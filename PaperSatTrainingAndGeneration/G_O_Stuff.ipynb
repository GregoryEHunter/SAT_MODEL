{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e95e1ea-be8a-4b2d-a687-6459a35b095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/lib/python3/dist-packages (from sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.8/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from sentence-transformers) (1.23.2)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.8/site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: torchvision in /usr/lib/python3/dist-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (7.0)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.8)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3.1'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e708b06-e633-4c8b-b21e-79ccc5e94333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import Model_Import_6\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b54264-bb2c-4e97-8e80-a0e6ef87d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56f370e-c280-4a3d-a4fc-cf47f6e9ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a033925-2eb0-4e33-972d-dc437a998494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20c87cd-0cb7-4d10-abfd-96cb685bbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_styles_w_dev(model, optimizer, pos_context_embeds_list, neg_context_embeds_list, logits_list, token_ids_list, style_list_train, dev_logits, dev_pos_context, dev_neg_context, dev_token_ids, dev_style, epochs, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list, style_list_train))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list, style_list_train = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            if style_list_train[example] == 0:\n",
    "                context_embeds_list = neg_context_embeds_list\n",
    "            else:\n",
    "                context_embeds_list = pos_context_embeds_list\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        model.eval()\n",
    "        CELoss_dev = nn.CrossEntropyLoss()\n",
    "        dev_loss_acum = 0\n",
    "        for dev_example in range(len(dev_logits)):\n",
    "            if style_list_train[dev_example] == 0:\n",
    "                dev_context = dev_neg_context\n",
    "            else:\n",
    "                dev_context = dev_pos_context\n",
    "            random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "            stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "            \n",
    "            dev_network_output = model(stacked_context_sample_dev.to(device), dev_logits[dev_example].to(device))\n",
    "            \n",
    "            shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "            dev_loss_acum += dev_loss.item()\n",
    "        full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49dd10ad-6216-4f5f-a25a-199c7fa6f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_str_for_sen_embed(text_list, max_len=384):\n",
    "    shortened_texts = []\n",
    "    for text in text_list:\n",
    "        start = 0\n",
    "        end = max_len\n",
    "        while True:\n",
    "            if len(text) <= max_len:\n",
    "                shortened_texts.append(text)\n",
    "                break\n",
    "            shortened_texts.append(text[:max_len])\n",
    "            text = text[max_len:]\n",
    "            \n",
    "    return shortened_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863cb3ff-8c02-47ea-a79f-dcb4a67f87cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Darwin_Gatsby/gatsby_raw.txt', 'r') as file:\n",
    "    gatsby = file.read().replace('\\n', ' ')\n",
    "\n",
    "with open('/home/ubuntu/Darwin_Gatsby/origin_of_species_raw.txt', 'r') as file:\n",
    "    origin = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189f12b5-ce44-4805-9784-a966c231d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby_shortened = truncate_str_for_sen_embed([gatsby], max_len=384)\n",
    "\n",
    "origin_shortened = truncate_str_for_sen_embed([origin], max_len=384)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb59974-885a-424e-b65e-09fe4e73cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby_shortened_train, gatsby_shortened_test = train_test_split(gatsby_shortened, test_size=0.2, random_state=42)\n",
    "origin_shortened_train, origin_shortened_test = train_test_split(origin_shortened, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99813d11-e9e1-4857-82be-b33d96ba918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gatsby_shortened_train + origin_shortened_train\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "tranformed_corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "test_corpus =  gatsby_shortened_test + origin_shortened_test\n",
    "\n",
    "transformed_test = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c5d9916-5e0f-4cce-9b0e-495bf17ef1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "141\n",
      "1864\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(len(gatsby_shortened_train))\n",
    "print(len(gatsby_shortened_test))\n",
    "print(len(origin_shortened_train))\n",
    "print(len(origin_shortened_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc886033-6dcc-459c-8142-53aea8921905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2427\n",
      "2427\n",
      "608\n",
      "608\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "test_labels = []\n",
    "for ex in range(len(corpus)):\n",
    "    if ex <= 563:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "print(len(corpus))\n",
    "print(len(labels))\n",
    "for ex in range(len(test_corpus)):\n",
    "    if ex <= 141:\n",
    "        test_labels.append(0)\n",
    "    else:\n",
    "        test_labels.append(1)\n",
    "print(len(test_corpus))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a7869f-83c7-4532-88e4-61c55d7d863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(tranformed_corpus, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19003c6b-1f74-4e5e-af8d-63cba2ede364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993421052631579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(transformed_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "63060108-1730-4f88-9734-ddf331a555f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_style_generate(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples = 100, num_tokens_to_generate = 1, sen_to_generate = 10):\n",
    "  #Put models in eval mode\n",
    "    SAT_model.eval()\n",
    "    GPT_transformer.eval()\n",
    "    \n",
    "    for sent in range(sen_to_generate):\n",
    "        random_context_samples = random.sample(context_to_sample, num_samples)\n",
    "        model_context_input =  torch.stack(random_context_samples, dim = 0).to(device)\n",
    "        expierment_list = []\n",
    "        sentences = []\n",
    "        # tokenize prompt\n",
    "        current_tokenization = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "        for generation in range(num_tokens_to_generate):\n",
    "            # put tokenized prompt through GPT_transformer to get GPT Logits\n",
    "            GPT_logits = GPT_transformer(current_tokenization).last_hidden_state.squeeze()\n",
    "\n",
    "            # put model_context_input and the GPT Logits into SAT model\n",
    "            adjusted_output = SAT_model(model_context_input, GPT_logits) \n",
    "\n",
    "            # Funtional softmax the SAT model output\n",
    "            SM_adjusted_output = torch.nn.functional.softmax(adjusted_output, dim = 1)\n",
    "\n",
    "            # argmax to get predicted token\n",
    "            # predicted_tokens = torch.argmax(SM_adjusted_output, dim =1)\n",
    "\n",
    "            # get topk tokens\n",
    "            top_predicted_tokens = torch.topk(SM_adjusted_output[-1], 3, dim =0).indices\n",
    "            top_predicted_tokens_prob = torch.topk(SM_adjusted_output[-1], 3, dim =0).values\n",
    "            top_predicted_tokens_numpy = top_predicted_tokens.cpu().detach().numpy()\n",
    "            top_predicted_tokens_prob_numpy = top_predicted_tokens_prob.cpu().detach().numpy()\n",
    "            token_prob_sum = np.sum(top_predicted_tokens_prob_numpy)\n",
    "            token_distribution = top_predicted_tokens_prob_numpy/token_prob_sum\n",
    "            \n",
    "            predicted_token =  torch.from_numpy(np.array(np.random.choice(top_predicted_tokens_numpy, p = token_distribution))).to(device)\n",
    "            \n",
    "            # print(top_predicted_tokens)\n",
    "             # tok_k_predicted_words = tokenizer.decode(top_predicted_tokens, skip_special_tokens=True)\n",
    "#             decoded_tokens = []\n",
    "#             for token_k in top_predicted_tokens:\n",
    "#                 decoded_tokens.append(tokenizer.decode(token_k, skip_special_tokens=True))\n",
    "                \n",
    "#             top_predicted_tokens_prob_cpu = top_predicted_tokens_prob.cpu()\n",
    "            del top_predicted_tokens_prob\n",
    "            # expierment_list.append((top_predicted_tokens_prob_cpu, decoded_tokens))\n",
    "\n",
    "\n",
    "                \n",
    "            #dif way of getting top predicted\n",
    "            # predicted_token = top_predicted_tokens[0]\n",
    "            # print(current_tokenization[-1][0])\n",
    "            # print(predicted_token)\n",
    "            # if predicted_token == 247 or predicted_token == current_tokenization[-1][0]:\n",
    "            if predicted_token == 247:\n",
    "                print(\"WEIRD TOKEN PREDICTED\")\n",
    "                predicted_token = top_predicted_tokens[1]\n",
    "\n",
    "            # print(torch.amax(SM_adjusted_output[-1]))\n",
    "\n",
    "            # print(predicted_tokens[-1].unsqueeze(0).unsqueeze(0))\n",
    "            # print(current_tokenization)\n",
    "\n",
    "            current_tokenization = torch.cat((current_tokenization, predicted_token.unsqueeze(0).unsqueeze(0)), 1)\n",
    "            del predicted_token\n",
    "            gc.collect()\n",
    "            # print(current_tokenization)\n",
    "\n",
    "        # decode\n",
    "        sentence = []\n",
    "        for i, beam in enumerate(current_tokenization):\n",
    "            # print(f\"{i}: {tokenizer.decode(beam)}\")\n",
    "            # print(f\"{i}: {current_tokenization}\")\n",
    "            #print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "            sentence.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "        sentences.append(sentence)\n",
    "        del current_tokenization\n",
    "        del random_context_samples\n",
    "        del model_context_input\n",
    "        gc.collect()\n",
    "        # for prediction in expierment_list:\n",
    "        #     print(prediction)\n",
    "        print(\".\")\n",
    "    return sentences\n",
    "    # for i, beam in enumerate(predicted_tokens):\n",
    "    #     # if i == 0:\n",
    "    #     #   continue\n",
    "    #     print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)} token_id: {beam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa37d84-ca95-4ec0-b1a2-6e4098d2c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpora_and_split(style_1, style_2):\n",
    "    truncated_style_1 = truncate_str_for_sen_embed([style_1], max_len=384)\n",
    "    truncated_style_2 = truncate_str_for_sen_embed([style_2], max_len=384)\n",
    "    truncated_style_1_train, truncated_style_1_test = train_test_split(truncated_style_1, test_size=0.2, random_state=42)\n",
    "    truncated_style_2_train, truncated_style_2_test = train_test_split(truncated_style_2, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return truncated_style_1_train, truncated_style_1_test, truncated_style_2_train, truncated_style_2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b751f7c-aa1f-4754-8def-17b71301bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorizer_and_labels(truncated_style_1_train, truncated_style_1_test, truncated_style_2_train, truncated_style_2_test):\n",
    "    ## make vectorizer\n",
    "    labels = []\n",
    "    test_lables = []\n",
    "    corpus = truncated_style_1_train + truncated_style_2_train\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "    tranformed_corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    test_corpus =  truncated_style_1_test + truncated_style_2_test\n",
    "\n",
    "    transformed_test = vectorizer.transform(test_corpus)\n",
    "    \n",
    "    for ex in range(len(corpus)):\n",
    "        if ex <= len(truncated_style_1_train):\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    for ex in range(len(corpus)):\n",
    "        if ex <= len(truncated_style_1_test):\n",
    "            test_labels.append(0)\n",
    "        else:\n",
    "            test_labels.append(1)\n",
    "    \n",
    "    return tranformed_corpus, transformed_test, labels, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daac39d7-99f1-4b4e-8271-eeb24b331111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_embed(model, sentence_list):\n",
    "    # sen_embeds = []\n",
    "    # count = 0\n",
    "    with torch.no_grad():\n",
    "        # for sen in sentence_list:\n",
    "        #     if len(sen) == 0:\n",
    "        #         print(\"empty sentence\")\n",
    "        #         continue\n",
    "        \n",
    "        sen_embeds = model.encode(sentence_list)\n",
    "        return sen_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5a20b8-792a-464c-a050-cdb2fe382c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logits(model, tokenizer, book_sentence_list):\n",
    "    logits = []\n",
    "    token_ids = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for sen in book_sentence_list:\n",
    "            if len(sen) == 0:\n",
    "                print(\"empty sentence\")\n",
    "                continue\n",
    "            gpt_tokenized = tokenizer(sen, return_tensors = 'pt', truncation=True).to(device)\n",
    "            gpt_embed = model(**gpt_tokenized).last_hidden_state.squeeze()\n",
    "            gpt_tokenized_cpu = gpt_tokenized.to('cpu')\n",
    "            del gpt_tokenized\n",
    "            token_ids.append(gpt_tokenized_cpu) ## on the cpu!!!!\n",
    "            gpt_embed_cpu = gpt_embed.to('cpu')\n",
    "            del gpt_embed\n",
    "            logits.append(gpt_embed_cpu)\n",
    "            if count%1000 == 0:\n",
    "                # t = torch.cuda.get_device_properties(0).total_memory\n",
    "                # r = torch.cuda.memory_reserved(0)\n",
    "                # a = torch.cuda.memory_allocated(0)\n",
    "                # f = r-a  # free inside reserved\n",
    "                # print(t/1000000000)\n",
    "                # print(r/1000000000)\n",
    "                # print(a/1000000000)\n",
    "                # print(f/1000000000)\n",
    "                print(count)\n",
    "            count += 1\n",
    "            \n",
    "        return logits, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f1b1ada-8819-4a86-b906-cabe6ef3da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2427, 768)\n"
     ]
    }
   ],
   "source": [
    "sen_embeds = make_context_embed(sentence_model, corpus)\n",
    "print(sen_embeds.shape)\n",
    "# sen_embeds_list = sen_embeds.tolist()\n",
    "dev_sen_embeds = make_context_embed(sentence_model, test_corpus)\n",
    "# dev_sen_embeds_list = dev_sen_embeds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b989ce6-72a8-48c7-8c77-3c3e8797ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_embeds_list = []\n",
    "dev_sen_embeds_list = []\n",
    "for embed in sen_embeds:\n",
    "    sen_embeds_list.append(torch.from_numpy(embed))\n",
    "for embed in dev_sen_embeds:\n",
    "    dev_sen_embeds_list.append(torch.from_numpy(embed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198b968a-c8c4-41d8-b242-6b35568bad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2427\n",
      "2427\n",
      "2427\n"
     ]
    }
   ],
   "source": [
    "print(len(sen_embeds))\n",
    "print(len(sen_embeds_list))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc2527e3-fc3a-4216-bfde-d3f546fb7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sen_embeds = make_context_embed(sentence_model, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd682bb-76d7-4275-835c-158f1e6a2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "logits, token_ids = make_logits(Model_Import_6.head_transformer, Model_Import_6.tokenizer, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36a53b62-54fe-44d0-bcc4-f45609408091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dev_logits, dev_token_ids = make_logits(Model_Import_6.head_transformer, Model_Import_6.tokenizer, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b0e4138-0eaf-43af-9665-a4f9a15e2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, optimizer, pos_context_embeds_list, neg_context_embeds_list, logits_list, token_ids_list, style_list_train, dev_logits, dev_pos_context, dev_neg_context, dev_token_ids, dev_style, epochs, num_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91053492-c422-4fe4-8e2a-289b55683876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "1864\n",
      "141\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "C_gatsby_embeds = sen_embeds_list[:563]\n",
    "C_origin_embeds = sen_embeds_list[563:]\n",
    "print(len(C_gatsby_embeds))\n",
    "print(len(C_origin_embeds))\n",
    "C_gatsby_embeds_dev = dev_sen_embeds_list[:141]\n",
    "C_origin_embeds_dev = dev_sen_embeds_list[141:]\n",
    "print(len(C_gatsby_embeds_dev))\n",
    "print(len(C_origin_embeds_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5de43fd6-cec6-4c94-b1f1-a51cf13b3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_gatsby_embeds = []\n",
    "# context_origin_embeds = []\n",
    "# for embed in sen_embeds:\n",
    "#     torch_embedsembeds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfad7b5d-2253-4d08-836d-18ddf6564a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_list_gatsby = []\n",
    "style_list_origin = []\n",
    "for embed in C_gatsby_embeds:\n",
    "    style_list_gatsby.append(0)\n",
    "for embed in C_origin_embeds:\n",
    "    style_list_origin.append(1)\n",
    "    \n",
    "style_list_train = style_list_gatsby + style_list_origin\n",
    "\n",
    "style_list_gatsby_dev = []\n",
    "style_list_origin_dev = []\n",
    "for embed in C_gatsby_embeds_dev:\n",
    "    style_list_gatsby_dev.append(0)\n",
    "for embed in C_origin_embeds_dev:\n",
    "    style_list_origin_dev.append(1)\n",
    "    \n",
    "style_list_dev = style_list_gatsby_dev + style_list_origin_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20aaf302-1471-4fca-947e-af2d4862e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21189e4c-2ef2-45fe-b7cf-3e24f4ecf436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    }
   ],
   "source": [
    "print(len(C_gatsby_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53262d16-f08a-428e-a3c4-a27ceff78bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93, 1600])\n"
     ]
    }
   ],
   "source": [
    "print(logits[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a00cadcf-e788-4699-b635-ed7ec55805ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(C_gatsby_embeds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2545b95e-45a2-492c-8fd3-c4038690d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_basic = Model_Import_6.MultiHeadModel_PyTorch_Stacked_Positional(C_gatsby_embeds[0].shape[0], logits[0].shape[1], heads = num_heads, attention_dim = int(logits[0].shape[1])).to(device) #\n",
    "# pytorch_basic = Model_Import_6.MultiHeadModel_PyTorch_Positional(C_gatsby_embeds[0].shape[0], logits[0].shape[1], heads = num_heads, attention_dim = int(logits[0].shape[1])).to(device) #\n",
    "# pytorch_basic = Model_Import_6.MultiHeadModel_PyTorch_Stacked_One_Alt(C_gatsby_embeds[0].shape[0], logits[0].shape[1], heads = num_heads, attention_dim = int(logits[0].shape[1])).to(device) #\n",
    "# pytorch_basic = Model_Import_6.MultiHeadModel_PyTorch_Stacked(C_gatsby_embeds[0].shape[0], logits[0].shape[1], heads = num_heads, attention_dim = int(logits[0].shape[1])).to(device) #\n",
    "\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "optimizer_basic = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2b60c6b9-a53a-42be-8ded-d77ae3ff4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105838400\n",
      "105838400\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in pytorch_basic.parameters()\n",
    ")\n",
    "print(total_params)\n",
    "\n",
    "trainable_params = sum(\n",
    "\tp.numel() for p in pytorch_basic.parameters() if p.requires_grad\n",
    ")\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "495f6024-7554-49e3-8bb8-29aafc7b796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Epoch: 0, Epoch Examples: 2427\n",
      "TRAIN LOSS: 2.916487216949463\n",
      "DEV LOSS: 3.2152166148941768\n",
      "----------------------------------------\n",
      "...Epoch: 1, Epoch Examples: 2427\n",
      "TRAIN LOSS: 2.6174700260162354\n",
      "DEV LOSS: 3.2419174588040303\n",
      "----------------------------------------\n",
      "...Epoch: 2, Epoch Examples: 2427\n",
      "TRAIN LOSS: 2.299574613571167\n",
      "DEV LOSS: 3.3021455058141758\n",
      "----------------------------------------\n",
      "...Epoch: 3, Epoch Examples: 2427\n",
      "TRAIN LOSS: 1.965745210647583\n",
      "DEV LOSS: 3.372939638401333\n",
      "----------------------------------------\n",
      "...Epoch: 4, Epoch Examples: 2427\n",
      "TRAIN LOSS: 1.622340440750122\n",
      "DEV LOSS: 3.4998063225495186\n",
      "----------------------------------------\n",
      "...Epoch: 5, Epoch Examples: 2427\n",
      "TRAIN LOSS: 1.2937655448913574\n",
      "DEV LOSS: 3.650838586844896\n",
      "----------------------------------------\n",
      "...Epoch: 6, Epoch Examples: 2427\n",
      "TRAIN LOSS: 0.9995564818382263\n",
      "DEV LOSS: 3.7840108330312527\n",
      "----------------------------------------\n",
      "...Epoch: 7, Epoch Examples: 2427\n",
      "TRAIN LOSS: 0.7638644576072693\n",
      "DEV LOSS: 3.929200784156197\n",
      "----------------------------------------\n",
      "...Epoch: 8, Epoch Examples: 2427\n",
      "TRAIN LOSS: 0.5842196345329285\n",
      "DEV LOSS: 4.112100469830789\n",
      "----------------------------------------\n",
      "...Epoch: 9, Epoch Examples: 2427\n",
      "TRAIN LOSS: 0.4563828110694885\n",
      "DEV LOSS: 4.237458749429176\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_two_styles_w_dev(pytorch_basic, optimizer_basic, C_origin_embeds, C_gatsby_embeds, logits, token_ids, style_list_train, dev_logits, C_origin_embeds_dev, C_gatsby_embeds_dev, dev_token_ids, style_list_dev, 10, num_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6c4b67ae-856e-4bf9-b5ac-bd684cd20a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "7.94820608\n",
      "7.273941504\n",
      "0.674264576\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1dca1e68-2b3c-47bf-9146-b83b8b7b7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"The\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a03f0fd5-de99-4f19-b6f2-e0099f952b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Darwin_Gatsby/gatsby_raw.txt', 'r') as file:\n",
    "    gatsby = file.read().replace('\\n', ' ')\n",
    "\n",
    "with open('/home/ubuntu/Darwin_Gatsby/origin_of_species_raw.txt', 'r') as file:\n",
    "    origin = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eb6806c2-0871-4506-8cbc-1e25f33f5e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439\n",
      "3981\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gatsby_sen_list = nltk.sent_tokenize(gatsby)\n",
    "origin_sen_list = nltk.sent_tokenize(origin)\n",
    "print(len(gatsby_sen_list))\n",
    "print(len(origin_sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f2521c38-61a8-42a6-bdca-7449e46d4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_overlap: {'And we', 'From the', 'Those who', 'We could', 'I see', 'At first', 'But I', 'Some little', 'There is', 'He must', 'I would', 'Do they', 'But no', 'They are', 'The masters', 'As soon', 'The eyes', 'And the', 'But with', 'It makes', 'He always', 'I will', 'Were all', 'He has', 'Of course', 'This is', 'When I', 'It was', 'At this', 'The day', 'To my', 'And what', 'For some', 'If there', 'Nothing at', 'After the', 'In fact,', 'How the', 'In two', 'But how', 'A man', 'By the', 'In my', 'Now I', 'Now, in', 'With the', 'A new', 'If it', 'For several', 'I am', 'The little', 'It seems', 'I could', 'I believe', 'Let the', 'But we', 'The three', 'All the', 'I remember', 'I was', 'And it', 'The old', 'A large', 'But it', 'And I', 'I think', 'The most', 'The truth', 'The whole', 'On the', 'A little', 'And when', 'The flowers', 'Now it', 'If I', 'After a', 'But the', 'Most of', 'Let us', 'So we', 'A few', 'I have', 'I should', 'And as', 'I know', 'It is', 'Not that', 'He would', 'As I', 'Even when', 'I must', 'The only', 'When we', 'Now, if', 'It occurred', 'In a', 'One of', 'See how', 'All these', 'Look at', 'At a', 'And all', 'But when', 'The practical', 'Not only', 'On a', 'But what', 'In one', 'The other', 'At the', 'As we', 'How do', 'In the', 'No one', 'It might', 'To the', 'Even if', 'This was', 'And now', 'As the', 'When the', 'And if', 'I feel', 'How much', 'A white', 'There must', 'As he', 'If we', 'In this', 'But there', 'The man', 'The fact', 'I cannot', 'But in'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ref https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "gatsby_first_words_list = []\n",
    "for sen_g in gatsby_sen_list:\n",
    "    sen_g = re.sub(r'^\\s*|\\s\\s*', ' ', sen_g).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_g = filter(lambda x: x in printable, sen_g)\n",
    "    sen_g = ''.join(filter(lambda x: x in printable, sen_g))\n",
    "    sen_g_list = sen_g.split()\n",
    "    #print(sen_g_list[0:3])\n",
    "    gatsby_first_words_list.append(sen_g_list[0:3])\n",
    "    \n",
    "origin_first_words_list = []  \n",
    "for sen_o in origin_sen_list:\n",
    "    sen_o = re.sub(r'^\\s*|\\s\\s*', ' ', sen_o).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_o = filter(lambda x: x in printable, sen_o)\n",
    "    sen_o = ''.join(filter(lambda x: x in printable, sen_o))\n",
    "    sen_o_list = sen_o.split()\n",
    "    # print(sen_o_list[0:3])\n",
    "    origin_first_words_list.append(sen_o_list[0:3]) \n",
    "    \n",
    "initial_unigrams_gatsby = defaultdict(int)\n",
    "initial_bigrams_gatsby = defaultdict(int)\n",
    "initial_trigrams_gatsby = defaultdict(int)\n",
    "initial_unigrams_origin = defaultdict(int)\n",
    "initial_bigrams_origin = defaultdict(int)\n",
    "initial_trigrams_origin = defaultdict(int)\n",
    "for first_words in gatsby_first_words_list:\n",
    "    initial_unigrams_gatsby[first_words[0]] += 1\n",
    "    initial_bigrams_gatsby[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_gatsby[' '.join(first_words)] += 1\n",
    "    \n",
    "for first_words in origin_first_words_list:\n",
    "    initial_unigrams_origin[first_words[0]] += 1\n",
    "    initial_bigrams_origin[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_origin[' '.join(first_words)] += 1\n",
    "    \n",
    "unigram_overlap = set(initial_unigrams_gatsby.keys())\n",
    "unigram_overlap = unigram_overlap.intersection(initial_unigrams_origin.keys())\n",
    "\n",
    "bigram_overlap = set(initial_bigrams_gatsby.keys())\n",
    "bigram_overlap = bigram_overlap.intersection(initial_bigrams_origin.keys())\n",
    "   \n",
    "trigram_overlap = set(initial_trigrams_gatsby.keys())\n",
    "trigram_overlap = trigram_overlap.intersection(initial_trigrams_origin.keys())\n",
    "\n",
    "\n",
    "# print(f\"unigram_overlap: {str(unigram_overlap)}\")\n",
    "# print(\"\\n\")\n",
    "print(f\"bigram_overlap: {str(bigram_overlap)}\")\n",
    "print(\"\\n\")\n",
    "# print(f\"trigram_overlap: {str(trigram_overlap)}\")\n",
    "\n",
    "bigram_list = list(bigram_overlap)\n",
    "    \n",
    "# combine the list\n",
    "\n",
    "# combined_g_o_list = gatsby_first_words_list + origin_first_words_list\n",
    "\n",
    "# mini_sen_list = []\n",
    "# for begin in combined_g_o_list:\n",
    "#     mini_sen_list.append(' '.join(begin))\n",
    "    \n",
    "# # ref https://stackoverflow.com/questions/43473736/most-common-2-grams-using-python\n",
    "# bigrams = zip(mini_sen_list, mini_sen_list[1:])\n",
    "# counts = Counter(bigrams)\n",
    "# print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a35981ae-9c37-4efc-94d3-2c27e7a86f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = random.sample(bigram_list,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b74a534-8efb-41d0-bd32-97fda2894a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "origins_SAT_generations = []\n",
    "for prompt_ in lst:\n",
    "    origins_SAT_generations.append(one_style_generate(prompt_, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, C_origin_embeds_dev, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "62168e7a-9f4b-4aed-b3a3-521b015031a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      "WEIRD TOKEN PREDICTED\n",
      ".\n",
      ".\n",
      "WEIRD TOKEN PREDICTED\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "gatsby_SAT_generations = []\n",
    "for prompt_ in lst:\n",
    "    gatsby_SAT_generations.append(one_style_generate(prompt_, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, C_gatsby_embeds_dev, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "959b1cf4-b820-46bf-b855-e29ed2a34eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "7.94820608\n",
      "7.273941504\n",
      "0.674264576\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "87eceac4-f998-4918-9fee-3ffcc5dabc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_strings(strings):\n",
    "    cleaned_strings = []\n",
    "    #print(strings[:20])\n",
    "    for sen in strings:\n",
    "        #print(sen)\n",
    "        sen = re.sub(r\"\\\\\", \"\", sen[0])    # bizarrely, each string is inside of a list containing nothing but that string\n",
    "        sen = re.sub(r\"\\n\", \" \", sen)\n",
    "        sen = filter(lambda x: x in printable, sen)\n",
    "        sen = ''.join(filter(lambda x: x in printable, sen))\n",
    "        cleaned_strings.append(sen)\n",
    "    #print(cleaned_strings[:20])\n",
    "    \n",
    "    return cleaned_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "951e7eb7-4403-4b36-aef2-bec8de3b3af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_gatsby_SAT_generations = clean_strings(gatsby_SAT_generations)\n",
    "clean_origins_SAT_generations = clean_strings(origins_SAT_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9a56bc8e-d407-4979-bf7b-0a6437d486bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_generations = clean_gatsby_SAT_generations + clean_origins_SAT_generations\n",
    "\n",
    "\n",
    "generations_vectorized = vectorizer.transform(all_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5090b336-df5b-43be-b74a-2745630c6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatsby_gen_correct_labels = [0 for i in range(10)]\n",
    "origin_gen_correct_labels = [1 for i in range(10)]\n",
    "\n",
    "generation_labels = gatsby_gen_correct_labels + origin_gen_correct_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1fb75995-c695-45e2-85b0-7bb26fcdd2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(generations_vectorized, generation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bfad9384-61cf-4d66-91b2-7399065c4809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['But in hermaphrodite organisms which cross only one of the two sexes, that there are two sexes which cross. This case is sometimes called complement-giving. In the case of hermaphrodite animals which cross only once, that there', 'Let us all take the temperature of the house. Is it hot or cold, and if it is a shade of yellow or green, will it last? Is the colour pleasant and pleasing to our eyes? Is the room clean and neat? Has our dog left', 'I see. I see.  I broke away.  I see.  But I want to get one of those fancy dogs with fur between the legs before I do anything to it. I had them all lined up', 'This was my faultand it was on my head. But I had made that mistake before. It was in college. It was in a spring, when we were just starting out. Ill let you in on my little secret. We were', 'No one supposes that I am going to give up my right, as I have done, of saying that it is a mistake to see the whole organisation in the light of the principle of natural selection, that it is at least at present in somedegree ', 'The day-of-tournament, he was awful.  Going back to his house, I saw him sitting downstairs, staring at his computer, staring at some thing, and I was like, whats that thing? And', 'But it was an unmistakable statement that Mr. Gatsby of West Egg, Long Island, was exempt from my scrutiny.  Anything goes between them, said my cousin. Anything that Gatsby does is all right with', 'But no one supposes that I was a lawful man.  I was a man without a motherwithout even a brother.                     ', 'It makes sense to me. I had a good time, a great time with him. I have a really good relationship with him now. It was unfortunate that he came down with a little thing like that. It was unfortunate that he came down with a little', 'I cannot tell whether my wife is lying, or what exactly is going on, but I have felt an unpleasantness, as of something firmly, firmly, firmly-like, as of something firmly-like-like, firmly-like, firmly-like,']\n"
     ]
    }
   ],
   "source": [
    "print(clean_gatsby_SAT_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b81ed1a0-be2a-4131-98a2-386d7dfdb9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_seqs = []\n",
    "# for seq in clean_gatsby_SAT_generations:\n",
    "#     all_seqs.append(('GATSBY_SAT', seq))\n",
    "# for seq in clean_origins_SAT_generations:\n",
    "#     all_seqs.append(('ORIGIN_SAT', seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74ab4d09-77a2-404e-8044-b9df2d5cf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.DataFrame(all_seqs, columns = ['Source', 'Sequence'])\n",
    "\n",
    "# df.to_csv('gatsby_origin_sat.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e30394-3abd-424c-8d7d-e5494668a01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
