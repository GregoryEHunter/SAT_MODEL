{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a52c8c-bcbd-48d0-93b0-f4aa7a18ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.8/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.8/site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.8/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/lib/python3/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/lib/python3/dist-packages (from sentence-transformers) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /usr/lib/python3/dist-packages (from sentence-transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from sentence-transformers) (1.23.2)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers) (7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3.1'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3fca79-9117-4406-a2e0-b37a812e532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel\n",
    "import math\n",
    "# import Model_Import\n",
    "import Model_Import_6 ## eating mem ## changed version commented out above\n",
    "from torch import optim\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbc57a5-fe62-4b84-9857-813d032d30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfa6b04-9190-483f-b869-296d0a44ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a98c73-b0a0-4726-ad62-be081ed5d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36940d20-50e4-4a72-bd8f-847e49810c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "R_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "Roberta_model = RobertaModel.from_pretrained(\"roberta-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2062f935-8a36-4740-82aa-687bdd4d442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9984cb-166d-4231-88e4-6eadb09a8272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n",
      "And, after boasting this way of my tolerance, I come to the admission that it has a limit.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "text = \"\"\"And, after boasting this way of my tolerance, I come to the admission\n",
    "that it has a limit. Conduct may be founded on the hard rock or the\n",
    "wet marshes, but after a certain point I don’t care what it’s founded\n",
    "on. When I came back from the East last autumn I felt that I wanted\n",
    "the world to be in uniform and at a sort of moral attention forever; I\n",
    "wanted no more riotous excursions with privileged glimpses into the\n",
    "human heart. Only Gatsby, the man who gives his name to this book, was\n",
    "exempt from my reaction—Gatsby, who represented everything for which I\n",
    "have an unaffected scorn. If personality is an unbroken series of\n",
    "successful gestures, then there was something gorgeous about him, some\n",
    "heightened sensitivity to the promises of life, as if he were related\n",
    "to one of those intricate machines that register earthquakes ten\n",
    "thousand miles away. This responsiveness had nothing to do with that\n",
    "flabby impressionability which is dignified under the name of the\n",
    "“creative temperament”—it was an extraordinary gift for hope, a\n",
    "romantic readiness such as I have never found in any other person and\n",
    "which it is not likely I shall ever find again. No—Gatsby turned out\n",
    "all right at the end; it is what preyed on Gatsby, what foul dust\n",
    "floated in the wake of his dreams that temporarily closed out my\n",
    "interest in the abortive sorrows and short-winded elations of men.\n",
    "And, after boasting this way of my tolerance, I come to the admission\n",
    "that it has a limit. Conduct may be founded on the hard rock or the\n",
    "wet marshes, but after a certain point I don’t care what it’s founded\n",
    "on. When I came back from the East last autumn I felt that I wanted\n",
    "the world to be in uniform and at a sort of moral attention forever; I\n",
    "wanted no more riotous excursions with privileged glimpses into the\n",
    "human heart. Only Gatsby, the man who gives his name to this book, was\n",
    "exempt from my reaction—Gatsby, who represented everything for which I\n",
    "have an unaffected scorn. If personality is an unbroken series of\n",
    "successful gestures, then there was something gorgeous about him, some\n",
    "heightened sensitivity to the promises of life, as if he were related\n",
    "to one of those intricate machines that register earthquakes ten\n",
    "thousand miles away. This responsiveness had nothing to do with that\n",
    "flabby impressionability which is dignified under the name of the\n",
    "“creative temperament”—it was an extraordinary gift for hope, a\n",
    "romantic readiness such as I have never found in any other person and\n",
    "which it is not likely I shall ever find again. No—Gatsby turned out\n",
    "all right at the end; it is what preyed on Gatsby, what foul dust\n",
    "floated in the wake of his dreams that temporarily closed out my\n",
    "interest in the abortive sorrows and short-winded elations of men.\n",
    "And, after boasting this way of my tolerance, I come to the admission\n",
    "that it has a limit. Conduct may be founded on the hard rock or the\n",
    "wet marshes, but after a certain point I don’t care what it’s founded\n",
    "on. When I came back from the East last autumn I felt that I wanted\n",
    "the world to be in uniform and at a sort of moral attention forever; I\n",
    "wanted no more riotous excursions with privileged glimpses into the\n",
    "human heart. Only Gatsby, the man who gives his name to this book, was\n",
    "exempt from my reaction—Gatsby, who represented everything for which I\n",
    "have an unaffected scorn. If personality is an unbroken series of\n",
    "successful gestures, then there was something gorgeous about him, some\n",
    "heightened sensitivity to the promises of life, as if he were related\n",
    "to one of those intricate machines that register earthquakes ten\n",
    "thousand miles away. This responsiveness had nothing to do with that\n",
    "flabby impressionability which is dignified under the name of the\n",
    "“creative temperament”—it was an extraordinary gift for hope, a\n",
    "romantic readiness such as I have never found in any other person and\n",
    "which it is not likely I shall ever find again. No—Gatsby turned out\n",
    "all right at the end; it is what preyed on Gatsby, what foul dust\n",
    "floated in the wake of his dreams that temporarily closed out my\n",
    "interest in the abortive sorrows and short-winded elations of men.\n",
    "\"\"\"\n",
    "text = text.replace('\\n', ' ')\n",
    "sent_text = nltk.sent_tokenize(text)\n",
    "print(len(text))\n",
    "print(sent_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b41b49c-9cff-4d87-ae81-0a4f52bbc91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And,', 'after', 'boasting', 'this', 'way', 'of', 'my', 'tolerance,', 'I', 'come']\n",
      "And, after boasting this way of my tolerance, I come\n",
      "to the admission that it has a limit. Conduct may be founded on the hard rock or the wet marshes, but after a certain point I don’t care what it’s founded on. When I came back from the East last autumn I felt that I wanted the world to be in uniform and at a sort of moral attention forever; I wanted no more riotous excursions with privileged glimpses into the human heart. Only Gatsby, the man who gives his name to this book, was exempt from my reaction—Gatsby, who represented everything for which I have an unaffected scorn. If personality is an unbroken series of successful gestures, then there was something gorgeous about him, some heightened sensitivity to the promises of life, as if he were related to one of those intricate machines that register earthquakes ten thousand miles away. This responsiveness had nothing to do with that flabby impressionability which is dignified under the name of the “creative temperament”—it was an extraordinary gift for hope, a romantic readiness such as I have never found in any other person and which it is not likely I shall ever find again. No—Gatsby turned out all right at the end; it is what preyed on Gatsby, what foul dust floated in the wake of his dreams that temporarily closed out my interest in the abortive sorrows and short-winded elations of men. And, after boasting this way of my tolerance, I come to the admission that it has a limit. Conduct may be founded on the hard rock or the wet marshes, but after a certain point I don’t care what it’s founded on. When I came back from the East last autumn I felt that I wanted the world to be in uniform and at a sort of moral attention forever; I wanted no more riotous excursions with privileged glimpses into the human heart. Only Gatsby, the man who gives his name to this book, was exempt from my reaction—Gatsby, who represented everything for which I have an unaffected scorn. If personality is an unbroken series of successful gestures, then there was something gorgeous about him, some heightened sensitivity to the promises of life, as if he were related to one of those intricate machines that register earthquakes ten thousand miles away. This responsiveness had nothing to do with that flabby impressionability which is dignified under the name of the “creative temperament”—it was an extraordinary gift for hope, a romantic readiness such as I have never found in any other person and which it is not likely I shall ever find again. No—Gatsby turned out all right at the end; it is what preyed on Gatsby, what foul dust floated in the wake of his dreams that temporarily closed out my interest in the abortive sorrows and short-winded elations of men. And, after boasting this way of my tolerance, I come to the admission that it has a limit. Conduct may be founded on the hard rock or the wet marshes, but after a certain point I don’t care what it’s founded on. When I came back from the East last autumn I felt that I wanted the world to be in uniform and at a sort of moral attention forever; I wanted no more riotous excursions with privileged glimpses into the human heart. Only Gatsby, the man who gives his name to this book, was exempt from my reaction—Gatsby, who represented everything for which I have an unaffected scorn. If personality is an unbroken series of successful gestures, then there was something gorgeous about him, some heightened sensitivity to the promises of life, as if he were related to one of those intricate machines that register earthquakes ten thousand miles away. This responsiveness had nothing to do with that flabby impressionability which is dignified under the name of the “creative temperament”—it was an extraordinary gift for hope, a romantic readiness such as I have never found in any other person and which it is not likely I shall ever find again. No—Gatsby turned out all right at the end; it is what preyed on Gatsby, what foul dust floated in the wake of his dreams that temporarily closed out my interest in the abortive sorrows and short-winded elations of men.\n"
     ]
    }
   ],
   "source": [
    "list_text = text.split()\n",
    "print(list_text[:10])\n",
    "print(' '.join(list_text[:10]))\n",
    "print(' '.join(list_text[10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca51af7-4a8d-4e27-9056-544a51355d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Darwin_Gatsby/gatsby_raw.txt', 'r') as file:\n",
    "    gatsby = file.read().replace('\\n', ' ')\n",
    "\n",
    "with open('/home/ubuntu/Darwin_Gatsby/origin_of_species_raw.txt', 'r') as file:\n",
    "    origin = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9033d864-22b7-44aa-ad13-47a968553f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439\n",
      "3981\n"
     ]
    }
   ],
   "source": [
    "gatsby_sen_list = nltk.sent_tokenize(gatsby)\n",
    "origin_sen_list = nltk.sent_tokenize(origin)\n",
    "print(len(gatsby_sen_list))\n",
    "print(len(origin_sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87791fca-272c-4511-b620-53c5fea38627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_overlap: {'Where', 'Who', 'Next', 'Call', 'Nevertheless', 'Take', 'Look', 'This', 'By', 'When,', 'Have', 'But,', 'Just', 'Nothing', 'Not', 'My', 'With', 'Until', 'the', 'Now,', 'Yet', 'I', 'Mr.', 'Each', 'In', 'Though', 'So', 'Why', 'When', 'First', 'Certainly', 'Almost', 'On', 'Its', 'What', 'Over', 'We', 'Both', 'Or', 'Finally', 'After', 'Let', 'See', 'Before', 'Our', 'As', 'Throw', 'Had', 'One', 'Thus', 'His', 'Were', 'She', 'The', 'From', 'Two', 'How', 'He', 'Do', 'All', 'An', 'Probably', 'Still', 'Every', 'Here,', 'Or,', 'Would', 'Such', 'A', 'There', 'Something', 'For', 'If', 'These', 'Having', 'Now', 'Most', 'Of', 'Much', 'Then,', 'Turning', 'Some', 'But', 'Only', 'Whenever', 'Moreover', 'And,', 'Several', 'To', 'Good', 'At', 'Why,', 'Generally', 'They', 'Those', 'No', 'Under', 'Neither', 'Again', 'Their', 'That', 'It', 'And', 'Even', 'Perhaps'}\n",
      "\n",
      "\n",
      "bigram_overlap: {'The practical', 'I will', 'There is', 'To my', 'As he', 'But there', 'Not that', 'For some', 'They are', 'No one', 'It seems', 'Now, if', 'The little', 'He would', 'It is', 'After the', 'I believe', 'If we', 'I am', 'Look at', 'At first', 'So we', 'Not only', 'The old', 'At the', 'In two', 'This is', 'This was', 'If I', 'As I', 'A little', 'In a', 'Now it', 'At this', 'As we', 'I must', 'Even if', 'Now, in', 'The day', 'To the', 'One of', 'The eyes', 'Of course', 'The three', 'And the', 'But no', 'Now I', 'When I', 'When we', 'Those who', 'A new', 'As soon', 'As the', 'And I', 'We could', 'The other', 'I feel', 'All these', 'Most of', 'Nothing at', 'And if', 'But with', 'There must', 'If there', 'Let us', 'I cannot', 'And all', 'On a', 'The only', 'The most', 'At a', 'And now', 'But I', 'It occurred', 'A large', 'I would', 'I was', 'But in', 'In my', 'The fact', 'The man', 'In fact,', 'If it', 'But it', 'It makes', 'From the', 'A man', 'A few', 'In this', 'But we', 'Even when', 'He must', 'Do they', 'How do', 'By the', 'The truth', 'I should', 'See how', 'A white', 'After a', 'But what', 'With the', 'In the', 'I think', 'How the', 'Were all', 'But how', 'Let the', 'I remember', 'But the', 'When the', 'And we', 'The flowers', 'The whole', 'Some little', 'And when', 'In one', 'The masters', 'And as', 'I have', 'I see', 'I know', 'For several', 'It was', 'I could', 'He has', 'How much', 'And it', 'All the', 'It might', 'On the', 'But when', 'He always', 'And what'}\n",
      "\n",
      "\n",
      "trigram_overlap: {'One of the', 'The eyes of', 'This was a', 'It occurred to', 'In one of', 'It is a', 'I have been', 'I think we', 'But I am', 'It is an', 'But there was', 'And as the', 'I was so', 'But no one', 'As soon as', 'This is the', 'I think the', 'But in the', 'It might have', 'I think it', 'This is a', 'On the contrary,', 'On the other', 'If we were', 'Most of the', 'I believe that'}\n"
     ]
    }
   ],
   "source": [
    "# ref https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "gatsby_first_words_list = []\n",
    "for sen_g in gatsby_sen_list:\n",
    "    sen_g = re.sub(r'^\\s*|\\s\\s*', ' ', sen_g).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_g = filter(lambda x: x in printable, sen_g)\n",
    "    sen_g = ''.join(filter(lambda x: x in printable, sen_g))\n",
    "    sen_g_list = sen_g.split()\n",
    "    #print(sen_g_list[0:3])\n",
    "    gatsby_first_words_list.append(sen_g_list[0:3])\n",
    "    \n",
    "origin_first_words_list = []  \n",
    "for sen_o in origin_sen_list:\n",
    "    sen_o = re.sub(r'^\\s*|\\s\\s*', ' ', sen_o).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_o = filter(lambda x: x in printable, sen_o)\n",
    "    sen_o = ''.join(filter(lambda x: x in printable, sen_o))\n",
    "    sen_o_list = sen_o.split()\n",
    "    # print(sen_o_list[0:3])\n",
    "    origin_first_words_list.append(sen_o_list[0:3]) \n",
    "    \n",
    "initial_unigrams_gatsby = defaultdict(int)\n",
    "initial_bigrams_gatsby = defaultdict(int)\n",
    "initial_trigrams_gatsby = defaultdict(int)\n",
    "initial_unigrams_origin = defaultdict(int)\n",
    "initial_bigrams_origin = defaultdict(int)\n",
    "initial_trigrams_origin = defaultdict(int)\n",
    "for first_words in gatsby_first_words_list:\n",
    "    initial_unigrams_gatsby[first_words[0]] += 1\n",
    "    initial_bigrams_gatsby[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_gatsby[' '.join(first_words)] += 1\n",
    "    \n",
    "for first_words in origin_first_words_list:\n",
    "    initial_unigrams_origin[first_words[0]] += 1\n",
    "    initial_bigrams_origin[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_origin[' '.join(first_words)] += 1\n",
    "    \n",
    "unigram_overlap = set(initial_unigrams_gatsby.keys())\n",
    "unigram_overlap = unigram_overlap.intersection(initial_unigrams_origin.keys())\n",
    "\n",
    "bigram_overlap = set(initial_bigrams_gatsby.keys())\n",
    "bigram_overlap = bigram_overlap.intersection(initial_bigrams_origin.keys())\n",
    "   \n",
    "trigram_overlap = set(initial_trigrams_gatsby.keys())\n",
    "trigram_overlap = trigram_overlap.intersection(initial_trigrams_origin.keys())\n",
    "\n",
    "\n",
    "print(f\"unigram_overlap: {str(unigram_overlap)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"bigram_overlap: {str(bigram_overlap)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"trigram_overlap: {str(trigram_overlap)}\")\n",
    "\n",
    "\n",
    "    \n",
    "# combine the list\n",
    "\n",
    "# combined_g_o_list = gatsby_first_words_list + origin_first_words_list\n",
    "\n",
    "# mini_sen_list = []\n",
    "# for begin in combined_g_o_list:\n",
    "#     mini_sen_list.append(' '.join(begin))\n",
    "    \n",
    "# # ref https://stackoverflow.com/questions/43473736/most-common-2-grams-using-python\n",
    "# bigrams = zip(mini_sen_list, mini_sen_list[1:])\n",
    "# counts = Counter(bigrams)\n",
    "# print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fe308-2114-48a9-915c-a6eccaa24743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa94377-751b-4ed9-8643-e66041da041e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541d8c1-f727-425d-83c4-9effdc8b17e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35a27b-281e-41b6-bca3-06b0f65a2aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c4f90-a413-48b6-ac07-dd52aacf8f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2e294ac-fa49-4b94-a680-121360abf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_long_str(sentence_splits, truncated_sentences):\n",
    "#     for sentence_part in range(len(sentence_splits)):\n",
    "#         sen_part_words = sentence_splits[sentence_part].split()\n",
    "#         len_sen = len(sen_part_words)\n",
    "#         # print(len_sen)\n",
    "    \n",
    "#         if len_sen > 384:\n",
    "#             print(len_sen)\n",
    "#             truncated_sentences.append(' '.join(sen_part_words[:384]))\n",
    "#             # print(' '.join(sen_part_words[0:384]))\n",
    "#             sentence_splits[sentence_part] = sentence_splits[sentence_part][384:]\n",
    "#             #print(' '.join(sen_part_words[-384:]))\n",
    "#             split_long_str(sentence_splits, truncated_sentences)\n",
    "\n",
    "#         else:\n",
    "#             return truncated_sentences.append(sentence_splits[sentence_part])\n",
    "\n",
    "\n",
    "# def truncate_str_for_sen_embed(text_list):\n",
    "#     truncated_sentences = []\n",
    "#     for text in text_list:\n",
    "#         # print(sentence)\n",
    "#         split_long_str([text], truncated_sentences)\n",
    "#     print(truncated_sentences)\n",
    "\n",
    "# def truncate_str_for_sen_embed(text_list):\n",
    "#     trucated_sentences = []\n",
    "#     while len(text_list):\n",
    "#         current_text = text_list.pop().split()\n",
    "#         if len(current_text) > 384:\n",
    "#             trucated_sentences.append(' '.join(current_text[:384]))\n",
    "            \n",
    "            \n",
    "def truncate_str_for_sen_embed(text_list, max_len=384): #ref christian\n",
    "    shortened_texts = []\n",
    "    for text in text_list:\n",
    "        start = 0\n",
    "        end = max_len\n",
    "        while True:\n",
    "            if len(text) <= max_len:\n",
    "                shortened_texts.append(text)\n",
    "                break\n",
    "            shortened_texts.append(text[:max_len])\n",
    "            text = text[max_len:]\n",
    "            \n",
    "    return shortened_texts\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc266253-0328-4709-827e-d25a3fbc8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sentence = truncate_str_for_sen_embed([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f094e98-3d85-4f2f-ac92-f6269d4b6afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4095\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(shortened_sentence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f19688-26d7-4b81-8477-408db58619d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence_embed(model, tokenizer, sentence_list):\n",
    "    sen_embeds = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for sen in book_sentence_list:\n",
    "            if len(sen) == 0:\n",
    "                print(\"empty sentence\")\n",
    "                continue\n",
    "           \n",
    "            count += 1\n",
    "        return sen_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18db49e-8144-4d8a-8fe1-ecd08fccd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "14.294188032\n",
      "14.265847808\n",
      "0.028340224\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad036333-c827-49ef-8ed2-3b1225366731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logits(model, tokenizer, book_sentence_list):\n",
    "    logits = []\n",
    "    token_ids = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for sen in book_sentence_list:\n",
    "            if len(sen) == 0:\n",
    "                print(\"empty sentence\")\n",
    "                continue\n",
    "            gpt_tokenized = tokenizer(sen, return_tensors = 'pt', truncation=True).to(device)\n",
    "            gpt_embed = model(**gpt_tokenized).last_hidden_state.squeeze()\n",
    "            gpt_tokenized_cpu = gpt_tokenized.to('cpu')\n",
    "            del gpt_tokenized\n",
    "            token_ids.append(gpt_tokenized_cpu) ## on the cpu!!!!\n",
    "            gpt_embed_cpu = gpt_embed.to('cpu')\n",
    "            del gpt_embed\n",
    "            logits.append(gpt_embed_cpu)\n",
    "            if count%1000 == 0:\n",
    "                # t = torch.cuda.get_device_properties(0).total_memory\n",
    "                # r = torch.cuda.memory_reserved(0)\n",
    "                # a = torch.cuda.memory_allocated(0)\n",
    "                # f = r-a  # free inside reserved\n",
    "                # print(t/1000000000)\n",
    "                # print(r/1000000000)\n",
    "                # print(a/1000000000)\n",
    "                # print(f/1000000000)\n",
    "                print(count)\n",
    "            count += 1\n",
    "            \n",
    "        return logits, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f576eac-6583-44f5-9c92-01face3cde50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "14.294188032\n",
      "14.265847808\n",
      "0.028340224\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99640e61-23f4-4869-aef3-497e43e21e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_style(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        # print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c7280f-ba27-428a-87b9-7a318213b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_style_generate(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples = 100, num_tokens_to_generate = 1, sen_to_generate = 10):\n",
    "  #Put models in eval mode\n",
    "    SAT_model.eval()\n",
    "    GPT_transformer.eval()\n",
    "    for sent in range(sen_to_generate):\n",
    "        random_context_samples = random.sample(context_to_sample, num_samples)\n",
    "        model_context_input =  torch.stack(random_context_samples, dim = 0)\n",
    "\n",
    "        # tokenize prompt\n",
    "        current_tokenization = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "        for generation in range(num_tokens_to_generate):\n",
    "            # put tokenized prompt through GPT_transformer to get GPT Logits\n",
    "            GPT_logits = GPT_transformer(current_tokenization).last_hidden_state.squeeze()\n",
    "\n",
    "            # put model_context_input and the GPT Logits into SAT model\n",
    "            adjusted_output = SAT_model(model_context_input, GPT_logits)\n",
    "\n",
    "            # Funtional softmax the SAT model output\n",
    "            SM_adjusted_output = torch.nn.functional.softmax(adjusted_output, dim = 1)\n",
    "\n",
    "            # argmax to get predicted token\n",
    "            predicted_tokens = torch.argmax(SM_adjusted_output, dim =1)\n",
    "\n",
    "            # get topk tokens\n",
    "            top_predicted_tokens = torch.topk(SM_adjusted_output[-1], 2, dim =0).indices\n",
    "\n",
    "            #dif way of getting top predicted\n",
    "            predicted_token = top_predicted_tokens[0]\n",
    "            if predicted_token == 247:\n",
    "                print(\"WEIRD TOKEN PREDICTED\")\n",
    "                predicted_token = top_predicted_tokens[1]\n",
    "\n",
    "            # print(torch.amax(SM_adjusted_output[-1]))\n",
    "\n",
    "            # print(predicted_tokens[-1].unsqueeze(0).unsqueeze(0))\n",
    "            # print(current_tokenization)\n",
    "\n",
    "            current_tokenization = torch.cat((current_tokenization, predicted_token.unsqueeze(0).unsqueeze(0)), 1)\n",
    "            # print(current_tokenization)\n",
    "\n",
    "        # decode\n",
    "        for i, beam in enumerate(current_tokenization):\n",
    "            # print(f\"{i}: {tokenizer.decode(beam)}\")\n",
    "            # print(f\"{i}: {current_tokenization}\")\n",
    "            print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    # for i, beam in enumerate(predicted_tokens):\n",
    "    #     # if i == 0:\n",
    "    #     #   continue\n",
    "    #     print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)} token_id: {beam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bccd7c67-a151-4d6e-a816-5267ccb3561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_path = \"/home/ubuntu/IMDB_train/pos/\"\n",
    "neg_path = \"/home/ubuntu/IMDB_train/neg/\"\n",
    "\n",
    "pos_token_list = []\n",
    "pos_text_list = []\n",
    "neg_token_list = []\n",
    "neg_text_list = []\n",
    "\n",
    "os.chdir(pos_path)\n",
    "  \n",
    "\n",
    "for file in os.listdir():\n",
    "    with open(pos_path+file, 'r') as f:\n",
    "        pos_text = f.read()\n",
    "        # put into text list\n",
    "        pos_text_list.append(pos_text)\n",
    "        #tokenize and put into token list\n",
    "        #pos_token_list.append(base_tokenizer.encode(pos_text, return_tensors = 'pt'))\n",
    "\n",
    "os.chdir(neg_path)\n",
    "\n",
    "for file in os.listdir():\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "    with open(neg_path+file, 'r') as f:\n",
    "        neg_text = f.read()\n",
    "        # put into text list\n",
    "        neg_text_list.append(neg_text)\n",
    "        #tokenize and put into token list\n",
    "        #neg_token_list.append(base_tokenizer.encode(neg_text_list, return_tensors = 'pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4439f18c-753f-423d-b954-4e46609e6a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You know the story - a group of plucky no-hopers enter a competition they seemingly have no chance of winning - it's a tale that has been done to death by Hollywood (Bring It On, The Karate Kid, Escape to Victory, Best of the Best etc). Now Korea gives it a go with a Taekwondo team struggling for glory  and guess what  the result is predictable but ultimately satisfying.<br /><br />The fact that this movie doesn't fall flat on its face is down to the talented young cast who really make you care about the characters, and this in turn keeps you watching to the end.<br /><br />Fans of your typical martial arts movie may be disappointed  Taekwondo does not deliver the usual flurry of moves and acrobatics seen in most Kung Fu films; the action is limited to (albeit impressive) kicking and the occasional punch. This doesn't matter though, since it is the interaction of the characters and their fight to make something of themselves which makes this movie a success.\n"
     ]
    }
   ],
   "source": [
    "print(pos_text_list[20])\n",
    "# test_text_pos = pos_text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701f5f1c-340e-42ce-b434-1b7a39675f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# ref https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "# cleaned = test_text_pos.replace('<br /><br />', ' ')\n",
    "# print(cleaned)\n",
    "\n",
    "def clean_imdb(review_list):\n",
    "    for review in range(len(review_list)):\n",
    "        cleaned_review = strip_tags(review_list[review])\n",
    "        review_list[review] = cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0fc2007-6c39-4d7f-a266-555f5fe44b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imdb(pos_text_list)\n",
    "clean_imdb(neg_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47469eda-abcf-4da1-b38b-3a0320a98499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legend of Dragoon is one of those little-known games that people either love or hate. Some people claim it's far too similar to other games, namely the Final Fantasy series--which is understandable, since it was originally intended to be Sony's equivalent of Final Fantasy. Honestly I can't comment on the similarities beyond that, as I'm not very familiar with the FF games.I think my favorite aspect of the game is the battle system. Not only do you have the ability to change into a more powerful dragoon form, but every time you attack, you have to pay attention in order to complete the attack by pressing buttons at the correct time. Not only that, sometimes enemies will attack you back right in the middle of a sequence, which means you have to press different buttons in order to avoid taking damage. Even the use of certain attack items requires a bit of button-mashing. If you don't want to attack, you can always guard, which not only cuts any damage taken in half, but raises your hit points without the use of healing potions.The FMVs are quite well-done, about the same quality as Final Fantasy 8's. However, the graphics during game play aren't quite up to that standard. They're nice, but they could have been--and honestly, should have been--better. The translation as well leaves something to be desired. Not only does it raise interesting character relationship questions, but there are also some grammatical mistakes that simply shouldn't have been allowed to pass.Another thing I found interesting was that you lose main party characters--one dies, and the other basically becomes useless to the party and leaves. While the death of the one character is often said to have no point, it makes you realize early on that the characters, while heroes, are still just as mortal as the next person. The people who replace the lost characters simply gain all their stats, so the transition game play-wise is fairly smooth. Perhaps my one complaint about the characters is the main character's love interest, Shana. She is the epitome of the helpless female in need of rescuing, pathetic to the point of driving a player to screaming with frustration. While you can use her in your party, she is insanely weak--I don't even know what her dragoon powers are like, as I disliked her so much I never used her. The character Rose, by contrast, is probably my favorite female character in any game ever. She's no wimp, and some of her dragoon magic is extremely useful. Meru is quite strong as well, while sometimes being an annoying talkative brat.The character designers were, as most are, inclined to make the female characters appear pretty or whatever, and didn't give much thought to the actual usefulness of the outfits. Seriously, no armor and having most of your skin exposed is not helpful when fighting monsters. But I will give them props, as they do have females serving as knights in the various countries.I can't comment much on the plot, as honestly I didn't pay much attention to it beyond where I needed to go to next. I'm not sure if this says something about the plot itself, or my gaming style.All in all, it's a very enjoyable game. It has its flaws, but for me it struck just the right balance of having to think and just pressing buttons and killing monsters.\n"
     ]
    }
   ],
   "source": [
    "print(pos_text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0573afd1-40f3-47c0-a4f6-f7db6604011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG LEN: 1323.44904\n"
     ]
    }
   ],
   "source": [
    "len_sum = 0\n",
    "count = 0\n",
    "for sentence in pos_text_list:\n",
    "    len_sum += len(sentence)\n",
    "    count += 1\n",
    "print(f\"AVG LEN: {len_sum/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92341fdc-935f-4dca-9089-2e4c8bdf17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "empty sentence\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "R_pos_embeds = make_context_embeddings_with_mean(Roberta_model, R_tokenizer, pos_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d990aa-5ccf-45c7-9e70-828915e11f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "empty sentence\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "pos_logits, pos_token_ids = make_logits(Model_Import_6.head_transformer, Model_Import_6.tokenizer, pos_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abceb836-55cb-4e1d-b12d-d670ecd8f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12499\n",
      "12499\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_logits))\n",
    "print(len(R_pos_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56b5c609-4beb-4955-99bb-6de8cee197a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "R_neg_embeds = make_context_embeddings_with_mean(Roberta_model, R_tokenizer, neg_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df595b5b-2020-4e7e-8b8c-9ca9845b944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc7f3de-7e32-42b6-b138-fa287c2b3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_logits, neg_token_ids = make_logits(Model_Import.head_transformer, Model_Import.tokenizer, neg_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9536131-141f-46cb-8c75-bdab74c4c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(R_pos_embeds, '/home/ubuntu/R_pos_embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84550461-b1a2-4d7f-aa8b-9f38aa80d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pos_logits, '/home/ubuntu/pos_logits.pt')\n",
    "torch.save(pos_token_ids, '/home/ubuntu/pos_token_ids.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0917dc7-53c1-49e4-a122-a7c922767664",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(R_neg_embeds, '/home/ubuntu/R_neg_embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6574fb31-a990-40bc-bace-223a9cd268db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(neg_logits, '/home/ubuntu/neg_logits.pt')\n",
    "torch.save(neg_token_ids, '/home/ubuntu/neg_token_ids.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71c0a3c0-2c02-46f9-b932-25a7a2b15f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor([0, 1, 2, 3, 4])\n",
    "# torch.save(x, 'tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "873dbba3-c565-4259-bb22-89c56d405fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/IMDB_train/neg\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ef7d9-d250-4714-b2db-1392a5df495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model_Import_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909fcc9-6b5a-4cb1-82c6-f5572c0dabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model_Import_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dec7b5-aae8-480c-8ee7-5fb9d79b27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_alone_model = Model_Import_5.DeeperModel(R_pos_embeds[0].shape[0], pos_logits[0].shape[1], attention_dim = None) # 1:42 per epoch on old deep model (2 layers)\n",
    "pos_optimizer = optim.Adam(pos_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340218e-ccc9-4aa7-b2e8-3ead60b6f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(pos_alone_model, pos_optimizer, R_pos_embeds, pos_logits, pos_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416225a9-5ef3-4eb5-8413-52470ef6f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import.ProposedModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None) # 1:08\n",
    "neg_alone_model = Model_Import_5.DeeperModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None)\n",
    "neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895d0fc-ec0f-4267-85aa-bdf5645b8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721fb81-dd3e-4ba5-b797-7760cc23b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model_Import_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a025b-dd3c-4c70-94a1-de68f1a8ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None)\n",
    "# neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b93f62-97a0-4ed3-9151-849b3076851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d11521-0cda-4e0e-aa21-e78b99949ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import.tokenizer, pos_alone_model, Model_Import.head_transformer, R_pos_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dcbe89-7ea1-4d8e-a8d1-246c71be4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import.tokenizer, neg_alone_model, Model_Import.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b31fd4-a48c-49ef-843b-5f4e915cfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids = Model_Import.tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "base_generation = Model_Import.head_model.generate(text_ids, max_length=20)\n",
    "for i, beam in enumerate(base_generation):\n",
    "      print(f\"{i}: {Model_Import.tokenizer.decode(beam, skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76346ac-44ba-4bca-8e6d-8fb3b62dc815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c46242-1c94-46bb-ab5b-14a10968bc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
