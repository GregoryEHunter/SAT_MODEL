{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd2434a8-e8b6-4aea-9d33-7c86a1e11540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c991b7-67bd-4589-8005-f8e94f5bcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel\n",
    "import math\n",
    "import Model_Import_6\n",
    "from torch import optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "# from transformers import WarmupLinearSchedule\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e09a10c-4395-4195-96d8-35b6ef55506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e50717c6-34e7-41fb-b6ac-c3c13e0acacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stuff(context_tensors, generator_logits, text_ids_loc):\n",
    "    context = torch.load(context_tensors)\n",
    "    logits = torch.load(generator_logits)\n",
    "    text_ids = torch.load(text_ids_loc)\n",
    "    return context, logits, text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1654d0-149b-401a-9b18-a18ba91a8e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "R_neg_embeds, neg_logits, neg_token_ids = load_stuff('R_neg_embeds.pt', 'neg_logits.pt', 'neg_token_ids.pt')\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c364522e-5812-4ca5-a813-e2ff4764a1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "R_pos_embeds, pos_logits, pos_token_ids = load_stuff('R_pos_embeds.pt', 'pos_logits.pt', 'pos_token_ids.pt')\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b604c9d-5029-40fa-b924-311903ffe054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_style_generate(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples = 100, num_tokens_to_generate = 1, sen_to_generate = 10):\n",
    "  #Put models in eval mode\n",
    "    SAT_model.eval()\n",
    "    GPT_transformer.eval()\n",
    "    \n",
    "    for sent in range(sen_to_generate):\n",
    "        random_context_samples = random.sample(context_to_sample, num_samples)\n",
    "        model_context_input =  torch.stack(random_context_samples, dim = 0).to(device)\n",
    "        expierment_list = []\n",
    "        sentences = []\n",
    "        # tokenize prompt\n",
    "        current_tokenization = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "        for generation in range(num_tokens_to_generate):\n",
    "            # put tokenized prompt through GPT_transformer to get GPT Logits\n",
    "            GPT_logits = GPT_transformer(current_tokenization).last_hidden_state.squeeze()\n",
    "\n",
    "            # put model_context_input and the GPT Logits into SAT model\n",
    "            adjusted_output = SAT_model(model_context_input, GPT_logits)\n",
    "\n",
    "            # Funtional softmax the SAT model output\n",
    "            SM_adjusted_output = torch.nn.functional.softmax(adjusted_output, dim = 1)\n",
    "\n",
    "            # argmax to get predicted token\n",
    "            # predicted_tokens = torch.argmax(SM_adjusted_output, dim =1)\n",
    "\n",
    "            # get topk tokens\n",
    "            top_predicted_tokens = torch.topk(SM_adjusted_output[-1], 3, dim =0).indices\n",
    "            top_predicted_tokens_prob = torch.topk(SM_adjusted_output[-1], 3, dim =0).values\n",
    "            top_predicted_tokens_numpy = top_predicted_tokens.cpu().detach().numpy()\n",
    "            top_predicted_tokens_prob_numpy = top_predicted_tokens_prob.cpu().detach().numpy()\n",
    "            token_prob_sum = np.sum(top_predicted_tokens_prob_numpy)\n",
    "            token_distribution = top_predicted_tokens_prob_numpy/token_prob_sum\n",
    "            \n",
    "            predicted_token =  torch.from_numpy(np.array(np.random.choice(top_predicted_tokens_numpy, p = token_distribution))).to(device)\n",
    "            \n",
    "            # print(top_predicted_tokens)\n",
    "             # tok_k_predicted_words = tokenizer.decode(top_predicted_tokens, skip_special_tokens=True)\n",
    "#             decoded_tokens = []\n",
    "#             for token_k in top_predicted_tokens:\n",
    "#                 decoded_tokens.append(tokenizer.decode(token_k, skip_special_tokens=True))\n",
    "                \n",
    "#             top_predicted_tokens_prob_cpu = top_predicted_tokens_prob.cpu()\n",
    "            del top_predicted_tokens_prob\n",
    "            # expierment_list.append((top_predicted_tokens_prob_cpu, decoded_tokens))\n",
    "\n",
    "\n",
    "                \n",
    "            #dif way of getting top predicted\n",
    "            # predicted_token = top_predicted_tokens[0]\n",
    "            # print(current_tokenization[-1][0])\n",
    "            # print(predicted_token)\n",
    "            # if predicted_token == 247 or predicted_token == current_tokenization[-1][0]:\n",
    "            if predicted_token == 247:\n",
    "                print(\"WEIRD TOKEN PREDICTED\")\n",
    "                predicted_token = top_predicted_tokens[1]\n",
    "\n",
    "            # print(torch.amax(SM_adjusted_output[-1]))\n",
    "\n",
    "            # print(predicted_tokens[-1].unsqueeze(0).unsqueeze(0))\n",
    "            # print(current_tokenization)\n",
    "\n",
    "            current_tokenization = torch.cat((current_tokenization, predicted_token.unsqueeze(0).unsqueeze(0)), 1)\n",
    "            del predicted_token\n",
    "            gc.collect()\n",
    "            # print(current_tokenization)\n",
    "\n",
    "        # decode\n",
    "        sentence = []\n",
    "        for i, beam in enumerate(current_tokenization):\n",
    "            # print(f\"{i}: {tokenizer.decode(beam)}\")\n",
    "            # print(f\"{i}: {current_tokenization}\")\n",
    "            #print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "            sentence.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "        sentences.append(sentence)\n",
    "        del current_tokenization\n",
    "        del random_context_samples\n",
    "        del model_context_input\n",
    "        gc.collect()\n",
    "        # for prediction in expierment_list:\n",
    "        #     print(prediction)\n",
    "        print(\"\\n\\n\")\n",
    "    return sentences\n",
    "    # for i, beam in enumerate(predicted_tokens):\n",
    "    #     # if i == 0:\n",
    "    #     #   continue\n",
    "    #     print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)} token_id: {beam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d88604-b0d3-49c0-ba0e-47eb82f78afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_embeds = R_neg_embeds+R_pos_embeds\n",
    "style_list_neg = []\n",
    "style_list_pos = []\n",
    "for embed in R_neg_embeds:\n",
    "    style_list_neg.append(0)\n",
    "for embed in R_pos_embeds:\n",
    "    style_list_pos.append(1)\n",
    "    \n",
    "style_list = style_list_neg + style_list_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d52da60-5fd1-4e8e-a8df-07c6881f94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_train, logits_test, token_ids_train, token_ids_test, style_train, style_test = train_test_split(neg_logits+pos_logits, neg_token_ids+pos_token_ids, style_list, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d859555-7d89-447b-9d4c-80149715c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_embeds_neg_train, R_embeds_neg_test = train_test_split(R_neg_embeds, test_size=0.2, random_state=42)\n",
    "R_embeds_pos_train, R_embeds_pos_test = train_test_split(R_pos_embeds, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f96630c-0f31-4a2a-8729-e0ca4a295208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_styles_w_dev(model, optimizer, pos_context_embeds_list, neg_context_embeds_list, logits_list, token_ids_list, style_list_train, dev_logits, dev_pos_context, dev_neg_context, dev_token_ids, dev_style, epochs, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list, style_list_train))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list, style_list_train = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            if style_list_train[example] == 0:\n",
    "                context_embeds_list = neg_context_embeds_list\n",
    "            else:\n",
    "                context_embeds_list = pos_context_embeds_list\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        model.eval()\n",
    "        CELoss_dev = nn.CrossEntropyLoss()\n",
    "        dev_loss_acum = 0\n",
    "        for dev_example in range(len(dev_logits)):\n",
    "            if style_list_train[dev_example] == 0:\n",
    "                dev_context = dev_neg_context\n",
    "            else:\n",
    "                dev_context = dev_pos_context\n",
    "            random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "            stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "            \n",
    "            dev_network_output = model(stacked_context_sample_dev.to(device), dev_logits[dev_example].to(device))\n",
    "            \n",
    "            shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "            dev_loss_acum += dev_loss.item()\n",
    "        full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dee0297e-eef8-42aa-a515-b524bb2931e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40733118-683a-4f1a-837e-e9be1ae24612",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28fb8d20-d08e-4a57-b091-163e4a1d65f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_two_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_two_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b660a15-3a6f-49f8-a98a-6dd9548e9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Epoch: 0, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.261791944503784\n",
      "DEV LOSS: 3.24163070807457\n",
      "----------------------------------------\n",
      "....................Epoch: 1, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.2213380336761475\n",
      "DEV LOSS: 3.2333539516448973\n",
      "----------------------------------------\n",
      "....................Epoch: 2, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.2006990909576416\n",
      "DEV LOSS: 3.230662031984329\n",
      "----------------------------------------\n",
      "....................Epoch: 3, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.1808905601501465\n",
      "DEV LOSS: 3.2353980966329576\n",
      "----------------------------------------\n",
      "....................Epoch: 4, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.162254810333252\n",
      "DEV LOSS: 3.2333726744413376\n",
      "----------------------------------------\n",
      "....................Epoch: 5, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.144083023071289\n",
      "DEV LOSS: 3.2388961870670316\n",
      "----------------------------------------\n",
      "....................Epoch: 6, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.1267900466918945\n",
      "DEV LOSS: 3.239118070960045\n",
      "----------------------------------------\n",
      "....................Epoch: 7, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.110502004623413\n",
      "DEV LOSS: 3.244857153391838\n",
      "----------------------------------------\n",
      "....................Epoch: 8, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.095285415649414\n",
      "DEV LOSS: 3.2489637910246847\n",
      "----------------------------------------\n",
      "....................Epoch: 9, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.0811853408813477\n",
      "DEV LOSS: 3.248293466258049\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_two_styles_w_dev(pytorch_stacked_two_cross, neg_optimizer, R_embeds_pos_train, R_embeds_neg_train, logits_train, token_ids_train, style_train, logits_test, R_embeds_pos_test, R_embeds_neg_test, token_ids_test, style_test, 10, num_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0a2d1ca-9155-4f3f-9da9-45a31906ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_basic = Model_Import_6.MultiHeadModel_PyTorch(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer_basic = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac4cecdd-4e1a-428d-903d-854b506f0491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Epoch: 0, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.2572386264801025\n",
      "DEV LOSS: 3.237956048643589\n",
      "----------------------------------------\n",
      "....................Epoch: 1, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.2205159664154053\n",
      "DEV LOSS: 3.2310818600654603\n",
      "----------------------------------------\n",
      "....................Epoch: 2, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.199735164642334\n",
      "DEV LOSS: 3.233080428123474\n",
      "----------------------------------------\n",
      "....................Epoch: 3, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.1804263591766357\n",
      "DEV LOSS: 3.2303701306462287\n",
      "----------------------------------------\n",
      "....................Epoch: 4, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.1619551181793213\n",
      "DEV LOSS: 3.231329568481445\n",
      "----------------------------------------\n",
      "....................Epoch: 5, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.1449155807495117\n",
      "DEV LOSS: 3.234548204445839\n",
      "----------------------------------------\n",
      "....................Epoch: 6, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.129037857055664\n",
      "DEV LOSS: 3.2349632293820383\n",
      "----------------------------------------\n",
      "....................Epoch: 7, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.114431142807007\n",
      "DEV LOSS: 3.2374271211504935\n",
      "----------------------------------------\n",
      "....................Epoch: 8, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.100231885910034\n",
      "DEV LOSS: 3.2430139830350875\n",
      "----------------------------------------\n",
      "....................Epoch: 9, Epoch Examples: 19999\n",
      "TRAIN LOSS: 3.0879478454589844\n",
      "DEV LOSS: 3.243356535542011\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_two_styles_w_dev(pytorch_basic, neg_optimizer_basic, R_embeds_pos_train, R_embeds_neg_train, logits_train, token_ids_train, style_train, logits_test, R_embeds_pos_test, R_embeds_neg_test, token_ids_test, style_test, 10, num_samples = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "053a5545-ff14-4033-90c9-9a1e9b134b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_two_alt = Model_Import_6.MultiHeadModel_PyTorch_Stacked_One_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer_basic = optim.RAdam(pytorch_stacked_two_alt.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff125e-249e-41ea-bb1d-979484c4b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_two_styles_w_dev(pytorch_stacked_two_alt, neg_optimizer_basic, R_embeds_pos_train, R_embeds_neg_train, logits_train, token_ids_train, style_train, logits_test, R_embeds_pos_test, R_embeds_neg_test, token_ids_test, style_test, 10, num_samples = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b796b9-7325-4648-bad3-43d15bbb48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_alt, Model_Import_6.head_transformer, R_embeds_neg_test, num_samples = 10, num_tokens_to_generate = 50, sen_to_generate = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46c8c2-4c1f-41ee-895d-9049f8e815d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_alt, Model_Import_6.head_transformer, R_embeds_pos_test, num_samples = 10, num_tokens_to_generate = 50, sen_to_generate = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca0f67f9-3c80-423f-91c3-860a09c4762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The movie was so bad it was good. It was so terrible that the acting was so good that the movie was actually good. It was so bad it was good, it was so bad that the acting wasn't so good that the movie was actually good.\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I actually turned it off after the first half hour. The acting was bad. The script was bad. The editing was bad. The directing was bad and the editing was so bad that it was almost like watching paint dry. I\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was not the worst movie I have ever seen but it was the first one that I thought was bad. I was very disappointed. The acting was bad. The plot was bad. It was a movie that I didn't want to watch. The only\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that it was banned in Germany. I was so disappointed that the movie was so bad that it was banned in Germany, and I was so disappointed that the movie was so bad that it was banned in Germany. I was so disappointed that\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't believe it was even made. I thought it was so bad that I was going to kill myself. It was so bad that I was so angry that I was actually thinking about killing myself. It was so bad that\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a remake of a 1971 Japanese film called \"The Iron Giant\". It was a huge hit at the time and it's a remake of that movie. The movie was so popular, it was made into two sequels, \"The Iron Giant 2:\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad that it has become a joke, and the only reason it's still being made is because it's a movie. I'm not even kidding. I'm not even being sarcastic. It has become so bad, that I'm embarrassed to\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad, it's almost as if someone wrote the script with a pen and paper and then threw it on the screen, and then someone else wrote the script with the script and then threw it on the screen, and then someone else wrote the\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so awful I couldn't believe it was the first time I saw this movie. The only reason I watched it is that the cover looked so good. It was so bad I couldn't believe I'd ever heard about this movie. The only reason\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad it's funny! I saw this movie on the shelf at Blockbuster and was so disappointed. The plot is terrible. It's not even a movie. It's a collection of scenes that were shot on a video camera. The actors\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that it became a cult favorite. The plot is ridiculous and the actors are terrible, but the movie is so bad that it's actually fun. I don't know if it was the acting or the script, but I found myself laughing\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a remake of the original film with the same title, but with a few new scenes and a new plot. It is about a man who is sent to prison and is then released. The movie is a bit slow, but it is still a\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was not a complete success. The film was released in the United States in late 1979, and it was not well-received. It was a box-office disappointment and it did not make a lot of money. The film was released on video,\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was a big hit in Germany, and I was very pleased. But the movie didn't do so well in the US, where I had to work with some of the worst actors. I was very disappointed, and it's not because the actors are\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts with an old man (Barry Nelson) who is being pursued by a man (John Ritter) who wants to kill him for some reason. The chase is so intense, it's almost as if the movie is trying to make us feel\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a little slow and boring but the actors are good and the story is pretty cool. The only thing is that it is a little slow and the story is pretty cool. It is a little slow, but it has some good effects and some good\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts out with a very good performance by David Strathairn, but the rest of the cast is pretty bad. The movie is very slow, and the plot doesn't make much sense. The acting is pretty good, and the movie is worth\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad it is funny. It's so bad it's funny. It's so bad it is funny. It's so bad it is funny. It's so bad it is funny. It's so bad it is funny. It's so\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was a big disappointment for me, and I was really disappointed that it wasn't a good film. It was very bad. I don't know if it was the director or the script, but it was just bad. I think it was a good\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I actually turned it off after the first 15 minutes, and I couldn't watch it again. The acting was so bad, and the script so bad, that it was impossible to watch the movie. It wasn't even entertaining.\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is about a group of people who are all on the same page and all having a great time. It's not a horror film, it's a comedy, and it's about a group of people having a great time. So, it was really\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is about a young woman, who has a crush on a man, and he is attracted to her. She tries to convince him to go out with her. But, he is attracted to her too, and they are in love. But, she\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is about a young man (Dillon) who has been raised in a very strict, religious, conservative family, and is about to be sent to a boarding school in the country where he is to spend his first year of high school. He is\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a bit slow and the acting is pretty poor, but it's still pretty funny, and it's not too bad. It's a pretty good movie, but I don't think I'll be watching it again. It's a good movie,\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad that it is hard to believe that it is based on a true story. It is so bad that it is hard to believe that a real person would actually make a movie based on it. It is so bad that it is hard to\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a remake of \"The Phantom of the Opera\". The Phantom is the Phantom of the Opera, the Phantom of the opera, the Phantom, the Phantom, the Phantom, the Phantom...The Phantom...The Phantom of the Opera...The Phantom of\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that the producers decided to remake it. The original movie was so awful that they decided to remake it. The remake was so bad, that they decided to remake it again. And again. And again. And again... The movie was\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a mess. I'm not saying that it's bad, I'm saying it's bad in the same way a bad movie can be good. I'm not saying it's a great movie, I'm saying it is a bad movie. The\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is so bad that it is not even worth mentioning. The plot is so bad, that even the actors cannot act. The movie is so bad that it makes you want to vomit. The movie is so bad, that it is not even funny.\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts out slow, but it's very interesting and it's very well made. The plot is very interesting, but it's very slow. It doesn't get going until the last 15 minutes. The first half of the movie is about the relationship between\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts off with a lot of suspense. We are introduced to the main characters, the main antagonist and his henchmen. We are given a little background on each of them. Then we get to know about their past. We get to know their\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is about a group of people who are trying to get a hold of a nuclear weapon and they are being pursued by the government, who are using the police and the military to do their dirty work. The movie is very entertaining, although it does contain\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so awful, it's hard to know where to start. It's so bad, it's so bad, it's so bad. It's so bad, it's so bad, it's so bad. It's so bad it's so\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't believe it when I first watched the trailer. I thought it was going to be some cheesy action flick. I didn't think it was going to be so bad. I thought it was going to be some cheesy action\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so good that the first time I saw it, I laughed so much I couldn't stand up. I was laughing so hard that the next day, I couldn't stop laughing. It's so funny. I was laughing so hard I was almost\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is about a man (John Cusack), who is sent to a psychiatric ward after his wife (Jennifer Jason Leigh) dies. He meets with a psychiatrist (David Strathairn) and is diagnosed with bipolar disorder. The psychiatrist is convinced\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts with a very good plot and a good idea. It's about two young people who get involved in a murder investigation. The first one is played by David Strickland and the second one is played by Robert Downey, Jr. The film\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't even finish it. The acting was so bad. I couldn't even watch it because it was so bad. I couldn't even finish it. It was so bad, it was like watching a movie. I was\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a remake of the classic \"The Little Prince\" (which was a French film), and is a remake, not a remake of the original. The plot is the same as the original movie. The only thing that is different is that the prince\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that it was banned from being shown in some cinemas, and the movie was so bad it was banned from being shown in some cinemas, and I don't know why. I don't know if it was because it's so\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't believe it was even made. I was so disgusted by the script that I turned it off. I was so disgusted with the movie that I couldn't believe it was even made. It's so bad that I couldn\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts out with a lot of good humor and some good action scenes. The movie is very entertaining and the actors are very good. The movie is very entertaining and the actors are very good. The movie has some great action sequences, but it is not\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't even watch it on TV. I couldn't even sit through the movie. The only thing I could watch was my own copy. I was embarrassed to see it. It was so bad, I couldn't even sit\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was a big hit in the UK and the rest of the world, and it was one of the most-watched movies in the UK in 2002 and 2003. I was very pleased with the movie, and I thought it was very well made.\n",
      "\n",
      "\n",
      "\n",
      "0: The movie starts with a lot of nudity and a lot of sexual scenes. It was a very interesting idea to start with, but it was a very bad idea for the movie. It was very hard to get it done. It was a very difficult movie to\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so awful that it was almost impossible to watch, and I couldn't believe that the director would actually allow himself to make it. It's not that I didn't want to like it, I just found it so bad. I couldn't believe\n",
      "\n",
      "\n",
      "\n",
      "0: The movie was so bad that I couldn't even finish it. I couldn't believe it. I was so angry that I couldn't even finish it. I was so angry. I couldn't finish it. It was so bad. I couldn't even finish\n",
      "\n",
      "\n",
      "\n",
      "0: The movie is a bit slow and the acting isn't that great, but I like the idea of a movie about a guy that gets into trouble with his girlfriend, but is saved by his girlfriend's ex-boyfriend (played by the always good-looking\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3ca674f0683c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneg_SAT_generations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_style_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_stacked_two_cross\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_embeds_neg_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-133117482b2c>\u001b[0m in \u001b[0;36mone_style_generate\u001b[0;34m(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples, num_tokens_to_generate, sen_to_generate)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_tokenization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tokenization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mpredicted_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# print(current_tokenization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neg_SAT_generations = one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_embeds_neg_test, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5943084-0cff-4b2c-b25d-b2f6bc84c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_SAT_generations = one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_embeds_pos_test, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80a1e1-06a1-432c-bf81-b549739dc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_SAT_generations_basic = one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_embeds_neg_test, num_samples = 10, num_tokens_to_generate = 50, sen_to_generate = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff767dc1-2f18-4031-9059-72dfbf0c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_SAT_generations_basic = one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_embeds_pos_test, num_samples = 10, num_tokens_to_generate = 50, sen_to_generate = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49ea5cd2-ab77-48b9-adb3-ce1b99cb8bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Darwin_Gatsby/gatsby_raw.txt', 'r') as file:\n",
    "    gatsby = file.read().replace('\\n', ' ')\n",
    "\n",
    "with open('/home/ubuntu/Darwin_Gatsby/origin_of_species_raw.txt', 'r') as file:\n",
    "    origin = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "870b741a-efc4-4528-b84e-1f511a0f5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439\n",
      "3981\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "gatsby_sen_list = nltk.sent_tokenize(gatsby)\n",
    "origin_sen_list = nltk.sent_tokenize(origin)\n",
    "print(len(gatsby_sen_list))\n",
    "print(len(origin_sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68ff2141-c928-4863-8961-7cad28924c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram_overlap: {'We could', 'I think', 'The only', 'How do', 'In two', 'I remember', 'Let the', 'And when', 'I cannot', 'The day', 'A new', 'But the', 'No one', 'All these', 'I feel', 'But there', 'It is', 'But we', 'But in', 'Not that', 'By the', 'See how', 'The practical', 'Were all', 'In the', 'I must', 'And as', 'The little', 'A few', 'Of course', 'He has', 'If we', 'How the', 'With the', 'At a', 'In fact,', 'He always', 'As soon', 'I believe', 'I see', 'But it', 'I will', 'Even if', 'Look at', 'There must', 'They are', 'One of', 'A little', 'A man', 'This was', 'I am', 'To the', 'But with', 'But I', 'The eyes', 'And what', 'I could', 'At this', 'The most', 'I should', 'At first', 'In my', 'The whole', 'As we', 'Some little', 'Even when', 'I would', 'I know', 'The fact', 'On a', 'But what', 'A large', 'After a', 'The old', 'And I', 'The truth', 'It might', 'But how', 'If I', 'As I', 'A white', 'I was', 'It seems', 'Now I', 'Let us', 'After the', 'The flowers', 'I have', 'And we', 'But no', 'The masters', 'It makes', 'In one', 'How much', 'Now it', 'If it', 'The man', 'In this', 'And if', 'If there', 'All the', 'This is', 'The three', 'He would', 'As the', 'Most of', 'The other', 'On the', 'When I', 'It occurred', 'Do they', 'He must', 'So we', 'To my', 'And now', 'It was', 'And the', 'Not only', 'And all', 'In a', 'But when', 'And it', 'For several', 'From the', 'When we', 'Now, in', 'At the', 'Those who', 'When the', 'As he', 'For some', 'Nothing at', 'Now, if', 'There is'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ref https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "gatsby_first_words_list = []\n",
    "for sen_g in gatsby_sen_list:\n",
    "    sen_g = re.sub(r'^\\s*|\\s\\s*', ' ', sen_g).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_g = filter(lambda x: x in printable, sen_g)\n",
    "    sen_g = ''.join(filter(lambda x: x in printable, sen_g))\n",
    "    sen_g_list = sen_g.split()\n",
    "    #print(sen_g_list[0:3])\n",
    "    gatsby_first_words_list.append(sen_g_list[0:3])\n",
    "    \n",
    "origin_first_words_list = []  \n",
    "for sen_o in origin_sen_list:\n",
    "    sen_o = re.sub(r'^\\s*|\\s\\s*', ' ', sen_o).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_o = filter(lambda x: x in printable, sen_o)\n",
    "    sen_o = ''.join(filter(lambda x: x in printable, sen_o))\n",
    "    sen_o_list = sen_o.split()\n",
    "    # print(sen_o_list[0:3])\n",
    "    origin_first_words_list.append(sen_o_list[0:3]) \n",
    "    \n",
    "initial_unigrams_gatsby = defaultdict(int)\n",
    "initial_bigrams_gatsby = defaultdict(int)\n",
    "initial_trigrams_gatsby = defaultdict(int)\n",
    "initial_unigrams_origin = defaultdict(int)\n",
    "initial_bigrams_origin = defaultdict(int)\n",
    "initial_trigrams_origin = defaultdict(int)\n",
    "for first_words in gatsby_first_words_list:\n",
    "    initial_unigrams_gatsby[first_words[0]] += 1\n",
    "    initial_bigrams_gatsby[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_gatsby[' '.join(first_words)] += 1\n",
    "    \n",
    "for first_words in origin_first_words_list:\n",
    "    initial_unigrams_origin[first_words[0]] += 1\n",
    "    initial_bigrams_origin[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_origin[' '.join(first_words)] += 1\n",
    "    \n",
    "unigram_overlap = set(initial_unigrams_gatsby.keys())\n",
    "unigram_overlap = unigram_overlap.intersection(initial_unigrams_origin.keys())\n",
    "\n",
    "bigram_overlap = set(initial_bigrams_gatsby.keys())\n",
    "bigram_overlap = bigram_overlap.intersection(initial_bigrams_origin.keys())\n",
    "   \n",
    "trigram_overlap = set(initial_trigrams_gatsby.keys())\n",
    "trigram_overlap = trigram_overlap.intersection(initial_trigrams_origin.keys())\n",
    "\n",
    "\n",
    "# print(f\"unigram_overlap: {str(unigram_overlap)}\")\n",
    "# print(\"\\n\")\n",
    "print(f\"bigram_overlap: {str(bigram_overlap)}\")\n",
    "print(\"\\n\")\n",
    "# print(f\"trigram_overlap: {str(trigram_overlap)}\")\n",
    "\n",
    "bigram_list = list(bigram_overlap)\n",
    "    \n",
    "# combine the list\n",
    "\n",
    "# combined_g_o_list = gatsby_first_words_list + origin_first_words_list\n",
    "\n",
    "# mini_sen_list = []\n",
    "# for begin in combined_g_o_list:\n",
    "#     mini_sen_list.append(' '.join(begin))\n",
    "    \n",
    "# # ref https://stackoverflow.com/questions/43473736/most-common-2-grams-using-python\n",
    "# bigrams = zip(mini_sen_list, mini_sen_list[1:])\n",
    "# counts = Counter(bigrams)\n",
    "# print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35137848-7682-46fd-91c3-143234d42744",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = random.sample(bigram_list,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8c65da8-ac98-4576-8ae4-b6bb457dfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_SAT_generations = []\n",
    "for prompt_ in lst:\n",
    "    neg_SAT_generations.append(one_style_generate(prompt_, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_embeds_neg_test, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0bac101-46ee-48a5-b862-5cc58f1c48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_SAT_generations = []\n",
    "for prompt_ in lst:\n",
    "    pos_SAT_generations.append(one_style_generate(prompt_, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_embeds_pos_test, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18eef3b0-972c-45bb-b0e7-351e9e56665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_strings(strings):\n",
    "    cleaned_strings = []\n",
    "    #print(strings[:20])\n",
    "    for sen in strings:\n",
    "        #print(sen)\n",
    "        sen = re.sub(r\"\\\\\", \"\", sen[0])    # bizarrely, each string is inside of a list containing nothing but that string\n",
    "        sen = re.sub(r\"\\n\", \" \", sen)\n",
    "        sen = filter(lambda x: x in printable, sen)\n",
    "        sen = ''.join(filter(lambda x: x in printable, sen))\n",
    "        cleaned_strings.append(sen)\n",
    "    #print(cleaned_strings[:20])\n",
    "    \n",
    "    return cleaned_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9e45764-696a-42af-aa56-ac046ce903c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pos_SAT_generations = clean_strings(pos_SAT_generations)\n",
    "clean_neg_SAT_generations = clean_strings(neg_SAT_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91455679-3233-4fc5-9726-fdb71877586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqs = []\n",
    "for seq in clean_pos_SAT_generations:\n",
    "    all_seqs.append(('POS_SAT', seq))\n",
    "for seq in clean_neg_SAT_generations:\n",
    "    all_seqs.append(('NEG_SAT', seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "744c620a-a747-471d-b328-c82eeca27e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_seqs, columns = ['Source', 'Sequence'])\n",
    "\n",
    "df.to_csv('pos_neg_sat.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b2cac-0bf9-42be-9fed-c8a8352d679a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
