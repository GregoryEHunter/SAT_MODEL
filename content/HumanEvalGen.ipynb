{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f4e9d2-5f9c-4390-b790-e96b46c4c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23beaaa-1a7b-499d-aa3d-43af27e0bb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, os, re, pandas as pd, json\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from torch import optim\n",
    "import gc\n",
    "from torch.nn import DataParallel\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78cd2911-de22-4177-b058-5fa43a5112ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae44f0b-41be-47df-a9b0-3ede68945565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "print(str(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113b8ccc-bd98-431a-9e75-389d49e42975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f472ab09-306f-4c43-8b03-8ad46546dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/Darwin_Gatsby/gatsby_raw.txt', 'r') as file:\n",
    "    gatsby = file.read().replace('\\n', ' ')\n",
    "\n",
    "with open('/home/ubuntu/Darwin_Gatsby/origin_of_species_raw.txt', 'r') as file:\n",
    "    origin = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a37281-db9d-42fd-b4ce-09d5ed6305a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2439\n",
      "3981\n"
     ]
    }
   ],
   "source": [
    "gatsby_sen_list = nltk.sent_tokenize(gatsby)\n",
    "origin_sen_list = nltk.sent_tokenize(origin)\n",
    "print(len(gatsby_sen_list))\n",
    "print(len(origin_sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe03a4d-095f-40e5-a5a6-8a250d045e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "gatsby_first_words_list = []\n",
    "for sen_g in gatsby_sen_list:\n",
    "    sen_g = re.sub(r'^\\s*|\\s\\s*', ' ', sen_g).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_g = filter(lambda x: x in printable, sen_g)\n",
    "    sen_g = ''.join(filter(lambda x: x in printable, sen_g))\n",
    "    sen_g_list = sen_g.split()\n",
    "    #print(sen_g_list[0:3])\n",
    "    gatsby_first_words_list.append(sen_g_list[0:3])\n",
    "    \n",
    "origin_first_words_list = []  \n",
    "for sen_o in origin_sen_list:\n",
    "    sen_o = re.sub(r'^\\s*|\\s\\s*', ' ', sen_o).strip().rstrip().lstrip()\n",
    "    printable = set(string.printable)\n",
    "    sen_o = filter(lambda x: x in printable, sen_o)\n",
    "    sen_o = ''.join(filter(lambda x: x in printable, sen_o))\n",
    "    sen_o_list = sen_o.split()\n",
    "    # print(sen_o_list[0:3])\n",
    "    origin_first_words_list.append(sen_o_list[0:3]) \n",
    "    \n",
    "initial_unigrams_gatsby = defaultdict(int)\n",
    "initial_bigrams_gatsby = defaultdict(int)\n",
    "initial_trigrams_gatsby = defaultdict(int)\n",
    "initial_unigrams_origin = defaultdict(int)\n",
    "initial_bigrams_origin = defaultdict(int)\n",
    "initial_trigrams_origin = defaultdict(int)\n",
    "for first_words in gatsby_first_words_list:\n",
    "    initial_unigrams_gatsby[first_words[0]] += 1\n",
    "    initial_bigrams_gatsby[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_gatsby[' '.join(first_words)] += 1\n",
    "    \n",
    "for first_words in origin_first_words_list:\n",
    "    initial_unigrams_origin[first_words[0]] += 1\n",
    "    initial_bigrams_origin[' '.join(first_words[:2])] += 1\n",
    "    initial_trigrams_origin[' '.join(first_words)] += 1\n",
    "    \n",
    "unigram_overlap = set(initial_unigrams_gatsby.keys())\n",
    "unigram_overlap = unigram_overlap.intersection(initial_unigrams_origin.keys())\n",
    "\n",
    "bigram_overlap = set(initial_bigrams_gatsby.keys())\n",
    "bigram_overlap = bigram_overlap.intersection(initial_bigrams_origin.keys())\n",
    "   \n",
    "trigram_overlap = set(initial_trigrams_gatsby.keys())\n",
    "trigram_overlap = trigram_overlap.intersection(initial_trigrams_origin.keys())\n",
    "\n",
    "\n",
    "# print(f\"unigram_overlap: {str(unigram_overlap)}\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"bigram_overlap: {str(bigram_overlap)}\")\n",
    "# print(\"\\n\")\n",
    "# print(f\"trigram_overlap: {str(trigram_overlap)}\")\n",
    "\n",
    "bigram_list = list(bigram_overlap)\n",
    "    \n",
    "# combine the list\n",
    "\n",
    "# combined_g_o_list = gatsby_first_words_list + origin_first_words_list\n",
    "\n",
    "# mini_sen_list = []\n",
    "# for begin in combined_g_o_list:\n",
    "#     mini_sen_list.append(' '.join(begin))\n",
    "    \n",
    "# # ref https://stackoverflow.com/questions/43473736/most-common-2-grams-using-python\n",
    "# bigrams = zip(mini_sen_list, mini_sen_list[1:])\n",
    "# counts = Counter(bigrams)\n",
    "# print(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffbf63b9-5552-40c8-a0f4-f26dd2d222b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0f8410-75a2-4ed6-8a77-8b97aeed8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n",
    "# # head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "head_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "# head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "# standard_gpt = GPT2LMHeadModel.from_pretrained('gpt2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb705c47-3798-408d-8563-07ddd1585056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "# head_model.load_state_dict(torch.load(\"/home/ubuntu/pos_imdb_model\"))\n",
    "head_model.load_state_dict(torch.load(\"/home/ubuntu/small_neg_midterm_version\"))\n",
    "# head_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed21663-96ca-4680-ab41-7a981cc53616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920c4bfc-4822-4923-a73d-f8e400dccbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16dcf93c-2f8c-4096-bf17-0f948742645a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Most of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>With the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0      At a\n",
       "1   Most of\n",
       "2   But how\n",
       "3    By the\n",
       "4  The eyes\n",
       "5   After a\n",
       "6   I could\n",
       "7  With the\n",
       "8     It is\n",
       "9    I have"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# list of strings\n",
    "# lst = ['I', 'Movie', 'He', 'What', \n",
    "#             'The', 'We', 'I thought', 'I sat', 'How about', 'It was', \"This is going\", \"The nature\", ]\n",
    "  \n",
    "lst = random.sample(bigram_list,10)\n",
    "# Calling DataFrame constructor on list\n",
    "df = pd.DataFrame(lst)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "859386d1-ad7e-4f3d-a2b8-97afd2976b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=50, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=.5,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated.to(device), labels=generated.to(device))\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated.to(device), next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().cpu().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().cpu().numpy())\n",
    "                # output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "                output_text = f\"{tokenizer.decode(output_list)}\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data, model, tokenizer):\n",
    "    generated_lyrics = []\n",
    "    for i in range(len(test_data)):\n",
    "        x = generate(model, tokenizer, test_data[0][i], entry_count=1)\n",
    "        generated_lyrics.append(x)\n",
    "    return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_text = text_generation(df, head_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaa15876-1808-4f68-9a2e-989440abc6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At a minimum, the three-year minimum wage is $12.25 an hour, and the minimum wage for a full-time worker is $12.50. The minimum wage for a full-time worker is $12.50. The minimum wage'] \n",
      "\n",
      "['Most of the people who were in the hospital were in shock and had no idea what was going on. The doctor told me that the patient had been taken to the hospital for surgery, but had not been able to see any of the other patients. He said'] \n",
      "\n",
      "['But how much of this is true? Why is it that the \"evidence\" that the \"evidence\" that the \"evidence\" is \"valid\" is so weak? Why is it that the \"evidence\" that the \"evidence\" is \"valid\" is'] \n",
      "\n",
      "['By the time the game was released, the story was already in the public domain. The game was released on the PC, but the PC version was not released until the end of the year. The game was released on the PC, but the PC version was'] \n",
      "\n",
      "['The eyes of the people who were watching, and the people who were watching, and the people who were watching, and the people who were watching, and the people who were watching, and the people who were watching, and the people who were watching, and'] \n",
      "\n",
      "['After a few days, I started to feel more comfortable with my body. I was able to sit down and relax, and I felt better. I was able to take a shower and relax. I felt more comfortable with my body. I was able to sit'] \n",
      "\n",
      "['I could not find the piece of paper that said \"I have a problem with this piece of paper\". I was not able to find the piece of paper that said \"I have a problem with this piece of paper\". I could not find the piece of paper'] \n",
      "\n",
      "[\"With the exception of the first two, the first two were the most successful in their respective careers. The first was the last of the three, which was a very good start to the career of the 'Titanic' character. The second was the '\"] \n",
      "\n",
      "[\"It is not a good idea to use the word 'dumb' to describe a device that is not designed to be used. The term 'dumb' is used to describe a device that is not designed to be used. This is a very bad idea\"] \n",
      "\n",
      "['I have a few more questions for you, dear reader. I am not a doctor, but I have a few questions for you. First of all, I am not a doctor. I am a former student of the history of medicine. I have a few'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in generated_text:\n",
    "    print(text, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961ce68-befd-477c-b996-3bd765623b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538d47f-6590-4e63-b412-d84415372564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
