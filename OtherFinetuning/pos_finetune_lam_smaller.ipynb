{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56ca7c0-996c-42d6-b331-9994049ac0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in ./.local/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a0d701-69c1-4d5d-a2a1-751b5e72e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7fafd4331820>,\n",
       " <torch.cuda.device at 0x7fafd43318e0>,\n",
       " <torch.cuda.device at 0x7fafd4331910>,\n",
       " <torch.cuda.device at 0x7fafd4317a00>,\n",
       " <torch.cuda.device at 0x7fafd42ec250>,\n",
       " <torch.cuda.device at 0x7fafd42ec400>,\n",
       " <torch.cuda.device at 0x7fafd42ec520>,\n",
       " <torch.cuda.device at 0x7fafd42ec640>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08052d9-593b-4b2a-badd-e5ad9678eea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch, os, re, pandas as pd, json\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoConfig\n",
    "\n",
    "import pandas as pd\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "from torch import optim\n",
    "import gc\n",
    "from torch.nn import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8344b304-4ce7-4f78-a014-9568478977b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda:0,1,2,3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ab763e-7474-4e70-bc74-02ecd18f2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87421647-abe6-486a-8d6e-0558a2f72918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n",
    "head_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d92019ff-6f5a-4f82-a763-b2c05142924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_path = \"/home/ubuntu/IMDB_train/pos/\"\n",
    "neg_path = \"/home/ubuntu/IMDB_train/neg/\"\n",
    "\n",
    "\n",
    "pos_token_list = []\n",
    "pos_text_list = []\n",
    "neg_token_list = []\n",
    "neg_text_list = []\n",
    "\n",
    "os.chdir(pos_path)\n",
    "  \n",
    "\n",
    "for file in os.listdir():\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "    with open(pos_path+file, 'r') as f:\n",
    "        pos_text = f.read()\n",
    "        # put into text list\n",
    "        pos_text_list.append(pos_text)\n",
    "        #tokenize and put into token list\n",
    "        #pos_token_list.append(base_tokenizer.encode(pos_text, return_tensors = 'pt'))\n",
    "\n",
    "os.chdir(neg_path)\n",
    "\n",
    "for file in os.listdir():\n",
    "    if not file.endswith('.txt'):\n",
    "        continue\n",
    "    with open(neg_path+file, 'r') as f:\n",
    "        neg_text = f.read()\n",
    "        # put into text list\n",
    "        neg_text_list.append(neg_text)\n",
    "        #tokenize and put into token list\n",
    "        #neg_token_list.append(base_tokenizer.encode(neg_text_list, return_tensors = 'pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7bd9f6c-9674-42e2-9fff-3f8c355b3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "# ref https://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)\n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "# cleaned = test_text_pos.replace('<br /><br />', ' ')\n",
    "# print(cleaned)\n",
    "\n",
    "def clean_imdb(review_list):\n",
    "    for review in range(len(review_list)):\n",
    "        cleaned_review = strip_tags(review_list[review])\n",
    "        review_list[review] = cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6f4bf3-8a03-4ac1-b9a2-88f27f4937d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imdb(pos_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22aef918-a39a-47b1-8427-89376cd4ecf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text\n",
      "0      Alright this was quite a sensitive little numb...\n",
      "1      I'm not sure under what circumstances director...\n",
      "2      Didn't Mystic Pizza win the Oscar for that yea...\n",
      "3      I really enjoyed this film because I have a tr...\n",
      "4      Director Douglas Sirk scores again with this, ...\n",
      "...                                                  ...\n",
      "12495  When this first came out, my dad brought it ho...\n",
      "12496  I'm watching the series again now that it's ou...\n",
      "12497  Note that I did not say that it is better...ju...\n",
      "12498  I was 10 years old when this show was on TV. B...\n",
      "12499  I admit I've only seen about three of Shakespe...\n",
      "\n",
      "[12500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_pos_tokens = pd.DataFrame(pos_token_list, columns = ['tokens'])\n",
    "# print(df_pos_tokens)\n",
    "df_pos_text = pd.DataFrame(pos_text_list, columns = ['text'])\n",
    "print(df_pos_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00e1d3d-cffe-4669-a617-1c1b91acd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"I work as a data scientist\"\n",
    "# text_ids = base_tokenizer.encode(text, return_tensors = 'pt')\n",
    "# print(text_ids.shape)\n",
    "# print(head_model(text_ids.to(device)).logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6e0d99-dae4-4878-ab93-d87d79d746d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbPos(Dataset):  \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = base_tokenizer # can change\n",
    "        self.text = []\n",
    "\n",
    "        for row in df_pos_text['text']:\n",
    "            self.text.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))               \n",
    "        if truncate:\n",
    "            self.text = self.text[:20000]\n",
    "        self.text_count = len(self.text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.text_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.text[item]\n",
    "    \n",
    "dataset = ImdbPos(df_pos_text['text'], truncate=True, gpt2_type=\"gpt2\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226ab723-d55e-400b-b322-b5aeef49abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    # print(packed_tensor)\n",
    "    # print(new_tensor)\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b36d924-11e4-42af-bcb6-d21a7a7938df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.zeros(5)\n",
    "# t2 = torch.zeros(5)\n",
    "# t3 = torch.zeros(5)\n",
    "\n",
    "# first_stacked = torch.stack((t1, t2))\n",
    "# print(first_stacked.shape)\n",
    "# second_stacked = torch.cat((first_stacked, t3.unsqueeze(0)))\n",
    "# print(second_stacked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182b97c5-19a3-4997-a022-e73f0f76413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    max_len = float(\"-inf\")\n",
    "    stacked_batch = None\n",
    "    for ten in batch:\n",
    "        if ten.size()[0] > max_len:\n",
    "            max_len = ten.size()[0]\n",
    "    for ten in batch:\n",
    "        cur_ten_len = ten.size()[0]\n",
    "        # running_ten = ten\n",
    "        for cur_ten_idx in range(cur_ten_len, max_len):\n",
    "            # print(type(ten))\n",
    "            ten = torch.cat((ten ,torch.tensor(base_tokenizer.encode(base_tokenizer.eos_token))))\n",
    "        if stacked_batch is None:\n",
    "            stacked_batch = ten.unsqueeze(0)\n",
    "        else:\n",
    "            stacked_batch = torch.cat((stacked_batch, ten.unsqueeze(0)))\n",
    "            \n",
    "    return stacked_batch\n",
    "    # base_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b11fc4-7960-4263-ad7c-8ce89486bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(base_tokenizer.encode(base_tokenizer.eos_token)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c58ce37-4cfc-42b3-8628-247166c7782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=10, epochs=20, lr=2e-6,\n",
    "    max_seq_len=1000, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "    acc_steps = 100\n",
    "    # device=torch.device(\"cuda\")\n",
    "    model = DataParallel(model, device_ids = [3])\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=lr)\n",
    "    # scheduler = get_linear_schedule_with_warmup(\n",
    "    #     optimizer, num_warmup_steps=warmup_steps, num_training_steps=1250*20\n",
    "    # )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=pad_collate, shuffle=True)\n",
    "\n",
    "    # train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        # print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            \n",
    "            #print(len(train_dataloader))\n",
    "            # print(f\"0 {entry}\")\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 1024)\n",
    "            # print(f\"1 {input_tensor}\")\n",
    "            del entry\n",
    "            # print(f\"2 {input_tensor}\")\n",
    "            del remainder\n",
    "            # print(f\"3 {input_tensor}\")\n",
    "            gc.collect()\n",
    "            # print(\"before\")\n",
    "            # print(carry_on)\n",
    "            # print(idx)\n",
    "            # print(len(train_dataloader) - 1)\n",
    "            # print(f\"4 {input_tensor}\")\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "            # print(\"after\")\n",
    "            # print(f\"5 {input_tensor}\")\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            if idx%100 == 0:\n",
    "                print(f\"LOSS: {loss}\")\n",
    "            loss.mean().backward()\n",
    "            del input_tensor\n",
    "            del outputs\n",
    "            del loss #?\n",
    "            gc.collect()\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                model.zero_grad(set_to_none=True)\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            # del input_tensor\n",
    "            # del outputs\n",
    "            # del loss #?\n",
    "            gc.collect()\n",
    "            input_tensor = None\n",
    "        loss = 0\n",
    "        input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83f8e1b-23a0-4a2c-bd3f-dec5498f3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # pos_imdb_model = head_model\n",
    "# torch.cuda.empty_cache()\n",
    "# t = torch.cuda.get_device_properties(1).total_memory\n",
    "# r = torch.cuda.memory_reserved(1)\n",
    "# a = torch.cuda.memory_allocated(1)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1000000000)\n",
    "# print(r/1000000000)\n",
    "# print(a/1000000000)\n",
    "# print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65189357-d72a-446c-ae3a-5bc3d75265f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [49:29,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [49:47,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [49:41,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [49:44,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12500it [49:33,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# pos_imdb_model = train(dataset, head_model, base_tokenizer, epochs = 20,  max_seq_len=1000, batch_size=16)\n",
    "\n",
    "pos_imdb_model = train(dataset, head_model, base_tokenizer, epochs = 5, max_seq_len=1000, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7009f1cd-fddd-4c61-9dfc-eec409b7f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(head_model.state_dict(), \"/home/ubuntu/small_pos_imdb_model_5_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7a0aa-d830-43e3-873e-13110465c879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c6406-fe89-4242-98d0-40f4acae12bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
