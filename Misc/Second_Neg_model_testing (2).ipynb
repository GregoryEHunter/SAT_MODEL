{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddef53-ede0-4ffc-9f40-6ad93418c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd405ed2-293c-4d3c-bb87-1e1150aa2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafc495-a842-47cf-96f7-155f69521510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel\n",
    "import math\n",
    "import Model_Import_6\n",
    "from torch import optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "# from transformers import WarmupLinearSchedule\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf1815-6397-49ac-bcd3-074b1ef61d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8bcf7-6566-41ac-b285-d9bfcb2a0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f930d95-76c1-493f-b794-5e351470a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa1d718-5043-4be0-8e40-b34d729a79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stuff(context_tensors, generator_logits, text_ids_loc):\n",
    "    context = torch.load(context_tensors, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    logits = torch.load(generator_logits, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    text_ids = torch.load(text_ids_loc, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    return context, logits, text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa216cd-b0b8-4065-865a-935fb06d6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_stuff(context_tensors, generator_logits, text_ids_loc):\n",
    "#     context = torch.load(context_tensors)\n",
    "#     logits = torch.load(generator_logits)\n",
    "#     text_ids = torch.load(text_ids_loc)\n",
    "#     return context, logits, text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d2496-5881-4887-8840-9fce52baca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "29.69567232\n",
      "29.573845504\n",
      "0.121826816\n"
     ]
    }
   ],
   "source": [
    "R_neg_embeds, neg_logits, neg_token_ids = load_stuff('R_neg_embeds.pt', 'neg_logits.pt', 'neg_token_ids.pt')\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019f7c6-1b9e-4ee2-8959-28d7af7e98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_pos_embeds, pos_logits, pos_token_ids = load_stuff('R_pos_embeds.pt', 'pos_logits.pt', 'pos_token_ids.pt')\n",
    "# t = torch.cuda.get_device_properties(0).total_memory\n",
    "# r = torch.cuda.memory_reserved(0)\n",
    "# a = torch.cuda.memory_allocated(0)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1000000000)\n",
    "# print(r/1000000000)\n",
    "# print(a/1000000000)\n",
    "# print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dc6f5-d95c-4809-9c06-dbcce9691b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_embeds_train, R_embeds_test, logits_train, logits_test, token_ids_train, token_ids_test = train_test_split(R_neg_embeds+R_pos_embeds, neg_logits+pos_logits, neg_token_ids+pos_token_ids,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81545d7a-d007-4ac2-b18c-991b73500773",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_neg_embeds_train, R_neg_embeds_test, neg_logits_train, neg_logits_test, neg_token_ids_train, neg_token_ids_test = train_test_split(R_neg_embeds, neg_logits, neg_token_ids,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc9a5b-12f3-4ecd-9013-b2c5a81075a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(R_neg_embeds_train))\n",
    "print(len(neg_logits_train))\n",
    "print(len(neg_token_ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ea769-a7ae-4fe3-a8ed-763df012e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(R_embeds_train))\n",
    "# print(len(logits_train))\n",
    "# print(len(token_ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4712a-bdcd-4103-b169-b87cba7db517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_style(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        # print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caa994-f113-420e-a14d-1d0f920c2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_style_w_dev(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        model.eval()\n",
    "        CELoss_dev = nn.CrossEntropyLoss()\n",
    "        dev_loss_acum = 0\n",
    "        for dev_example in range(len(dev_logits)):\n",
    "            random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "            stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "            dev_network_output = model(stacked_context_sample_dev, dev_logits[dev_example])\n",
    "            shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous()\n",
    "            dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "            dev_loss_acum += dev_loss.item()\n",
    "        full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61317d09-38bd-4f21-a71e-7bf916ae62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_style_generate(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples = 100, num_tokens_to_generate = 1, sen_to_generate = 10):\n",
    "  #Put models in eval mode\n",
    "    SAT_model.eval()\n",
    "    GPT_transformer.eval()\n",
    "    \n",
    "    for sent in range(sen_to_generate):\n",
    "        random_context_samples = random.sample(context_to_sample, num_samples)\n",
    "        model_context_input =  torch.stack(random_context_samples, dim = 0)\n",
    "        expierment_list = []\n",
    "        # tokenize prompt\n",
    "        current_tokenization = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "        for generation in range(num_tokens_to_generate):\n",
    "            # put tokenized prompt through GPT_transformer to get GPT Logits\n",
    "            GPT_logits = GPT_transformer(current_tokenization).last_hidden_state.squeeze()\n",
    "\n",
    "            # put model_context_input and the GPT Logits into SAT model\n",
    "            adjusted_output = SAT_model(model_context_input, GPT_logits)\n",
    "\n",
    "            # Funtional softmax the SAT model output\n",
    "            SM_adjusted_output = torch.nn.functional.softmax(adjusted_output, dim = 1)\n",
    "\n",
    "            # argmax to get predicted token\n",
    "            # predicted_tokens = torch.argmax(SM_adjusted_output, dim =1)\n",
    "\n",
    "            # get topk tokens\n",
    "            top_predicted_tokens = torch.topk(SM_adjusted_output[-1], 5, dim =0).indices\n",
    "            top_predicted_tokens_prob = torch.topk(SM_adjusted_output[-1], 5, dim =0).values\n",
    "            top_predicted_tokens_numpy = top_predicted_tokens.cpu().detach().numpy()\n",
    "            top_predicted_tokens_prob_numpy = top_predicted_tokens_prob.cpu().detach().numpy()\n",
    "            token_prob_sum = np.sum(top_predicted_tokens_prob_numpy)\n",
    "            token_distribution = top_predicted_tokens_prob_numpy/token_prob_sum\n",
    "            \n",
    "            predicted_token =  torch.from_numpy(np.array(np.random.choice(top_predicted_tokens_numpy, p = token_distribution))).to(device)\n",
    "            \n",
    "            # print(top_predicted_tokens)\n",
    "             # tok_k_predicted_words = tokenizer.decode(top_predicted_tokens, skip_special_tokens=True)\n",
    "            decoded_tokens = []\n",
    "            for token_k in top_predicted_tokens:\n",
    "                decoded_tokens.append(tokenizer.decode(token_k, skip_special_tokens=True))\n",
    "                \n",
    "            top_predicted_tokens_prob_cpu = top_predicted_tokens_prob.cpu()\n",
    "            del top_predicted_tokens_prob\n",
    "            expierment_list.append((top_predicted_tokens_prob_cpu, decoded_tokens))\n",
    "\n",
    "\n",
    "                \n",
    "            #dif way of getting top predicted\n",
    "            # predicted_token = top_predicted_tokens[0]\n",
    "            # print(current_tokenization[-1][0])\n",
    "            # print(predicted_token)\n",
    "            # if predicted_token == 247 or predicted_token == current_tokenization[-1][0]:\n",
    "            if predicted_token == 247:\n",
    "                print(\"WEIRD TOKEN PREDICTED\")\n",
    "                predicted_token = top_predicted_tokens[1]\n",
    "\n",
    "            # print(torch.amax(SM_adjusted_output[-1]))\n",
    "\n",
    "            # print(predicted_tokens[-1].unsqueeze(0).unsqueeze(0))\n",
    "            # print(current_tokenization)\n",
    "\n",
    "            current_tokenization = torch.cat((current_tokenization, predicted_token.unsqueeze(0).unsqueeze(0)), 1)\n",
    "            del predicted_token\n",
    "            gc.collect()\n",
    "            # print(current_tokenization)\n",
    "\n",
    "        # decode\n",
    "        for i, beam in enumerate(current_tokenization):\n",
    "            # print(f\"{i}: {tokenizer.decode(beam)}\")\n",
    "            # print(f\"{i}: {current_tokenization}\")\n",
    "            print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "        del current_tokenization\n",
    "        del random_context_samples\n",
    "        del model_context_input\n",
    "        gc.collect()\n",
    "        for prediction in expierment_list:\n",
    "            print(prediction)\n",
    "        print(\"\\n\\n\")\n",
    "    # for i, beam in enumerate(predicted_tokens):\n",
    "    #     # if i == 0:\n",
    "    #     #   continue\n",
    "    #     print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)} token_id: {beam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4683f-85b0-4486-b2c4-9b84c004c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_two_style_w_dev(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100):\n",
    "#     CELoss = nn.CrossEntropyLoss()\n",
    "#     total_count = 0\n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "#         random.shuffle(to_shuffle)\n",
    "\n",
    "#         logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "#         model.train()\n",
    "#         ag_loss_epoch = 0\n",
    "#         epoch_count = 0\n",
    "#         for example in range(len(logits_list)):\n",
    "#             random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "#             stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "#             # print(stacked_context_sample.shape)\n",
    "#             optimizer.zero_grad()\n",
    "#             network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "#             if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "#                 print(\"ONE text id\")\n",
    "#                 continue\n",
    "#             shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "#             shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "#             loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "#             ag_loss_epoch += loss\n",
    "#             epoch_count += 1\n",
    "#             total_count += 1\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if example%1000 == 0:\n",
    "#                 print(\".\", end = \"\")\n",
    "#         model.eval()\n",
    "#         CELoss_dev = nn.CrossEntropyLoss()\n",
    "#         dev_loss_acum = 0\n",
    "#         for dev_example in range(len(dev_logits)):\n",
    "#             random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "#             stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "#             dev_network_output = model(stacked_context_sample_dev, dev_logits[dev_example])\n",
    "#             shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "#             shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous()\n",
    "#             dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "#             dev_loss_acum += dev_loss.item()\n",
    "#         full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "#         print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "#         print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "#         print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "#         print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c7e5f-2860-452b-b022-c594156ecd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "# neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018d55c-a003-4842-b382-dbe4ff5e6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_one_style(neg_alone_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1b2b0-a94e-40b7-8ad7-4e27aab2a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "# neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beba6c-7638-458e-93e2-1d5883647729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_one_style_w_dev(neg_alone_model, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 20, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8cf27-0c8e-40ad-ae02-4c92ca7b245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "29.69567232\n",
      "29.573845504\n",
      "0.121826816\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d83a3-062c-4d48-b9a5-e8559520d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b92b5d-a83c-43fe-af1e-501a6f944e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb2915-9853-453a-b8ca-15b0afd33eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_two_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_two_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375fac0-d115-4562-bf0c-45ae5d2c3b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2931504249572754\n",
      "DEV LOSS: 3.246247928380966\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2244300842285156\n",
      "DEV LOSS: 3.234160268497467\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.175823211669922\n",
      "DEV LOSS: 3.237887247514725\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.122915267944336\n",
      "DEV LOSS: 3.240265000295639\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.06491756439209\n",
      "DEV LOSS: 3.254282879114151\n",
      "----------------------------------------\n",
      "..........Epoch: 5, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.003410577774048\n",
      "DEV LOSS: 3.2699905432224274\n",
      "----------------------------------------\n",
      "..........Epoch: 6, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.937988519668579\n",
      "DEV LOSS: 3.285699222135544\n",
      "----------------------------------------\n",
      "..........Epoch: 7, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.872974395751953\n",
      "DEV LOSS: 3.3122050407409667\n",
      "----------------------------------------\n",
      "..........Epoch: 8, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.8064165115356445\n",
      "DEV LOSS: 3.353215451431274\n",
      "----------------------------------------\n",
      "..........Epoch: 9, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.741619348526001\n",
      "DEV LOSS: 3.378151350927353\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_two_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 10, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c5c68-e187-4b20-bbf0-7cfc7825be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought that this film was pretty boring. The only reason I watched it was because I was curious about it. But the acting wasn't too bad. The story is not that great either. The acting was good enough for a B-Movie movie. I\n",
      "(tensor([0.3836, 0.1720, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3320, 0.2363, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4145, 0.1970, 0.1690, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6391, 0.0585, 0.0580, 0.0559, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1007, 0.0720, 0.0701, 0.0441, 0.0339], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1270, 0.0969, 0.0869, 0.0650, 0.0502], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' atro'])\n",
      "(tensor([0.3230, 0.2953, 0.1496, 0.0408, 0.0155], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '...'])\n",
      "(tensor([0.1770, 0.1761, 0.1543, 0.0364, 0.0196], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.1856, 0.0623, 0.0580, 0.0477, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' only', ' movie', ' plot'])\n",
      "(tensor([0.2880, 0.1840, 0.0666, 0.0513, 0.0475], grad_fn=<ToCopyBackward0>), [' thing', ' interesting', ' good', ' funny', ' reason'])\n",
      "(tensor([0.6726, 0.0862, 0.0753, 0.0669, 0.0446], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' to', ' i'])\n",
      "(tensor([0.4499, 0.0745, 0.0568, 0.0335, 0.0267], grad_fn=<ToCopyBackward0>), [' watched', ' even', ' gave', ' didn', \"'m\"])\n",
      "(tensor([0.8710, 0.0555, 0.0535, 0.0103, 0.0011], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' all'])\n",
      "(tensor([0.7153, 0.1323, 0.0339, 0.0101, 0.0082], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' from'])\n",
      "(tensor([0.5384, 0.1810, 0.0669, 0.0446, 0.0343], grad_fn=<ToCopyBackward0>), [' because', ' to', ' for', ' the', ' that'])\n",
      "(tensor([0.4096, 0.1437, 0.1180, 0.0730, 0.0315], grad_fn=<ToCopyBackward0>), [' I', ' of', ' the', ' it', ' my'])\n",
      "(tensor([0.1285, 0.1196, 0.0961, 0.0674, 0.0496], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' like', ' really'])\n",
      "(tensor([0.1398, 0.0736, 0.0583, 0.0566, 0.0402], grad_fn=<ToCopyBackward0>), [' a', ' in', ' curious', ' looking', ' so'])\n",
      "(tensor([0.5164, 0.1355, 0.0829, 0.0682, 0.0326], grad_fn=<ToCopyBackward0>), [' to', ' about', ' as', '.', ' what'])\n",
      "(tensor([0.3831, 0.2249, 0.0390, 0.0317, 0.0238], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' what'])\n",
      "(tensor([0.5872, 0.2077, 0.0441, 0.0198, 0.0124], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' being'])\n",
      "(tensor([0.3423, 0.1318, 0.1099, 0.0364, 0.0166], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' If'])\n",
      "(tensor([0.1474, 0.1197, 0.1094, 0.1021, 0.0380], grad_fn=<ToCopyBackward0>), [' I', ' after', ' it', ' the', ','])\n",
      "(tensor([0.1405, 0.1275, 0.0812, 0.0484, 0.0365], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' ending'])\n",
      "(tensor([0.6483, 0.0614, 0.0555, 0.0510, 0.0330], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' wasn', ' in'])\n",
      "(tensor([9.9618e-01, 1.1630e-03, 3.8003e-04, 2.5945e-04, 2.0240e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.1394, 0.1285, 0.1106, 0.0906, 0.0717], grad_fn=<ToCopyBackward0>), [' good', ' very', ' that', ' even', ' too'])\n",
      "(tensor([0.6197, 0.1500, 0.0337, 0.0264, 0.0207], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' great', ' convincing', ' special'])\n",
      "(tensor([0.3879, 0.2684, 0.1192, 0.0318, 0.0267], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' either', ' for'])\n",
      "(tensor([0.1686, 0.1530, 0.1528, 0.0614, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.1792, 0.1721, 0.0983, 0.0564, 0.0360], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' only', ' movie', ' acting'])\n",
      "(tensor([0.5613, 0.1300, 0.0364, 0.0245, 0.0225], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' line', ' seemed'])\n",
      "(tensor([0.2382, 0.0634, 0.0554, 0.0412, 0.0388], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' interesting', ' very', ' OK'])\n",
      "(tensor([0.3121, 0.2087, 0.0821, 0.0710, 0.0535], grad_fn=<ToCopyBackward0>), [' that', ' too', ' very', ' interesting', ' really'])\n",
      "(tensor([0.3979, 0.1820, 0.0986, 0.0452, 0.0252], grad_fn=<ToCopyBackward0>), [' great', ' interesting', ' good', ' bad', ' exciting'])\n",
      "(tensor([0.3459, 0.2182, 0.1813, 0.0828, 0.0380], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' either', ' but'])\n",
      "(tensor([0.7822, 0.1178, 0.0191, 0.0120, 0.0062], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' ('])\n",
      "(tensor([0.1892, 0.1678, 0.1251, 0.0429, 0.0315], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' There', ' But'])\n",
      "(tensor([0.2036, 0.0904, 0.0624, 0.0476, 0.0404], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' main', ' story'])\n",
      "(tensor([0.3962, 0.2501, 0.0573, 0.0514, 0.0229], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' wasn', ' in'])\n",
      "(tensor([0.1504, 0.1471, 0.1429, 0.1149, 0.0400], grad_fn=<ToCopyBackward0>), [' not', ' okay', ' pretty', ' OK', ' good'])\n",
      "(tensor([0.1968, 0.1213, 0.1095, 0.1031, 0.1028], grad_fn=<ToCopyBackward0>), [' though', ',', ' but', ' enough', '.'])\n",
      "(tensor([0.4185, 0.2487, 0.1017, 0.0664, 0.0373], grad_fn=<ToCopyBackward0>), [' to', ' for', ' though', ' that', ','])\n",
      "(tensor([0.5449, 0.1590, 0.0789, 0.0773, 0.0250], grad_fn=<ToCopyBackward0>), [' me', ' it', ' the', ' a', ' what'])\n",
      "(tensor([0.2976, 0.1705, 0.0495, 0.0454, 0.0225], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' low', ' B', ' soap'])\n",
      "(tensor([0.6455, 0.0767, 0.0721, 0.0480, 0.0228], grad_fn=<ToCopyBackward0>), ['-', ' movie', ' film', ' grade', 'ollywood'])\n",
      "(tensor([0.4029, 0.3466, 0.0470, 0.0436, 0.0271], grad_fn=<ToCopyBackward0>), ['Movie', 'movie', 'level', 'film', 'grade'])\n",
      "(tensor([0.3139, 0.0895, 0.0857, 0.0427, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' though', ' movie'])\n",
      "(tensor([0.6159, 0.0994, 0.0572, 0.0547, 0.0248], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' like', ' though'])\n",
      "(tensor([0.2069, 0.1610, 0.0959, 0.0896, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' The', ' But', ' It', ' There'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a really bad movie. And I really don't know if it was the direction the director was taking the movie, or the actors or whatever, but it just was bad. It had no point, it was just bad. It\n",
      "(tensor([0.3832, 0.1726, 0.0905, 0.0771, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3333, 0.2353, 0.0581, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4141, 0.1975, 0.1692, 0.0494, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6398, 0.0582, 0.0582, 0.0558, 0.0175], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1008, 0.0722, 0.0701, 0.0442, 0.0339], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1290, 0.0766, 0.0559, 0.0513, 0.0481], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.4327, 0.0503, 0.0471, 0.0271, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' boring', ' cheesy', ' funny'])\n",
      "(tensor([0.1860, 0.1192, 0.0965, 0.0595, 0.0593], grad_fn=<ToCopyBackward0>), [' idea', ' movie', ' film', ' copy', ' joke'])\n",
      "(tensor([0.5647, 0.0787, 0.0468, 0.0390, 0.0266], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', '....'])\n",
      "(tensor([0.2729, 0.1420, 0.0821, 0.0294, 0.0268], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Not'])\n",
      "(tensor([0.4861, 0.0630, 0.0619, 0.0543, 0.0380], grad_fn=<ToCopyBackward0>), [' I', ' then', ' it', ' that', ' the'])\n",
      "(tensor([0.1439, 0.0961, 0.0810, 0.0466, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' thought', ' have'])\n",
      "(tensor([0.1033, 0.0787, 0.0538, 0.0529, 0.0442], grad_fn=<ToCopyBackward0>), [' wanted', ' thought', ' don', ' didn', ' think'])\n",
      "(tensor([9.9590e-01, 1.4330e-03, 5.8768e-04, 2.5706e-04, 2.0183e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3012, 0.1689, 0.1529, 0.1399, 0.0341], grad_fn=<ToCopyBackward0>), [' like', ' think', ' know', ' understand', ' want'])\n",
      "(tensor([0.5143, 0.1190, 0.0935, 0.0900, 0.0814], grad_fn=<ToCopyBackward0>), [' why', ' how', ' where', ' what', ' if'])\n",
      "(tensor([0.3904, 0.1705, 0.1653, 0.0571, 0.0418], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' this', ' the'])\n",
      "(tensor([0.4792, 0.1526, 0.0666, 0.0514, 0.0317], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' could', ' would'])\n",
      "(tensor([0.3581, 0.1556, 0.0360, 0.0299, 0.0271], grad_fn=<ToCopyBackward0>), [' a', ' the', ' as', ' because', ' an'])\n",
      "(tensor([0.1071, 0.0895, 0.0673, 0.0540, 0.0461], grad_fn=<ToCopyBackward0>), [' story', ' script', ' right', ' acting', ' direction'])\n",
      "(tensor([0.1616, 0.1088, 0.1054, 0.0911, 0.0597], grad_fn=<ToCopyBackward0>), [' the', ' or', ',', ' that', '.'])\n",
      "(tensor([0.2572, 0.1585, 0.1129, 0.0829, 0.0421], grad_fn=<ToCopyBackward0>), [' director', ' movie', ' story', ' film', ' studio'])\n",
      "(tensor([0.6092, 0.1897, 0.0599, 0.0270, 0.0147], grad_fn=<ToCopyBackward0>), [' was', ' wanted', ' took', ' had', ' gave'])\n",
      "(tensor([0.4164, 0.1832, 0.0887, 0.0403, 0.0216], grad_fn=<ToCopyBackward0>), [' going', ' taking', ' trying', ' in', ' shooting'])\n",
      "(tensor([0.5686, 0.2083, 0.0241, 0.0239, 0.0223], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' or', ' me'])\n",
      "(tensor([0.4335, 0.3703, 0.1135, 0.0131, 0.0053], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' film', ' picture', ' audience'])\n",
      "(tensor([0.4558, 0.2453, 0.1025, 0.0573, 0.0200], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' or', ' and'])\n",
      "(tensor([0.4661, 0.2454, 0.1055, 0.0243, 0.0208], grad_fn=<ToCopyBackward0>), [' but', ' or', ' the', ' because', ' I'])\n",
      "(tensor([0.5430, 0.1877, 0.0420, 0.0247, 0.0242], grad_fn=<ToCopyBackward0>), [' the', ' if', ' what', ' it', ' because'])\n",
      "(tensor([0.1066, 0.0634, 0.0594, 0.0586, 0.0575], grad_fn=<ToCopyBackward0>), [' script', ' actors', ' story', ' writing', ' acting'])\n",
      "(tensor([0.3160, 0.1326, 0.0965, 0.0785, 0.0627], grad_fn=<ToCopyBackward0>), [',', ' or', '.', ' that', \"'\"])\n",
      "(tensor([0.8363, 0.0261, 0.0234, 0.0079, 0.0061], grad_fn=<ToCopyBackward0>), [' the', ' what', ' whatever', ' whoever', ' just'])\n",
      "(tensor([0.4610, 0.1739, 0.1389, 0.0806, 0.0380], grad_fn=<ToCopyBackward0>), [',', ' the', '.', ' it', ' was'])\n",
      "(tensor([0.9361, 0.0118, 0.0095, 0.0081, 0.0034], grad_fn=<ToCopyBackward0>), [' but', ' or', ' because', ' I', ' the'])\n",
      "(tensor([0.3058, 0.1765, 0.1157, 0.1054, 0.0293], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' that'])\n",
      "(tensor([0.3855, 0.3011, 0.0679, 0.0473, 0.0444], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' seemed', ' really'])\n",
      "(tensor([0.4018, 0.1491, 0.0750, 0.0696, 0.0260], grad_fn=<ToCopyBackward0>), [' seemed', ' didn', ' wasn', ' was', ' just'])\n",
      "(tensor([0.2136, 0.1854, 0.0621, 0.0545, 0.0540], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' bad', ' just'])\n",
      "(tensor([0.7132, 0.0522, 0.0349, 0.0334, 0.0278], grad_fn=<ToCopyBackward0>), ['.', ' and', ' movie', ',', ' in'])\n",
      "(tensor([0.1754, 0.1541, 0.1339, 0.0531, 0.0324], grad_fn=<ToCopyBackward0>), [' It', ' And', ' I', ' The', 'I'])\n",
      "(tensor([0.5975, 0.0996, 0.0689, 0.0445, 0.0308], grad_fn=<ToCopyBackward0>), [' was', ' just', \"'s\", ' wasn', ' had'])\n",
      "(tensor([0.1994, 0.1938, 0.0836, 0.0769, 0.0653], grad_fn=<ToCopyBackward0>), [' a', ' no', ' the', ' so', ' all'])\n",
      "(tensor([0.2978, 0.1088, 0.0545, 0.0454, 0.0365], grad_fn=<ToCopyBackward0>), [' point', ' story', ' real', ' heart', ' plot'])\n",
      "(tensor([0.7009, 0.1566, 0.0349, 0.0218, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' of', ' and'])\n",
      "(tensor([0.2767, 0.1473, 0.0941, 0.0871, 0.0458], grad_fn=<ToCopyBackward0>), [' it', ' and', ' the', ' no', ' there'])\n",
      "(tensor([0.4814, 0.2240, 0.0999, 0.0723, 0.0276], grad_fn=<ToCopyBackward0>), [' had', ' was', ' didn', ' just', ' really'])\n",
      "(tensor([0.4711, 0.0860, 0.0769, 0.0595, 0.0266], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' stupid', ' pointless', ' not'])\n",
      "(tensor([0.1892, 0.1028, 0.0912, 0.0794, 0.0694], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' pointless', ' stupid', ' boring'])\n",
      "(tensor([0.7364, 0.0592, 0.0289, 0.0270, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ',', ' movie', ' and', ' acting'])\n",
      "(tensor([0.1944, 0.1303, 0.1214, 0.0408, 0.0361], grad_fn=<ToCopyBackward0>), [' And', ' It', ' I', ' The', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the worst movie of all time. I thought it was a really bad movie. I was really disappointed in this movie. It's a really bad movie. The movie is so stupid and predictable. I'm not even going to comment much on\n",
      "(tensor([0.3847, 0.1711, 0.0894, 0.0770, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4384, 0.2424, 0.1962, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4803, 0.1359, 0.1035, 0.0558, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7386, 0.0315, 0.0259, 0.0253, 0.0179], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.6893, 0.0858, 0.0163, 0.0159, 0.0133], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' Christmas', ' horror'])\n",
      "(tensor([0.6267, 0.2445, 0.0753, 0.0088, 0.0068], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.7268, 0.1097, 0.0295, 0.0167, 0.0083], grad_fn=<ToCopyBackward0>), [' all', ' the', ' 2001', ' my', ' 2009'])\n",
      "(tensor([9.9265e-01, 1.2844e-03, 1.2582e-03, 1.2259e-03, 8.6589e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', ' the', ' movies', '-'])\n",
      "(tensor([0.5316, 0.0937, 0.0797, 0.0645, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ',', ' until', ' when', '!'])\n",
      "(tensor([0.3863, 0.0987, 0.0893, 0.0254, 0.0228], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', ' This'])\n",
      "(tensor([0.0960, 0.0748, 0.0691, 0.0584, 0.0575], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' really', ' can'])\n",
      "(tensor([0.4184, 0.2911, 0.0684, 0.0618, 0.0491], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' I'])\n",
      "(tensor([0.7695, 0.0815, 0.0187, 0.0138, 0.0137], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' should'])\n",
      "(tensor([0.2086, 0.1800, 0.0741, 0.0572, 0.0288], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' just'])\n",
      "(tensor([0.0807, 0.0632, 0.0567, 0.0517, 0.0476], grad_fn=<ToCopyBackward0>), [' really', ' big', ' total', ' piece', ' complete'])\n",
      "(tensor([0.5589, 0.2118, 0.0263, 0.0212, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ',', ' dumb', ' boring', ' stupid'])\n",
      "(tensor([0.6231, 0.0668, 0.0595, 0.0270, 0.0201], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' idea', ' joke'])\n",
      "(tensor([0.7959, 0.0518, 0.0289, 0.0153, 0.0103], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' from'])\n",
      "(tensor([0.3265, 0.1143, 0.0578, 0.0449, 0.0239], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.1329, 0.0965, 0.0883, 0.0529, 0.0500], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.3863, 0.1298, 0.0573, 0.0466, 0.0424], grad_fn=<ToCopyBackward0>), [' really', ' very', ' in', ' so', ' actually'])\n",
      "(tensor([0.2730, 0.1458, 0.0982, 0.0399, 0.0311], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ',', ' looking', ' shocked'])\n",
      "(tensor([0.3269, 0.2271, 0.2010, 0.0777, 0.0594], grad_fn=<ToCopyBackward0>), ['.', ' in', ' with', ' when', ' by'])\n",
      "(tensor([0.5884, 0.1157, 0.1017, 0.0825, 0.0299], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' myself', ' that'])\n",
      "(tensor([0.8601, 0.0611, 0.0223, 0.0129, 0.0032], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ','])\n",
      "(tensor([0.6638, 0.0762, 0.0722, 0.0281, 0.0268], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' when', ' and'])\n",
      "(tensor([0.3862, 0.1554, 0.0416, 0.0396, 0.0311], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.4310, 0.2139, 0.0519, 0.0325, 0.0270], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' really'])\n",
      "(tensor([0.2357, 0.1342, 0.1249, 0.1050, 0.0620], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' one', ' really'])\n",
      "(tensor([0.4292, 0.0689, 0.0597, 0.0535, 0.0389], grad_fn=<ToCopyBackward0>), [' really', ' bad', ' movie', ' big', ' very'])\n",
      "(tensor([0.6698, 0.0933, 0.0489, 0.0214, 0.0158], grad_fn=<ToCopyBackward0>), [' bad', ',', ' boring', ' stupid', ' dumb'])\n",
      "(tensor([0.8579, 0.0273, 0.0150, 0.0105, 0.0046], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ',', ' idea'])\n",
      "(tensor([0.7733, 0.0680, 0.0240, 0.0159, 0.0149], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', ' that'])\n",
      "(tensor([0.2644, 0.2403, 0.0406, 0.0306, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.2004, 0.0927, 0.0879, 0.0578, 0.0329], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' story', ' plot'])\n",
      "(tensor([0.3848, 0.1421, 0.0695, 0.0670, 0.0373], grad_fn=<ToCopyBackward0>), [' is', ' was', ' just', ' starts', ' has'])\n",
      "(tensor([0.1268, 0.1212, 0.0969, 0.0820, 0.0674], grad_fn=<ToCopyBackward0>), [' not', ' so', ' a', ' really', ' just'])\n",
      "(tensor([0.4809, 0.0991, 0.0318, 0.0306, 0.0190], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' boring', ' predictable', ' terrible'])\n",
      "(tensor([0.2285, 0.1913, 0.1711, 0.1575, 0.0858], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' it'])\n",
      "(tensor([0.2126, 0.1109, 0.0786, 0.0541, 0.0479], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' dumb', ' the', ' I'])\n",
      "(tensor([0.3816, 0.2529, 0.1331, 0.1263, 0.0115], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' that', ' in'])\n",
      "(tensor([0.2677, 0.1974, 0.1131, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' You'])\n",
      "(tensor([0.1284, 0.0942, 0.0835, 0.0688, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' don', ' can'])\n",
      "(tensor([0.2595, 0.1377, 0.1096, 0.0605, 0.0480], grad_fn=<ToCopyBackward0>), [' not', ' a', ' really', ' just', ' so'])\n",
      "(tensor([0.4940, 0.1247, 0.0866, 0.0393, 0.0347], grad_fn=<ToCopyBackward0>), [' even', ' a', ' going', ' sure', ' one'])\n",
      "(tensor([0.4254, 0.1918, 0.1015, 0.0515, 0.0428], grad_fn=<ToCopyBackward0>), [' going', ' gonna', ' kidding', ' sure', ' joking'])\n",
      "(tensor([9.8654e-01, 4.5132e-03, 1.3710e-03, 1.2296e-03, 8.4083e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' into', ' in', ' there', ' back'])\n",
      "(tensor([0.1700, 0.1066, 0.0790, 0.0681, 0.0642], grad_fn=<ToCopyBackward0>), [' talk', ' try', ' comment', ' mention', ' go'])\n",
      "(tensor([0.5362, 0.1561, 0.0441, 0.0419, 0.0399], grad_fn=<ToCopyBackward0>), [' on', ' about', ' much', ' because', ' any'])\n",
      "(tensor([0.3282, 0.2631, 0.1498, 0.0863, 0.0403], grad_fn=<ToCopyBackward0>), [' about', ' on', ' more', ' because', '.'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty lame, but I was willing to give it another chance. I was really looking forward to seeing it in the theater because I thought it could be funny, but it was just boring. It had some funny parts, but the movie\n",
      "(tensor([0.3825, 0.1726, 0.0905, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4998, 0.0603, 0.0342, 0.0154, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6240, 0.0399, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2590, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1969, 0.1243, 0.1046, 0.0784, 0.0679], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3554, 0.2185, 0.1639, 0.0251, 0.0218], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' at'])\n",
      "(tensor([0.2588, 0.0767, 0.0481, 0.0310, 0.0296], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' so', ' I'])\n",
      "(tensor([0.1904, 0.1677, 0.1130, 0.0569, 0.0418], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' the', ' at'])\n",
      "(tensor([0.1104, 0.0894, 0.0559, 0.0434, 0.0425], grad_fn=<ToCopyBackward0>), [' was', ' really', ' guess', \"'m\", ' didn'])\n",
      "(tensor([0.1107, 0.0450, 0.0422, 0.0381, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' willing', ' looking', ' a', ' in'])\n",
      "(tensor([9.9811e-01, 4.7864e-04, 2.1901e-04, 1.1638e-04, 8.3762e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' at', ' in', ' and'])\n",
      "(tensor([0.5403, 0.0627, 0.0566, 0.0387, 0.0360], grad_fn=<ToCopyBackward0>), [' give', ' overlook', ' admit', ' take', ' watch'])\n",
      "(tensor([0.9382, 0.0202, 0.0072, 0.0039, 0.0018], grad_fn=<ToCopyBackward0>), [' it', ' the', ' them', ' this', ' some'])\n",
      "(tensor([0.7691, 0.0591, 0.0523, 0.0261, 0.0165], grad_fn=<ToCopyBackward0>), [' a', ' another', ' the', ' that', ' some'])\n",
      "(tensor([0.3844, 0.3025, 0.1600, 0.0527, 0.0268], grad_fn=<ToCopyBackward0>), [' chance', ' try', ' shot', ' go', ' watch'])\n",
      "(tensor([0.4700, 0.1104, 0.0859, 0.0531, 0.0421], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' to', ' after'])\n",
      "(tensor([0.2919, 0.0970, 0.0917, 0.0406, 0.0281], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1206, 0.0749, 0.0451, 0.0409, 0.0409], grad_fn=<ToCopyBackward0>), [' was', ' rented', ' really', ' watched', \"'m\"])\n",
      "(tensor([0.1821, 0.1540, 0.0872, 0.0385, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' actually', ' willing', ' so'])\n",
      "(tensor([0.3142, 0.2674, 0.0950, 0.0628, 0.0200], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' surprised', ' hoping', ' impressed'])\n",
      "(tensor([9.9552e-01, 2.9466e-03, 8.3332e-04, 1.9042e-04, 1.8023e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' up'])\n",
      "(tensor([9.9340e-01, 2.4628e-03, 8.7848e-04, 7.4995e-04, 2.6517e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', ' the', ' on'])\n",
      "(tensor([0.3553, 0.1317, 0.1129, 0.0967, 0.0780], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' this', ' watching', ' it'])\n",
      "(tensor([0.3247, 0.1910, 0.1414, 0.0328, 0.0306], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' how', ' what'])\n",
      "(tensor([0.2961, 0.1169, 0.1155, 0.0803, 0.0585], grad_fn=<ToCopyBackward0>), [' again', ',', ' in', '.', ' because'])\n",
      "(tensor([0.6110, 0.1006, 0.0726, 0.0276, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' its', ' a', ' theaters', ' my'])\n",
      "(tensor([0.6397, 0.1759, 0.0395, 0.0243, 0.0074], grad_fn=<ToCopyBackward0>), [' theater', ' theatre', ' theaters', ' cinema', ' theat'])\n",
      "(tensor([0.2008, 0.1851, 0.0871, 0.0827, 0.0561], grad_fn=<ToCopyBackward0>), [' because', '.', ' when', ',', ' after'])\n",
      "(tensor([0.5873, 0.1558, 0.0468, 0.0309, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' of', ' my'])\n",
      "(tensor([0.1252, 0.1090, 0.0979, 0.0913, 0.0597], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' like', \"'m\", ' was'])\n",
      "(tensor([0.6641, 0.1439, 0.0449, 0.0422, 0.0210], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ' this'])\n",
      "(tensor([0.3689, 0.3532, 0.0800, 0.0405, 0.0341], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' had'])\n",
      "(tensor([0.6207, 0.0946, 0.0698, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' be', ' have', ' possibly', ' get', ' really'])\n",
      "(tensor([0.2490, 0.1943, 0.1462, 0.1028, 0.0333], grad_fn=<ToCopyBackward0>), [' funny', ' pretty', ' a', ' really', ' better'])\n",
      "(tensor([0.3114, 0.2280, 0.1341, 0.0450, 0.0332], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' at'])\n",
      "(tensor([0.4024, 0.0751, 0.0300, 0.0265, 0.0260], grad_fn=<ToCopyBackward0>), [' but', ' and', ' too', ' especially', ' like'])\n",
      "(tensor([0.2363, 0.2153, 0.0962, 0.0588, 0.0338], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' then'])\n",
      "(tensor([0.4031, 0.1598, 0.0769, 0.0645, 0.0442], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' just', ' didn', ' really'])\n",
      "(tensor([0.2181, 0.1491, 0.0912, 0.0476, 0.0457], grad_fn=<ToCopyBackward0>), [' just', ' not', ' really', ' more', ' pretty'])\n",
      "(tensor([0.1456, 0.0721, 0.0702, 0.0646, 0.0563], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' boring', ' too'])\n",
      "(tensor([0.4461, 0.2871, 0.0662, 0.0493, 0.0123], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' to'])\n",
      "(tensor([0.3054, 0.1597, 0.1569, 0.0230, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.3566, 0.1563, 0.1029, 0.0689, 0.0540], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' didn', ' had'])\n",
      "(tensor([0.1280, 0.1057, 0.1014, 0.0832, 0.0621], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' no', ' so'])\n",
      "(tensor([0.1999, 0.1532, 0.0783, 0.0701, 0.0435], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' potential', ' of', ' really'])\n",
      "(tensor([0.2619, 0.2443, 0.1198, 0.0854, 0.0781], grad_fn=<ToCopyBackward0>), [' parts', ' scenes', ' lines', ' moments', ' bits'])\n",
      "(tensor([0.6812, 0.1092, 0.0549, 0.0429, 0.0166], grad_fn=<ToCopyBackward0>), [',', ' and', ' but', ' in', ' that'])\n",
      "(tensor([0.8236, 0.0533, 0.0251, 0.0096, 0.0080], grad_fn=<ToCopyBackward0>), [' but', ' like', ' and', ' some', ' I'])\n",
      "(tensor([0.1682, 0.1522, 0.1011, 0.0730, 0.0562], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' they', ' that'])\n",
      "(tensor([0.2585, 0.2173, 0.0588, 0.0512, 0.0404], grad_fn=<ToCopyBackward0>), [' movie', ' rest', ' whole', ' acting', ' story'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this is an awful, awful movie. It's one of my least favorite movies of all time. I can't believe that the studio would make something so horrible. It's one of those movies where you can't believe that this movie was made\n",
      "(tensor([0.3832, 0.1725, 0.0904, 0.0771, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3331, 0.2357, 0.0581, 0.0561, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4139, 0.1970, 0.1697, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.3745, 0.2196, 0.0972, 0.0621, 0.0313], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' not', ' an'])\n",
      "(tensor([0.1188, 0.0606, 0.0509, 0.0469, 0.0436], grad_fn=<ToCopyBackward0>), [' awful', ' opportunity', ' absolutely', ' insult', ' OK'])\n",
      "(tensor([0.5986, 0.2187, 0.0480, 0.0192, 0.0093], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ',', ' script'])\n",
      "(tensor([0.3943, 0.0789, 0.0333, 0.0239, 0.0182], grad_fn=<ToCopyBackward0>), [' awful', ' terrible', ' boring', ' horrible', ' really'])\n",
      "(tensor([0.5731, 0.2600, 0.0421, 0.0317, 0.0140], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' script', ','])\n",
      "(tensor([0.8009, 0.0428, 0.0369, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', '!!'])\n",
      "(tensor([0.2778, 0.1900, 0.0559, 0.0373, 0.0224], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.4251, 0.1644, 0.0911, 0.0471, 0.0263], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' has', ' looks'])\n",
      "(tensor([0.2081, 0.1271, 0.1222, 0.1036, 0.0379], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' one', ' like'])\n",
      "(tensor([0.8908, 0.0445, 0.0097, 0.0045, 0.0038], grad_fn=<ToCopyBackward0>), [' of', ' that', ' thing', ' where', ' the'])\n",
      "(tensor([0.6728, 0.2819, 0.0332, 0.0037, 0.0008], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' a'])\n",
      "(tensor([0.3904, 0.2403, 0.0724, 0.0667, 0.0417], grad_fn=<ToCopyBackward0>), [' all', ' least', ' worst', ' favorites', ' favorite'])\n",
      "(tensor([0.6889, 0.1447, 0.0793, 0.0098, 0.0071], grad_fn=<ToCopyBackward0>), [' favorite', ' favorites', '-', ' liked', ' favourite'])\n",
      "(tensor([0.6710, 0.0703, 0.0228, 0.0161, 0.0104], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' comed', ' movie', ' horror'])\n",
      "(tensor([0.2305, 0.2193, 0.1364, 0.1014, 0.0843], grad_fn=<ToCopyBackward0>), ['.', ' of', ' ever', ',', ' in'])\n",
      "(tensor([9.7358e-01, 1.9609e-02, 1.4275e-03, 6.3293e-04, 5.3254e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' this', ' ALL'])\n",
      "(tensor([9.9041e-01, 3.7183e-03, 3.7019e-03, 1.1400e-03, 1.5117e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' the', ' movies'])\n",
      "(tensor([0.7341, 0.1608, 0.0180, 0.0110, 0.0078], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' ('])\n",
      "(tensor([0.2346, 0.1719, 0.0666, 0.0501, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.0778, 0.0735, 0.0685, 0.0588, 0.0562], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' was', ' really', ' don'])\n",
      "(tensor([0.6206, 0.0499, 0.0487, 0.0395, 0.0290], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' honestly', ' remember', ' watch'])\n",
      "(tensor([0.3556, 0.2052, 0.0555, 0.0492, 0.0466], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' stand', ' imagine', ' remember'])\n",
      "(tensor([0.5679, 0.0691, 0.0617, 0.0418, 0.0285], grad_fn=<ToCopyBackward0>), [' that', ' I', ' this', ' the', ' it'])\n",
      "(tensor([0.1595, 0.1457, 0.1051, 0.0514, 0.0429], grad_fn=<ToCopyBackward0>), [' I', ' the', ' a', ' someone', ' so'])\n",
      "(tensor([0.1836, 0.1082, 0.0628, 0.0495, 0.0409], grad_fn=<ToCopyBackward0>), [' people', ' studio', ' director', ' M', ' creator'])\n",
      "(tensor([0.1930, 0.1373, 0.0605, 0.0591, 0.0541], grad_fn=<ToCopyBackward0>), [' would', ' was', ' put', ' is', ' thought'])\n",
      "(tensor([0.1650, 0.1441, 0.1438, 0.0491, 0.0476], grad_fn=<ToCopyBackward0>), [' actually', ' put', ' have', ' allow', ' make'])\n",
      "(tensor([0.4353, 0.1441, 0.1409, 0.1233, 0.1009], grad_fn=<ToCopyBackward0>), [' this', ' it', ' such', ' something', ' a'])\n",
      "(tensor([0.4193, 0.3275, 0.1189, 0.0620, 0.0212], grad_fn=<ToCopyBackward0>), [' so', ' like', ' this', ' as', ' that'])\n",
      "(tensor([0.1468, 0.0925, 0.0623, 0.0547, 0.0302], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' horrible', ' awful', ' incredibly'])\n",
      "(tensor([0.5546, 0.0928, 0.0861, 0.0459, 0.0390], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' and', ' to'])\n",
      "(tensor([0.2630, 0.1401, 0.0687, 0.0545, 0.0325], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.5302, 0.0956, 0.0637, 0.0392, 0.0245], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' has'])\n",
      "(tensor([0.2327, 0.1482, 0.0919, 0.0732, 0.0669], grad_fn=<ToCopyBackward0>), [' just', ' not', ' one', ' a', ' like'])\n",
      "(tensor([0.9533, 0.0128, 0.0053, 0.0027, 0.0025], grad_fn=<ToCopyBackward0>), [' of', ' thing', ' big', ' that', ' the'])\n",
      "(tensor([0.5659, 0.3874, 0.0275, 0.0057, 0.0025], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' their'])\n",
      "(tensor([0.9269, 0.0335, 0.0033, 0.0018, 0.0013], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' really', ' rare'])\n",
      "(tensor([0.5631, 0.3058, 0.0320, 0.0194, 0.0105], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', '.'])\n",
      "(tensor([0.2891, 0.2429, 0.1362, 0.0588, 0.0563], grad_fn=<ToCopyBackward0>), [' you', ' I', ' the', ' it', ' if'])\n",
      "(tensor([0.1211, 0.1107, 0.0799, 0.0535, 0.0413], grad_fn=<ToCopyBackward0>), [' can', ' just', \"'re\", ' think', ' feel'])\n",
      "(tensor([0.5386, 0.1357, 0.1100, 0.0236, 0.0184], grad_fn=<ToCopyBackward0>), [\"'t\", ' tell', ' see', ' watch', ' say'])\n",
      "(tensor([0.3372, 0.2560, 0.0524, 0.0417, 0.0258], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' really', ' get', ' watch'])\n",
      "(tensor([0.4963, 0.1543, 0.0642, 0.0388, 0.0369], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' what', ' how'])\n",
      "(tensor([0.2458, 0.1160, 0.0860, 0.0776, 0.0598], grad_fn=<ToCopyBackward0>), [' the', ' this', ' they', ' a', ' you'])\n",
      "(tensor([0.4007, 0.1696, 0.0989, 0.0943, 0.0241], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' could', ' was', ' can'])\n",
      "(tensor([0.3327, 0.2781, 0.0731, 0.0520, 0.0488], grad_fn=<ToCopyBackward0>), [' was', ' is', ' could', ' got', ' even'])\n",
      "(tensor([0.6138, 0.1661, 0.0725, 0.0307, 0.0132], grad_fn=<ToCopyBackward0>), [' made', ' even', ' actually', ' ever', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this was a pretty bad movie... I'm not even going to try to explain it. I just don't get it. It was just awful. I can't explain why I didn't get it. I don't understand why I didn't\n",
      "(tensor([0.3851, 0.1710, 0.0893, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3308, 0.2373, 0.0582, 0.0558, 0.0230], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4152, 0.1955, 0.1691, 0.0498, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.5001, 0.1505, 0.0848, 0.0607, 0.0126], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' probably'])\n",
      "(tensor([0.1237, 0.0891, 0.0825, 0.0765, 0.0751], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' really', ' sequel', ' pretty'])\n",
      "(tensor([0.2616, 0.0547, 0.0344, 0.0302, 0.0267], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' dumb', ' interesting', ' original'])\n",
      "(tensor([0.8498, 0.0808, 0.0145, 0.0041, 0.0037], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' horror', ' script'])\n",
      "(tensor([0.6269, 0.0948, 0.0333, 0.0307, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', '...', '!', '....'])\n",
      "(tensor([0.0677, 0.0344, 0.0328, 0.0312, 0.0300], grad_fn=<ToCopyBackward0>), [' I', 'and', 'the', 'but', 'I'])\n",
      "(tensor([0.1083, 0.0861, 0.0829, 0.0774, 0.0594], grad_fn=<ToCopyBackward0>), [' was', ' mean', ' really', ' thought', \"'m\"])\n",
      "(tensor([0.2103, 0.1957, 0.0644, 0.0566, 0.0544], grad_fn=<ToCopyBackward0>), [' not', ' a', ' sure', ' just', ' really'])\n",
      "(tensor([0.2478, 0.1614, 0.1354, 0.1092, 0.0528], grad_fn=<ToCopyBackward0>), [' a', ' sure', ' even', ' one', ' going'])\n",
      "(tensor([0.2944, 0.2009, 0.1408, 0.0706, 0.0373], grad_fn=<ToCopyBackward0>), [' going', ' sure', ' gonna', ' a', ' kidding'])\n",
      "(tensor([0.9867, 0.0034, 0.0019, 0.0014, 0.0012], grad_fn=<ToCopyBackward0>), [' to', ' into', ' that', ' out', ' there'])\n",
      "(tensor([0.1123, 0.0945, 0.0904, 0.0638, 0.0593], grad_fn=<ToCopyBackward0>), [' call', ' comment', ' try', ' bother', ' mention'])\n",
      "(tensor([0.8452, 0.0997, 0.0105, 0.0032, 0.0028], grad_fn=<ToCopyBackward0>), [' to', ' and', ' it', '.', ','])\n",
      "(tensor([0.1295, 0.0761, 0.0589, 0.0443, 0.0436], grad_fn=<ToCopyBackward0>), [' describe', ' explain', ' give', ' understand', ' be'])\n",
      "(tensor([0.5601, 0.1066, 0.0899, 0.0657, 0.0624], grad_fn=<ToCopyBackward0>), [' it', ' how', ' why', ' the', ' that'])\n",
      "(tensor([0.3208, 0.2735, 0.1320, 0.0642, 0.0198], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' because', '...'])\n",
      "(tensor([0.3311, 0.1519, 0.0747, 0.0257, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.1840, 0.0922, 0.0568, 0.0553, 0.0477], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' was', ' don', ' can'])\n",
      "(tensor([0.1218, 0.0788, 0.0701, 0.0519, 0.0490], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' watched', ' couldn', ' found'])\n",
      "(tensor([9.9709e-01, 6.7522e-04, 3.7884e-04, 1.8243e-04, 1.5723e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3713, 0.2120, 0.1159, 0.1008, 0.0611], grad_fn=<ToCopyBackward0>), [' get', ' understand', ' know', ' think', ' like'])\n",
      "(tensor([0.8900, 0.0392, 0.0225, 0.0221, 0.0085], grad_fn=<ToCopyBackward0>), [' it', ' why', ' how', ' the', ' this'])\n",
      "(tensor([0.7996, 0.0582, 0.0448, 0.0228, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', '...', '!'])\n",
      "(tensor([0.2820, 0.1067, 0.1023, 0.0270, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.5256, 0.1089, 0.0491, 0.0323, 0.0284], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' is', ' seems'])\n",
      "(tensor([0.1566, 0.0915, 0.0866, 0.0817, 0.0301], grad_fn=<ToCopyBackward0>), [' just', ' like', ' so', ' a', ' boring'])\n",
      "(tensor([0.1254, 0.1029, 0.0613, 0.0556, 0.0482], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' terrible', ' so', ' awful'])\n",
      "(tensor([0.6913, 0.0871, 0.0697, 0.0597, 0.0357], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2859, 0.1032, 0.0830, 0.0357, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', 'I'])\n",
      "(tensor([0.0942, 0.0783, 0.0761, 0.0702, 0.0618], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' can', ' was', ' really'])\n",
      "(tensor([0.7282, 0.0671, 0.0349, 0.0241, 0.0150], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' only', ' understand', ' honestly'])\n",
      "(tensor([0.3114, 0.2469, 0.0880, 0.0481, 0.0384], grad_fn=<ToCopyBackward0>), [' even', ' believe', ' understand', ' explain', ' really'])\n",
      "(tensor([0.5936, 0.1782, 0.0996, 0.0257, 0.0184], grad_fn=<ToCopyBackward0>), [' it', ' why', ' how', ' to', ' what'])\n",
      "(tensor([0.2554, 0.2199, 0.1552, 0.0715, 0.0476], grad_fn=<ToCopyBackward0>), [' I', ' it', '.', ',', ' this'])\n",
      "(tensor([0.1962, 0.0618, 0.0589, 0.0420, 0.0410], grad_fn=<ToCopyBackward0>), [' didn', ' don', \"'m\", ' found', ' felt'])\n",
      "(tensor([9.9813e-01, 2.6829e-04, 1.8761e-04, 1.3983e-04, 8.8450e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '\"'])\n",
      "(tensor([0.1988, 0.1927, 0.1326, 0.0544, 0.0364], grad_fn=<ToCopyBackward0>), [' get', ' like', ' enjoy', ' think', ' find'])\n",
      "(tensor([0.7436, 0.0648, 0.0340, 0.0181, 0.0167], grad_fn=<ToCopyBackward0>), [' it', ' into', ' the', ' this', ' that'])\n",
      "(tensor([0.6713, 0.1260, 0.0628, 0.0311, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', '...', ' because'])\n",
      "(tensor([0.4603, 0.1132, 0.0466, 0.0239, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Maybe', 'I'])\n",
      "(tensor([0.1295, 0.0900, 0.0761, 0.0751, 0.0743], grad_fn=<ToCopyBackward0>), [' just', \"'m\", ' can', ' don', ' was'])\n",
      "(tensor([9.9809e-01, 2.4039e-04, 2.0675e-04, 1.4947e-04, 9.2702e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", \"'\", '`', ',', '\"'])\n",
      "(tensor([0.2295, 0.2281, 0.1938, 0.1076, 0.0996], grad_fn=<ToCopyBackward0>), [' know', ' even', ' understand', ' get', ' think'])\n",
      "(tensor([0.5026, 0.2037, 0.0941, 0.0500, 0.0340], grad_fn=<ToCopyBackward0>), [' why', ' it', ' how', ' the', ' what'])\n",
      "(tensor([0.3327, 0.1552, 0.0895, 0.0657, 0.0504], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' they', ' people'])\n",
      "(tensor([0.1754, 0.0603, 0.0517, 0.0489, 0.0475], grad_fn=<ToCopyBackward0>), [' didn', ' watched', ' was', \"'m\", ' thought'])\n",
      "(tensor([9.9866e-01, 1.2947e-04, 1.0333e-04, 9.9971e-05, 7.1934e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '\"'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty lame, but at least it wasn't as bad as \"Plan 9 From Outer Space\". This movie had no plot at all, just a bunch of random clips, some of which are funny... but not in the way you would\n",
      "(tensor([0.3832, 0.1724, 0.0903, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4369, 0.2445, 0.1963, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0594, 0.0361, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1375, 0.0700, 0.0663, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1757, 0.1510, 0.1086, 0.0947, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.3829, 0.1678, 0.1165, 0.0297, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' when'])\n",
      "(tensor([0.2211, 0.0975, 0.0507, 0.0363, 0.0308], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' I', ' it'])\n",
      "(tensor([0.2302, 0.1701, 0.1049, 0.0639, 0.0570], grad_fn=<ToCopyBackward0>), [' it', ' I', ' then', ' at', ' the'])\n",
      "(tensor([0.6795, 0.2807, 0.0056, 0.0055, 0.0044], grad_fn=<ToCopyBackward0>), [' least', ' the', ' a', ' some', ' one'])\n",
      "(tensor([0.3378, 0.1584, 0.1511, 0.0339, 0.0265], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' there', ' i'])\n",
      "(tensor([0.2575, 0.1682, 0.1025, 0.0471, 0.0366], grad_fn=<ToCopyBackward0>), [' was', ' had', ' wasn', ' made', ' didn'])\n",
      "(tensor([9.9590e-01, 1.7581e-03, 3.4140e-04, 3.3427e-04, 1.4755e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", ';'])\n",
      "(tensor([0.6776, 0.0294, 0.0280, 0.0178, 0.0176], grad_fn=<ToCopyBackward0>), [' as', ' too', ' a', ' even', ' so'])\n",
      "(tensor([0.3369, 0.1663, 0.0869, 0.0595, 0.0352], grad_fn=<ToCopyBackward0>), [' bad', ' lame', ' boring', ' funny', ' terrible'])\n",
      "(tensor([9.8758e-01, 2.0416e-03, 1.9720e-03, 1.0892e-03, 9.4114e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' as', ' a', '.', ' in', '...'])\n",
      "(tensor([0.0997, 0.0825, 0.0490, 0.0486, 0.0438], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' this', ' Man', ' I'])\n",
      "(tensor([0.0901, 0.0518, 0.0223, 0.0170, 0.0166], grad_fn=<ToCopyBackward0>), ['Plan', 'The', 'Night', 'G', 'Dead'])\n",
      "(tensor([0.8161, 0.0763, 0.0399, 0.0105, 0.0076], grad_fn=<ToCopyBackward0>), [' 9', ' Nine', ' nine', ' 7', ' B'])\n",
      "(tensor([0.5898, 0.2373, 0.0420, 0.0344, 0.0293], grad_fn=<ToCopyBackward0>), [' From', ' from', '\"', '.\"', '\".'])\n",
      "(tensor([0.9757, 0.0062, 0.0032, 0.0015, 0.0010], grad_fn=<ToCopyBackward0>), [' Outer', ' Out', ' outer', ' OUT', ' out'])\n",
      "(tensor([9.8475e-01, 7.1345e-03, 1.7858e-03, 6.8732e-04, 3.9654e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Space', ' space', 'Space', ' Darkness', ' Spac'])\n",
      "(tensor([0.3900, 0.3191, 0.1892, 0.0589, 0.0143], grad_fn=<ToCopyBackward0>), ['\".', '\"', '.\"', '\",', ',\"'])\n",
      "(tensor([0.1749, 0.1393, 0.0849, 0.0750, 0.0274], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'The'])\n",
      "(tensor([0.5374, 0.1055, 0.0992, 0.0865, 0.0355], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' is', ' was', ' film'])\n",
      "(tensor([0.1748, 0.1292, 0.0826, 0.0469, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' sucked', ' has'])\n",
      "(tensor([0.1816, 0.0915, 0.0810, 0.0678, 0.0581], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' so', ' no'])\n",
      "(tensor([0.4356, 0.1171, 0.0778, 0.0278, 0.0268], grad_fn=<ToCopyBackward0>), [' plot', ' redeem', ' real', ' humor', ' story'])\n",
      "(tensor([0.4063, 0.1942, 0.0684, 0.0513, 0.0455], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' at', ' or'])\n",
      "(tensor([0.9841, 0.0047, 0.0038, 0.0036, 0.0011], grad_fn=<ToCopyBackward0>), [' all', ' the', ' least', ' ALL', ' first'])\n",
      "(tensor([0.4591, 0.2128, 0.1542, 0.0283, 0.0183], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '!', '...'])\n",
      "(tensor([0.2293, 0.1624, 0.0687, 0.0676, 0.0584], grad_fn=<ToCopyBackward0>), [' and', ' just', ' no', ' the', ' so'])\n",
      "(tensor([0.1788, 0.1250, 0.0974, 0.0437, 0.0381], grad_fn=<ToCopyBackward0>), [' a', ' random', ' some', ' boring', ' one'])\n",
      "(tensor([0.6152, 0.0427, 0.0231, 0.0212, 0.0173], grad_fn=<ToCopyBackward0>), [' bunch', ' guy', ' lot', ' random', ' group'])\n",
      "(tensor([9.9254e-01, 8.0132e-04, 5.4275e-04, 5.1782e-04, 3.4768e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', 'a', ' o', ' people', ' characters'])\n",
      "(tensor([0.1061, 0.0631, 0.0574, 0.0536, 0.0467], grad_fn=<ToCopyBackward0>), [' people', ' lame', ' random', ' stupid', ' silly'])\n",
      "(tensor([0.3838, 0.0619, 0.0413, 0.0406, 0.0337], grad_fn=<ToCopyBackward0>), [' scenes', ' clips', ' characters', ' people', ' shots'])\n",
      "(tensor([0.4210, 0.3247, 0.0391, 0.0381, 0.0267], grad_fn=<ToCopyBackward0>), [' from', ' of', ' that', ' and', ','])\n",
      "(tensor([0.1944, 0.1744, 0.0791, 0.0329, 0.0312], grad_fn=<ToCopyBackward0>), [' and', ' some', ' which', ' clips', ' a'])\n",
      "(tensor([0.4061, 0.0347, 0.0310, 0.0256, 0.0159], grad_fn=<ToCopyBackward0>), [' of', ' even', ' that', ' funny', ' people'])\n",
      "(tensor([0.8291, 0.1272, 0.0130, 0.0031, 0.0020], grad_fn=<ToCopyBackward0>), [' which', ' them', ' the', ' it', ' crappy'])\n",
      "(tensor([0.4196, 0.1221, 0.0439, 0.0355, 0.0353], grad_fn=<ToCopyBackward0>), [' were', ' are', ' I', ' looked', ' weren'])\n",
      "(tensor([0.1071, 0.0995, 0.0666, 0.0587, 0.0491], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' quite', ' pretty', ' just'])\n",
      "(tensor([0.3890, 0.3625, 0.0848, 0.0546, 0.0204], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' (', '...'])\n",
      "(tensor([0.2156, 0.1000, 0.0781, 0.0689, 0.0312], grad_fn=<ToCopyBackward0>), [' and', ' but', 'but', 'and', ' I'])\n",
      "(tensor([0.0947, 0.0898, 0.0890, 0.0790, 0.0680], grad_fn=<ToCopyBackward0>), [' the', ' some', ' that', ' not', ' most'])\n",
      "(tensor([0.2073, 0.1587, 0.1065, 0.0989, 0.0717], grad_fn=<ToCopyBackward0>), [' really', ' much', ' in', ' very', ' funny'])\n",
      "(tensor([0.4084, 0.3913, 0.1287, 0.0139, 0.0121], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' this', ' an'])\n",
      "(tensor([0.6228, 0.0529, 0.0333, 0.0235, 0.0191], grad_fn=<ToCopyBackward0>), [' way', ' least', ' right', ' \"', ' funny'])\n",
      "(tensor([0.2372, 0.1744, 0.1228, 0.0812, 0.0802], grad_fn=<ToCopyBackward0>), [' you', ' that', ' the', ' \"', ' this'])\n",
      "(tensor([0.3840, 0.1920, 0.1122, 0.0595, 0.0547], grad_fn=<ToCopyBackward0>), [' would', \"'d\", ' think', ' might', ' expect'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that a sequel would not be a good idea because this was a good movie, a good movie for kids... but I thought this would be a great story for kids and a great story for adults. I was very disappointed. It's not as good\n",
      "(tensor([0.3834, 0.1719, 0.0902, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3318, 0.2363, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.3453, 0.2848, 0.2643, 0.0092, 0.0066], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' sequel', ' documentary', ' remake'])\n",
      "(tensor([0.5781, 0.1848, 0.0614, 0.0322, 0.0221], grad_fn=<ToCopyBackward0>), [' to', ' was', ' would', ' of', ' is'])\n",
      "(tensor([0.7213, 0.0815, 0.0367, 0.0322, 0.0149], grad_fn=<ToCopyBackward0>), [' be', ' have', ' not', ' never', ' make'])\n",
      "(tensor([0.5291, 0.2712, 0.0382, 0.0271, 0.0183], grad_fn=<ToCopyBackward0>), [' be', ' only', ' have', ' work', ' make'])\n",
      "(tensor([0.3624, 0.0797, 0.0778, 0.0713, 0.0478], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' as', ' too'])\n",
      "(tensor([0.7911, 0.0719, 0.0158, 0.0104, 0.0093], grad_fn=<ToCopyBackward0>), [' good', ' great', ' bad', ' sequel', ' very'])\n",
      "(tensor([0.7524, 0.0693, 0.0464, 0.0395, 0.0140], grad_fn=<ToCopyBackward0>), [' idea', ' movie', ' move', ' sequel', ' thing'])\n",
      "(tensor([0.3026, 0.2533, 0.1049, 0.0633, 0.0598], grad_fn=<ToCopyBackward0>), [' for', '.', ',', ' because', ' as'])\n",
      "(tensor([0.1443, 0.1411, 0.1242, 0.1147, 0.0965], grad_fn=<ToCopyBackward0>), [' the', ' this', ' I', ' of', ' it'])\n",
      "(tensor([0.3961, 0.2048, 0.1461, 0.0437, 0.0307], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' one', ' film'])\n",
      "(tensor([0.3812, 0.1960, 0.0762, 0.0577, 0.0427], grad_fn=<ToCopyBackward0>), [' a', ' the', ' such', ' not', ' an'])\n",
      "(tensor([0.2715, 0.1269, 0.0932, 0.0572, 0.0516], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' great', ' very', ' real'])\n",
      "(tensor([9.3029e-01, 3.4639e-02, 1.6948e-02, 4.6010e-03, 8.7913e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' sequel', ' book'])\n",
      "(tensor([0.7240, 0.0648, 0.0624, 0.0219, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '!'])\n",
      "(tensor([0.3412, 0.1105, 0.0850, 0.0526, 0.0432], grad_fn=<ToCopyBackward0>), [' but', ' and', ' a', ' so', ' the'])\n",
      "(tensor([0.3739, 0.1806, 0.0850, 0.0832, 0.0243], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' great', ' real'])\n",
      "(tensor([0.4472, 0.2351, 0.0342, 0.0296, 0.0266], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' sequel', ' idea', ' film'])\n",
      "(tensor([0.2653, 0.1899, 0.1017, 0.0677, 0.0654], grad_fn=<ToCopyBackward0>), [' that', ' with', '.', ' in', ' for'])\n",
      "(tensor([0.6295, 0.0858, 0.0255, 0.0234, 0.0221], grad_fn=<ToCopyBackward0>), [' kids', ' the', ' everybody', ' young', ' adults'])\n",
      "(tensor([0.4056, 0.1103, 0.0761, 0.0482, 0.0304], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.1176, 0.0837, 0.0823, 0.0639, 0.0452], grad_fn=<ToCopyBackward0>), [' I', 'I', ' but', 'but', ' and'])\n",
      "(tensor([0.1164, 0.0840, 0.0639, 0.0590, 0.0375], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' a'])\n",
      "(tensor([0.1834, 0.0700, 0.0540, 0.0513, 0.0446], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' guess', \"'m\"])\n",
      "(tensor([0.3482, 0.1932, 0.0558, 0.0539, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' the', ' kids'])\n",
      "(tensor([0.3809, 0.2115, 0.0958, 0.0775, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' movie', ' would', ' is', ' could'])\n",
      "(tensor([0.9118, 0.0159, 0.0108, 0.0101, 0.0070], grad_fn=<ToCopyBackward0>), [' be', ' make', ' have', ' get', ' not'])\n",
      "(tensor([0.7291, 0.0528, 0.0216, 0.0205, 0.0178], grad_fn=<ToCopyBackward0>), [' a', ' an', ' better', ' something', ' the'])\n",
      "(tensor([0.6490, 0.2036, 0.0334, 0.0152, 0.0134], grad_fn=<ToCopyBackward0>), [' good', ' great', ' really', ' very', ' sequel'])\n",
      "(tensor([0.7444, 0.0925, 0.0440, 0.0227, 0.0204], grad_fn=<ToCopyBackward0>), [' movie', ' sequel', ' story', ' idea', ' film'])\n",
      "(tensor([0.3299, 0.2904, 0.1135, 0.0616, 0.0596], grad_fn=<ToCopyBackward0>), [' for', ' to', ' and', '.', ','])\n",
      "(tensor([0.3651, 0.2710, 0.1543, 0.0451, 0.0339], grad_fn=<ToCopyBackward0>), [' kids', ' a', ' the', ' children', ' adults'])\n",
      "(tensor([0.2379, 0.1880, 0.1558, 0.0965, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ' to', '...', ',', ' and'])\n",
      "(tensor([0.1454, 0.1375, 0.1047, 0.0597, 0.0560], grad_fn=<ToCopyBackward0>), [' adults', ' I', ' a', ' that', ' for'])\n",
      "(tensor([0.6911, 0.2176, 0.0192, 0.0081, 0.0073], grad_fn=<ToCopyBackward0>), [' great', ' good', ' really', ' very', ' story'])\n",
      "(tensor([0.4930, 0.3253, 0.0351, 0.0308, 0.0137], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' family', ' sequel', ' way'])\n",
      "(tensor([0.8893, 0.0631, 0.0090, 0.0064, 0.0050], grad_fn=<ToCopyBackward0>), [' for', ' to', '.', '...', ' about'])\n",
      "(tensor([0.8712, 0.0218, 0.0198, 0.0155, 0.0144], grad_fn=<ToCopyBackward0>), [' adults', ' kids', ' parents', ' the', ' families'])\n",
      "(tensor([0.5625, 0.1273, 0.0743, 0.0538, 0.0291], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', '...', ' as'])\n",
      "(tensor([0.2620, 0.1951, 0.0856, 0.0697, 0.0450], grad_fn=<ToCopyBackward0>), [' I', ' So', ' And', ' It', ' But'])\n",
      "(tensor([0.1513, 0.1160, 0.0843, 0.0509, 0.0462], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' think', \"'m\"])\n",
      "(tensor([0.3112, 0.1054, 0.0867, 0.0636, 0.0370], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' really', ' so', ' not'])\n",
      "(tensor([0.3629, 0.1411, 0.1072, 0.0968, 0.0285], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' wrong', ' excited', ' much'])\n",
      "(tensor([0.2970, 0.2106, 0.1405, 0.0826, 0.0624], grad_fn=<ToCopyBackward0>), [' when', ' with', '.', ' by', ' in'])\n",
      "(tensor([0.2778, 0.1517, 0.0708, 0.0570, 0.0489], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.3667, 0.2663, 0.0626, 0.0339, 0.0310], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' just', ' seemed'])\n",
      "(tensor([0.2154, 0.1824, 0.1550, 0.0359, 0.0322], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' like', ' really'])\n",
      "(tensor([0.3916, 0.1263, 0.0980, 0.0796, 0.0385], grad_fn=<ToCopyBackward0>), [' a', ' as', ' the', ' even', ' that'])\n",
      "(tensor([0.3401, 0.1603, 0.0307, 0.0302, 0.0290], grad_fn=<ToCopyBackward0>), [' good', ' if', ' though', ' bad', ' interesting'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was one of the worst movies I have ever seen, and I have seen a lot. I really don't even know where I saw this movie. I don't really know where I even got it, but I know I rented it from a\n",
      "(tensor([0.3831, 0.1722, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2442, 0.1957, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4799, 0.1354, 0.1037, 0.0554, 0.0254], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.9005, 0.0167, 0.0077, 0.0068, 0.0066], grad_fn=<ToCopyBackward0>), [' of', ' really', ' more', ' the', ' that'])\n",
      "(tensor([0.8493, 0.1164, 0.0142, 0.0015, 0.0009], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' a'])\n",
      "(tensor([0.5760, 0.0963, 0.0949, 0.0428, 0.0284], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' best', ' weakest', ' funn'])\n",
      "(tensor([0.7962, 0.0917, 0.0114, 0.0044, 0.0036], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' western', ' British'])\n",
      "(tensor([0.7148, 0.1294, 0.0736, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' to'])\n",
      "(tensor([0.5015, 0.3088, 0.0859, 0.0514, 0.0264], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([0.9157, 0.0755, 0.0020, 0.0014, 0.0012], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9086, 0.0213, 0.0170, 0.0083, 0.0052], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' wasted', ' been'])\n",
      "(tensor([0.6193, 0.1188, 0.0816, 0.0498, 0.0249], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' in', '...'])\n",
      "(tensor([0.1898, 0.1576, 0.0598, 0.0320, 0.0318], grad_fn=<ToCopyBackward0>), [' and', ' but', ' the', ' with', ' not'])\n",
      "(tensor([0.5730, 0.1095, 0.0497, 0.0254, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' was'])\n",
      "(tensor([0.1622, 0.1057, 0.0850, 0.0744, 0.0632], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', \"'m\", ' really'])\n",
      "(tensor([0.8145, 0.0272, 0.0180, 0.0143, 0.0101], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' watched', ' to', ' only'])\n",
      "(tensor([0.4974, 0.2505, 0.1108, 0.0294, 0.0144], grad_fn=<ToCopyBackward0>), [' some', ' quite', ' a', ' many', ' worse'])\n",
      "(tensor([0.7360, 0.1139, 0.0563, 0.0200, 0.0191], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' LOT', ' couple', ' ton'])\n",
      "(tensor([0.7103, 0.2246, 0.0235, 0.0113, 0.0086], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' in', '!'])\n",
      "(tensor([0.2072, 0.1386, 0.0846, 0.0813, 0.0399], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.1121, 0.0832, 0.0807, 0.0482, 0.0413], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', ' really', ' can'])\n",
      "(tensor([0.1056, 0.0572, 0.0539, 0.0483, 0.0477], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' have', ' think', ' do'])\n",
      "(tensor([9.9361e-01, 2.6193e-03, 8.0819e-04, 5.7466e-04, 2.4280e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.3411, 0.1665, 0.1603, 0.0963, 0.0422], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' like', ' even'])\n",
      "(tensor([0.4296, 0.2163, 0.0577, 0.0557, 0.0549], grad_fn=<ToCopyBackward0>), [' know', ' remember', ' think', ' like', ' want'])\n",
      "(tensor([0.4559, 0.2166, 0.1719, 0.0485, 0.0464], grad_fn=<ToCopyBackward0>), [' where', ' how', ' if', ' why', ' what'])\n",
      "(tensor([0.9641, 0.0132, 0.0088, 0.0050, 0.0017], grad_fn=<ToCopyBackward0>), [' to', ' I', ' it', ' the', ' this'])\n",
      "(tensor([0.2115, 0.0989, 0.0635, 0.0630, 0.0467], grad_fn=<ToCopyBackward0>), [' am', ' was', ' saw', \"'m\", ' put'])\n",
      "(tensor([0.7345, 0.1653, 0.0475, 0.0154, 0.0035], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.5154, 0.1296, 0.1164, 0.0597, 0.0255], grad_fn=<ToCopyBackward0>), [' movie', '.', ' one', ',', ' film'])\n",
      "(tensor([0.4837, 0.3355, 0.0201, 0.0132, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' at', ' before'])\n",
      "(tensor([0.4455, 0.1358, 0.0539, 0.0181, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0869, 0.0806, 0.0741, 0.0728, 0.0541], grad_fn=<ToCopyBackward0>), [' am', ' think', ' was', ' have', ' don'])\n",
      "(tensor([9.9615e-01, 1.1408e-03, 5.7369e-04, 2.5213e-04, 1.7665e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.4826, 0.1905, 0.1161, 0.0837, 0.0349], grad_fn=<ToCopyBackward0>), [' even', ' remember', ' know', ' think', ' really'])\n",
      "(tensor([0.3591, 0.2065, 0.0963, 0.0555, 0.0440], grad_fn=<ToCopyBackward0>), [' remember', ' know', ' even', ' think', ' have'])\n",
      "(tensor([0.2157, 0.1896, 0.1768, 0.1111, 0.0896], grad_fn=<ToCopyBackward0>), [' where', ' how', ' if', ' what', ' why'])\n",
      "(tensor([0.3823, 0.2511, 0.1576, 0.0758, 0.0251], grad_fn=<ToCopyBackward0>), [' I', ' it', ' to', ' this', ' the'])\n",
      "(tensor([0.3827, 0.0829, 0.0643, 0.0517, 0.0402], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' found', ' even', ' have'])\n",
      "(tensor([0.4717, 0.1145, 0.0581, 0.0462, 0.0308], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' got', ' watched', ' see'])\n",
      "(tensor([0.5885, 0.2107, 0.0718, 0.0254, 0.0195], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' my'])\n",
      "(tensor([0.5670, 0.1952, 0.1342, 0.0219, 0.0115], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', ' in', ' because'])\n",
      "(tensor([0.5403, 0.0670, 0.0349, 0.0344, 0.0341], grad_fn=<ToCopyBackward0>), [' but', ' I', ' so', ' except', ' although'])\n",
      "(tensor([0.6438, 0.1409, 0.0126, 0.0115, 0.0110], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' my', ' this'])\n",
      "(tensor([0.1076, 0.0689, 0.0588, 0.0559, 0.0481], grad_fn=<ToCopyBackward0>), [' have', ' really', \"'m\", ' am', ' know'])\n",
      "(tensor([0.4494, 0.1987, 0.1405, 0.0490, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' where', ' someone'])\n",
      "(tensor([0.1511, 0.0965, 0.0846, 0.0487, 0.0370], grad_fn=<ToCopyBackward0>), [' have', ' saw', ' watched', \"'m\", ' rented'])\n",
      "(tensor([0.9703, 0.0077, 0.0073, 0.0022, 0.0020], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' something', ' a'])\n",
      "(tensor([0.2779, 0.1983, 0.1001, 0.0601, 0.0564], grad_fn=<ToCopyBackward0>), [' from', '.', ' at', ' because', ' and'])\n",
      "(tensor([0.2528, 0.1392, 0.0892, 0.0666, 0.0546], grad_fn=<ToCopyBackward0>), [' Block', ' a', ' some', ' Netflix', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie would be more like \"Rocky Horror Picture Show\" than \"The Incredible Melting Man\", but it was pretty funny and interesting, though it's hard to tell if it was the acting or the plot. The acting was pretty good,\n",
      "(tensor([0.3827, 0.1725, 0.0905, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2452, 0.1956, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6534, 0.0594, 0.0360, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.8300, 0.0817, 0.0134, 0.0116, 0.0080], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' get'])\n",
      "(tensor([0.1440, 0.1144, 0.0935, 0.0926, 0.0752], grad_fn=<ToCopyBackward0>), [' more', ' better', ' great', ' a', ' really'])\n",
      "(tensor([0.3965, 0.1125, 0.0565, 0.0445, 0.0286], grad_fn=<ToCopyBackward0>), [' like', ' entertaining', ' of', ' along', ' boring'])\n",
      "(tensor([0.2051, 0.1870, 0.1372, 0.0575, 0.0365], grad_fn=<ToCopyBackward0>), [' a', ' \"', ' the', ' an', \" '\"])\n",
      "(tensor([0.0563, 0.0354, 0.0222, 0.0181, 0.0154], grad_fn=<ToCopyBackward0>), ['The', 'Plan', 'Ext', 'G', 'Rock'])\n",
      "(tensor([0.7577, 0.0591, 0.0124, 0.0117, 0.0074], grad_fn=<ToCopyBackward0>), ['y', 'n', ' Star', ' Hudson', ' n'])\n",
      "(tensor([0.7972, 0.0282, 0.0254, 0.0180, 0.0119], grad_fn=<ToCopyBackward0>), [' Horror', '\"', ' Bal', '\",', '\".'])\n",
      "(tensor([0.9555, 0.0124, 0.0041, 0.0025, 0.0024], grad_fn=<ToCopyBackward0>), [' Picture', ' Show', '\",', '\".', '\"'])\n",
      "(tensor([9.8957e-01, 2.5770e-03, 2.5452e-03, 1.2363e-03, 4.2469e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Show', ' Shows', ' SHOW', 'Show', ' Theater'])\n",
      "(tensor([0.6760, 0.0959, 0.0807, 0.0634, 0.0594], grad_fn=<ToCopyBackward0>), ['\"', '\",', '\".', ',\"', '.\"'])\n",
      "(tensor([0.2468, 0.1582, 0.0870, 0.0818, 0.0632], grad_fn=<ToCopyBackward0>), [' than', ' or', ' with', ' but', ' and'])\n",
      "(tensor([0.4485, 0.1770, 0.1459, 0.0284, 0.0212], grad_fn=<ToCopyBackward0>), [' \"', ' a', ' it', ' the', ' an'])\n",
      "(tensor([0.0897, 0.0732, 0.0382, 0.0227, 0.0157], grad_fn=<ToCopyBackward0>), ['Rock', 'Plan', 'The', 'C', 'My'])\n",
      "(tensor([0.0973, 0.0866, 0.0699, 0.0499, 0.0407], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Incredible', ' Shining', ' Wizard'])\n",
      "(tensor([0.3097, 0.3007, 0.0660, 0.0630, 0.0471], grad_fn=<ToCopyBackward0>), [' Mel', ' Mr', ' Dr', ' B', ' Sh'])\n",
      "(tensor([0.9329, 0.0155, 0.0061, 0.0059, 0.0043], grad_fn=<ToCopyBackward0>), ['ting', 'ted', 'ons', 'anch', 'ts'])\n",
      "(tensor([9.7694e-01, 6.3904e-03, 1.2792e-03, 7.8092e-04, 7.8045e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Man', ' Woman', 'Man', ' Thing', ' Pot'])\n",
      "(tensor([0.3752, 0.2939, 0.1151, 0.1055, 0.0717], grad_fn=<ToCopyBackward0>), ['\".', '.\"', '\",', '\"', ',\"'])\n",
      "(tensor([0.5629, 0.0682, 0.0475, 0.0230, 0.0204], grad_fn=<ToCopyBackward0>), [' but', ' and', ' which', ' with', ' I'])\n",
      "(tensor([0.3554, 0.1112, 0.0595, 0.0452, 0.0326], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' maybe'])\n",
      "(tensor([0.2055, 0.0730, 0.0681, 0.0625, 0.0484], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' is', ' turned', \"'s\"])\n",
      "(tensor([0.1709, 0.0808, 0.0715, 0.0473, 0.0462], grad_fn=<ToCopyBackward0>), [' pretty', ' actually', ' still', ' worse', ' more'])\n",
      "(tensor([0.2639, 0.1164, 0.0776, 0.0595, 0.0557], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' boring', ' awful', ' cool'])\n",
      "(tensor([0.4762, 0.1797, 0.0857, 0.0275, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '!'])\n",
      "(tensor([0.1246, 0.1015, 0.0931, 0.0560, 0.0428], grad_fn=<ToCopyBackward0>), [' I', ' interesting', ' funny', ' entertaining', ' the'])\n",
      "(tensor([0.2979, 0.1502, 0.0769, 0.0614, 0.0566], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', ' at'])\n",
      "(tensor([0.1451, 0.1297, 0.1280, 0.0640, 0.0609], grad_fn=<ToCopyBackward0>), [' though', ' too', ' especially', ' and', ' considering'])\n",
      "(tensor([0.1791, 0.1266, 0.1155, 0.0761, 0.0546], grad_fn=<ToCopyBackward0>), [' not', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.1433, 0.1254, 0.0779, 0.0675, 0.0580], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' did', ' had', ' didn'])\n",
      "(tensor([0.2060, 0.1020, 0.0891, 0.0682, 0.0427], grad_fn=<ToCopyBackward0>), [' not', ' a', ' too', ' more', ' hard'])\n",
      "(tensor([9.6828e-01, 2.0040e-02, 8.0740e-03, 2.8977e-04, 2.0630e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' not', ' at', ' on'])\n",
      "(tensor([0.1870, 0.0934, 0.0756, 0.0677, 0.0632], grad_fn=<ToCopyBackward0>), [' imagine', ' tell', ' compare', ' believe', ' follow'])\n",
      "(tensor([0.1946, 0.0981, 0.0913, 0.0873, 0.0601], grad_fn=<ToCopyBackward0>), [' how', ' what', ' from', ' if', ' whether'])\n",
      "(tensor([0.3101, 0.2461, 0.0762, 0.0609, 0.0453], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3933, 0.2431, 0.0589, 0.0438, 0.0251], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' would', ' will', ' is'])\n",
      "(tensor([0.2198, 0.1473, 0.0688, 0.0422, 0.0318], grad_fn=<ToCopyBackward0>), [' the', ' a', ' meant', ' supposed', ' more'])\n",
      "(tensor([0.1755, 0.0964, 0.0534, 0.0274, 0.0230], grad_fn=<ToCopyBackward0>), [' actors', ' same', ' acting', ' funny', ' fact'])\n",
      "(tensor([0.7487, 0.1591, 0.0239, 0.0142, 0.0132], grad_fn=<ToCopyBackward0>), [' or', ',', ' of', ' that', ' ('])\n",
      "(tensor([0.9014, 0.0111, 0.0062, 0.0057, 0.0045], grad_fn=<ToCopyBackward0>), [' the', ' story', ' writing', ' just', ' not'])\n",
      "(tensor([0.1687, 0.1361, 0.0969, 0.0217, 0.0210], grad_fn=<ToCopyBackward0>), [' story', ' script', ' plot', ' humor', ' special'])\n",
      "(tensor([0.5912, 0.1577, 0.0725, 0.0414, 0.0311], grad_fn=<ToCopyBackward0>), [' that', '.', ',', ' which', ' or'])\n",
      "(tensor([0.2100, 0.1495, 0.0957, 0.0377, 0.0341], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'The', ' There'])\n",
      "(tensor([0.0831, 0.0791, 0.0383, 0.0293, 0.0282], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ' acting', ' main'])\n",
      "(tensor([0.6697, 0.0738, 0.0519, 0.0192, 0.0190], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.3118, 0.0687, 0.0396, 0.0286, 0.0282], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' very', ' weak', ' bad'])\n",
      "(tensor([0.5024, 0.1548, 0.0227, 0.0214, 0.0209], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' poor', ' weak', ' funny'])\n",
      "(tensor([0.5565, 0.1192, 0.1037, 0.0361, 0.0207], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' for', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a good movie. I think it's a good movie. But the only reason I watched it is because I thought that this movie had to be in theaters. It was not a movie that I wanted to sit through, but if it\n",
      "(tensor([0.3830, 0.1722, 0.0902, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2443, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6525, 0.0595, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0662, 0.0548, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0850, 0.0616, 0.0528, 0.0427, 0.0407], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.3097, 0.1906, 0.0864, 0.0349, 0.0293], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' story', ' premise'])\n",
      "(tensor([0.3240, 0.1274, 0.0885, 0.0761, 0.0577], grad_fn=<ToCopyBackward0>), ['.', '...', '....', '!', ','])\n",
      "(tensor([0.3265, 0.2301, 0.0793, 0.0280, 0.0153], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2052, 0.1358, 0.0547, 0.0522, 0.0415], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' just'])\n",
      "(tensor([0.4451, 0.1474, 0.0753, 0.0494, 0.0190], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' they'])\n",
      "(tensor([0.3173, 0.2336, 0.0711, 0.0524, 0.0451], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' could', ' has', ' is'])\n",
      "(tensor([0.4072, 0.0739, 0.0402, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' one', ' funny', ' an', ' very'])\n",
      "(tensor([0.4142, 0.0721, 0.0717, 0.0575, 0.0487], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' movie', ' great'])\n",
      "(tensor([0.8772, 0.0348, 0.0247, 0.0159, 0.0039], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' story', ' plot'])\n",
      "(tensor([0.3963, 0.1688, 0.0456, 0.0434, 0.0433], grad_fn=<ToCopyBackward0>), ['.', ' for', ' in', ' if', ','])\n",
      "(tensor([0.3826, 0.1046, 0.0869, 0.0343, 0.0311], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.1600, 0.1375, 0.1081, 0.0824, 0.0267], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' when'])\n",
      "(tensor([0.1343, 0.1302, 0.0672, 0.0539, 0.0431], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' story', ' only', ' one'])\n",
      "(tensor([0.3407, 0.1675, 0.1243, 0.0522, 0.0512], grad_fn=<ToCopyBackward0>), [' reason', ' thing', ' way', ' person', ' good'])\n",
      "(tensor([0.6353, 0.1090, 0.0704, 0.0459, 0.0409], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' this', ' it'])\n",
      "(tensor([0.1371, 0.1265, 0.1217, 0.0479, 0.0427], grad_fn=<ToCopyBackward0>), [' gave', ' watched', \"'m\", ' didn', ' was'])\n",
      "(tensor([0.8107, 0.1107, 0.0373, 0.0094, 0.0019], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.4388, 0.1705, 0.0757, 0.0243, 0.0237], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' I'])\n",
      "(tensor([0.7169, 0.0714, 0.0507, 0.0217, 0.0194], grad_fn=<ToCopyBackward0>), [' because', ' I', ' that', ' the', ' for'])\n",
      "(tensor([0.6089, 0.0675, 0.0651, 0.0612, 0.0439], grad_fn=<ToCopyBackward0>), [' I', ' my', ' the', ' it', ' of'])\n",
      "(tensor([0.1623, 0.1414, 0.1215, 0.0865, 0.0332], grad_fn=<ToCopyBackward0>), [\"'m\", ' thought', ' like', ' was', ' really'])\n",
      "(tensor([0.5034, 0.1115, 0.0893, 0.0598, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ','])\n",
      "(tensor([0.2037, 0.1228, 0.0886, 0.0690, 0.0375], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' maybe'])\n",
      "(tensor([0.3984, 0.2302, 0.1926, 0.0297, 0.0113], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' guy', ' girl'])\n",
      "(tensor([0.4900, 0.1079, 0.0618, 0.0234, 0.0199], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' would', ' could'])\n",
      "(tensor([0.3035, 0.2667, 0.0771, 0.0484, 0.0293], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' some', ' an'])\n",
      "(tensor([0.7447, 0.0837, 0.0684, 0.0160, 0.0113], grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', ' come', ' go'])\n",
      "(tensor([0.1515, 0.0900, 0.0724, 0.0569, 0.0369], grad_fn=<ToCopyBackward0>), [' made', ' a', ' funny', ' in', ' good'])\n",
      "(tensor([0.5818, 0.1380, 0.0694, 0.0336, 0.0273], grad_fn=<ToCopyBackward0>), [' the', ' there', ' my', ' theaters', ' it'])\n",
      "(tensor([0.4067, 0.1904, 0.0932, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' to', ' in'])\n",
      "(tensor([0.3148, 0.0956, 0.0901, 0.0378, 0.0377], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' So'])\n",
      "(tensor([0.2977, 0.1900, 0.1142, 0.0796, 0.0358], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' has', ' is'])\n",
      "(tensor([0.1693, 0.1093, 0.0980, 0.0863, 0.0632], grad_fn=<ToCopyBackward0>), [' like', ' so', ' not', ' a', ' just'])\n",
      "(tensor([0.1777, 0.0941, 0.0606, 0.0565, 0.0435], grad_fn=<ToCopyBackward0>), [' a', ' like', ' just', ' funny', ' something'])\n",
      "(tensor([0.1113, 0.0539, 0.0356, 0.0322, 0.0320], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' very', ' waste', ' bad'])\n",
      "(tensor([0.5698, 0.1869, 0.0751, 0.0396, 0.0237], grad_fn=<ToCopyBackward0>), [' that', ' I', '.', ' for', ' to'])\n",
      "(tensor([0.5226, 0.1559, 0.0494, 0.0218, 0.0196], grad_fn=<ToCopyBackward0>), [' I', ' was', ' a', ' you', ' the'])\n",
      "(tensor([0.1299, 0.0905, 0.0597, 0.0588, 0.0522], grad_fn=<ToCopyBackward0>), [' would', ' was', ' wanted', ' really', ' could'])\n",
      "(tensor([0.8085, 0.0510, 0.0374, 0.0107, 0.0088], grad_fn=<ToCopyBackward0>), [' to', '.', ' on', ' out', ' in'])\n",
      "(tensor([0.3927, 0.2069, 0.0944, 0.0615, 0.0470], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' make', ' sit', ' be'])\n",
      "(tensor([0.7283, 0.1774, 0.0242, 0.0170, 0.0155], grad_fn=<ToCopyBackward0>), [' through', ' down', ' and', ' around', ' in'])\n",
      "(tensor([0.1833, 0.1346, 0.0950, 0.0780, 0.0485], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' just'])\n",
      "(tensor([0.3962, 0.0546, 0.0457, 0.0433, 0.0360], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' so', ' like'])\n",
      "(tensor([0.4695, 0.2242, 0.0276, 0.0215, 0.0140], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' if', ' at'])\n",
      "(tensor([0.3871, 0.2326, 0.2279, 0.0200, 0.0175], grad_fn=<ToCopyBackward0>), [' I', ' it', ' you', ' there', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this movie was pretty awful. It was slow moving with a lot of boring scenes and acting. It was not scary at all. There was no tension. The acting was so bad, I was not entertained at all. I was not even scared\n",
      "(tensor([0.3838, 0.1717, 0.0900, 0.0772, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3317, 0.2365, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4149, 0.1967, 0.1688, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.5735, 0.0626, 0.0570, 0.0477, 0.0273], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.0899, 0.0876, 0.0655, 0.0531, 0.0410], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' very', ' so', ' really'])\n",
      "(tensor([0.1798, 0.1534, 0.0867, 0.0836, 0.0777], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' awful', ' boring', ' lame'])\n",
      "(tensor([0.6090, 0.0810, 0.0774, 0.0375, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2235, 0.1430, 0.1414, 0.0220, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.2898, 0.1659, 0.0754, 0.0562, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' has'])\n",
      "(tensor([0.0658, 0.0607, 0.0584, 0.0572, 0.0562], grad_fn=<ToCopyBackward0>), [' just', ' slow', ' boring', ' a', ' so'])\n",
      "(tensor([0.3545, 0.2933, 0.1951, 0.0560, 0.0145], grad_fn=<ToCopyBackward0>), [' and', ' moving', ',', ' paced', '-'])\n",
      "(tensor([0.5905, 0.2870, 0.0376, 0.0182, 0.0072], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' with', ' ('])\n",
      "(tensor([0.1784, 0.1596, 0.0501, 0.0486, 0.0468], grad_fn=<ToCopyBackward0>), [' no', ' a', ' bad', ' lots', ' stupid'])\n",
      "(tensor([0.2300, 0.0926, 0.0529, 0.0440, 0.0316], grad_fn=<ToCopyBackward0>), [' lot', ' weak', ' very', ' few', ' bunch'])\n",
      "(tensor([9.9097e-01, 1.5135e-03, 1.4034e-03, 4.3509e-04, 4.1624e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' less', ' more', ' to', ' going'])\n",
      "(tensor([0.0985, 0.0497, 0.0494, 0.0248, 0.0184], grad_fn=<ToCopyBackward0>), [' scenes', ' pointless', ' boring', ' wasted', ' plot'])\n",
      "(tensor([0.3121, 0.1409, 0.0610, 0.0167, 0.0154], grad_fn=<ToCopyBackward0>), [' scenes', ' dialogue', ' dialog', ' visuals', ' talking'])\n",
      "(tensor([0.5604, 0.1385, 0.0927, 0.0413, 0.0251], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' that', ' of'])\n",
      "(tensor([0.0765, 0.0746, 0.0514, 0.0461, 0.0428], grad_fn=<ToCopyBackward0>), [' a', ' stupid', ' the', ' acting', ' it'])\n",
      "(tensor([0.4141, 0.3251, 0.0744, 0.0406, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' and', ' with'])\n",
      "(tensor([0.2278, 0.1549, 0.1443, 0.0373, 0.0284], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', 'The'])\n",
      "(tensor([0.3246, 0.0825, 0.0817, 0.0780, 0.0713], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', ' wasn', \"'s\"])\n",
      "(tensor([0.1199, 0.0988, 0.0764, 0.0720, 0.0580], grad_fn=<ToCopyBackward0>), [' not', ' also', ' boring', ' a', ' just'])\n",
      "(tensor([0.4413, 0.2222, 0.0631, 0.0232, 0.0226], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' entertaining', ' a', ' very'])\n",
      "(tensor([0.4341, 0.1762, 0.0893, 0.0462, 0.0417], grad_fn=<ToCopyBackward0>), [' at', ' or', ',', ' and', '.'])\n",
      "(tensor([9.9631e-01, 1.0215e-03, 7.5475e-04, 6.0882e-04, 3.6801e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' least', ' the', ' ALL', ' any'])\n",
      "(tensor([0.6242, 0.1352, 0.0760, 0.0233, 0.0193], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.2041, 0.1955, 0.1550, 0.0340, 0.0263], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', 'The'])\n",
      "(tensor([0.7147, 0.1110, 0.0839, 0.0378, 0.0102], grad_fn=<ToCopyBackward0>), [' was', ' were', ' wasn', ' is', ' are'])\n",
      "(tensor([0.2880, 0.1270, 0.0911, 0.0830, 0.0819], grad_fn=<ToCopyBackward0>), [' no', ' not', ' nothing', ' a', ' one'])\n",
      "(tensor([0.4803, 0.1683, 0.0392, 0.0229, 0.0226], grad_fn=<ToCopyBackward0>), [' suspense', ' gore', ' horror', ' plot', ' tension'])\n",
      "(tensor([0.2111, 0.1504, 0.1270, 0.0860, 0.0825], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' whatsoever', ' at'])\n",
      "(tensor([0.2313, 0.2138, 0.1477, 0.0795, 0.0384], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.3777, 0.0425, 0.0358, 0.0280, 0.0269], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' actors', ' movie'])\n",
      "(tensor([0.8674, 0.0400, 0.0103, 0.0086, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' and', ' in'])\n",
      "(tensor([0.1496, 0.0836, 0.0728, 0.0701, 0.0660], grad_fn=<ToCopyBackward0>), [' not', ' terrible', ' bad', ' horrible', ' so'])\n",
      "(tensor([0.4598, 0.0674, 0.0439, 0.0401, 0.0335], grad_fn=<ToCopyBackward0>), [' bad', ' fake', '-', ' wooden', ' cor'])\n",
      "(tensor([0.5630, 0.0902, 0.0662, 0.0622, 0.0579], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.4731, 0.1680, 0.0697, 0.0495, 0.0292], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' and'])\n",
      "(tensor([0.1015, 0.1007, 0.0923, 0.0869, 0.0737], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' actually', ' could', ' almost'])\n",
      "(tensor([0.1024, 0.0841, 0.0792, 0.0564, 0.0537], grad_fn=<ToCopyBackward0>), [' not', ' embarrassed', ' thinking', ' wondering', ' surprised'])\n",
      "(tensor([0.3024, 0.0691, 0.0666, 0.0497, 0.0446], grad_fn=<ToCopyBackward0>), [' even', ' entertained', ' surprised', ' expecting', ' able'])\n",
      "(tensor([0.2701, 0.2468, 0.1048, 0.0781, 0.0700], grad_fn=<ToCopyBackward0>), [' by', ' at', '.', ' for', ' even'])\n",
      "(tensor([9.8867e-01, 6.8837e-03, 1.4650e-03, 1.0026e-03, 5.6426e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' ALL', ' least'])\n",
      "(tensor([0.7134, 0.1041, 0.0315, 0.0196, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' by', ',', ' and', ' at'])\n",
      "(tensor([0.2708, 0.2020, 0.1141, 0.0338, 0.0295], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1036, 0.0847, 0.0597, 0.0595, 0.0504], grad_fn=<ToCopyBackward0>), [' was', ' think', ' really', ' would', ' just'])\n",
      "(tensor([0.1063, 0.0706, 0.0630, 0.0492, 0.0454], grad_fn=<ToCopyBackward0>), [' not', ' just', ' very', ' really', ' so'])\n",
      "(tensor([0.2038, 0.1032, 0.0720, 0.0705, 0.0595], grad_fn=<ToCopyBackward0>), [' even', ' impressed', ' scared', ' entertained', ' expecting'])\n",
      "(tensor([0.1199, 0.1091, 0.0990, 0.0597, 0.0351], grad_fn=<ToCopyBackward0>), [' creep', ' scared', ' bored', ' interested', ' entertained'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie was pretty funny. I think that it was a pretty good movie. I just thought it wasn't very believable and it just didn't feel to me as a viewer, as a storyteller, as an actor, that this was\n",
      "(tensor([0.3843, 0.1714, 0.0896, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3307, 0.2372, 0.0583, 0.0559, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2635, 0.0545, 0.0269, 0.0218, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5308, 0.0605, 0.0537, 0.0233, 0.0206], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.1790, 0.0810, 0.0522, 0.0426, 0.0358], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' a', ' so', ' terrible'])\n",
      "(tensor([0.1903, 0.1174, 0.0817, 0.0803, 0.0608], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3077, 0.1816, 0.1439, 0.0466, 0.0444], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' but'])\n",
      "(tensor([0.2807, 0.1434, 0.0694, 0.0339, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1148, 0.0826, 0.0812, 0.0546, 0.0542], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' mean', ' think'])\n",
      "(tensor([0.3384, 0.2077, 0.1389, 0.0349, 0.0289], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' I', ' this'])\n",
      "(tensor([0.3011, 0.2324, 0.0693, 0.0178, 0.0167], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' there', ' I'])\n",
      "(tensor([0.2519, 0.1951, 0.1196, 0.0507, 0.0261], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' has', ' had'])\n",
      "(tensor([0.2367, 0.1244, 0.1098, 0.0472, 0.0204], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' pretty', ' very', ' more'])\n",
      "(tensor([0.1900, 0.1898, 0.1028, 0.0385, 0.0331], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' very', ' lot', ' great'])\n",
      "(tensor([0.1612, 0.1169, 0.0755, 0.0434, 0.0423], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' accurate', ' average', ' boring'])\n",
      "(tensor([0.6318, 0.1898, 0.0279, 0.0123, 0.0120], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' satire', ' spoof', ' film'])\n",
      "(tensor([0.6565, 0.0802, 0.0354, 0.0352, 0.0216], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' for', '....'])\n",
      "(tensor([0.2994, 0.1077, 0.0687, 0.0450, 0.0331], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.1100, 0.1095, 0.0713, 0.0662, 0.0660], grad_fn=<ToCopyBackward0>), [' think', ' thought', ' really', ' just', ' mean'])\n",
      "(tensor([0.1460, 0.1419, 0.0971, 0.0705, 0.0665], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' think', ' wish', ' didn'])\n",
      "(tensor([0.3687, 0.3377, 0.1357, 0.0129, 0.0116], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.6623, 0.0475, 0.0319, 0.0301, 0.0244], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' wasn', ' could'])\n",
      "(tensor([9.9743e-01, 7.4067e-04, 3.0100e-04, 1.4555e-04, 1.4195e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2459, 0.1938, 0.1840, 0.0409, 0.0346], grad_fn=<ToCopyBackward0>), [' as', ' very', ' funny', ' the', ' that'])\n",
      "(tensor([0.3396, 0.2159, 0.0722, 0.0550, 0.0424], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' well', ' believable', ' scary'])\n",
      "(tensor([0.2637, 0.2219, 0.1004, 0.0917, 0.0607], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' to'])\n",
      "(tensor([0.2046, 0.1602, 0.1181, 0.0929, 0.0647], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' believable'])\n",
      "(tensor([0.2894, 0.2074, 0.1662, 0.0737, 0.0454], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' didn', ' just', ' seemed'])\n",
      "(tensor([0.4210, 0.2256, 0.0819, 0.0133, 0.0120], grad_fn=<ToCopyBackward0>), [' seemed', ' didn', ' wasn', ' was', ' seems'])\n",
      "(tensor([9.9759e-01, 6.9868e-04, 2.8767e-04, 1.4823e-04, 1.2023e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.1608, 0.0808, 0.0680, 0.0625, 0.0606], grad_fn=<ToCopyBackward0>), [' have', ' ring', ' feel', ' come', ' move'])\n",
      "(tensor([0.3745, 0.1611, 0.0726, 0.0676, 0.0451], grad_fn=<ToCopyBackward0>), [' real', ' like', ' very', ' to', ' believable'])\n",
      "(tensor([0.8822, 0.0331, 0.0240, 0.0086, 0.0065], grad_fn=<ToCopyBackward0>), [' me', ' be', ' bad', ' good', ' much'])\n",
      "(tensor([0.3159, 0.1511, 0.0979, 0.0876, 0.0630], grad_fn=<ToCopyBackward0>), [' that', ' as', ' to', ' like', '.'])\n",
      "(tensor([0.3383, 0.1163, 0.1109, 0.0809, 0.0565], grad_fn=<ToCopyBackward0>), [' believable', ' if', ' a', ' realistic', ' though'])\n",
      "(tensor([0.2134, 0.1248, 0.1236, 0.0972, 0.0846], grad_fn=<ToCopyBackward0>), [' viewer', ' movie', ' story', ' real', ' character'])\n",
      "(tensor([0.3695, 0.1772, 0.1297, 0.0982, 0.0357], grad_fn=<ToCopyBackward0>), [' that', '.', ',', ' as', ' to'])\n",
      "(tensor([0.3855, 0.0829, 0.0558, 0.0472, 0.0389], grad_fn=<ToCopyBackward0>), [' as', ' the', ' that', ' like', ' I'])\n",
      "(tensor([0.5435, 0.1597, 0.0667, 0.0398, 0.0348], grad_fn=<ToCopyBackward0>), [' a', ' an', ' to', ' someone', ' I'])\n",
      "(tensor([0.2496, 0.0870, 0.0798, 0.0702, 0.0662], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' person', ' viewer', ' film'])\n",
      "(tensor([0.7454, 0.0983, 0.0454, 0.0352, 0.0133], grad_fn=<ToCopyBackward0>), ['te', '-', ' tell', ' telling', ','])\n",
      "(tensor([9.9187e-01, 3.7861e-03, 8.3504e-04, 7.9672e-04, 2.9606e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ller', 'acher', 'llers', 'aser', 'll'])\n",
      "(tensor([0.7686, 0.0699, 0.0412, 0.0353, 0.0129], grad_fn=<ToCopyBackward0>), [',', '.', ' that', ' and', ' or'])\n",
      "(tensor([0.5255, 0.2834, 0.0491, 0.0313, 0.0171], grad_fn=<ToCopyBackward0>), [' as', ' that', ' the', ' what', ' and'])\n",
      "(tensor([0.6675, 0.2133, 0.0260, 0.0145, 0.0134], grad_fn=<ToCopyBackward0>), [' a', ' an', ' someone', ' the', ' somebody'])\n",
      "(tensor([0.7461, 0.1135, 0.0247, 0.0159, 0.0083], grad_fn=<ToCopyBackward0>), [' actor', ' artist', ' actress', ' acting', ' artistic'])\n",
      "(tensor([0.5775, 0.1247, 0.0719, 0.0262, 0.0243], grad_fn=<ToCopyBackward0>), [',', '.', ' that', ' and', ' to'])\n",
      "(tensor([0.6053, 0.2421, 0.0285, 0.0270, 0.0240], grad_fn=<ToCopyBackward0>), [' as', ' that', ' the', ' and', ' what'])\n",
      "(tensor([0.3200, 0.2577, 0.1320, 0.0622, 0.0466], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.5674, 0.0974, 0.0803, 0.0652, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' is', ' could', ' movie', ' really'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a sequel to a really good movie. I'm sure it was not a sequel to the good movie. I'm sure it was a totally different movie. I'm not sure if I liked it. I don't really remember. But it\n",
      "(tensor([0.3840, 0.1717, 0.0898, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1166, 0.0397, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1840, 0.1457, 0.0508, 0.0454, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1645, 0.1160, 0.0576, 0.0492, 0.0449], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8522, 0.0310, 0.0140, 0.0123, 0.0085], grad_fn=<ToCopyBackward0>), [' to', ' of', ',', '...', ' that'])\n",
      "(tensor([0.1701, 0.0653, 0.0591, 0.0367, 0.0355], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.1922, 0.1524, 0.0636, 0.0577, 0.0540], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' real', ' classic', ' good'])\n",
      "(tensor([0.3910, 0.3174, 0.0473, 0.0202, 0.0175], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' great', ' old', ','])\n",
      "(tensor([0.5521, 0.1037, 0.0440, 0.0320, 0.0266], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' action', ' story', ' horror'])\n",
      "(tensor([0.5223, 0.0777, 0.0658, 0.0475, 0.0274], grad_fn=<ToCopyBackward0>), ['.', ',', ' called', ' that', ' I'])\n",
      "(tensor([0.2653, 0.1004, 0.0615, 0.0613, 0.0446], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.1307, 0.1150, 0.1049, 0.0476, 0.0300], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' just'])\n",
      "(tensor([0.2018, 0.1497, 0.1005, 0.0673, 0.0646], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' sure', ' just'])\n",
      "(tensor([0.1716, 0.1663, 0.1206, 0.1073, 0.0879], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' there', ' this'])\n",
      "(tensor([0.3367, 0.2322, 0.1280, 0.0682, 0.0538], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' will', ' wasn'])\n",
      "(tensor([0.3329, 0.0787, 0.0719, 0.0528, 0.0473], grad_fn=<ToCopyBackward0>), [' a', '.', ' not', ' something', ','])\n",
      "(tensor([0.2813, 0.0990, 0.0625, 0.0549, 0.0454], grad_fn=<ToCopyBackward0>), [' a', ' as', ' even', '.', ' the'])\n",
      "(tensor([0.3804, 0.1265, 0.1196, 0.1147, 0.0234], grad_fn=<ToCopyBackward0>), [' sequel', ' great', ' remake', ' good', ' big'])\n",
      "(tensor([0.7713, 0.0778, 0.0398, 0.0206, 0.0144], grad_fn=<ToCopyBackward0>), [' to', ',', '.', ' at', ' but'])\n",
      "(tensor([0.4237, 0.1084, 0.0468, 0.0454, 0.0341], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' any', ' this'])\n",
      "(tensor([0.2534, 0.1697, 0.1235, 0.0773, 0.0590], grad_fn=<ToCopyBackward0>), [' original', ' great', ' best', ' good', ' excellent'])\n",
      "(tensor([0.9107, 0.0109, 0.0075, 0.0039, 0.0020], grad_fn=<ToCopyBackward0>), [' movie', ' original', ' book', ' one', ' movies'])\n",
      "(tensor([0.5037, 0.2475, 0.0516, 0.0248, 0.0241], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' because', ' I'])\n",
      "(tensor([0.3656, 0.0981, 0.0458, 0.0383, 0.0364], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' But'])\n",
      "(tensor([0.1170, 0.0752, 0.0723, 0.0634, 0.0589], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' don', ' just', ' really'])\n",
      "(tensor([0.4072, 0.1928, 0.1129, 0.0430, 0.0174], grad_fn=<ToCopyBackward0>), [' sure', ' not', ' just', ' a', ' really'])\n",
      "(tensor([0.3173, 0.1278, 0.1177, 0.0867, 0.0791], grad_fn=<ToCopyBackward0>), [' it', ' that', ' the', ' there', ' they'])\n",
      "(tensor([0.6869, 0.1159, 0.0779, 0.0215, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' had', ' is'])\n",
      "(tensor([0.3864, 0.2382, 0.0674, 0.0379, 0.0336], grad_fn=<ToCopyBackward0>), [' a', ' not', ' something', ' just', ' more'])\n",
      "(tensor([0.1368, 0.1291, 0.0662, 0.0622, 0.0528], grad_fn=<ToCopyBackward0>), [' good', ' really', ' sequel', ' very', ' totally'])\n",
      "(tensor([0.6877, 0.0743, 0.0264, 0.0127, 0.0107], grad_fn=<ToCopyBackward0>), [' original', ' different', ' new', ' unrelated', ' weird'])\n",
      "(tensor([0.8866, 0.0444, 0.0159, 0.0087, 0.0065], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' thing', ' kind'])\n",
      "(tensor([0.5256, 0.1404, 0.0500, 0.0386, 0.0330], grad_fn=<ToCopyBackward0>), ['.', ',', ' than', ' from', ' with'])\n",
      "(tensor([0.2444, 0.0987, 0.0602, 0.0411, 0.0383], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.1784, 0.1132, 0.0737, 0.0566, 0.0558], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' can', ' was', ' really'])\n",
      "(tensor([0.3018, 0.2921, 0.0973, 0.0289, 0.0265], grad_fn=<ToCopyBackward0>), [' sure', ' not', ' just', ' a', ' really'])\n",
      "(tensor([0.3223, 0.2364, 0.0641, 0.0463, 0.0448], grad_fn=<ToCopyBackward0>), [' sure', ' even', ' a', ' quite', ' going'])\n",
      "(tensor([0.2192, 0.1321, 0.1013, 0.0721, 0.0605], grad_fn=<ToCopyBackward0>), [' if', '.', ' it', ' what', ' that'])\n",
      "(tensor([0.4723, 0.0991, 0.0773, 0.0660, 0.0533], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' that', ' this'])\n",
      "(tensor([0.0897, 0.0806, 0.0789, 0.0576, 0.0554], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' even', ' liked', ' was'])\n",
      "(tensor([0.6769, 0.1533, 0.0502, 0.0437, 0.0211], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' any'])\n",
      "(tensor([0.3082, 0.1971, 0.1895, 0.0757, 0.0408], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' at', ' as'])\n",
      "(tensor([0.4728, 0.0923, 0.0519, 0.0367, 0.0255], grad_fn=<ToCopyBackward0>), [' I', ' It', ' Maybe', 'I', ' The'])\n",
      "(tensor([0.1261, 0.0713, 0.0704, 0.0700, 0.0640], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' really', ' don', ' was'])\n",
      "(tensor([9.9676e-01, 5.4846e-04, 4.5642e-04, 2.5229e-04, 1.5378e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3377, 0.1604, 0.1388, 0.1205, 0.0643], grad_fn=<ToCopyBackward0>), [' know', ' think', ' really', ' remember', ' even'])\n",
      "(tensor([0.3205, 0.2528, 0.1107, 0.0633, 0.0391], grad_fn=<ToCopyBackward0>), [' remember', ' know', ' like', ' think', ' have'])\n",
      "(tensor([0.3728, 0.1918, 0.1312, 0.0403, 0.0394], grad_fn=<ToCopyBackward0>), [' it', ' the', '.', ' much', ' that'])\n",
      "(tensor([0.2996, 0.1060, 0.0629, 0.0550, 0.0334], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' Maybe', ' The'])\n",
      "(tensor([0.3053, 0.1565, 0.0682, 0.0353, 0.0273], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' that'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. It's not even funny. There's a guy who plays a genie. He comes into this town. He's got magic powers. And he's supposed to make a film. He makes a documentary about it.\n",
      "(tensor([0.3834, 0.1724, 0.0902, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2449, 0.1958, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7770, 0.0502, 0.0283, 0.0262, 0.0100], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1267, 0.1050, 0.0742, 0.0603], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5982, 0.0806, 0.0744, 0.0363, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2062, 0.1829, 0.1379, 0.0233, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.2557, 0.1761, 0.0682, 0.0656, 0.0397], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' is'])\n",
      "(tensor([0.2347, 0.0945, 0.0671, 0.0630, 0.0562], grad_fn=<ToCopyBackward0>), [' not', ' a', ' like', ' one', ' just'])\n",
      "(tensor([0.5789, 0.0793, 0.0614, 0.0409, 0.0316], grad_fn=<ToCopyBackward0>), [' even', ' as', ' funny', ' that', ' a'])\n",
      "(tensor([0.5284, 0.0570, 0.0338, 0.0313, 0.0287], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' a', ' entertaining', ' worth'])\n",
      "(tensor([0.5128, 0.1308, 0.0572, 0.0404, 0.0364], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' in', ' because'])\n",
      "(tensor([0.2579, 0.1572, 0.1451, 0.0269, 0.0155], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' This'])\n",
      "(tensor([0.2931, 0.2021, 0.1883, 0.1763, 0.0594], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' are', ' was', ' were'])\n",
      "(tensor([0.3000, 0.1271, 0.1135, 0.1060, 0.0351], grad_fn=<ToCopyBackward0>), [' no', ' not', ' a', ' nothing', ' some'])\n",
      "(tensor([0.2466, 0.1929, 0.0621, 0.0415, 0.0333], grad_fn=<ToCopyBackward0>), [' guy', ' lot', ' couple', ' really', ' scene'])\n",
      "(tensor([0.4204, 0.1010, 0.1003, 0.0381, 0.0365], grad_fn=<ToCopyBackward0>), [' who', ' that', ' named', ' with', ' called'])\n",
      "(tensor([0.0931, 0.0677, 0.0622, 0.0552, 0.0549], grad_fn=<ToCopyBackward0>), [' plays', ' falls', \"'s\", ' is', ' thinks'])\n",
      "(tensor([0.6531, 0.0638, 0.0363, 0.0247, 0.0173], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' himself', ' his'])\n",
      "(tensor([0.0996, 0.0606, 0.0509, 0.0208, 0.0206], grad_fn=<ToCopyBackward0>), [' gen', ' guy', ' really', ' doctor', ' character'])\n",
      "(tensor([0.9448, 0.0179, 0.0130, 0.0073, 0.0013], grad_fn=<ToCopyBackward0>), ['ie', 'oc', 'ial', 'ious', 'ki'])\n",
      "(tensor([0.5685, 0.0572, 0.0536, 0.0520, 0.0508], grad_fn=<ToCopyBackward0>), [' who', ' named', ' that', '.', ','])\n",
      "(tensor([0.3751, 0.0950, 0.0575, 0.0550, 0.0492], grad_fn=<ToCopyBackward0>), [' He', ' The', ' And', ' There', ' I'])\n",
      "(tensor([0.1508, 0.1082, 0.0569, 0.0482, 0.0381], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' comes', ' can', ' wants'])\n",
      "(tensor([0.2529, 0.2126, 0.1197, 0.0885, 0.0835], grad_fn=<ToCopyBackward0>), [' in', ' out', ' into', ' back', ' to'])\n",
      "(tensor([0.4553, 0.1806, 0.1205, 0.1146, 0.0332], grad_fn=<ToCopyBackward0>), [' a', ' the', ' your', ' this', ' my'])\n",
      "(tensor([0.1282, 0.1164, 0.0917, 0.0748, 0.0479], grad_fn=<ToCopyBackward0>), [' hotel', ' movie', ' town', ' house', ' apartment'])\n",
      "(tensor([0.3622, 0.2579, 0.0562, 0.0520, 0.0475], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' with', ' where'])\n",
      "(tensor([0.5505, 0.0913, 0.0825, 0.0291, 0.0282], grad_fn=<ToCopyBackward0>), [' He', ' The', ' And', ' I', ' There'])\n",
      "(tensor([0.1392, 0.1259, 0.1240, 0.0569, 0.0468], grad_fn=<ToCopyBackward0>), [' wants', ' has', \"'s\", ' says', ' makes'])\n",
      "(tensor([0.1592, 0.1238, 0.0743, 0.0494, 0.0311], grad_fn=<ToCopyBackward0>), [' supposed', ' a', ' got', ' not', ' been'])\n",
      "(tensor([0.3541, 0.1400, 0.0754, 0.0567, 0.0552], grad_fn=<ToCopyBackward0>), [' this', ' a', ' all', ' the', ' magic'])\n",
      "(tensor([0.7786, 0.0246, 0.0136, 0.0120, 0.0074], grad_fn=<ToCopyBackward0>), [' powers', ' eyes', ' tricks', '.', ' that'])\n",
      "(tensor([0.6660, 0.1473, 0.0512, 0.0311, 0.0107], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' to'])\n",
      "(tensor([0.5183, 0.1468, 0.0546, 0.0315, 0.0232], grad_fn=<ToCopyBackward0>), [' He', ' And', ' The', ' I', ' So'])\n",
      "(tensor([0.6080, 0.0568, 0.0449, 0.0219, 0.0218], grad_fn=<ToCopyBackward0>), [' he', ' the', ' then', ' they', ' his'])\n",
      "(tensor([0.2059, 0.1612, 0.0812, 0.0357, 0.0285], grad_fn=<ToCopyBackward0>), [\"'s\", ' wants', ' has', ' tries', ' makes'])\n",
      "(tensor([0.1463, 0.1173, 0.0830, 0.0672, 0.0428], grad_fn=<ToCopyBackward0>), [' got', ' supposed', ' been', ' trying', ' like'])\n",
      "(tensor([9.9941e-01, 1.5674e-04, 1.1741e-04, 1.6684e-05, 1.6093e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ',', ' be', ' in', ' -'])\n",
      "(tensor([0.2176, 0.1951, 0.1712, 0.0948, 0.0219], grad_fn=<ToCopyBackward0>), [' help', ' be', ' make', ' bring', ' protect'])\n",
      "(tensor([0.2630, 0.2182, 0.1029, 0.0361, 0.0348], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' all', ' everybody'])\n",
      "(tensor([0.4862, 0.2810, 0.0159, 0.0063, 0.0059], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' documentary', ' really', ' wish'])\n",
      "(tensor([0.1734, 0.1681, 0.0867, 0.0802, 0.0764], grad_fn=<ToCopyBackward0>), ['.', ' for', ' that', ' with', ' about'])\n",
      "(tensor([0.2360, 0.1614, 0.0875, 0.0664, 0.0591], grad_fn=<ToCopyBackward0>), [' And', ' He', ' But', ' The', ' So'])\n",
      "(tensor([0.2026, 0.0877, 0.0736, 0.0580, 0.0501], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' goes', ' makes', ' can'])\n",
      "(tensor([0.4134, 0.3294, 0.0535, 0.0522, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' this', ' it', ' the', ' some'])\n",
      "(tensor([0.4600, 0.2019, 0.0395, 0.0379, 0.0324], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' stupid', ' documentary', ' really'])\n",
      "(tensor([0.6255, 0.0990, 0.0928, 0.0225, 0.0127], grad_fn=<ToCopyBackward0>), [' about', ' on', '.', ' called', ' with'])\n",
      "(tensor([0.2346, 0.1949, 0.1179, 0.0962, 0.0699], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' a', ' his'])\n",
      "(tensor([0.8386, 0.0720, 0.0277, 0.0066, 0.0059], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was so bad that I thought it would be better to make a sequel. I think this is the worst movie I have ever made. It was so bad. I have never made a movie so bad, and I have made some really good\n",
      "(tensor([0.3831, 0.1724, 0.0902, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4998, 0.0602, 0.0342, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6241, 0.0399, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.5385, 0.0405, 0.0323, 0.0297, 0.0250], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' atro', ' boring', ' terrible'])\n",
      "(tensor([0.2687, 0.2584, 0.1541, 0.0618, 0.0435], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.5330, 0.1157, 0.0823, 0.0517, 0.0392], grad_fn=<ToCopyBackward0>), [' I', ' it', ' i', ' the', ' even'])\n",
      "(tensor([0.1631, 0.0699, 0.0613, 0.0440, 0.0337], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' couldn', ' just'])\n",
      "(tensor([0.3442, 0.1753, 0.1327, 0.0610, 0.0608], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' maybe', ' this'])\n",
      "(tensor([0.3268, 0.2630, 0.1548, 0.0521, 0.0416], grad_fn=<ToCopyBackward0>), [' was', ' would', ' must', ' might', ' had'])\n",
      "(tensor([0.7026, 0.0633, 0.0423, 0.0374, 0.0339], grad_fn=<ToCopyBackward0>), [' be', ' have', ' never', ' get', ' make'])\n",
      "(tensor([0.3057, 0.0727, 0.0631, 0.0531, 0.0433], grad_fn=<ToCopyBackward0>), [' funny', ' better', ' good', ' funn', ' a'])\n",
      "(tensor([0.7165, 0.0757, 0.0226, 0.0200, 0.0151], grad_fn=<ToCopyBackward0>), [' to', ' if', ' than', ' with', ' as'])\n",
      "(tensor([0.1704, 0.0888, 0.0835, 0.0466, 0.0463], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' make', ' have', ' be'])\n",
      "(tensor([0.6380, 0.0873, 0.0435, 0.0350, 0.0329], grad_fn=<ToCopyBackward0>), [' a', ' another', ' it', ' an', ' the'])\n",
      "(tensor([0.1693, 0.1559, 0.1003, 0.0331, 0.0255], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' second', ' really', ' short'])\n",
      "(tensor([0.4299, 0.1122, 0.0878, 0.0502, 0.0328], grad_fn=<ToCopyBackward0>), ['.', ' than', ' to', ',', ' that'])\n",
      "(tensor([0.2906, 0.0580, 0.0563, 0.0501, 0.0480], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' But'])\n",
      "(tensor([0.1149, 0.0904, 0.0758, 0.0402, 0.0377], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' really', ' think'])\n",
      "(tensor([0.1756, 0.1628, 0.1092, 0.0999, 0.0532], grad_fn=<ToCopyBackward0>), [' that', ' the', ' this', ' it', ' I'])\n",
      "(tensor([0.3128, 0.2480, 0.1214, 0.0686, 0.0362], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' was', ' one', ' sequel'])\n",
      "(tensor([0.4268, 0.3418, 0.0403, 0.0230, 0.0128], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' one', ' more'])\n",
      "(tensor([0.1510, 0.1449, 0.1447, 0.1412, 0.0899], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' second', ' only', ' first'])\n",
      "(tensor([0.5086, 0.3745, 0.0362, 0.0108, 0.0052], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' film', ' thing', ' adaptation'])\n",
      "(tensor([0.6323, 0.1274, 0.0781, 0.0540, 0.0236], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' i', ' of'])\n",
      "(tensor([0.5947, 0.3287, 0.0433, 0.0056, 0.0042], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' ever', ' had', ' can'])\n",
      "(tensor([0.8640, 0.1020, 0.0134, 0.0038, 0.0019], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' made', ' had', ' done'])\n",
      "(tensor([0.6099, 0.2209, 0.0316, 0.0296, 0.0130], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' had', ' been', ' directed'])\n",
      "(tensor([0.6934, 0.1047, 0.0928, 0.0188, 0.0139], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', '.\"'])\n",
      "(tensor([0.3130, 0.1364, 0.0857, 0.0466, 0.0278], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.3395, 0.2420, 0.1632, 0.0366, 0.0355], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' was', ' has', ' looks'])\n",
      "(tensor([0.2529, 0.1174, 0.0667, 0.0572, 0.0407], grad_fn=<ToCopyBackward0>), [' so', ' a', ' like', ' just', ' not'])\n",
      "(tensor([0.6668, 0.0561, 0.0367, 0.0283, 0.0191], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ' awful'])\n",
      "(tensor([0.4396, 0.2744, 0.0702, 0.0679, 0.0370], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.3616, 0.1443, 0.0820, 0.0314, 0.0199], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.0904, 0.0818, 0.0710, 0.0645, 0.0529], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' have', ' think', ' don'])\n",
      "(tensor([0.2221, 0.1886, 0.0917, 0.0825, 0.0534], grad_fn=<ToCopyBackward0>), [' never', ' to', ' made', ' a', ' been'])\n",
      "(tensor([0.2206, 0.2115, 0.1865, 0.1017, 0.0238], grad_fn=<ToCopyBackward0>), [' made', ' seen', ' been', ' had', ' done'])\n",
      "(tensor([0.7697, 0.1061, 0.0415, 0.0249, 0.0144], grad_fn=<ToCopyBackward0>), [' a', ' such', ' anything', ' any', ' something'])\n",
      "(tensor([0.5229, 0.3233, 0.0519, 0.0273, 0.0187], grad_fn=<ToCopyBackward0>), [' worse', ' movie', ' bad', ' film', ' sequel'])\n",
      "(tensor([0.2929, 0.1259, 0.1068, 0.0923, 0.0921], grad_fn=<ToCopyBackward0>), [' so', ' that', ' as', ' this', ' worse'])\n",
      "(tensor([0.8210, 0.0628, 0.0261, 0.0167, 0.0074], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ' awful'])\n",
      "(tensor([0.2887, 0.2695, 0.2077, 0.0535, 0.0425], grad_fn=<ToCopyBackward0>), [' in', ' that', '.', ',', ' before'])\n",
      "(tensor([0.2705, 0.1269, 0.1176, 0.1175, 0.0690], grad_fn=<ToCopyBackward0>), [' so', ' I', ' but', ' and', ' it'])\n",
      "(tensor([0.7058, 0.0641, 0.0370, 0.0220, 0.0156], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' so'])\n",
      "(tensor([0.3839, 0.0635, 0.0455, 0.0397, 0.0382], grad_fn=<ToCopyBackward0>), [' have', ' am', ' don', \"'m\", ' was'])\n",
      "(tensor([0.4928, 0.0837, 0.0686, 0.0607, 0.0305], grad_fn=<ToCopyBackward0>), [' made', ' been', ' done', ' never', ' to'])\n",
      "(tensor([0.3810, 0.1814, 0.0492, 0.0489, 0.0289], grad_fn=<ToCopyBackward0>), [' some', ' a', ' many', ' quite', ' very'])\n",
      "(tensor([0.2134, 0.1998, 0.1306, 0.1221, 0.0835], grad_fn=<ToCopyBackward0>), [' really', ' movies', ' very', ' pretty', ' bad'])\n",
      "(tensor([0.5052, 0.1987, 0.1412, 0.0586, 0.0247], grad_fn=<ToCopyBackward0>), [' bad', ' good', ',', ' great', ' funny'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a waste of time. I'm a huge Van Damme fan and even he couldn't save this one. The movie starts off strong with Van Damme in the lead, then he is pushed aside to the side and then completely humiliated\n",
      "(tensor([0.3849, 0.1711, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2427, 0.1962, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6513, 0.0599, 0.0362, 0.0355, 0.0264], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1367, 0.0702, 0.0658, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0856, 0.0612, 0.0531, 0.0423, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.9688, 0.0080, 0.0021, 0.0019, 0.0014], grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' because'])\n",
      "(tensor([0.5771, 0.0885, 0.0884, 0.0430, 0.0164], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' 90', ' 2'])\n",
      "(tensor([0.4436, 0.1747, 0.1162, 0.0264, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' because', '...'])\n",
      "(tensor([0.2371, 0.1609, 0.1432, 0.0242, 0.0222], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1043, 0.0796, 0.0624, 0.0461, 0.0442], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' thought', ' have'])\n",
      "(tensor([0.2223, 0.1792, 0.0641, 0.0403, 0.0353], grad_fn=<ToCopyBackward0>), [' not', ' a', ' sure', ' glad', ' so'])\n",
      "(tensor([0.4407, 0.1469, 0.1261, 0.0300, 0.0159], grad_fn=<ToCopyBackward0>), [' big', ' fan', ' huge', ' movie', ' Christian'])\n",
      "(tensor([0.1570, 0.1296, 0.0907, 0.0292, 0.0267], grad_fn=<ToCopyBackward0>), [' fan', ' Van', ' Adam', ' Steven', ' Michael'])\n",
      "(tensor([0.8791, 0.0182, 0.0163, 0.0076, 0.0059], grad_fn=<ToCopyBackward0>), [' Dam', 'esa', 'ishing', ' Hels', ' Wild'])\n",
      "(tensor([9.9312e-01, 1.4758e-03, 1.1263e-03, 5.9191e-04, 2.9165e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['me', 'm', 'en', 'mer', 'ne'])\n",
      "(tensor([0.9635, 0.0051, 0.0049, 0.0037, 0.0029], grad_fn=<ToCopyBackward0>), [' fan', ' and', ' comple', '/', ','])\n",
      "(tensor([0.5611, 0.1840, 0.0973, 0.0522, 0.0290], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' but', ' ('])\n",
      "(tensor([0.1291, 0.1183, 0.0906, 0.0446, 0.0422], grad_fn=<ToCopyBackward0>), [' this', ' I', ' even', ' thought', ' was'])\n",
      "(tensor([0.3000, 0.1435, 0.0825, 0.0494, 0.0304], grad_fn=<ToCopyBackward0>), [' though', ' he', ' thought', ' I', ' his'])\n",
      "(tensor([0.1126, 0.0859, 0.0829, 0.0780, 0.0532], grad_fn=<ToCopyBackward0>), [' doesn', ' couldn', ' can', ' has', \"'s\"])\n",
      "(tensor([9.9741e-01, 6.3031e-04, 2.8224e-04, 1.6152e-04, 1.1787e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.7359, 0.0562, 0.0193, 0.0130, 0.0126], grad_fn=<ToCopyBackward0>), [' save', ' make', ' do', ' have', ' keep'])\n",
      "(tensor([0.9038, 0.0454, 0.0237, 0.0083, 0.0028], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' Van', ' a'])\n",
      "(tensor([0.4861, 0.1648, 0.0789, 0.0250, 0.0197], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' film', '.', ' fl'])\n",
      "(tensor([0.8752, 0.0399, 0.0212, 0.0139, 0.0044], grad_fn=<ToCopyBackward0>), ['.', ' from', '!', ',', '...'])\n",
      "(tensor([0.2111, 0.1676, 0.0701, 0.0278, 0.0267], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' Van', ' This'])\n",
      "(tensor([0.1540, 0.1469, 0.0959, 0.0866, 0.0742], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' acting', ' story', ' plot'])\n",
      "(tensor([0.1671, 0.1291, 0.0529, 0.0469, 0.0441], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' starts', ' just'])\n",
      "(tensor([0.4420, 0.3949, 0.0328, 0.0217, 0.0150], grad_fn=<ToCopyBackward0>), [' out', ' off', ' with', ' of', ' well'])\n",
      "(tensor([0.1030, 0.0988, 0.0812, 0.0733, 0.0632], grad_fn=<ToCopyBackward0>), [' with', ' great', ' looking', ' strong', ' well'])\n",
      "(tensor([0.2469, 0.2279, 0.2169, 0.1193, 0.0316], grad_fn=<ToCopyBackward0>), [' and', ' but', ' with', ',', ' enough'])\n",
      "(tensor([0.5522, 0.1182, 0.0880, 0.0454, 0.0158], grad_fn=<ToCopyBackward0>), [' Van', ' a', ' the', ' some', ' an'])\n",
      "(tensor([9.9838e-01, 5.4855e-04, 2.7383e-04, 1.6647e-04, 4.9396e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' Dam', 'Dam', ' dam', ' D', ' being'])\n",
      "(tensor([9.9676e-01, 1.8935e-03, 3.2634e-04, 1.9066e-04, 1.7789e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['me', 'mes', 'm', 'men', 'mel'])\n",
      "(tensor([0.2943, 0.0805, 0.0796, 0.0771, 0.0376], grad_fn=<ToCopyBackward0>), [' as', ' and', \"'s\", ' in', ' getting'])\n",
      "(tensor([0.5814, 0.1493, 0.0771, 0.0317, 0.0313], grad_fn=<ToCopyBackward0>), [' a', ' the', ' his', ' some', ' an'])\n",
      "(tensor([0.6442, 0.0202, 0.0168, 0.0146, 0.0128], grad_fn=<ToCopyBackward0>), [' lead', ' role', ' beginning', ' opening', ' main'])\n",
      "(tensor([0.2294, 0.1778, 0.1495, 0.1372, 0.1083], grad_fn=<ToCopyBackward0>), [' and', ' role', ',', ' as', ' but'])\n",
      "(tensor([0.6715, 0.0512, 0.0197, 0.0121, 0.0084], grad_fn=<ToCopyBackward0>), [' but', ' and', ' then', ' fighting', ' he'])\n",
      "(tensor([0.2413, 0.2224, 0.0622, 0.0350, 0.0324], grad_fn=<ToCopyBackward0>), [' it', ' the', ' he', ' goes', ' falls'])\n",
      "(tensor([0.1958, 0.1125, 0.1086, 0.0694, 0.0543], grad_fn=<ToCopyBackward0>), [\"'s\", ' falls', ' gets', ' goes', ' is'])\n",
      "(tensor([0.1841, 0.0314, 0.0291, 0.0279, 0.0214], grad_fn=<ToCopyBackward0>), [' completely', ' paired', ' surrounded', ' reduced', ' pushed'])\n",
      "(tensor([0.3497, 0.1507, 0.0770, 0.0717, 0.0703], grad_fn=<ToCopyBackward0>), [' to', ' down', ' aside', ' out', ' around'])\n",
      "(tensor([0.4782, 0.1303, 0.0896, 0.0676, 0.0670], grad_fn=<ToCopyBackward0>), [' by', ' and', ' to', ' for', ' in'])\n",
      "(tensor([0.4630, 0.1102, 0.0423, 0.0415, 0.0202], grad_fn=<ToCopyBackward0>), [' be', ' the', ' a', ' make', ' \"'])\n",
      "(tensor([0.2385, 0.0809, 0.0357, 0.0300, 0.0284], grad_fn=<ToCopyBackward0>), [' side', ' point', ' second', ' bad', ' background'])\n",
      "(tensor([0.2069, 0.2046, 0.1492, 0.0813, 0.0599], grad_fn=<ToCopyBackward0>), [' and', ' by', ' for', ' to', ' of'])\n",
      "(tensor([0.1939, 0.0293, 0.0264, 0.0251, 0.0224], grad_fn=<ToCopyBackward0>), [' the', ' forgotten', ' played', ' his', ' then'])\n",
      "(tensor([0.0629, 0.0472, 0.0413, 0.0406, 0.0324], grad_fn=<ToCopyBackward0>), [' the', ' replaced', ' he', ' completely', ' pushed'])\n",
      "(tensor([0.2869, 0.0512, 0.0368, 0.0348, 0.0241], grad_fn=<ToCopyBackward0>), [' forgotten', ' dis', ' wasted', ' ignored', ' humiliated'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty funny watching this movie as a kid. I mean, this movie is just dumb! It's just stupid! I mean the acting is bad, and they have a lot of dumb plot devices, and I mean, I'm not even\n",
      "(tensor([0.3842, 0.1715, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1165, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1455, 0.0509, 0.0455, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1141, 0.0954, 0.0749, 0.0610, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.2840, 0.0858, 0.0657, 0.0654, 0.0539], grad_fn=<ToCopyBackward0>), [' when', ' how', ' to', ' watching', ' and'])\n",
      "(tensor([0.6909, 0.0933, 0.0134, 0.0065, 0.0063], grad_fn=<ToCopyBackward0>), [' this', ' the', ' all', ' Will', ' it'])\n",
      "(tensor([0.3628, 0.0838, 0.0396, 0.0350, 0.0298], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ' show', ' on'])\n",
      "(tensor([0.1946, 0.1577, 0.1327, 0.0677, 0.0429], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' at', ' as'])\n",
      "(tensor([0.9074, 0.0293, 0.0223, 0.0135, 0.0107], grad_fn=<ToCopyBackward0>), [' a', ' an', ' I', ' the', ' it'])\n",
      "(tensor([0.1993, 0.0644, 0.0515, 0.0370, 0.0364], grad_fn=<ToCopyBackward0>), [' kid', ' teenager', ' child', ' young', ' 10'])\n",
      "(tensor([0.5015, 0.2007, 0.0640, 0.0339, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' because'])\n",
      "(tensor([0.4078, 0.1069, 0.0566, 0.0169, 0.0122], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1274, 0.0668, 0.0574, 0.0565, 0.0464], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' remember', ' mean'])\n",
      "(tensor([0.4846, 0.0887, 0.0563, 0.0390, 0.0256], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' this'])\n",
      "(tensor([0.1836, 0.1293, 0.1124, 0.0647, 0.0294], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' \"'])\n",
      "(tensor([0.3570, 0.2606, 0.2367, 0.0140, 0.0120], grad_fn=<ToCopyBackward0>), [' is', ' was', ' movie', ' thing', ' kid'])\n",
      "(tensor([0.4084, 0.1416, 0.0566, 0.0409, 0.0300], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' tries'])\n",
      "(tensor([0.1817, 0.1392, 0.1225, 0.0767, 0.0582], grad_fn=<ToCopyBackward0>), [' so', ' about', ' not', ' a', ' just'])\n",
      "(tensor([0.1707, 0.0727, 0.0659, 0.0496, 0.0370], grad_fn=<ToCopyBackward0>), [' stupid', ' a', ' so', ' dumb', ' bad'])\n",
      "(tensor([0.2506, 0.1307, 0.1062, 0.0866, 0.0612], grad_fn=<ToCopyBackward0>), ['.', ' as', '!', ' fun', ' and'])\n",
      "(tensor([0.1494, 0.0937, 0.0804, 0.0346, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.6488, 0.0582, 0.0462, 0.0326, 0.0244], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' makes'])\n",
      "(tensor([0.1507, 0.1406, 0.1271, 0.1271, 0.0661], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' just', ' not', ' like'])\n",
      "(tensor([0.4697, 0.2388, 0.0503, 0.0247, 0.0191], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' a', ' silly', ' so'])\n",
      "(tensor([0.4760, 0.1400, 0.1320, 0.0900, 0.0217], grad_fn=<ToCopyBackward0>), ['!', ' and', '.', ',', ' fun'])\n",
      "(tensor([0.1958, 0.1251, 0.0732, 0.0574, 0.0248], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.2100, 0.0776, 0.0672, 0.0625, 0.0528], grad_fn=<ToCopyBackward0>), [' mean', \"'m\", ' was', ' don', ' can'])\n",
      "(tensor([0.7660, 0.0382, 0.0249, 0.0180, 0.0136], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' this', '...'])\n",
      "(tensor([0.1769, 0.0651, 0.0628, 0.0615, 0.0482], grad_fn=<ToCopyBackward0>), [' acting', ' whole', ' plot', ' guy', ' idea'])\n",
      "(tensor([0.4941, 0.1866, 0.1267, 0.0215, 0.0179], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', \"'s\", ' sucks'])\n",
      "(tensor([0.2234, 0.1223, 0.1201, 0.0643, 0.0511], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' bad', ' terrible', ' so'])\n",
      "(tensor([0.3987, 0.1527, 0.1515, 0.1350, 0.0279], grad_fn=<ToCopyBackward0>), [',', '!', '.', ' and', '...'])\n",
      "(tensor([0.6217, 0.0987, 0.0639, 0.0603, 0.0316], grad_fn=<ToCopyBackward0>), [' the', ' but', ' it', ' and', ' there'])\n",
      "(tensor([0.5914, 0.1090, 0.0673, 0.0384, 0.0283], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' there', ' they'])\n",
      "(tensor([0.1079, 0.0733, 0.0723, 0.0697, 0.0635], grad_fn=<ToCopyBackward0>), [\"'re\", ' try', ' should', ' have', ' show'])\n",
      "(tensor([0.1358, 0.1085, 0.0947, 0.0852, 0.0699], grad_fn=<ToCopyBackward0>), [' to', ' a', ' stupid', ' no', ' the'])\n",
      "(tensor([0.3643, 0.1136, 0.0655, 0.0535, 0.0309], grad_fn=<ToCopyBackward0>), [' stupid', ' really', ' dumb', ' lot', ' guy'])\n",
      "(tensor([9.8482e-01, 7.0771e-03, 3.0467e-03, 4.4704e-04, 3.9404e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' to', ' more', ' wrong', ' less'])\n",
      "(tensor([0.0668, 0.0526, 0.0299, 0.0288, 0.0268], grad_fn=<ToCopyBackward0>), [' dumb', ' people', ' nudity', ' stupid', ' lame'])\n",
      "(tensor([0.2878, 0.0640, 0.0545, 0.0412, 0.0327], grad_fn=<ToCopyBackward0>), [' plot', ' music', ' lines', ' dialogue', ' special'])\n",
      "(tensor([0.2368, 0.1089, 0.1059, 0.0708, 0.0660], grad_fn=<ToCopyBackward0>), [' points', ' devices', ' twists', ' lines', ' holes'])\n",
      "(tensor([0.2810, 0.2501, 0.1293, 0.0542, 0.0516], grad_fn=<ToCopyBackward0>), [',', '.', '!', ' and', ' in'])\n",
      "(tensor([0.4580, 0.3614, 0.0487, 0.0099, 0.0090], grad_fn=<ToCopyBackward0>), [' but', ' and', ' like', ' the', ' so'])\n",
      "(tensor([0.2374, 0.2096, 0.1277, 0.0482, 0.0416], grad_fn=<ToCopyBackward0>), [' the', ' it', ' they', ' I', ' there'])\n",
      "(tensor([0.2114, 0.1007, 0.0908, 0.0825, 0.0712], grad_fn=<ToCopyBackward0>), [' just', ' don', ' think', \"'m\", ' mean'])\n",
      "(tensor([0.2854, 0.1054, 0.0775, 0.0465, 0.0368], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' really', ' this'])\n",
      "(tensor([0.1764, 0.1219, 0.1072, 0.0858, 0.0539], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' what'])\n",
      "(tensor([0.1313, 0.0989, 0.0654, 0.0520, 0.0440], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' think', ' can', ' know'])\n",
      "(tensor([0.3431, 0.2119, 0.0949, 0.0514, 0.0266], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' still'])\n",
      "(tensor([0.3880, 0.1091, 0.1037, 0.0560, 0.0342], grad_fn=<ToCopyBackward0>), [' even', ' a', ' going', ' one', ' gonna'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I was going to be in for another long night watching this. This one was a bit better than my first. I really thought I could do better. I was really looking forward to watching this. I really enjoyed it. I think I could\n",
      "(tensor([0.3839, 0.1717, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3312, 0.2363, 0.0584, 0.0561, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2146, 0.1253, 0.1123, 0.0826, 0.0740], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.4719, 0.1360, 0.0312, 0.0289, 0.0225], grad_fn=<ToCopyBackward0>), [' going', ' in', ' so', ' watching', ' a'])\n",
      "(tensor([9.8503e-01, 2.6783e-03, 1.9229e-03, 1.1664e-03, 9.1092e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' through', ' in', ' into'])\n",
      "(tensor([0.2914, 0.2073, 0.1065, 0.0924, 0.0713], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' get', ' see', ' have'])\n",
      "(tensor([0.0819, 0.0704, 0.0492, 0.0424, 0.0414], grad_fn=<ToCopyBackward0>), [' a', ' in', ' very', ' the', ' able'])\n",
      "(tensor([0.2841, 0.2381, 0.0847, 0.0720, 0.0594], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' this', ' trouble'])\n",
      "(tensor([0.7160, 0.0795, 0.0248, 0.0208, 0.0189], grad_fn=<ToCopyBackward0>), [' a', ' an', ' another', ' some', ' the'])\n",
      "(tensor([0.1017, 0.0496, 0.0488, 0.0414, 0.0312], grad_fn=<ToCopyBackward0>), [' movie', ' long', ' torture', ' one', ' disappointment'])\n",
      "(tensor([0.1668, 0.0759, 0.0285, 0.0276, 0.0201], grad_fn=<ToCopyBackward0>), [' night', ' day', ' 90', ' summer', ' movie'])\n",
      "(tensor([0.2476, 0.2436, 0.1166, 0.0399, 0.0343], grad_fn=<ToCopyBackward0>), [' watching', ' of', '.', ' when', ','])\n",
      "(tensor([0.8653, 0.0365, 0.0196, 0.0171, 0.0051], grad_fn=<ToCopyBackward0>), [' this', ' the', ' paint', ' it', ' a'])\n",
      "(tensor([0.3632, 0.1246, 0.1207, 0.0573, 0.0257], grad_fn=<ToCopyBackward0>), [' movie', ' film', '.', ' one', ' thing'])\n",
      "(tensor([0.2964, 0.1203, 0.0892, 0.0251, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' But'])\n",
      "(tensor([0.3603, 0.2100, 0.1762, 0.0587, 0.0350], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4552, 0.0906, 0.0387, 0.0340, 0.0233], grad_fn=<ToCopyBackward0>), [' was', ' is', ' really', ' had', ' however'])\n",
      "(tensor([0.0941, 0.0818, 0.0584, 0.0493, 0.0482], grad_fn=<ToCopyBackward0>), [' really', ' a', ' just', ' supposed', ' so'])\n",
      "(tensor([0.3201, 0.2044, 0.0695, 0.0321, 0.0225], grad_fn=<ToCopyBackward0>), [' little', ' real', ' bit', ' complete', ' lot'])\n",
      "(tensor([0.3693, 0.1108, 0.1107, 0.0863, 0.0532], grad_fn=<ToCopyBackward0>), [' more', ' longer', ' better', ' of', ' easier'])\n",
      "(tensor([0.4116, 0.3957, 0.0363, 0.0330, 0.0329], grad_fn=<ToCopyBackward0>), [' than', '.', ',', ' but', ' as'])\n",
      "(tensor([0.7408, 0.0500, 0.0407, 0.0361, 0.0203], grad_fn=<ToCopyBackward0>), [' the', ' I', ' most', ' my', ' any'])\n",
      "(tensor([0.3249, 0.1309, 0.0869, 0.0640, 0.0596], grad_fn=<ToCopyBackward0>), [' other', ' first', ' others', ' previous', ' last'])\n",
      "(tensor([0.2931, 0.0987, 0.0769, 0.0565, 0.0475], grad_fn=<ToCopyBackward0>), [' one', ' attempt', '.', ' viewing', ' review'])\n",
      "(tensor([0.3204, 0.2109, 0.0859, 0.0342, 0.0242], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'I'])\n",
      "(tensor([0.1566, 0.0809, 0.0585, 0.0490, 0.0424], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' actually'])\n",
      "(tensor([0.1238, 0.0898, 0.0679, 0.0664, 0.0445], grad_fn=<ToCopyBackward0>), [' enjoyed', ' thought', ' like', ' wanted', ' wish'])\n",
      "(tensor([0.3727, 0.2255, 0.1603, 0.0690, 0.0549], grad_fn=<ToCopyBackward0>), [' I', ' that', ' this', ' it', ' the'])\n",
      "(tensor([0.6601, 0.1122, 0.0768, 0.0467, 0.0349], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', \"'d\", ' could'])\n",
      "(tensor([0.1813, 0.1228, 0.1100, 0.0772, 0.0555], grad_fn=<ToCopyBackward0>), [' see', ' get', ' make', ' have', ' do'])\n",
      "(tensor([0.5793, 0.1230, 0.0629, 0.0314, 0.0281], grad_fn=<ToCopyBackward0>), [' better', ' a', ' it', ' something', ' some'])\n",
      "(tensor([0.5308, 0.2356, 0.0633, 0.0282, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' but', ' with'])\n",
      "(tensor([0.4411, 0.0838, 0.0567, 0.0501, 0.0288], grad_fn=<ToCopyBackward0>), [' I', ' The', 'I', ' It', ' But'])\n",
      "(tensor([0.1570, 0.1108, 0.0648, 0.0491, 0.0421], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' actually'])\n",
      "(tensor([0.2396, 0.2027, 0.0440, 0.0336, 0.0303], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' so', ' very', ' actually'])\n",
      "(tensor([0.4100, 0.0900, 0.0620, 0.0298, 0.0265], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' hoping', ' not', ' in'])\n",
      "(tensor([9.9693e-01, 1.1204e-03, 6.0906e-04, 5.6166e-04, 1.9581e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' up'])\n",
      "(tensor([9.9050e-01, 2.0298e-03, 1.0432e-03, 1.0158e-03, 8.7772e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', '.', ' too'])\n",
      "(tensor([0.2583, 0.2130, 0.1626, 0.0915, 0.0683], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' watching', ' seeing'])\n",
      "(tensor([0.4106, 0.3194, 0.1426, 0.0179, 0.0099], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3424, 0.1793, 0.1338, 0.0354, 0.0190], grad_fn=<ToCopyBackward0>), [' one', ' because', '.', ' movie', ','])\n",
      "(tensor([0.3733, 0.1032, 0.0711, 0.0437, 0.0400], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', 'I'])\n",
      "(tensor([0.2051, 0.1702, 0.0982, 0.0401, 0.0298], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' watched'])\n",
      "(tensor([0.1634, 0.1624, 0.1446, 0.0594, 0.0467], grad_fn=<ToCopyBackward0>), [' thought', ' wanted', ' was', ' enjoyed', ' did'])\n",
      "(tensor([0.3752, 0.2667, 0.1254, 0.1152, 0.0129], grad_fn=<ToCopyBackward0>), [' it', ' watching', ' the', ' this', ' seeing'])\n",
      "(tensor([0.4683, 0.1229, 0.1090, 0.0461, 0.0346], grad_fn=<ToCopyBackward0>), ['.', ',', ' though', ' and', ' a'])\n",
      "(tensor([0.4252, 0.1753, 0.0715, 0.0341, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2129, 0.1933, 0.0928, 0.0475, 0.0276], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' think', ' just'])\n",
      "(tensor([0.3794, 0.1359, 0.1093, 0.0998, 0.0962], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' this'])\n",
      "(tensor([0.1485, 0.0705, 0.0698, 0.0477, 0.0471], grad_fn=<ToCopyBackward0>), [' was', ' enjoyed', ' could', ' did', ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a good movie, and I thought that it was a very interesting story, but I don't think I can really give it a 10. Maybe I'm being too generous. I don't really have anything to say about it.\n",
      "(tensor([0.3840, 0.1718, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3317, 0.2366, 0.0581, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4144, 0.1967, 0.1693, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6391, 0.0586, 0.0580, 0.0558, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1006, 0.0720, 0.0702, 0.0442, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1287, 0.0778, 0.0562, 0.0513, 0.0483], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2105, 0.1407, 0.0736, 0.0603, 0.0574], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' example', ' way'])\n",
      "(tensor([0.3457, 0.0968, 0.0940, 0.0760, 0.0391], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', ' in'])\n",
      "(tensor([0.5755, 0.0391, 0.0249, 0.0234, 0.0174], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' so', ' a'])\n",
      "(tensor([0.4274, 0.1462, 0.1058, 0.0434, 0.0252], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' then'])\n",
      "(tensor([0.1724, 0.1127, 0.1071, 0.0390, 0.0338], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' think'])\n",
      "(tensor([0.4767, 0.2742, 0.1140, 0.0340, 0.0116], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.3824, 0.2129, 0.1400, 0.0268, 0.0161], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.4560, 0.0723, 0.0722, 0.0527, 0.0352], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' had', ' represented'])\n",
      "(tensor([0.2337, 0.0953, 0.0600, 0.0443, 0.0380], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' interesting', ' well', ' very'])\n",
      "(tensor([0.3720, 0.0929, 0.0516, 0.0356, 0.0343], grad_fn=<ToCopyBackward0>), [' good', ' very', ' really', ' great', ' bad'])\n",
      "(tensor([0.1512, 0.1131, 0.0833, 0.0788, 0.0692], grad_fn=<ToCopyBackward0>), [' good', ' well', ' interesting', ' funny', ' boring'])\n",
      "(tensor([0.4249, 0.3415, 0.0711, 0.0192, 0.0143], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' story', ' concept', ' and'])\n",
      "(tensor([0.3245, 0.1503, 0.1026, 0.0965, 0.0606], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' with'])\n",
      "(tensor([0.5579, 0.2443, 0.0213, 0.0117, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' very', ' which'])\n",
      "(tensor([0.1797, 0.1109, 0.1043, 0.0536, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' when'])\n",
      "(tensor([0.1349, 0.1165, 0.0710, 0.0661, 0.0594], grad_fn=<ToCopyBackward0>), [' just', ' was', ' really', ' didn', ' don'])\n",
      "(tensor([9.9280e-01, 3.1983e-03, 1.0797e-03, 6.7266e-04, 2.4050e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.4325, 0.1628, 0.1289, 0.0505, 0.0265], grad_fn=<ToCopyBackward0>), [' think', ' really', ' know', ' like', ' necessarily'])\n",
      "(tensor([0.5464, 0.2242, 0.0618, 0.0497, 0.0252], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ' this', ' the'])\n",
      "(tensor([0.0945, 0.0926, 0.0706, 0.0695, 0.0645], grad_fn=<ToCopyBackward0>), [' would', ' can', \"'m\", ' was', ' could'])\n",
      "(tensor([0.2893, 0.2565, 0.0547, 0.0458, 0.0239], grad_fn=<ToCopyBackward0>), [' recommend', ' really', ' give', ' say', ' ever'])\n",
      "(tensor([0.2059, 0.1315, 0.0614, 0.0427, 0.0422], grad_fn=<ToCopyBackward0>), [' recommend', ' say', ' make', ' give', ' comment'])\n",
      "(tensor([0.5849, 0.1269, 0.0796, 0.0340, 0.0265], grad_fn=<ToCopyBackward0>), [' it', ' a', ' away', ' any', ' an'])\n",
      "(tensor([0.4601, 0.1569, 0.0742, 0.0482, 0.0398], grad_fn=<ToCopyBackward0>), [' a', ' more', ' away', ' any', ' an'])\n",
      "(tensor([0.1635, 0.0740, 0.0583, 0.0328, 0.0312], grad_fn=<ToCopyBackward0>), [' rating', ' vote', ' review', ' good', ' 10'])\n",
      "(tensor([0.5933, 0.0927, 0.0685, 0.0545, 0.0297], grad_fn=<ToCopyBackward0>), ['.', ' because', '/', ' out', ','])\n",
      "(tensor([0.2966, 0.1321, 0.0449, 0.0316, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Maybe', ' This'])\n",
      "(tensor([0.3134, 0.1787, 0.1322, 0.0537, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' I', ' it', ' if', ' that'])\n",
      "(tensor([0.2046, 0.1946, 0.0905, 0.0780, 0.0584], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' just', ' was', \"'ll\"])\n",
      "(tensor([0.1901, 0.1221, 0.1199, 0.0790, 0.0629], grad_fn=<ToCopyBackward0>), [' being', ' not', ' wrong', ' too', ' a'])\n",
      "(tensor([0.7129, 0.0887, 0.0222, 0.0210, 0.0115], grad_fn=<ToCopyBackward0>), [' too', ' a', ' hard', ' very', ' overly'])\n",
      "(tensor([0.2683, 0.2041, 0.0916, 0.0498, 0.0414], grad_fn=<ToCopyBackward0>), [' hard', ' generous', ' kind', ' nice', ' harsh'])\n",
      "(tensor([0.4992, 0.1458, 0.0837, 0.0545, 0.0372], grad_fn=<ToCopyBackward0>), ['.', ',', ' here', ' in', ' to'])\n",
      "(tensor([0.3109, 0.0964, 0.0846, 0.0667, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' It', ' Maybe', ' But', ' The'])\n",
      "(tensor([0.1263, 0.0772, 0.0733, 0.0683, 0.0618], grad_fn=<ToCopyBackward0>), [' can', \"'m\", ' just', ' really', ' don'])\n",
      "(tensor([9.9769e-01, 3.8526e-04, 3.1883e-04, 2.0237e-04, 1.1644e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3586, 0.2709, 0.2335, 0.0204, 0.0190], grad_fn=<ToCopyBackward0>), [' think', ' really', ' know', ' even', ' like'])\n",
      "(tensor([0.3338, 0.1853, 0.0826, 0.0709, 0.0667], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' have', ' understand'])\n",
      "(tensor([0.4197, 0.2119, 0.0746, 0.0690, 0.0529], grad_fn=<ToCopyBackward0>), [' a', ' an', ' anything', ' much', ' any'])\n",
      "(tensor([0.1974, 0.1671, 0.0613, 0.0580, 0.0540], grad_fn=<ToCopyBackward0>), [' good', ' to', ' more', ' positive', ' else'])\n",
      "(tensor([0.5766, 0.1614, 0.0383, 0.0365, 0.0286], grad_fn=<ToCopyBackward0>), [' say', ' compare', ' add', ' really', ' comment'])\n",
      "(tensor([0.7117, 0.0515, 0.0410, 0.0307, 0.0202], grad_fn=<ToCopyBackward0>), [' about', ' that', ' other', ' beyond', '.'])\n",
      "(tensor([0.7464, 0.1291, 0.0740, 0.0182, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' how'])\n",
      "(tensor([0.6110, 0.1920, 0.0547, 0.0274, 0.0150], grad_fn=<ToCopyBackward0>), ['.', ',', ' other', ' that', ' at'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was funny when this movie came out. I was in grade 11 when this was released. I'm still trying to get over how stupid it is. I can't even believe that people actually liked this. It has a really bad premise, and\n",
      "(tensor([0.3836, 0.1720, 0.0902, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7134, 0.1164, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1837, 0.1462, 0.0507, 0.0452, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.2108, 0.1262, 0.0831, 0.0781, 0.0706], grad_fn=<ToCopyBackward0>), [' when', ' to', ' at', ' how', ' that'])\n",
      "(tensor([0.4422, 0.1595, 0.0853, 0.0691, 0.0330], grad_fn=<ToCopyBackward0>), [' I', ' this', ' i', ' the', ' a'])\n",
      "(tensor([0.3490, 0.0386, 0.0298, 0.0285, 0.0282], grad_fn=<ToCopyBackward0>), [' movie', ' show', ' film', ' guy', ' was'])\n",
      "(tensor([0.2992, 0.2352, 0.1652, 0.0195, 0.0160], grad_fn=<ToCopyBackward0>), [' started', ' was', ' came', ' starts', ' finally'])\n",
      "(tensor([9.9408e-01, 2.5609e-03, 1.1112e-03, 6.1794e-04, 2.8323e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' out', ' on', ' up', ' along', ' about'])\n",
      "(tensor([0.1981, 0.1881, 0.1839, 0.0741, 0.0640], grad_fn=<ToCopyBackward0>), [',', ' in', '.', ' because', ' and'])\n",
      "(tensor([0.5284, 0.0942, 0.0392, 0.0201, 0.0133], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1619, 0.1018, 0.0552, 0.0483, 0.0454], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' mean', ' think'])\n",
      "(tensor([0.1497, 0.0898, 0.0719, 0.0660, 0.0456], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' in', ' so', ' like'])\n",
      "(tensor([0.2988, 0.1873, 0.1219, 0.0338, 0.0219], grad_fn=<ToCopyBackward0>), [' the', ' a', ' college', ' grade', ' it'])\n",
      "(tensor([0.3308, 0.1460, 0.0771, 0.0715, 0.0595], grad_fn=<ToCopyBackward0>), [' school', ' 11', ' 10', ' 6', ' 12'])\n",
      "(tensor([0.2883, 0.2812, 0.0811, 0.0510, 0.0440], grad_fn=<ToCopyBackward0>), [' when', ' at', ',', ' and', '.'])\n",
      "(tensor([0.4337, 0.2991, 0.1615, 0.0271, 0.0189], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' that'])\n",
      "(tensor([0.3290, 0.2646, 0.2396, 0.0306, 0.0125], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' came', ' one', ' thing'])\n",
      "(tensor([0.7543, 0.1049, 0.0122, 0.0111, 0.0107], grad_fn=<ToCopyBackward0>), [' released', ' made', ' in', ' filmed', ' originally'])\n",
      "(tensor([0.4043, 0.2350, 0.1539, 0.0681, 0.0451], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ' so'])\n",
      "(tensor([0.6860, 0.0646, 0.0264, 0.0242, 0.0167], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' We'])\n",
      "(tensor([0.1582, 0.1151, 0.0971, 0.0740, 0.0334], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' remember', \"'m\", ' still'])\n",
      "(tensor([0.2947, 0.1115, 0.0465, 0.0445, 0.0316], grad_fn=<ToCopyBackward0>), [' still', ' not', ' a', ' in', ' now'])\n",
      "(tensor([0.2141, 0.1142, 0.0549, 0.0487, 0.0329], grad_fn=<ToCopyBackward0>), [' in', ' trying', ' friends', ' a', ' not'])\n",
      "(tensor([9.9855e-01, 3.4107e-04, 1.2451e-04, 9.1226e-05, 6.4224e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' not', ' really', ' hard', ','])\n",
      "(tensor([0.2182, 0.1401, 0.0941, 0.0864, 0.0337], grad_fn=<ToCopyBackward0>), [' get', ' figure', ' understand', ' find', ' believe'])\n",
      "(tensor([0.2289, 0.1877, 0.1429, 0.0648, 0.0400], grad_fn=<ToCopyBackward0>), [' the', ' my', ' over', ' all', ' back'])\n",
      "(tensor([0.4120, 0.2923, 0.1346, 0.0566, 0.0103], grad_fn=<ToCopyBackward0>), [' it', ' how', ' the', ' that', ' this'])\n",
      "(tensor([0.3708, 0.1495, 0.0927, 0.0521, 0.0286], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' much', ' terrible', ' awful'])\n",
      "(tensor([0.4413, 0.3102, 0.0580, 0.0399, 0.0283], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' and'])\n",
      "(tensor([0.3376, 0.3214, 0.1581, 0.0425, 0.0396], grad_fn=<ToCopyBackward0>), [' is', ' was', ' really', ' all', ' actually'])\n",
      "(tensor([0.5064, 0.1237, 0.1026, 0.0332, 0.0289], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' in', ' to'])\n",
      "(tensor([0.3182, 0.1141, 0.0972, 0.0224, 0.0170], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1323, 0.0817, 0.0575, 0.0529, 0.0477], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' don', ' think'])\n",
      "(tensor([0.6813, 0.0376, 0.0344, 0.0252, 0.0236], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' tell', ' honestly', ' only'])\n",
      "(tensor([0.6150, 0.1181, 0.0468, 0.0221, 0.0189], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' understand', ' imagine', ' really'])\n",
      "(tensor([0.1524, 0.1203, 0.1133, 0.1107, 0.0935], grad_fn=<ToCopyBackward0>), [' describe', ' believe', ' explain', ' tell', ' begin'])\n",
      "(tensor([0.4916, 0.0952, 0.0678, 0.0458, 0.0452], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' how', ' I'])\n",
      "(tensor([0.1894, 0.0929, 0.0511, 0.0508, 0.0408], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' they', ' people'])\n",
      "(tensor([0.3400, 0.1163, 0.0627, 0.0425, 0.0322], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' think', ' like', ' liked'])\n",
      "(tensor([0.2870, 0.2334, 0.1036, 0.0613, 0.0445], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' made', ' like', ' think'])\n",
      "(tensor([0.6567, 0.3010, 0.0288, 0.0060, 0.0007], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' the', ' to'])\n",
      "(tensor([0.6415, 0.1174, 0.0723, 0.0189, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ',', ' stupid'])\n",
      "(tensor([0.3737, 0.1209, 0.0798, 0.0213, 0.0179], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' They'])\n",
      "(tensor([0.6381, 0.0976, 0.0513, 0.0361, 0.0248], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' has'])\n",
      "(tensor([0.1393, 0.1093, 0.1045, 0.0796, 0.0599], grad_fn=<ToCopyBackward0>), [' no', ' the', ' a', ' to', ' absolutely'])\n",
      "(tensor([0.1284, 0.0601, 0.0395, 0.0307, 0.0286], grad_fn=<ToCopyBackward0>), [' really', ' very', ' lot', ' totally', ' couple'])\n",
      "(tensor([0.2656, 0.0997, 0.0929, 0.0688, 0.0549], grad_fn=<ToCopyBackward0>), [' cheesy', ' dumb', ' bad', ' good', ' stupid'])\n",
      "(tensor([0.1281, 0.0910, 0.0435, 0.0352, 0.0318], grad_fn=<ToCopyBackward0>), [' premise', ' cast', ' storyline', ',', ' plot'])\n",
      "(tensor([0.3737, 0.2980, 0.1291, 0.0253, 0.0174], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' that', ' to'])\n",
      "(tensor([0.3495, 0.0832, 0.0692, 0.0507, 0.0422], grad_fn=<ToCopyBackward0>), [' and', ' but', ' bad', ' the', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a very boring film. I really thought it was boring because I was watching it and I really didn't care at all what was going on in the film. The acting was terrible in this film and that was really the only reason\n",
      "(tensor([0.3844, 0.1716, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3314, 0.2368, 0.0581, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4150, 0.1963, 0.1691, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6390, 0.0587, 0.0579, 0.0559, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1006, 0.0719, 0.0701, 0.0441, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1286, 0.0781, 0.0563, 0.0512, 0.0483], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.1168, 0.0684, 0.0508, 0.0446, 0.0400], grad_fn=<ToCopyBackward0>), [' boring', ' interesting', ' good', ' well', ' bad'])\n",
      "(tensor([0.4320, 0.2730, 0.0544, 0.0509, 0.0299], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' and', ','])\n",
      "(tensor([0.3182, 0.1166, 0.1132, 0.0573, 0.0446], grad_fn=<ToCopyBackward0>), ['.', '...', ',', ' to', '....'])\n",
      "(tensor([0.1968, 0.1640, 0.1569, 0.0369, 0.0226], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' And'])\n",
      "(tensor([0.1052, 0.0841, 0.0811, 0.0524, 0.0498], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' mean', ' don'])\n",
      "(tensor([0.2234, 0.1296, 0.0987, 0.0634, 0.0539], grad_fn=<ToCopyBackward0>), [' didn', ' thought', ' don', ' wanted', ' did'])\n",
      "(tensor([0.5073, 0.2715, 0.1184, 0.0360, 0.0072], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' the', ' I'])\n",
      "(tensor([0.7196, 0.1132, 0.0279, 0.0160, 0.0135], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', ' could', ' sucked'])\n",
      "(tensor([0.2767, 0.2188, 0.0594, 0.0506, 0.0378], grad_fn=<ToCopyBackward0>), [' boring', ' a', '.', ' very', ' one'])\n",
      "(tensor([0.3144, 0.3067, 0.0923, 0.0574, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' because', ' to'])\n",
      "(tensor([0.2295, 0.1974, 0.1699, 0.0860, 0.0721], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' there', ' of'])\n",
      "(tensor([0.0907, 0.0774, 0.0595, 0.0554, 0.0516], grad_fn=<ToCopyBackward0>), [' didn', ' was', ' really', \"'m\", ' thought'])\n",
      "(tensor([0.3151, 0.1650, 0.0855, 0.0466, 0.0309], grad_fn=<ToCopyBackward0>), [' expecting', ' not', ' watching', ' so', ' looking'])\n",
      "(tensor([0.6161, 0.1708, 0.0486, 0.0403, 0.0147], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' all'])\n",
      "(tensor([0.1587, 0.1560, 0.0740, 0.0712, 0.0597], grad_fn=<ToCopyBackward0>), [' in', ' and', ' with', ' at', ' on'])\n",
      "(tensor([0.3306, 0.1161, 0.0912, 0.0554, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' it', ' there', ' thinking', ' the'])\n",
      "(tensor([0.2469, 0.1022, 0.0737, 0.0719, 0.0659], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' just', ' didn', ' really'])\n",
      "(tensor([0.2170, 0.2145, 0.0928, 0.0543, 0.0444], grad_fn=<ToCopyBackward0>), [' thought', ' didn', ' wanted', ' felt', ' was'])\n",
      "(tensor([9.9567e-01, 1.6981e-03, 4.9344e-04, 3.4433e-04, 1.5712e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2640, 0.1526, 0.1125, 0.0850, 0.0792], grad_fn=<ToCopyBackward0>), [' care', ' get', ' enjoy', ' understand', ' like'])\n",
      "(tensor([0.2816, 0.1392, 0.1274, 0.0822, 0.0728], grad_fn=<ToCopyBackward0>), [' about', ' for', ' at', ' what', '.'])\n",
      "(tensor([0.9239, 0.0371, 0.0249, 0.0060, 0.0016], grad_fn=<ToCopyBackward0>), [' all', ' the', ' that', ' any', ' first'])\n",
      "(tensor([0.4562, 0.2482, 0.0546, 0.0405, 0.0336], grad_fn=<ToCopyBackward0>), [' about', '.', ' for', ',', ' what'])\n",
      "(tensor([0.4810, 0.3468, 0.0480, 0.0267, 0.0245], grad_fn=<ToCopyBackward0>), [' happened', ' was', ' the', ' they', ' it'])\n",
      "(tensor([0.5933, 0.1775, 0.1212, 0.0308, 0.0155], grad_fn=<ToCopyBackward0>), [' going', ' happening', ' in', ' on', ' said'])\n",
      "(tensor([9.7073e-01, 2.2805e-02, 2.0222e-03, 1.7954e-03, 4.8726e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' on', ' to', ' through', ' in', ' down'])\n",
      "(tensor([0.3917, 0.3668, 0.0572, 0.0353, 0.0270], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' with', ' and'])\n",
      "(tensor([0.4280, 0.3041, 0.1166, 0.0693, 0.0509], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' there', ' that'])\n",
      "(tensor([0.7509, 0.2103, 0.0099, 0.0028, 0.0023], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' story', ' background', ' script'])\n",
      "(tensor([0.6359, 0.1053, 0.0424, 0.0330, 0.0236], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' at'])\n",
      "(tensor([0.2709, 0.1089, 0.0859, 0.0732, 0.0382], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', 'I'])\n",
      "(tensor([0.2483, 0.1326, 0.0448, 0.0404, 0.0389], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' movie', ' story', ' film'])\n",
      "(tensor([0.7792, 0.0429, 0.0360, 0.0251, 0.0251], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' in', ',', ' and'])\n",
      "(tensor([0.1089, 0.0844, 0.0746, 0.0645, 0.0498], grad_fn=<ToCopyBackward0>), [' terrible', ' not', ' so', ' very', ' bad'])\n",
      "(tensor([0.2965, 0.2625, 0.2549, 0.0338, 0.0169], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '...'])\n",
      "(tensor([0.6542, 0.1378, 0.1240, 0.0254, 0.0124], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' my'])\n",
      "(tensor([0.8596, 0.1036, 0.0141, 0.0050, 0.0025], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', '.', ','])\n",
      "(tensor([0.5848, 0.1330, 0.1004, 0.0301, 0.0197], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' as'])\n",
      "(tensor([0.3570, 0.3300, 0.0704, 0.0579, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' there'])\n",
      "(tensor([0.3257, 0.2823, 0.2114, 0.0469, 0.0153], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' really', ' just'])\n",
      "(tensor([0.4204, 0.0894, 0.0597, 0.0456, 0.0394], grad_fn=<ToCopyBackward0>), [' the', ' one', ' really', ' a', ' probably'])\n",
      "(tensor([0.2863, 0.0768, 0.0729, 0.0683, 0.0465], grad_fn=<ToCopyBackward0>), [' the', ' sad', ' boring', ' disappointing', ','])\n",
      "(tensor([0.9198, 0.0103, 0.0078, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' only', ' reason', ' main', ' most', ' one'])\n",
      "(tensor([0.4095, 0.2925, 0.0508, 0.0404, 0.0360], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' redeem', ' good', ' part'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it would be interesting to see how the characters acted in real-life. I expected a good film with a good plot and decent acting. What a disappointment. The plot was so bad that even Christopher Lee couldn't save the movie. The acting,\n",
      "(tensor([0.3848, 0.1711, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7130, 0.1166, 0.0399, 0.0102, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.9410, 0.0268, 0.0064, 0.0053, 0.0018], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' not'])\n",
      "(tensor([0.6378, 0.0575, 0.0476, 0.0393, 0.0169], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' an', ' more', ' better'])\n",
      "(tensor([0.7088, 0.1274, 0.0340, 0.0231, 0.0210], grad_fn=<ToCopyBackward0>), [' to', ' for', ' if', ' and', ' as'])\n",
      "(tensor([0.7823, 0.0215, 0.0178, 0.0144, 0.0110], grad_fn=<ToCopyBackward0>), [' see', ' review', ' compare', ' have', ' watch'])\n",
      "(tensor([0.4932, 0.1178, 0.0927, 0.0834, 0.0331], grad_fn=<ToCopyBackward0>), [' how', ' the', ' if', ' what', ' a'])\n",
      "(tensor([0.1363, 0.1168, 0.0928, 0.0736, 0.0683], grad_fn=<ToCopyBackward0>), [' many', ' much', ' other', ' the', ' far'])\n",
      "(tensor([0.1026, 0.0666, 0.0638, 0.0543, 0.0468], grad_fn=<ToCopyBackward0>), [' original', ' actors', ' other', ' characters', ' real'])\n",
      "(tensor([0.2069, 0.1148, 0.0562, 0.0487, 0.0478], grad_fn=<ToCopyBackward0>), [' acted', ' would', ' did', ' performed', ' behaved'])\n",
      "(tensor([0.4564, 0.1126, 0.1025, 0.0504, 0.0348], grad_fn=<ToCopyBackward0>), [' in', ' when', ' around', ' on', ' after'])\n",
      "(tensor([0.2752, 0.2722, 0.1165, 0.0505, 0.0402], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' real', ' their'])\n",
      "(tensor([9.1351e-01, 6.3722e-02, 8.7865e-03, 1.7124e-03, 6.7258e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' life', '-', ' time', ' space', ' LIFE'])\n",
      "(tensor([0.9211, 0.0562, 0.0113, 0.0044, 0.0017], grad_fn=<ToCopyBackward0>), ['life', 'time', 'world', 'space', 'live'])\n",
      "(tensor([0.1918, 0.1889, 0.1528, 0.0424, 0.0354], grad_fn=<ToCopyBackward0>), ['.', ',', ' situations', ' when', ' and'])\n",
      "(tensor([0.4531, 0.0999, 0.0720, 0.0238, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' This'])\n",
      "(tensor([0.0933, 0.0808, 0.0387, 0.0385, 0.0374], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' expected', ' think', ' have'])\n",
      "(tensor([0.3045, 0.1364, 0.1134, 0.0681, 0.0644], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' them', ' it'])\n",
      "(tensor([0.1430, 0.1405, 0.0811, 0.0423, 0.0321], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' lot', ' film', ' \"'])\n",
      "(tensor([0.3148, 0.3126, 0.0751, 0.0403, 0.0239], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ',', ' comedy'])\n",
      "(tensor([0.2563, 0.1593, 0.1385, 0.0989, 0.0487], grad_fn=<ToCopyBackward0>), [',', '.', ' from', ' with', ' but'])\n",
      "(tensor([0.1955, 0.1629, 0.0729, 0.0526, 0.0351], grad_fn=<ToCopyBackward0>), [' good', ' a', ' interesting', ' some', ' funny'])\n",
      "(tensor([0.4952, 0.0374, 0.0349, 0.0322, 0.0265], grad_fn=<ToCopyBackward0>), [' good', ' lot', ' decent', ' strong', ' great'])\n",
      "(tensor([0.4456, 0.1555, 0.1534, 0.0575, 0.0483], grad_fn=<ToCopyBackward0>), [' plot', ' storyline', ' story', ' cast', ' script'])\n",
      "(tensor([0.2527, 0.2362, 0.1762, 0.1348, 0.0316], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', '.', ' with'])\n",
      "(tensor([0.3310, 0.1364, 0.0767, 0.0706, 0.0338], grad_fn=<ToCopyBackward0>), [' good', ' decent', ' interesting', ' a', ' acting'])\n",
      "(tensor([0.9617, 0.0057, 0.0054, 0.0041, 0.0025], grad_fn=<ToCopyBackward0>), [' acting', ' performances', ' actors', ' special', ' casting'])\n",
      "(tensor([0.7550, 0.1232, 0.0304, 0.0161, 0.0071], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' from', ' by'])\n",
      "(tensor([0.2257, 0.1702, 0.0728, 0.0636, 0.0483], grad_fn=<ToCopyBackward0>), [' What', ' I', ' But', ' Instead', ' The'])\n",
      "(tensor([0.9579, 0.0116, 0.0063, 0.0045, 0.0042], grad_fn=<ToCopyBackward0>), [' I', ' i', ' was', ' we', ' a'])\n",
      "(tensor([0.3505, 0.1548, 0.0723, 0.0392, 0.0288], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' let', ' mistake', ' load'])\n",
      "(tensor([0.4447, 0.4046, 0.0594, 0.0264, 0.0189], grad_fn=<ToCopyBackward0>), ['.', '!', '!!', '!!!', '...'])\n",
      "(tensor([0.3229, 0.1714, 0.0539, 0.0534, 0.0433], grad_fn=<ToCopyBackward0>), [' The', ' I', ' This', ' It', 'The'])\n",
      "(tensor([0.2060, 0.1267, 0.1136, 0.0710, 0.0526], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' acting', ' characters', ' actors'])\n",
      "(tensor([0.4452, 0.2771, 0.0585, 0.0137, 0.0122], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' had'])\n",
      "(tensor([0.1075, 0.0534, 0.0439, 0.0438, 0.0367], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' so', ' terrible', ' awful'])\n",
      "(tensor([0.1705, 0.0559, 0.0420, 0.0350, 0.0246], grad_fn=<ToCopyBackward0>), [' weak', ' bad', ' predictable', ' thin', ' stupid'])\n",
      "(tensor([0.4035, 0.1728, 0.1155, 0.1106, 0.0764], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' I', ' it'])\n",
      "(tensor([0.4155, 0.1890, 0.1041, 0.0847, 0.0300], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' even', ' you'])\n",
      "(tensor([0.3676, 0.0452, 0.0440, 0.0378, 0.0346], grad_fn=<ToCopyBackward0>), [' the', ' Christopher', ' if', ' I', ' after'])\n",
      "(tensor([0.5443, 0.1886, 0.1372, 0.0224, 0.0190], grad_fn=<ToCopyBackward0>), [' Lee', ' Walk', ' Pl', ' Guest', ' Lloyd'])\n",
      "(tensor([0.2682, 0.0610, 0.0500, 0.0444, 0.0353], grad_fn=<ToCopyBackward0>), [' couldn', \"'s\", ' was', ',', ' ('])\n",
      "(tensor([9.9525e-01, 1.3625e-03, 4.6694e-04, 2.7642e-04, 1.9858e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.7473, 0.0401, 0.0383, 0.0325, 0.0213], grad_fn=<ToCopyBackward0>), [' save', ' rescue', ' make', ' salvage', ' help'])\n",
      "(tensor([0.7087, 0.1416, 0.1084, 0.0093, 0.0074], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' me', ' his'])\n",
      "(tensor([0.5207, 0.3869, 0.0221, 0.0090, 0.0056], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' day', ' picture', ' poor'])\n",
      "(tensor([0.8102, 0.0390, 0.0378, 0.0320, 0.0133], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', '!', ' for'])\n",
      "(tensor([0.4859, 0.0860, 0.0382, 0.0327, 0.0265], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3470, 0.1147, 0.0725, 0.0435, 0.0389], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' story', ' actors'])\n",
      "(tensor([0.7575, 0.0515, 0.0296, 0.0171, 0.0136], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' is', ',', '?'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a really bad movie. I was in the audience when it was released and I thought it was a really bad movie. It was a big joke in my movie when I was in college, and I was really disappointed. I'm still not\n",
      "(tensor([0.3841, 0.1716, 0.0898, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1167, 0.0397, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1841, 0.1454, 0.0509, 0.0451, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1638, 0.1161, 0.0577, 0.0491, 0.0450], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.6034, 0.0516, 0.0459, 0.0383, 0.0233], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8282, 0.0665, 0.0241, 0.0073, 0.0053], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' horror', ' sequel'])\n",
      "(tensor([0.6668, 0.0630, 0.0363, 0.0345, 0.0291], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '...', '!'])\n",
      "(tensor([0.2885, 0.1358, 0.0640, 0.0330, 0.0296], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.1346, 0.0843, 0.0826, 0.0615, 0.0498], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', ' mean', \"'m\"])\n",
      "(tensor([0.4459, 0.0942, 0.0450, 0.0394, 0.0374], grad_fn=<ToCopyBackward0>), [' really', ' very', ' in', ' so', ' not'])\n",
      "(tensor([0.3283, 0.1709, 0.1366, 0.0280, 0.0240], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' awe', ' college'])\n",
      "(tensor([0.2608, 0.1290, 0.0693, 0.0284, 0.0239], grad_fn=<ToCopyBackward0>), [' theater', ' mood', ' audience', ' middle', ' army'])\n",
      "(tensor([0.3856, 0.1077, 0.0750, 0.0401, 0.0377], grad_fn=<ToCopyBackward0>), [' when', ' for', ' at', ' the', ' with'])\n",
      "(tensor([0.2920, 0.2046, 0.1960, 0.1407, 0.0829], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' they'])\n",
      "(tensor([0.5864, 0.1485, 0.0844, 0.0529, 0.0195], grad_fn=<ToCopyBackward0>), [' was', ' came', ' first', ' opened', ' started'])\n",
      "(tensor([0.3352, 0.0790, 0.0685, 0.0680, 0.0493], grad_fn=<ToCopyBackward0>), [' released', ' first', ' being', ' in', ' originally'])\n",
      "(tensor([0.3157, 0.2484, 0.1656, 0.1316, 0.0148], grad_fn=<ToCopyBackward0>), [' in', ',', ' and', '.', ' at'])\n",
      "(tensor([0.5113, 0.0915, 0.0543, 0.0209, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' that'])\n",
      "(tensor([0.1239, 0.1119, 0.0768, 0.0767, 0.0515], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' just', ' actually'])\n",
      "(tensor([0.8235, 0.0543, 0.0377, 0.0276, 0.0111], grad_fn=<ToCopyBackward0>), [' it', ' that', ' the', ' this', ','])\n",
      "(tensor([0.9218, 0.0089, 0.0083, 0.0063, 0.0061], grad_fn=<ToCopyBackward0>), [' was', ' really', ' had', ' wasn', \"'s\"])\n",
      "(tensor([0.4147, 0.0924, 0.0484, 0.0457, 0.0404], grad_fn=<ToCopyBackward0>), [' a', ' terrible', ' one', ' the', ' really'])\n",
      "(tensor([0.6916, 0.0638, 0.0230, 0.0229, 0.0200], grad_fn=<ToCopyBackward0>), [' really', ' bad', ' very', ' pretty', ' terrible'])\n",
      "(tensor([0.7566, 0.0976, 0.0264, 0.0240, 0.0103], grad_fn=<ToCopyBackward0>), [' bad', ',', ' dumb', ' stupid', ' terrible'])\n",
      "(tensor([0.9632, 0.0182, 0.0051, 0.0016, 0.0015], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' idea'])\n",
      "(tensor([0.9032, 0.0278, 0.0203, 0.0056, 0.0050], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', '!'])\n",
      "(tensor([0.3100, 0.1090, 0.0770, 0.0431, 0.0359], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.3532, 0.2533, 0.0521, 0.0316, 0.0301], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' has', ' had'])\n",
      "(tensor([0.0962, 0.0783, 0.0602, 0.0483, 0.0459], grad_fn=<ToCopyBackward0>), [' a', ' so', ' just', ' not', ' like'])\n",
      "(tensor([0.1778, 0.1114, 0.0599, 0.0409, 0.0269], grad_fn=<ToCopyBackward0>), [' big', ' really', ' bad', ' very', ' total'])\n",
      "(tensor([0.1944, 0.1620, 0.0652, 0.0521, 0.0458], grad_fn=<ToCopyBackward0>), [' bomb', ' disappointment', ',', ' waste', ' joke'])\n",
      "(tensor([0.1680, 0.1636, 0.1239, 0.0712, 0.0626], grad_fn=<ToCopyBackward0>), [' in', ' to', ' for', ' on', '.'])\n",
      "(tensor([0.7267, 0.0395, 0.0362, 0.0252, 0.0167], grad_fn=<ToCopyBackward0>), [' the', ' Hollywood', ' my', ' some', ' a'])\n",
      "(tensor([0.1144, 0.0970, 0.0677, 0.0526, 0.0505], grad_fn=<ToCopyBackward0>), [' family', ' class', ' movie', ' book', ' household'])\n",
      "(tensor([0.4552, 0.0689, 0.0574, 0.0322, 0.0308], grad_fn=<ToCopyBackward0>), [' class', ',', ' when', ' classes', ' theater'])\n",
      "(tensor([0.5687, 0.0842, 0.0813, 0.0430, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' they', ' the', ' it', ' my'])\n",
      "(tensor([0.1583, 0.1456, 0.0682, 0.0426, 0.0313], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' saw', \"'m\", ' played'])\n",
      "(tensor([0.2605, 0.0569, 0.0445, 0.0386, 0.0375], grad_fn=<ToCopyBackward0>), [' in', ' a', ' doing', ' at', ' actually'])\n",
      "(tensor([0.3174, 0.2601, 0.0989, 0.0588, 0.0326], grad_fn=<ToCopyBackward0>), [' the', ' college', ' it', ' high', ' grade'])\n",
      "(tensor([0.4099, 0.2133, 0.1077, 0.0338, 0.0316], grad_fn=<ToCopyBackward0>), [',', '.', ' when', ' that', ' and'])\n",
      "(tensor([0.1685, 0.1088, 0.0972, 0.0742, 0.0626], grad_fn=<ToCopyBackward0>), [' and', ' I', ' so', ' when', ' but'])\n",
      "(tensor([0.6263, 0.0654, 0.0414, 0.0333, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' now', ' that'])\n",
      "(tensor([0.1505, 0.1180, 0.0885, 0.0594, 0.0520], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' really', ' just'])\n",
      "(tensor([0.4155, 0.0651, 0.0585, 0.0541, 0.0478], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' in', ' very', ' like'])\n",
      "(tensor([0.1291, 0.0901, 0.0775, 0.0710, 0.0647], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' into', ' excited', ' embarrassed'])\n",
      "(tensor([0.2220, 0.2016, 0.1973, 0.1845, 0.0677], grad_fn=<ToCopyBackward0>), [' with', ' in', ' when', '.', ' that'])\n",
      "(tensor([0.3925, 0.0950, 0.0699, 0.0447, 0.0409], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' So'])\n",
      "(tensor([0.1227, 0.1137, 0.0759, 0.0662, 0.0464], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', \"'m\", ' think'])\n",
      "(tensor([0.2058, 0.1565, 0.1274, 0.0893, 0.0417], grad_fn=<ToCopyBackward0>), [' not', ' a', ' still', ' really', ' sure'])\n",
      "(tensor([0.1823, 0.1561, 0.1459, 0.0915, 0.0784], grad_fn=<ToCopyBackward0>), [' a', ' really', ' disappointed', ' very', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a joke from the start. The acting wasn't even good enough to make it worth watching. The only thing that kept me watching was the fact that I could hear the voice of the narrator. He's just a guy who says the\n",
      "(tensor([0.3840, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2436, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6521, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0614, 0.0529, 0.0427, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.2055, 0.1292, 0.1210, 0.1075, 0.0586], grad_fn=<ToCopyBackward0>), [' when', ' until', ' from', '.', ' at'])\n",
      "(tensor([0.8153, 0.0273, 0.0101, 0.0083, 0.0076], grad_fn=<ToCopyBackward0>), [' the', ' start', ' begin', ' when', ' beginning'])\n",
      "(tensor([0.2862, 0.2696, 0.1872, 0.0440, 0.0285], grad_fn=<ToCopyBackward0>), [' start', ' get', ' beginning', ' very', ' word'])\n",
      "(tensor([0.7726, 0.0825, 0.0215, 0.0196, 0.0128], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' when', ' and'])\n",
      "(tensor([0.2079, 0.1735, 0.1073, 0.0235, 0.0176], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' What'])\n",
      "(tensor([0.0907, 0.0704, 0.0647, 0.0385, 0.0334], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' movie', ' idea'])\n",
      "(tensor([0.7478, 0.0807, 0.0319, 0.0234, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' wasn', ' and'])\n",
      "(tensor([9.9609e-01, 1.4471e-03, 3.6415e-04, 3.0534e-04, 2.1965e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2387, 0.2063, 0.1014, 0.0950, 0.0476], grad_fn=<ToCopyBackward0>), [' even', ' bad', ' funny', ' good', ' very'])\n",
      "(tensor([0.3532, 0.1252, 0.0770, 0.0525, 0.0350], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' that', ' convincing', ' close'])\n",
      "(tensor([0.4103, 0.1836, 0.1733, 0.0566, 0.0228], grad_fn=<ToCopyBackward0>), ['.', ' enough', ',', ' and', ' in'])\n",
      "(tensor([0.7921, 0.0991, 0.0441, 0.0265, 0.0084], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ',', ' and'])\n",
      "(tensor([0.4125, 0.3332, 0.0262, 0.0180, 0.0173], grad_fn=<ToCopyBackward0>), [' make', ' be', ' keep', ' save', ' get'])\n",
      "(tensor([0.4030, 0.1805, 0.0940, 0.0838, 0.0758], grad_fn=<ToCopyBackward0>), [' it', ' you', ' this', ' a', ' me'])\n",
      "(tensor([0.3653, 0.0736, 0.0653, 0.0449, 0.0275], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' bear', ' worth', ' pass'])\n",
      "(tensor([0.7853, 0.0408, 0.0316, 0.0207, 0.0163], grad_fn=<ToCopyBackward0>), [' watching', ' the', ' sitting', ' seeing', ' it'])\n",
      "(tensor([0.6015, 0.0812, 0.0461, 0.0232, 0.0172], grad_fn=<ToCopyBackward0>), ['.', ' the', ',', ' for', '...'])\n",
      "(tensor([0.3152, 0.1134, 0.0911, 0.0298, 0.0243], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' This'])\n",
      "(tensor([0.2792, 0.0933, 0.0777, 0.0509, 0.0407], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' only', ' movie', ' script'])\n",
      "(tensor([0.2575, 0.1703, 0.1044, 0.0526, 0.0371], grad_fn=<ToCopyBackward0>), [' thing', ' good', ' reason', ' funny', ' actor'])\n",
      "(tensor([0.6621, 0.0825, 0.0427, 0.0235, 0.0212], grad_fn=<ToCopyBackward0>), [' that', ' I', ' this', ' interesting', ' worse'])\n",
      "(tensor([0.3759, 0.1442, 0.1298, 0.0537, 0.0301], grad_fn=<ToCopyBackward0>), [' was', ' made', ' kept', ' could', ' would'])\n",
      "(tensor([0.8913, 0.0430, 0.0315, 0.0070, 0.0066], grad_fn=<ToCopyBackward0>), [' me', ' watching', ' my', ' this', ' it'])\n",
      "(tensor([0.9291, 0.0235, 0.0099, 0.0037, 0.0033], grad_fn=<ToCopyBackward0>), [' watching', ' from', ' interested', ' going', ' watch'])\n",
      "(tensor([0.6361, 0.1604, 0.0770, 0.0546, 0.0116], grad_fn=<ToCopyBackward0>), [' was', ' the', ' it', ' this', ' were'])\n",
      "(tensor([0.6868, 0.0491, 0.0219, 0.0202, 0.0175], grad_fn=<ToCopyBackward0>), [' the', ' that', ' to', ' watching', ' because'])\n",
      "(tensor([0.0569, 0.0564, 0.0436, 0.0225, 0.0213], grad_fn=<ToCopyBackward0>), [' plot', ' beautiful', ' fact', ' nudity', ' \"'])\n",
      "(tensor([0.9138, 0.0244, 0.0167, 0.0081, 0.0055], grad_fn=<ToCopyBackward0>), [' that', ' the', ' I', ' it', ' of'])\n",
      "(tensor([0.4444, 0.1912, 0.1220, 0.0280, 0.0276], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' there', ' they'])\n",
      "(tensor([0.2609, 0.0758, 0.0618, 0.0443, 0.0422], grad_fn=<ToCopyBackward0>), [' was', ' could', ' had', ' wanted', ' really'])\n",
      "(tensor([0.4446, 0.1044, 0.0762, 0.0446, 0.0217], grad_fn=<ToCopyBackward0>), [' see', ' not', ' tell', ' hear', ' say'])\n",
      "(tensor([0.5130, 0.0241, 0.0223, 0.0219, 0.0209], grad_fn=<ToCopyBackward0>), [' the', ' that', ' what', ' how', ' Christopher'])\n",
      "(tensor([0.0567, 0.0323, 0.0204, 0.0194, 0.0189], grad_fn=<ToCopyBackward0>), [' \"', ' two', ' other', ' little', ' voice'])\n",
      "(tensor([0.8817, 0.0298, 0.0235, 0.0066, 0.0033], grad_fn=<ToCopyBackward0>), [' of', '-', ' over', ',', ' from'])\n",
      "(tensor([0.2060, 0.0237, 0.0223, 0.0194, 0.0187], grad_fn=<ToCopyBackward0>), [' the', ' William', ' John', ' Richard', ' Michael'])\n",
      "(tensor([0.1702, 0.0509, 0.0472, 0.0406, 0.0271], grad_fn=<ToCopyBackward0>), [' narrator', ' director', ' movie', ' actor', ' actress'])\n",
      "(tensor([0.1003, 0.0973, 0.0623, 0.0600, 0.0477], grad_fn=<ToCopyBackward0>), [' saying', '.', ' talking', ',', ' ('])\n",
      "(tensor([0.2489, 0.0830, 0.0813, 0.0372, 0.0339], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' He', ' This'])\n",
      "(tensor([0.2463, 0.0859, 0.0782, 0.0631, 0.0346], grad_fn=<ToCopyBackward0>), [' was', ' sounded', \"'s\", ' kept', ' said'])\n",
      "(tensor([0.1348, 0.0723, 0.0653, 0.0480, 0.0471], grad_fn=<ToCopyBackward0>), [' the', ' supposed', ' so', ' saying', ' just'])\n",
      "(tensor([0.1034, 0.0797, 0.0632, 0.0454, 0.0343], grad_fn=<ToCopyBackward0>), [' a', ' as', ' like', ' that', ' so'])\n",
      "(tensor([0.1051, 0.1003, 0.0956, 0.0646, 0.0432], grad_fn=<ToCopyBackward0>), [' really', ' voice', ' guy', ' kid', ' little'])\n",
      "(tensor([0.2195, 0.2002, 0.1068, 0.0940, 0.0558], grad_fn=<ToCopyBackward0>), [' who', ' that', ' with', ' in', ' talking'])\n",
      "(tensor([0.1999, 0.0792, 0.0514, 0.0450, 0.0296], grad_fn=<ToCopyBackward0>), [' talks', ' says', ' narr', ' keeps', \"'s\"])\n",
      "(tensor([0.3392, 0.2126, 0.0395, 0.0354, 0.0324], grad_fn=<ToCopyBackward0>), [' \"', ' the', ' that', ' a', ' all'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was pretty atrocious, so I went to see the film for free. The first thing I found was that the director had been paid to produce this film. This film had been written by someone who also appeared in \"The Naked Gun\" and\n",
      "(tensor([0.3840, 0.1718, 0.0899, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2439, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4801, 0.1356, 0.1035, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1266, 0.0736, 0.0625, 0.0621, 0.0415], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' atro', ' awful', ' obvious'])\n",
      "(tensor([9.9925e-01, 6.2431e-05, 4.1673e-05, 3.2910e-05, 2.6897e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'pic', 'cc'])\n",
      "(tensor([0.2320, 0.1035, 0.0960, 0.0383, 0.0325], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', '!'])\n",
      "(tensor([0.1886, 0.1063, 0.0856, 0.0403, 0.0343], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' as', ' so'])\n",
      "(tensor([0.8395, 0.0233, 0.0146, 0.0138, 0.0088], grad_fn=<ToCopyBackward0>), [' I', ' i', ' it', ' the', ' when'])\n",
      "(tensor([0.0812, 0.0791, 0.0738, 0.0486, 0.0410], grad_fn=<ToCopyBackward0>), [' was', ' decided', ' went', ' thought', \"'m\"])\n",
      "(tensor([0.5620, 0.1784, 0.0614, 0.0378, 0.0198], grad_fn=<ToCopyBackward0>), [' to', ' into', ' on', ' and', ' back'])\n",
      "(tensor([0.7052, 0.0554, 0.0550, 0.0434, 0.0184], grad_fn=<ToCopyBackward0>), [' see', ' the', ' watch', ' IM', ' check'])\n",
      "(tensor([0.3806, 0.1437, 0.0545, 0.0506, 0.0500], grad_fn=<ToCopyBackward0>), [' the', ' what', ' a', ' this', ' if'])\n",
      "(tensor([0.1870, 0.1230, 0.1100, 0.0442, 0.0218], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' video', ' director', ' people'])\n",
      "(tensor([0.1199, 0.0806, 0.0796, 0.0736, 0.0614], grad_fn=<ToCopyBackward0>), [' for', ' myself', ' with', ' in', ' to'])\n",
      "(tensor([0.4258, 0.1421, 0.0815, 0.0683, 0.0434], grad_fn=<ToCopyBackward0>), [' myself', ' free', ' my', ' the', ' a'])\n",
      "(tensor([0.2281, 0.1560, 0.1440, 0.0871, 0.0561], grad_fn=<ToCopyBackward0>), [' at', '.', ' in', ' with', ' on'])\n",
      "(tensor([0.2973, 0.1259, 0.1084, 0.0380, 0.0369], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' What', ' And'])\n",
      "(tensor([0.0878, 0.0724, 0.0701, 0.0505, 0.0300], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' film', ' first', ' people'])\n",
      "(tensor([0.5302, 0.0722, 0.0354, 0.0345, 0.0206], grad_fn=<ToCopyBackward0>), [' thing', ' question', ' comment', ' scene', ' impression'])\n",
      "(tensor([0.5334, 0.4180, 0.0104, 0.0057, 0.0056], grad_fn=<ToCopyBackward0>), [' I', ' that', ' you', ' i', ' they'])\n",
      "(tensor([0.4137, 0.1211, 0.0697, 0.0500, 0.0337], grad_fn=<ToCopyBackward0>), [' noticed', ' thought', ' did', ' saw', ' found'])\n",
      "(tensor([0.6602, 0.0703, 0.0272, 0.0251, 0.0225], grad_fn=<ToCopyBackward0>), [' was', ' out', ' when', ' in', ' that'])\n",
      "(tensor([0.5627, 0.1628, 0.1024, 0.0543, 0.0170], grad_fn=<ToCopyBackward0>), [' that', ' the', ' a', ' this', ' an'])\n",
      "(tensor([0.3319, 0.2236, 0.1184, 0.0697, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' there', ' I'])\n",
      "(tensor([0.1253, 0.0779, 0.0540, 0.0299, 0.0246], grad_fn=<ToCopyBackward0>), [' director', ' film', ' movie', ' people', ' \"'])\n",
      "(tensor([0.2032, 0.1738, 0.0821, 0.0251, 0.0236], grad_fn=<ToCopyBackward0>), [' had', ' was', ',', ' of', ' and'])\n",
      "(tensor([0.1224, 0.1211, 0.0612, 0.0394, 0.0308], grad_fn=<ToCopyBackward0>), [' completely', ' made', ' been', ' cut', ' not'])\n",
      "(tensor([0.0742, 0.0720, 0.0499, 0.0380, 0.0321], grad_fn=<ToCopyBackward0>), [' fired', ' in', ' working', ' paid', ' let'])\n",
      "(tensor([0.3465, 0.1778, 0.1642, 0.0680, 0.0434], grad_fn=<ToCopyBackward0>), [' to', ' by', ' for', ' a', ' off'])\n",
      "(tensor([0.1890, 0.1554, 0.0744, 0.0679, 0.0526], grad_fn=<ToCopyBackward0>), [' make', ' be', ' promote', ' produce', ' appear'])\n",
      "(tensor([0.6045, 0.2196, 0.0385, 0.0322, 0.0306], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' a', ' and'])\n",
      "(tensor([0.4790, 0.1230, 0.0554, 0.0408, 0.0238], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ' piece', ','])\n",
      "(tensor([0.5289, 0.1336, 0.0767, 0.0460, 0.0389], grad_fn=<ToCopyBackward0>), ['.', ',', ' by', '!', ' and'])\n",
      "(tensor([0.1447, 0.1305, 0.0810, 0.0449, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' The', ' He', ' That', ' This'])\n",
      "(tensor([0.2870, 0.2279, 0.0918, 0.0327, 0.0214], grad_fn=<ToCopyBackward0>), [' is', ' was', ' film', ' movie', ' made'])\n",
      "(tensor([0.2971, 0.2590, 0.0925, 0.0506, 0.0150], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' has', \"'s\"])\n",
      "(tensor([0.5269, 0.0929, 0.0884, 0.0395, 0.0285], grad_fn=<ToCopyBackward0>), [' been', ' nothing', ' no', ' not', ' already'])\n",
      "(tensor([0.3010, 0.1684, 0.0765, 0.0402, 0.0364], grad_fn=<ToCopyBackward0>), [' made', ' written', ' done', ' produced', ' directed'])\n",
      "(tensor([0.6701, 0.1602, 0.0580, 0.0127, 0.0092], grad_fn=<ToCopyBackward0>), [' by', ' and', ',', ' for', ' in'])\n",
      "(tensor([0.3231, 0.2050, 0.0676, 0.0660, 0.0612], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' someone'])\n",
      "(tensor([0.5399, 0.1131, 0.1028, 0.0234, 0.0232], grad_fn=<ToCopyBackward0>), [' else', ' with', ' who', ' that', ' in'])\n",
      "(tensor([0.2123, 0.2122, 0.0465, 0.0457, 0.0371], grad_fn=<ToCopyBackward0>), [' was', ' had', ' has', ' is', ' also'])\n",
      "(tensor([0.0893, 0.0785, 0.0696, 0.0662, 0.0641], grad_fn=<ToCopyBackward0>), [' wrote', ' had', ' did', ' appears', ' appeared'])\n",
      "(tensor([0.6572, 0.1665, 0.1070, 0.0350, 0.0114], grad_fn=<ToCopyBackward0>), [' in', ' on', ' as', ' to', ' at'])\n",
      "(tensor([0.3392, 0.2531, 0.0529, 0.0355, 0.0319], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' \"', ' another'])\n",
      "(tensor([0.0968, 0.0177, 0.0140, 0.0133, 0.0124], grad_fn=<ToCopyBackward0>), ['The', 'Re', 'C', 'B', 'G'])\n",
      "(tensor([0.0927, 0.0802, 0.0246, 0.0162, 0.0153], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Pro', ' House', ' Sop'])\n",
      "(tensor([0.7317, 0.0383, 0.0240, 0.0232, 0.0145], grad_fn=<ToCopyBackward0>), [' Gun', ' Man', ' Pre', ' City', ' War'])\n",
      "(tensor([0.7089, 0.0855, 0.0585, 0.0406, 0.0363], grad_fn=<ToCopyBackward0>), ['\"', '\".', '\",', '.\"', ',\"'])\n",
      "(tensor([0.1349, 0.0948, 0.0868, 0.0831, 0.0650], grad_fn=<ToCopyBackward0>), [' and', ' movies', ' movie', ' TV', ' films'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very well done. It had some good effects. But it seemed to be a bit too long and drawn out and predictable. The movie had some potential, but it just didn't deliver. I really didn't care for this movie at\n",
      "(tensor([0.3831, 0.1723, 0.0904, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2450, 0.1959, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0595, 0.0360, 0.0356, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0663, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5069, 0.0403, 0.0346, 0.0338, 0.0324], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' funny', ','])\n",
      "(tensor([0.5459, 0.2534, 0.1076, 0.0180, 0.0147], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.4158, 0.1612, 0.1353, 0.0447, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.1966, 0.1902, 0.1733, 0.0249, 0.0242], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' Unfortunately'])\n",
      "(tensor([0.4034, 0.1107, 0.0827, 0.0611, 0.0546], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' has', ' is'])\n",
      "(tensor([0.3173, 0.1128, 0.0878, 0.0665, 0.0563], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' good', ' great'])\n",
      "(tensor([0.1103, 0.0945, 0.0895, 0.0663, 0.0472], grad_fn=<ToCopyBackward0>), [' funny', ' great', ' of', ' good', ' really'])\n",
      "(tensor([0.2882, 0.1067, 0.0698, 0.0506, 0.0358], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' effects', ' special', ' action'])\n",
      "(tensor([0.3802, 0.1916, 0.1756, 0.0638, 0.0268], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' as'])\n",
      "(tensor([0.1665, 0.1485, 0.1478, 0.1257, 0.0478], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' But', ' Unfortunately'])\n",
      "(tensor([0.3539, 0.1493, 0.0591, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' when', ','])\n",
      "(tensor([0.2903, 0.1088, 0.0685, 0.0541, 0.0508], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' seemed'])\n",
      "(tensor([0.3666, 0.3339, 0.0575, 0.0468, 0.0364], grad_fn=<ToCopyBackward0>), [' to', ' like', ' that', ' as', ' more'])\n",
      "(tensor([0.4445, 0.1983, 0.0615, 0.0475, 0.0299], grad_fn=<ToCopyBackward0>), [' me', ' be', ' lack', ' have', ' fall'])\n",
      "(tensor([0.1218, 0.0947, 0.0870, 0.0307, 0.0300], grad_fn=<ToCopyBackward0>), [' trying', ' a', ' more', ' shot', ' very'])\n",
      "(tensor([0.1083, 0.0773, 0.0549, 0.0534, 0.0473], grad_fn=<ToCopyBackward0>), [' very', ' little', ' lot', ' bit', ' big'])\n",
      "(tensor([0.4347, 0.1665, 0.0630, 0.0411, 0.0244], grad_fn=<ToCopyBackward0>), [' too', ' of', ' over', ' more', ' slow'])\n",
      "(tensor([0.0988, 0.0920, 0.0423, 0.0409, 0.0401], grad_fn=<ToCopyBackward0>), [' long', ' much', ' talk', ' \"', ' preach'])\n",
      "(tensor([0.3625, 0.2229, 0.1603, 0.0665, 0.0281], grad_fn=<ToCopyBackward0>), ['.', ' and', ' for', ',', ' at'])\n",
      "(tensor([0.1298, 0.0670, 0.0665, 0.0522, 0.0333], grad_fn=<ToCopyBackward0>), [' too', ' boring', ' drawn', ' dragged', ' way'])\n",
      "(tensor([9.8603e-01, 8.0845e-03, 9.3043e-04, 8.0534e-04, 6.7848e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' out', '-', ' to', ' together', ' from'])\n",
      "(tensor([0.4555, 0.1460, 0.1047, 0.0693, 0.0607], grad_fn=<ToCopyBackward0>), ['.', ' for', ' to', ' and', ','])\n",
      "(tensor([0.1691, 0.0463, 0.0399, 0.0386, 0.0330], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' the', ' too', ' had'])\n",
      "(tensor([0.5928, 0.0686, 0.0678, 0.0614, 0.0482], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' for'])\n",
      "(tensor([0.2117, 0.1506, 0.1469, 0.0325, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.1435, 0.0862, 0.0843, 0.0573, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' ending', ' movie', ' story', ' whole'])\n",
      "(tensor([0.1476, 0.1453, 0.0840, 0.0828, 0.0327], grad_fn=<ToCopyBackward0>), [' just', ' was', ' seemed', ' had', ' dragged'])\n",
      "(tensor([0.1820, 0.1519, 0.0619, 0.0325, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' no', ' so'])\n",
      "(tensor([0.1202, 0.1146, 0.0508, 0.0490, 0.0427], grad_fn=<ToCopyBackward0>), [' potential', ' good', ' great', ' of', ' interesting'])\n",
      "(tensor([0.3784, 0.2825, 0.1037, 0.0654, 0.0368], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' to', ' and'])\n",
      "(tensor([0.9265, 0.0130, 0.0070, 0.0044, 0.0041], grad_fn=<ToCopyBackward0>), [' but', ' and', ' though', ' however', ' I'])\n",
      "(tensor([0.2214, 0.1627, 0.0970, 0.0509, 0.0253], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' was', ' didn'])\n",
      "(tensor([0.2355, 0.1026, 0.0990, 0.0846, 0.0643], grad_fn=<ToCopyBackward0>), [' just', ' was', ' seemed', ' needed', ' didn'])\n",
      "(tensor([0.2626, 0.2447, 0.0474, 0.0431, 0.0383], grad_fn=<ToCopyBackward0>), [' didn', ' seemed', ' ended', ' never', ' wasn'])\n",
      "(tensor([9.9395e-01, 2.8449e-03, 6.4710e-04, 5.9127e-04, 2.0090e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', '\"'])\n",
      "(tensor([0.1259, 0.0947, 0.0595, 0.0500, 0.0499], grad_fn=<ToCopyBackward0>), [' deliver', ' work', ' have', ' seem', ' come'])\n",
      "(tensor([0.6346, 0.1680, 0.0290, 0.0204, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ' on', ' for', ' as', ' the'])\n",
      "(tensor([0.2896, 0.1439, 0.1262, 0.0228, 0.0220], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.1013, 0.0895, 0.0631, 0.0619, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' really', ' would', ' think', \"'m\"])\n",
      "(tensor([0.1037, 0.0933, 0.0798, 0.0737, 0.0682], grad_fn=<ToCopyBackward0>), [' wanted', ' didn', ' would', ' don', ' wish'])\n",
      "(tensor([9.9745e-01, 8.0415e-04, 3.4515e-04, 1.4039e-04, 1.3895e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2327, 0.2278, 0.1130, 0.1118, 0.0884], grad_fn=<ToCopyBackward0>), [' care', ' like', ' get', ' enjoy', ' understand'])\n",
      "(tensor([0.6824, 0.1193, 0.0361, 0.0238, 0.0209], grad_fn=<ToCopyBackward0>), [' for', ' about', ' what', ' much', ' too'])\n",
      "(tensor([0.4366, 0.3323, 0.1484, 0.0428, 0.0037], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' any', ' how'])\n",
      "(tensor([0.7396, 0.1741, 0.0301, 0.0095, 0.0038], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', '.', ' flick'])\n",
      "(tensor([0.4283, 0.2242, 0.0975, 0.0430, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', ' one'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that it was a sequel to The Man From Snowy River. I had very little to compare it to, except maybe the Twilight Zone episode, \"The Man Who Wasn't There.\" The acting was atrocious, the script was atrocious and\n",
      "(tensor([0.3844, 0.1713, 0.0895, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3307, 0.2370, 0.0582, 0.0559, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.5333, 0.1179, 0.0422, 0.0373, 0.0248], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' had'])\n",
      "(tensor([0.3211, 0.0614, 0.0520, 0.0493, 0.0346], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' the', ' very'])\n",
      "(tensor([0.2478, 0.1079, 0.0600, 0.0546, 0.0376], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' great', ' pretty', ' really'])\n",
      "(tensor([0.7697, 0.0747, 0.0244, 0.0181, 0.0103], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ',', '...'])\n",
      "(tensor([0.1701, 0.0628, 0.0565, 0.0384, 0.0336], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.0466, 0.0421, 0.0274, 0.0191, 0.0173], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Boo', ' Legend', ' Man'])\n",
      "(tensor([0.2979, 0.2944, 0.2069, 0.0643, 0.0249], grad_fn=<ToCopyBackward0>), [' Who', ' From', 'ch', ' With', ' Called'])\n",
      "(tensor([9.5484e-01, 5.6520e-03, 5.4174e-03, 1.0819e-03, 8.6503e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Snow', ' UN', ' Earth', ' Planet', ' U'])\n",
      "(tensor([9.9409e-01, 1.0875e-03, 3.9713e-04, 2.3034e-04, 2.2254e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['y', 'ys', 'ies', ' City', 'don'])\n",
      "(tensor([0.9237, 0.0198, 0.0074, 0.0024, 0.0021], grad_fn=<ToCopyBackward0>), [' River', ' Mountain', ' Lake', ' City', ' Island'])\n",
      "(tensor([0.2944, 0.1563, 0.0628, 0.0598, 0.0535], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' which', ' so'])\n",
      "(tensor([0.2048, 0.0971, 0.0854, 0.0405, 0.0290], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' That'])\n",
      "(tensor([0.1706, 0.1123, 0.0445, 0.0402, 0.0356], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' think', ' had'])\n",
      "(tensor([0.1230, 0.1199, 0.1032, 0.0816, 0.0740], grad_fn=<ToCopyBackward0>), [' a', ' never', ' to', ' no', ' very'])\n",
      "(tensor([0.3834, 0.3388, 0.1126, 0.0420, 0.0305], grad_fn=<ToCopyBackward0>), [' high', ' low', ' little', ' good', ' much'])\n",
      "(tensor([0.1721, 0.1706, 0.1154, 0.0994, 0.0891], grad_fn=<ToCopyBackward0>), [' expectation', ' expectations', ' idea', ' knowledge', ' to'])\n",
      "(tensor([0.5723, 0.0708, 0.0680, 0.0590, 0.0303], grad_fn=<ToCopyBackward0>), [' do', ' go', ' work', ' compare', ' no'])\n",
      "(tensor([0.9448, 0.0149, 0.0062, 0.0061, 0.0037], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' The', ' to'])\n",
      "(tensor([0.8738, 0.1060, 0.0099, 0.0016, 0.0012], grad_fn=<ToCopyBackward0>), [' to', ' with', ' too', ',', '.'])\n",
      "(tensor([0.5733, 0.2895, 0.0169, 0.0165, 0.0132], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' and', ' other'])\n",
      "(tensor([0.3474, 0.1251, 0.0902, 0.0670, 0.0592], grad_fn=<ToCopyBackward0>), [' but', ' other', ' except', ' so', ' and'])\n",
      "(tensor([0.2538, 0.2001, 0.1028, 0.0992, 0.0372], grad_fn=<ToCopyBackward0>), [' for', ' maybe', ' that', ' the', ' The'])\n",
      "(tensor([0.2239, 0.1472, 0.0285, 0.0245, 0.0216], grad_fn=<ToCopyBackward0>), [' the', ' The', ' that', ' some', ' Snow'])\n",
      "(tensor([0.0628, 0.0604, 0.0275, 0.0172, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' first', ' TV', ' book', ' Twilight'])\n",
      "(tensor([9.9052e-01, 2.2819e-03, 2.1433e-03, 5.3978e-04, 3.5936e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Zone', ' zone', ' series', ' movies', ' movie'])\n",
      "(tensor([0.4089, 0.1609, 0.1022, 0.0638, 0.0314], grad_fn=<ToCopyBackward0>), [' episode', '.', ' episodes', ' movie', ','])\n",
      "(tensor([0.2555, 0.1292, 0.0993, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' \"', ',', ' of', ' where', ' that'])\n",
      "(tensor([0.2004, 0.1990, 0.1249, 0.1027, 0.0939], grad_fn=<ToCopyBackward0>), [' and', ' \"', ' but', \" '\", ' which'])\n",
      "(tensor([0.2101, 0.0604, 0.0159, 0.0155, 0.0124], grad_fn=<ToCopyBackward0>), ['The', 'Night', 'Time', 'A', 'Christmas'])\n",
      "(tensor([0.0678, 0.0620, 0.0208, 0.0183, 0.0146], grad_fn=<ToCopyBackward0>), [' Brain', ' Man', ' Blue', ' Straight', ' Grey'])\n",
      "(tensor([0.6382, 0.1290, 0.0834, 0.0313, 0.0164], grad_fn=<ToCopyBackward0>), [' From', ' Who', ' from', 'ch', ' In'])\n",
      "(tensor([0.2725, 0.1959, 0.0367, 0.0362, 0.0305], grad_fn=<ToCopyBackward0>), [' Was', ' Fell', ' Would', ' Came', ' K'])\n",
      "(tensor([0.3020, 0.0278, 0.0151, 0.0118, 0.0097], grad_fn=<ToCopyBackward0>), ['n', ' Naked', ' Blind', ' Dead', ' Drunk'])\n",
      "(tensor([0.9775, 0.0081, 0.0030, 0.0023, 0.0011], grad_fn=<ToCopyBackward0>), [\"'t\", '`', '\"', \"'\", ','])\n",
      "(tensor([0.8600, 0.0372, 0.0069, 0.0052, 0.0025], grad_fn=<ToCopyBackward0>), [' There', ' there', ' Me', ' Here', ' Even'])\n",
      "(tensor([0.3910, 0.2553, 0.1343, 0.1244, 0.0816], grad_fn=<ToCopyBackward0>), ['.\"', '\".', '\"', '\",', ',\"'])\n",
      "(tensor([0.1211, 0.0853, 0.0776, 0.0733, 0.0590], grad_fn=<ToCopyBackward0>), [' I', ' The', ' And', ' But', ' That'])\n",
      "(tensor([0.1104, 0.0749, 0.0377, 0.0297, 0.0281], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' Man', ' plot', ' special'])\n",
      "(tensor([0.4917, 0.1365, 0.1082, 0.0577, 0.0489], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' in'])\n",
      "(tensor([0.0705, 0.0625, 0.0619, 0.0525, 0.0479], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' awful', ' very', ' atro'])\n",
      "(tensor([9.9990e-01, 8.4930e-06, 5.2630e-06, 4.9977e-06, 3.6752e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'vol', 'par'])\n",
      "(tensor([0.3994, 0.2152, 0.1845, 0.0705, 0.0259], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' in', ';'])\n",
      "(tensor([0.5352, 0.1219, 0.1215, 0.0234, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' but', ' and', ' as', ' though'])\n",
      "(tensor([0.1349, 0.1074, 0.1035, 0.0774, 0.0522], grad_fn=<ToCopyBackward0>), [' script', ' plot', ' story', ' writing', ' production'])\n",
      "(tensor([0.7417, 0.0304, 0.0100, 0.0099, 0.0091], grad_fn=<ToCopyBackward0>), [' was', ',', ' sounded', ' wasn', ' had'])\n",
      "(tensor([0.1352, 0.1324, 0.0510, 0.0360, 0.0290], grad_fn=<ToCopyBackward0>), [' atro', ' awful', ' terrible', ' bad', ' a'])\n",
      "(tensor([9.9935e-01, 4.5853e-05, 2.4795e-05, 2.1589e-05, 2.1558e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'cular', 'cul', 'cept', 'cient'])\n",
      "(tensor([0.4883, 0.2448, 0.2037, 0.0130, 0.0057], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ';', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so terrible I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see\n",
      "(tensor([0.3833, 0.1719, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4375, 0.2442, 0.1961, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6524, 0.0596, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1374, 0.0700, 0.0663, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5192, 0.0474, 0.0457, 0.0420, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2037, 0.1736, 0.1379, 0.1202, 0.0641], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', '.', ','])\n",
      "(tensor([0.1177, 0.0852, 0.0671, 0.0589, 0.0580], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' could', ' was', ' had'])\n",
      "(tensor([0.1145, 0.0891, 0.0672, 0.0487, 0.0457], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' closed', ' started'])\n",
      "(tensor([0.1914, 0.1229, 0.0887, 0.0744, 0.0558], grad_fn=<ToCopyBackward0>), [' to', ' $', ' money', ' for', ' 1'])\n",
      "(tensor([0.3781, 0.1587, 0.1250, 0.0665, 0.0301], grad_fn=<ToCopyBackward0>), [' see', ' get', ' rent', ' watch', ' have'])\n",
      "(tensor([0.8478, 0.0498, 0.0374, 0.0074, 0.0023], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' something'])\n",
      "(tensor([0.3048, 0.1846, 0.0507, 0.0449, 0.0434], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' on', ' at'])\n",
      "(tensor([0.2057, 0.1313, 0.1215, 0.0691, 0.0318], grad_fn=<ToCopyBackward0>), [' the', ' theat', ' theaters', ' a', ' theater'])\n",
      "(tensor([0.4096, 0.3619, 0.1031, 0.0119, 0.0110], grad_fn=<ToCopyBackward0>), [' theatre', ' theater', ' bargain', ' Theatre', ' cinema'])\n",
      "(tensor([0.8047, 0.0229, 0.0191, 0.0148, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' just', ' because'])\n",
      "(tensor([0.3303, 0.1255, 0.0802, 0.0199, 0.0155], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' What'])\n",
      "(tensor([0.0961, 0.0820, 0.0482, 0.0468, 0.0370], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' can', ' actually', ' really'])\n",
      "(tensor([0.6126, 0.0412, 0.0339, 0.0303, 0.0168], grad_fn=<ToCopyBackward0>), [' paid', ' rented', ' thought', ' pay', ' made'])\n",
      "(tensor([0.5270, 0.0911, 0.0888, 0.0509, 0.0446], grad_fn=<ToCopyBackward0>), [' to', ' $', ' for', ' a', ' money'])\n",
      "(tensor([0.6948, 0.1178, 0.0599, 0.0240, 0.0161], grad_fn=<ToCopyBackward0>), [' see', ' rent', ' watch', ' go', ' get'])\n",
      "(tensor([0.4967, 0.3880, 0.0298, 0.0164, 0.0122], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' that'])\n",
      "(tensor([0.8089, 0.0713, 0.0288, 0.0247, 0.0152], grad_fn=<ToCopyBackward0>), [' in', ' at', ' on', '.', ' because'])\n",
      "(tensor([0.8078, 0.1082, 0.0313, 0.0114, 0.0060], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' theaters', ' an'])\n",
      "(tensor([0.9582, 0.0298, 0.0024, 0.0013, 0.0012], grad_fn=<ToCopyBackward0>), [' theater', ' theatre', ' Theater', ' theaters', ' cinema'])\n",
      "(tensor([0.6478, 0.0743, 0.0406, 0.0237, 0.0181], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' when'])\n",
      "(tensor([0.3483, 0.0627, 0.0438, 0.0418, 0.0271], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' That'])\n",
      "(tensor([0.1429, 0.0594, 0.0529, 0.0484, 0.0441], grad_fn=<ToCopyBackward0>), [' actually', ' really', ' think', \"'m\", ' thought'])\n",
      "(tensor([0.8195, 0.0406, 0.0175, 0.0104, 0.0082], grad_fn=<ToCopyBackward0>), [' paid', ' pay', ' thought', ' rented', ' watched'])\n",
      "(tensor([0.8955, 0.0381, 0.0118, 0.0112, 0.0061], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ' a', ' $'])\n",
      "(tensor([0.9722, 0.0069, 0.0031, 0.0023, 0.0022], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' go', ' get', ' buy'])\n",
      "(tensor([0.8977, 0.0634, 0.0068, 0.0064, 0.0027], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' that'])\n",
      "(tensor([0.9662, 0.0159, 0.0047, 0.0039, 0.0025], grad_fn=<ToCopyBackward0>), [' in', ' at', '.', ' on', ' because'])\n",
      "(tensor([9.7715e-01, 1.3647e-02, 4.3248e-03, 1.3725e-03, 6.9287e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' theaters', ' theater'])\n",
      "(tensor([9.9275e-01, 1.8321e-03, 1.7190e-03, 9.8754e-04, 7.8996e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' theater', ' movie', ' theatre', ' Theater', ' theaters'])\n",
      "(tensor([0.8531, 0.0280, 0.0206, 0.0123, 0.0089], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' when', '!'])\n",
      "(tensor([0.5545, 0.0421, 0.0414, 0.0345, 0.0193], grad_fn=<ToCopyBackward0>), [' I', 'I', ' It', ' This', ' And'])\n",
      "(tensor([0.7372, 0.0651, 0.0338, 0.0128, 0.0080], grad_fn=<ToCopyBackward0>), [' actually', ' even', ' really', ' think', ' thought'])\n",
      "(tensor([0.9566, 0.0107, 0.0028, 0.0022, 0.0015], grad_fn=<ToCopyBackward0>), [' paid', ' pay', ' thought', '...', ' watched'])\n",
      "(tensor([0.9773, 0.0072, 0.0066, 0.0013, 0.0011], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', '...', ','])\n",
      "(tensor([9.9173e-01, 2.6661e-03, 8.6059e-04, 5.8124e-04, 4.1451e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' see', ' watch', ' hear', ' get', ' go'])\n",
      "(tensor([0.9754, 0.0117, 0.0015, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([9.9042e-01, 3.1325e-03, 2.5710e-03, 6.2622e-04, 4.9987e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' in', '.', ' at', ' on', ' because'])\n",
      "(tensor([9.8578e-01, 3.7631e-03, 3.6560e-03, 5.4283e-04, 4.6540e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' theaters', ' theater'])\n",
      "(tensor([9.9405e-01, 1.9511e-03, 9.6096e-04, 7.0463e-04, 4.4009e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' theater', ' movie', ' theaters', ' Theater', ' theatre'])\n",
      "(tensor([0.9513, 0.0058, 0.0045, 0.0040, 0.0034], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' I'])\n",
      "(tensor([0.7901, 0.0466, 0.0163, 0.0151, 0.0065], grad_fn=<ToCopyBackward0>), [' I', 'I', ' This', ' It', ' You'])\n",
      "(tensor([0.8915, 0.0445, 0.0162, 0.0047, 0.0020], grad_fn=<ToCopyBackward0>), [' actually', ' even', ' really', ' think', ' was'])\n",
      "(tensor([9.7739e-01, 3.6733e-03, 1.7105e-03, 1.1079e-03, 8.5328e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' paid', ' pay', '...', ' thought', ' watched'])\n",
      "(tensor([9.8834e-01, 3.1176e-03, 2.3023e-03, 6.1779e-04, 4.5673e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', '.', ' for', ',', '...'])\n",
      "(tensor([9.8779e-01, 2.2967e-03, 1.3456e-03, 8.8954e-04, 6.7809e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' see', ' watch', ' hear', ' get', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of those films that is made for TV movies. I was so wrong. It was a great film with great actors. I was very disappointed with this film. I thought I would enjoy it more as it went on, but it was\n",
      "(tensor([0.3839, 0.1718, 0.0897, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1459, 0.0508, 0.0453, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9170, 0.0124, 0.0066, 0.0065, 0.0039], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7602, 0.1941, 0.0167, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.4908, 0.2099, 0.0214, 0.0133, 0.0075], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' \"', ' B', \" '\"])\n",
      "(tensor([0.5348, 0.2359, 0.0748, 0.0261, 0.0166], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', ' like'])\n",
      "(tensor([0.1621, 0.0644, 0.0446, 0.0442, 0.0388], grad_fn=<ToCopyBackward0>), [' was', ' would', ' the', ' you', ' is'])\n",
      "(tensor([0.2067, 0.1526, 0.0288, 0.0275, 0.0259], grad_fn=<ToCopyBackward0>), [' so', ' funny', ' made', ' fun', ' going'])\n",
      "(tensor([0.2812, 0.2084, 0.1376, 0.0825, 0.0570], grad_fn=<ToCopyBackward0>), [' for', ' in', ' by', ' to', ' on'])\n",
      "(tensor([0.3581, 0.1241, 0.1180, 0.0935, 0.0330], grad_fn=<ToCopyBackward0>), [' kids', ' TV', ' the', ' one', ' children'])\n",
      "(tensor([0.1391, 0.1179, 0.0974, 0.0289, 0.0230], grad_fn=<ToCopyBackward0>), [' movie', ' movies', '.', ' screens', ' audiences'])\n",
      "(tensor([0.2618, 0.1675, 0.0672, 0.0642, 0.0500], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' that', ' and'])\n",
      "(tensor([0.1732, 0.1010, 0.0978, 0.0444, 0.0437], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' And'])\n",
      "(tensor([0.1040, 0.0927, 0.0515, 0.0514, 0.0500], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' don', ' really', \"'m\"])\n",
      "(tensor([0.1639, 0.1596, 0.0532, 0.0516, 0.0409], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' surprised', ' looking'])\n",
      "(tensor([0.4436, 0.2531, 0.0588, 0.0255, 0.0238], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ' bored', ' surprised', ' excited'])\n",
      "(tensor([0.7618, 0.1319, 0.0164, 0.0124, 0.0109], grad_fn=<ToCopyBackward0>), ['.', '!', ' about', ' on', ','])\n",
      "(tensor([0.2373, 0.1930, 0.1528, 0.1046, 0.0419], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', 'This'])\n",
      "(tensor([0.3405, 0.2746, 0.1935, 0.0200, 0.0198], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' wasn', ' has'])\n",
      "(tensor([0.1785, 0.0965, 0.0453, 0.0417, 0.0350], grad_fn=<ToCopyBackward0>), [' so', ' a', ' really', ' one', ' awful'])\n",
      "(tensor([0.3487, 0.1123, 0.0709, 0.0475, 0.0216], grad_fn=<ToCopyBackward0>), [' really', ' very', ' real', ' great', ' major'])\n",
      "(tensor([0.1628, 0.1509, 0.0877, 0.0729, 0.0565], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' disappointment', ',', ' way'])\n",
      "(tensor([0.5334, 0.0771, 0.0581, 0.0498, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', '!', ' to'])\n",
      "(tensor([0.4482, 0.1521, 0.0448, 0.0324, 0.0263], grad_fn=<ToCopyBackward0>), [' great', ' a', ' an', ' very', ' wonderful'])\n",
      "(tensor([0.4138, 0.1776, 0.0974, 0.0481, 0.0229], grad_fn=<ToCopyBackward0>), [' actors', ' acting', ' characters', ' people', ' performances'])\n",
      "(tensor([0.2975, 0.2959, 0.2577, 0.0368, 0.0317], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' but', ' in'])\n",
      "(tensor([0.2660, 0.1559, 0.1101, 0.0425, 0.0335], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Unfortunately', ' But'])\n",
      "(tensor([0.0851, 0.0761, 0.0747, 0.0593, 0.0556], grad_fn=<ToCopyBackward0>), [' really', ' was', \"'m\", ' am', ' can'])\n",
      "(tensor([0.3158, 0.2554, 0.0630, 0.0206, 0.0176], grad_fn=<ToCopyBackward0>), [' very', ' really', ' so', ' disappointed', ' a'])\n",
      "(tensor([0.5503, 0.0802, 0.0506, 0.0310, 0.0277], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' much', ' impressed', ' excited'])\n",
      "(tensor([0.4182, 0.1625, 0.0941, 0.0687, 0.0673], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', ' by', ' that'])\n",
      "(tensor([0.3763, 0.2154, 0.2142, 0.0338, 0.0281], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' how', ' my'])\n",
      "(tensor([0.5781, 0.2574, 0.0475, 0.0084, 0.0061], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' sequel', ' turkey'])\n",
      "(tensor([0.5576, 0.1362, 0.1107, 0.0478, 0.0166], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' and', ' in'])\n",
      "(tensor([0.2473, 0.2282, 0.1435, 0.0333, 0.0186], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1332, 0.0877, 0.0606, 0.0528, 0.0483], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' think'])\n",
      "(tensor([0.5916, 0.1472, 0.0644, 0.0511, 0.0495], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3282, 0.2877, 0.1197, 0.0431, 0.0317], grad_fn=<ToCopyBackward0>), [' would', ' was', ' had', \"'d\", ' could'])\n",
      "(tensor([0.2242, 0.1567, 0.1103, 0.0850, 0.0729], grad_fn=<ToCopyBackward0>), [' enjoy', ' be', ' like', ' love', ' have'])\n",
      "(tensor([0.5875, 0.1004, 0.0975, 0.0613, 0.0314], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' watching', ' seeing'])\n",
      "(tensor([0.4548, 0.1751, 0.0771, 0.0632, 0.0493], grad_fn=<ToCopyBackward0>), [' more', ',', ' but', '.', ' a'])\n",
      "(tensor([0.1945, 0.1780, 0.1287, 0.1114, 0.0745], grad_fn=<ToCopyBackward0>), ['.', ' than', ' after', ' because', ' as'])\n",
      "(tensor([0.5042, 0.1587, 0.1498, 0.1178, 0.0156], grad_fn=<ToCopyBackward0>), [' a', ' an', ' I', ' it', ' the'])\n",
      "(tensor([0.3887, 0.0596, 0.0489, 0.0410, 0.0296], grad_fn=<ToCopyBackward0>), [' was', ' went', ' is', ' progressed', ' goes'])\n",
      "(tensor([0.7113, 0.2522, 0.0053, 0.0047, 0.0038], grad_fn=<ToCopyBackward0>), [' along', ' on', ' through', ' down', ' into'])\n",
      "(tensor([0.4234, 0.2994, 0.0602, 0.0534, 0.0442], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' the', ' because'])\n",
      "(tensor([0.8198, 0.0340, 0.0212, 0.0163, 0.0087], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' then', ' so'])\n",
      "(tensor([0.3439, 0.2303, 0.0622, 0.0517, 0.0317], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' as'])\n",
      "(tensor([0.3138, 0.2931, 0.0426, 0.0361, 0.0277], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', ' never', \"'s\"])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film had some potential to be something interesting, but I just couldn't get in the mood to watch it. It seemed too much like a movie that someone had filmed at a college fraternity party and was proud of. The plot was very weak\n",
      "(tensor([0.3828, 0.1726, 0.0903, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3319, 0.2362, 0.0581, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4140, 0.1971, 0.1695, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6394, 0.0584, 0.0581, 0.0558, 0.0175], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.3331, 0.1618, 0.0983, 0.0669, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' some', ' all'])\n",
      "(tensor([0.4147, 0.1236, 0.1102, 0.0453, 0.0425], grad_fn=<ToCopyBackward0>), [' potential', ' real', ' great', ' promise', ' good'])\n",
      "(tensor([0.4142, 0.2487, 0.1042, 0.0338, 0.0282], grad_fn=<ToCopyBackward0>), [',', '.', ' to', ' for', ' as'])\n",
      "(tensor([0.7645, 0.0312, 0.0235, 0.0211, 0.0200], grad_fn=<ToCopyBackward0>), [' be', ' do', ' really', ' make', ' become'])\n",
      "(tensor([0.1990, 0.1592, 0.1186, 0.1093, 0.0516], grad_fn=<ToCopyBackward0>), [' a', ' something', ' funny', ' really', ' very'])\n",
      "(tensor([0.1783, 0.1156, 0.1005, 0.0711, 0.0547], grad_fn=<ToCopyBackward0>), [' special', ' really', ' good', ' interesting', ' very'])\n",
      "(tensor([0.3189, 0.2313, 0.1493, 0.0892, 0.0345], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', '...'])\n",
      "(tensor([0.6068, 0.0407, 0.0317, 0.0228, 0.0214], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' something', ' to'])\n",
      "(tensor([0.2029, 0.1200, 0.0616, 0.0514, 0.0506], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' after', ' ultimately'])\n",
      "(tensor([0.3116, 0.1292, 0.0522, 0.0422, 0.0365], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', ' don', ' found'])\n",
      "(tensor([0.2733, 0.1448, 0.1145, 0.0799, 0.0268], grad_fn=<ToCopyBackward0>), [' didn', ' wasn', ' couldn', ' found', ' don'])\n",
      "(tensor([9.9435e-01, 2.6024e-03, 5.7849e-04, 5.5657e-04, 1.6789e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2401, 0.1589, 0.0772, 0.0558, 0.0428], grad_fn=<ToCopyBackward0>), [' believe', ' get', ' see', ' bring', ' figure'])\n",
      "(tensor([0.3071, 0.1066, 0.1019, 0.0815, 0.0625], grad_fn=<ToCopyBackward0>), [' into', ' over', ' on', ' in', ' behind'])\n",
      "(tensor([0.3220, 0.3070, 0.0465, 0.0456, 0.0333], grad_fn=<ToCopyBackward0>), [' the', ' to', ' there', ' on', '.'])\n",
      "(tensor([0.6441, 0.0332, 0.0155, 0.0131, 0.0125], grad_fn=<ToCopyBackward0>), [' mood', ' right', ' heads', ' frame', ' mindset'])\n",
      "(tensor([0.7627, 0.1403, 0.0507, 0.0045, 0.0036], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ' and', ' when'])\n",
      "(tensor([0.7499, 0.0701, 0.0199, 0.0182, 0.0156], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' sit', ' be', ' enjoy'])\n",
      "(tensor([0.8544, 0.0447, 0.0175, 0.0169, 0.0138], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' any'])\n",
      "(tensor([0.7427, 0.0499, 0.0174, 0.0166, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', ' again', ' at', ' after'])\n",
      "(tensor([0.2278, 0.1918, 0.1292, 0.0254, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'The', ' This'])\n",
      "(tensor([0.2784, 0.1554, 0.0830, 0.0692, 0.0534], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' just', ' wasn'])\n",
      "(tensor([0.2883, 0.2619, 0.0948, 0.0709, 0.0303], grad_fn=<ToCopyBackward0>), [' to', ' like', ' too', ' more', ' as'])\n",
      "(tensor([0.0961, 0.0763, 0.0555, 0.0430, 0.0291], grad_fn=<ToCopyBackward0>), [' pret', ' much', ' slow', ' long', ' artificial'])\n",
      "(tensor([0.7202, 0.0973, 0.0229, 0.0088, 0.0081], grad_fn=<ToCopyBackward0>), [' like', ' of', ' for', ' to', ' too'])\n",
      "(tensor([0.6116, 0.1447, 0.0236, 0.0223, 0.0216], grad_fn=<ToCopyBackward0>), [' a', ' an', ' one', ' it', ' another'])\n",
      "(tensor([0.0581, 0.0445, 0.0329, 0.0284, 0.0267], grad_fn=<ToCopyBackward0>), [' TV', ' drama', ' \"', ' movie', ' filmed'])\n",
      "(tensor([0.1605, 0.1579, 0.1214, 0.0633, 0.0553], grad_fn=<ToCopyBackward0>), [' about', ' that', ' for', ' made', '.'])\n",
      "(tensor([0.1715, 0.1455, 0.1125, 0.0802, 0.0519], grad_fn=<ToCopyBackward0>), [' someone', ' was', ' a', ' the', ' had'])\n",
      "(tensor([0.3754, 0.1315, 0.0973, 0.0377, 0.0290], grad_fn=<ToCopyBackward0>), [' made', ' else', ' had', ' at', ' was'])\n",
      "(tensor([0.7734, 0.0531, 0.0234, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [' made', ' written', ' filmed', ' put', ' done'])\n",
      "(tensor([0.1458, 0.0898, 0.0898, 0.0870, 0.0860], grad_fn=<ToCopyBackward0>), [' in', ' at', ' to', ' on', ' for'])\n",
      "(tensor([0.4728, 0.1323, 0.0991, 0.0502, 0.0194], grad_fn=<ToCopyBackward0>), [' a', ' the', ' home', ' an', ' someone'])\n",
      "(tensor([0.1109, 0.0677, 0.0563, 0.0486, 0.0394], grad_fn=<ToCopyBackward0>), [' friend', ' party', ' college', ' rave', ' friends'])\n",
      "(tensor([0.2074, 0.0708, 0.0538, 0.0439, 0.0425], grad_fn=<ToCopyBackward0>), [' party', ' reunion', ' campus', ' football', ' fraternity'])\n",
      "(tensor([0.3002, 0.2205, 0.0385, 0.0149, 0.0139], grad_fn=<ToCopyBackward0>), [' party', ' house', ' ha', ' function', ' dinner'])\n",
      "(tensor([0.3918, 0.1580, 0.1350, 0.0376, 0.0312], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' -'])\n",
      "(tensor([0.0917, 0.0916, 0.0796, 0.0626, 0.0453], grad_fn=<ToCopyBackward0>), [' was', ' edited', ' made', ' had', ' then'])\n",
      "(tensor([0.0807, 0.0549, 0.0337, 0.0201, 0.0200], grad_fn=<ToCopyBackward0>), [' trying', ' then', ' proud', ' hoping', ' making'])\n",
      "(tensor([9.3235e-01, 5.0483e-02, 6.4520e-03, 4.6498e-03, 8.0550e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' to', ' enough', ' that', ' as'])\n",
      "(tensor([0.5248, 0.0743, 0.0629, 0.0595, 0.0331], grad_fn=<ToCopyBackward0>), ['.', ',', ' for', ' it', ' the'])\n",
      "(tensor([0.1919, 0.1620, 0.1131, 0.0453, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.1789, 0.0675, 0.0435, 0.0431, 0.0386], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' characters', ' only', ' story'])\n",
      "(tensor([0.6074, 0.0808, 0.0281, 0.0251, 0.0134], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' seemed', ' and'])\n",
      "(tensor([0.0761, 0.0494, 0.0429, 0.0374, 0.0367], grad_fn=<ToCopyBackward0>), [' predictable', ' weak', ' too', ' very', ' so'])\n",
      "(tensor([0.5280, 0.0349, 0.0337, 0.0315, 0.0243], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' confusing', ' weak', ' cliché'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie would be great. It is not.I was really looking forward to seeing it, but it was not. It was slow, boring and had a few plot holes. The movie had a lot of potential, but was not. I would\n",
      "(tensor([0.3839, 0.1720, 0.0897, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2437, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6522, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.8301, 0.0816, 0.0135, 0.0115, 0.0080], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' get'])\n",
      "(tensor([0.1441, 0.1157, 0.0932, 0.0924, 0.0748], grad_fn=<ToCopyBackward0>), [' more', ' better', ' great', ' a', ' really'])\n",
      "(tensor([0.3179, 0.2666, 0.0777, 0.0772, 0.0764], grad_fn=<ToCopyBackward0>), ['.', ' for', ' because', ',', ' to'])\n",
      "(tensor([0.3114, 0.1360, 0.0718, 0.0513, 0.0201], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' So'])\n",
      "(tensor([0.2426, 0.2209, 0.0916, 0.0536, 0.0529], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' has'])\n",
      "(tensor([0.1498, 0.1258, 0.0934, 0.0655, 0.0611], grad_fn=<ToCopyBackward0>), [' not', '.', ',', ' a', ' so'])\n",
      "(tensor([0.5901, 0.0642, 0.0596, 0.0464, 0.0308], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' at', ' that'])\n",
      "(tensor([0.2150, 0.1896, 0.1227, 0.0446, 0.0233], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', 'The', 'I'])\n",
      "(tensor([0.0938, 0.0660, 0.0650, 0.0609, 0.0467], grad_fn=<ToCopyBackward0>), [' am', ' was', ' have', \"'m\", ' think'])\n",
      "(tensor([0.3300, 0.1004, 0.0501, 0.0479, 0.0454], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' expecting', ' not'])\n",
      "(tensor([0.6456, 0.0577, 0.0494, 0.0425, 0.0301], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' hoping', ' excited', ' surprised'])\n",
      "(tensor([9.9555e-01, 2.6168e-03, 7.9290e-04, 2.6924e-04, 2.4733e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' at'])\n",
      "(tensor([9.9074e-01, 4.9087e-03, 6.9240e-04, 5.1710e-04, 4.2665e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' on', ' and'])\n",
      "(tensor([0.2890, 0.2559, 0.2200, 0.0542, 0.0463], grad_fn=<ToCopyBackward0>), [' seeing', ' this', ' watching', ' it', ' the'])\n",
      "(tensor([0.4323, 0.2582, 0.0582, 0.0116, 0.0089], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.2606, 0.2300, 0.1059, 0.0795, 0.0628], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.2193, 0.1284, 0.0852, 0.0580, 0.0487], grad_fn=<ToCopyBackward0>), [' but', ' and', ' since', ' I', ' as'])\n",
      "(tensor([0.2551, 0.1919, 0.1129, 0.0950, 0.0325], grad_fn=<ToCopyBackward0>), [' it', ' I', ' after', ' the', ' was'])\n",
      "(tensor([0.3401, 0.1697, 0.0545, 0.0525, 0.0429], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' really', ' turned'])\n",
      "(tensor([0.2235, 0.1976, 0.1176, 0.0669, 0.0344], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' really'])\n",
      "(tensor([0.2140, 0.1719, 0.0847, 0.0720, 0.0511], grad_fn=<ToCopyBackward0>), ['.', ' at', ' as', ' that', ' funny'])\n",
      "(tensor([0.1395, 0.1203, 0.1099, 0.1010, 0.0983], grad_fn=<ToCopyBackward0>), [' It', ' I', 'I', ' The', 'The'])\n",
      "(tensor([0.4183, 0.1746, 0.1006, 0.0273, 0.0199], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' had', ' has'])\n",
      "(tensor([0.1926, 0.0822, 0.0769, 0.0642, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' slow', ' so'])\n",
      "(tensor([0.3362, 0.2755, 0.1706, 0.0936, 0.0194], grad_fn=<ToCopyBackward0>), [' moving', ' and', ',', ' paced', ' going'])\n",
      "(tensor([0.2961, 0.0692, 0.0550, 0.0367, 0.0240], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' and', ' dull', ' the'])\n",
      "(tensor([6.6049e-01, 3.2726e-01, 1.8256e-03, 1.7683e-03, 6.1964e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' &', ' with'])\n",
      "(tensor([0.1393, 0.0637, 0.0596, 0.0502, 0.0454], grad_fn=<ToCopyBackward0>), [' predictable', ' had', ' not', ' the', ' was'])\n",
      "(tensor([0.3333, 0.1264, 0.0791, 0.0233, 0.0224], grad_fn=<ToCopyBackward0>), [' no', ' a', ' some', ' the', ' nothing'])\n",
      "(tensor([0.1629, 0.0890, 0.0600, 0.0546, 0.0512], grad_fn=<ToCopyBackward0>), [' lot', ' very', ' stupid', ' weak', ' few'])\n",
      "(tensor([0.1362, 0.0716, 0.0534, 0.0455, 0.0307], grad_fn=<ToCopyBackward0>), [' plot', ' stupid', ' flaws', ' funny', ' parts'])\n",
      "(tensor([0.9577, 0.0073, 0.0062, 0.0058, 0.0029], grad_fn=<ToCopyBackward0>), [' holes', ' flaws', ' points', ' twists', '-'])\n",
      "(tensor([0.6420, 0.0621, 0.0554, 0.0521, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ' in', ' that', ',', ' and'])\n",
      "(tensor([0.1518, 0.1187, 0.0801, 0.0772, 0.0619], grad_fn=<ToCopyBackward0>), [' The', ' I', 'The', ' It', 'I'])\n",
      "(tensor([0.2367, 0.0777, 0.0761, 0.0364, 0.0351], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' plot', ' story'])\n",
      "(tensor([0.1940, 0.1140, 0.0950, 0.0472, 0.0329], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' just', ' did'])\n",
      "(tensor([0.2103, 0.0854, 0.0783, 0.0711, 0.0381], grad_fn=<ToCopyBackward0>), [' a', ' no', ' some', ' the', ' so'])\n",
      "(tensor([0.1181, 0.1116, 0.0717, 0.0521, 0.0341], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' great', ' very', ' really'])\n",
      "(tensor([0.9027, 0.0458, 0.0294, 0.0076, 0.0048], grad_fn=<ToCopyBackward0>), [' of', ' more', ' to', ' going', ' in'])\n",
      "(tensor([0.5316, 0.0587, 0.0305, 0.0196, 0.0151], grad_fn=<ToCopyBackward0>), [' potential', ' promise', ' people', ' nudity', ' the'])\n",
      "(tensor([0.4945, 0.1590, 0.1022, 0.0699, 0.0555], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' to'])\n",
      "(tensor([0.8790, 0.0268, 0.0091, 0.0069, 0.0068], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' it', ' if'])\n",
      "(tensor([0.1974, 0.1599, 0.0786, 0.0457, 0.0296], grad_fn=<ToCopyBackward0>), [' it', ' the', ' was', ' I', ' didn'])\n",
      "(tensor([0.1833, 0.1177, 0.0544, 0.0416, 0.0397], grad_fn=<ToCopyBackward0>), [' not', ' just', ' ruined', ' never', ' sadly'])\n",
      "(tensor([0.1386, 0.0525, 0.0488, 0.0403, 0.0383], grad_fn=<ToCopyBackward0>), ['.', ' worthy', ' as', ' able', ' worth'])\n",
      "(tensor([0.1657, 0.1131, 0.1039, 0.0718, 0.0463], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.1001, 0.0740, 0.0698, 0.0695, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' really', ' would', ' think', ' am'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was so bad that I actually made a better film. It's not even close to the worst movie I have ever made, I'm just saying it was so bad that it made the other films in the series look great! This is the\n",
      "(tensor([0.3841, 0.1713, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4381, 0.2434, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7761, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1275, 0.0769, 0.0660, 0.0556, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4452, 0.0487, 0.0405, 0.0300, 0.0288], grad_fn=<ToCopyBackward0>), [' bad', ' atro', ' awful', ' boring', ' terrible'])\n",
      "(tensor([0.2855, 0.2250, 0.2116, 0.0481, 0.0310], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', ' when'])\n",
      "(tensor([0.5353, 0.1432, 0.1194, 0.0342, 0.0270], grad_fn=<ToCopyBackward0>), [' I', ' it', ' i', ' the', ' even'])\n",
      "(tensor([0.1934, 0.0708, 0.0537, 0.0412, 0.0354], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' thought', ' had', ' couldn'])\n",
      "(tensor([0.1438, 0.0973, 0.0583, 0.0455, 0.0413], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' felt', ' started'])\n",
      "(tensor([0.5752, 0.1661, 0.0433, 0.0321, 0.0278], grad_fn=<ToCopyBackward0>), [' a', ' it', ' the', ' myself', ' an'])\n",
      "(tensor([0.5714, 0.0438, 0.0296, 0.0277, 0.0259], grad_fn=<ToCopyBackward0>), [' comment', ' note', ' review', ' list', ' better'])\n",
      "(tensor([0.6046, 0.1199, 0.0707, 0.0268, 0.0200], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' documentary', ' version'])\n",
      "(tensor([0.2096, 0.1354, 0.0998, 0.0661, 0.0402], grad_fn=<ToCopyBackward0>), [' myself', '.', ' with', ' than', ' just'])\n",
      "(tensor([0.3919, 0.0796, 0.0634, 0.0548, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', 'I'])\n",
      "(tensor([0.4170, 0.2611, 0.0552, 0.0299, 0.0181], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' has'])\n",
      "(tensor([0.1262, 0.1033, 0.1031, 0.0974, 0.0802], grad_fn=<ToCopyBackward0>), [' not', ' so', ' just', ' called', ' like'])\n",
      "(tensor([0.5883, 0.1118, 0.0707, 0.0339, 0.0238], grad_fn=<ToCopyBackward0>), [' even', ' as', ' that', ' a', ' like'])\n",
      "(tensor([0.2832, 0.1484, 0.1218, 0.0617, 0.0550], grad_fn=<ToCopyBackward0>), [' close', ' funny', ' a', ' good', ' that'])\n",
      "(tensor([0.5432, 0.3716, 0.0419, 0.0051, 0.0032], grad_fn=<ToCopyBackward0>), [' to', '.', ',', '!', ' in'])\n",
      "(tensor([0.2143, 0.1469, 0.1320, 0.1227, 0.0289], grad_fn=<ToCopyBackward0>), [' as', ' the', ' being', ' good', ' a'])\n",
      "(tensor([0.2235, 0.1322, 0.0916, 0.0443, 0.0442], grad_fn=<ToCopyBackward0>), [' worst', ' same', ' original', ' first', ' level'])\n",
      "(tensor([0.7113, 0.2139, 0.0252, 0.0066, 0.0024], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' I', ' thing', ' one'])\n",
      "(tensor([0.8142, 0.0787, 0.0350, 0.0282, 0.0132], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' i', ' of', ' that'])\n",
      "(tensor([0.6097, 0.3521, 0.0149, 0.0052, 0.0021], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' saw', ' can'])\n",
      "(tensor([0.8160, 0.1647, 0.0048, 0.0041, 0.0015], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' made', ' watched', ' had'])\n",
      "(tensor([0.8260, 0.0692, 0.0364, 0.0199, 0.0049], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' watched', ' had', ' been'])\n",
      "(tensor([0.3960, 0.3899, 0.0738, 0.0321, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' in', ' and'])\n",
      "(tensor([0.7460, 0.0408, 0.0295, 0.0161, 0.0158], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' though'])\n",
      "(tensor([0.1547, 0.0891, 0.0742, 0.0702, 0.0648], grad_fn=<ToCopyBackward0>), [\"'m\", ' think', ' mean', ' just', ' am'])\n",
      "(tensor([0.1622, 0.1193, 0.1011, 0.0563, 0.0531], grad_fn=<ToCopyBackward0>), [' not', ' just', ' sure', ' talking', ' still'])\n",
      "(tensor([0.1250, 0.0937, 0.0924, 0.0733, 0.0349], grad_fn=<ToCopyBackward0>), [' saying', ' really', ' not', ' a', ' trying'])\n",
      "(tensor([0.3426, 0.1283, 0.0919, 0.0494, 0.0447], grad_fn=<ToCopyBackward0>), [' it', ' that', '.', ' the', ' this'])\n",
      "(tensor([0.3049, 0.2537, 0.0894, 0.0830, 0.0333], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' wasn', ' is', '.'])\n",
      "(tensor([0.2659, 0.1550, 0.0582, 0.0378, 0.0304], grad_fn=<ToCopyBackward0>), [' so', ' better', ' pretty', ' more', ' worse'])\n",
      "(tensor([0.8208, 0.0315, 0.0276, 0.0189, 0.0084], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' horrible', ' much'])\n",
      "(tensor([0.4335, 0.3414, 0.0902, 0.0532, 0.0103], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.6535, 0.1824, 0.0148, 0.0139, 0.0121], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' the', ' my'])\n",
      "(tensor([0.3092, 0.1029, 0.0690, 0.0497, 0.0382], grad_fn=<ToCopyBackward0>), [' was', ' made', \"'s\", ' became', ' is'])\n",
      "(tensor([0.3269, 0.2807, 0.0402, 0.0386, 0.0312], grad_fn=<ToCopyBackward0>), [' me', ' a', ' something', ' the', ' better'])\n",
      "(tensor([0.2789, 0.0891, 0.0659, 0.0401, 0.0349], grad_fn=<ToCopyBackward0>), [' original', ' other', ' best', ' first', ' script'])\n",
      "(tensor([0.1654, 0.1277, 0.1233, 0.0780, 0.0580], grad_fn=<ToCopyBackward0>), [' movie', ' movies', ' film', ' one', ' films'])\n",
      "(tensor([0.2713, 0.2632, 0.1398, 0.0476, 0.0352], grad_fn=<ToCopyBackward0>), [' in', ' I', ' that', ' better', ' on'])\n",
      "(tensor([0.5387, 0.3295, 0.0423, 0.0211, 0.0189], grad_fn=<ToCopyBackward0>), [' my', ' the', ' this', ' which', ' that'])\n",
      "(tensor([0.8284, 0.0286, 0.0244, 0.0043, 0.0034], grad_fn=<ToCopyBackward0>), [' series', ' genre', ' same', ' process', ' category'])\n",
      "(tensor([0.3173, 0.1837, 0.0547, 0.0454, 0.0290], grad_fn=<ToCopyBackward0>), [' look', ' better', ' so', ' that', ','])\n",
      "(tensor([0.4917, 0.2759, 0.0397, 0.0255, 0.0205], grad_fn=<ToCopyBackward0>), [' like', ' good', ' pretty', ' great', ' better'])\n",
      "(tensor([0.7295, 0.0643, 0.0565, 0.0446, 0.0253], grad_fn=<ToCopyBackward0>), ['.', ' in', '!', '...', ','])\n",
      "(tensor([0.2114, 0.1286, 0.0862, 0.0529, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'The'])\n",
      "(tensor([0.2248, 0.2215, 0.2063, 0.1020, 0.0501], grad_fn=<ToCopyBackward0>), [' is', ' film', ' movie', ' was', ' one'])\n",
      "(tensor([0.1706, 0.1266, 0.0540, 0.0524, 0.0391], grad_fn=<ToCopyBackward0>), [' the', ' a', ' one', ' definitely', ' by'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a really bad movie. I thought the acting was awful. I thought the plot was ridiculous. I was really surprised that I enjoyed the movie so much. But I really enjoyed this movie too. I really did enjoy it. I really do\n",
      "(tensor([0.3835, 0.1721, 0.0904, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1160, 0.0395, 0.0101, 0.0086], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1832, 0.1468, 0.0505, 0.0452, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1642, 0.1158, 0.0582, 0.0491, 0.0444], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.6009, 0.0521, 0.0461, 0.0388, 0.0233], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8276, 0.0668, 0.0244, 0.0073, 0.0053], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' horror', ' sequel'])\n",
      "(tensor([0.6649, 0.0633, 0.0364, 0.0350, 0.0293], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '...', '!'])\n",
      "(tensor([0.2890, 0.1360, 0.0639, 0.0329, 0.0295], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.1344, 0.0846, 0.0824, 0.0618, 0.0498], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', ' mean', \"'m\"])\n",
      "(tensor([0.7384, 0.0816, 0.0486, 0.0364, 0.0197], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.1689, 0.1574, 0.0835, 0.0517, 0.0315], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' story', ' plot', ' guy'])\n",
      "(tensor([0.8583, 0.0497, 0.0163, 0.0129, 0.0124], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' in', ' and'])\n",
      "(tensor([0.2888, 0.1132, 0.0922, 0.0655, 0.0592], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' horrible', ' awful', ' really'])\n",
      "(tensor([0.5081, 0.2160, 0.2018, 0.0189, 0.0076], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ';'])\n",
      "(tensor([0.3205, 0.1497, 0.1167, 0.0923, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' The', ' And', ' It', 'I'])\n",
      "(tensor([0.2675, 0.0811, 0.0685, 0.0516, 0.0501], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' really', ' mean', ' don'])\n",
      "(tensor([0.6268, 0.1988, 0.0282, 0.0274, 0.0230], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' that', ' they'])\n",
      "(tensor([0.2603, 0.2018, 0.1471, 0.0424, 0.0348], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' writing', ' movie'])\n",
      "(tensor([0.9054, 0.0130, 0.0077, 0.0070, 0.0054], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' wasn', ',', ' had'])\n",
      "(tensor([0.1677, 0.1260, 0.0745, 0.0597, 0.0457], grad_fn=<ToCopyBackward0>), [' stupid', ' ridiculous', ' terrible', ' really', ' awful'])\n",
      "(tensor([0.6171, 0.2443, 0.0722, 0.0044, 0.0043], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ';'])\n",
      "(tensor([0.3768, 0.1360, 0.0628, 0.0549, 0.0513], grad_fn=<ToCopyBackward0>), [' I', ' And', ' The', ' It', ' But'])\n",
      "(tensor([0.3402, 0.0941, 0.0645, 0.0547, 0.0527], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' just', ' really'])\n",
      "(tensor([0.2924, 0.1324, 0.0458, 0.0436, 0.0422], grad_fn=<ToCopyBackward0>), [' really', ' very', ' a', ' not', ' so'])\n",
      "(tensor([0.3719, 0.1281, 0.1226, 0.0340, 0.0187], grad_fn=<ToCopyBackward0>), [' disappointed', ',', ' surprised', ' annoyed', ' offended'])\n",
      "(tensor([0.3078, 0.1796, 0.1687, 0.1182, 0.0728], grad_fn=<ToCopyBackward0>), [' when', ' that', ' by', ' at', ' to'])\n",
      "(tensor([0.1379, 0.1111, 0.0846, 0.0669, 0.0637], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' a'])\n",
      "(tensor([0.1033, 0.0936, 0.0766, 0.0541, 0.0519], grad_fn=<ToCopyBackward0>), [' liked', ' was', ' enjoyed', ' actually', ' watched'])\n",
      "(tensor([0.5631, 0.2027, 0.1347, 0.0211, 0.0109], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' watching', ' that'])\n",
      "(tensor([0.8524, 0.0262, 0.0178, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' whole', ' book', ' original'])\n",
      "(tensor([0.2443, 0.2323, 0.0929, 0.0820, 0.0647], grad_fn=<ToCopyBackward0>), ['.', ' at', ' so', ',', ' as'])\n",
      "(tensor([9.9760e-01, 3.2446e-04, 2.5675e-04, 2.5562e-04, 1.9618e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' much', ' little', ' well', ' very', ' that'])\n",
      "(tensor([0.7481, 0.0835, 0.0306, 0.0202, 0.0166], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' after', ' when'])\n",
      "(tensor([0.3878, 0.1187, 0.0416, 0.0358, 0.0317], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' But'])\n",
      "(tensor([0.2661, 0.1057, 0.0701, 0.0665, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' after'])\n",
      "(tensor([0.1342, 0.1139, 0.0555, 0.0537, 0.0383], grad_fn=<ToCopyBackward0>), [' was', ' really', ' did', ' just', ' have'])\n",
      "(tensor([0.0855, 0.0801, 0.0693, 0.0643, 0.0601], grad_fn=<ToCopyBackward0>), [' didn', ' don', ' enjoyed', ' was', ' did'])\n",
      "(tensor([0.4575, 0.2037, 0.0911, 0.0376, 0.0228], grad_fn=<ToCopyBackward0>), [' it', ' the', ' watching', ' this', ' \"'])\n",
      "(tensor([0.5811, 0.1214, 0.0512, 0.0463, 0.0166], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' episode', '.'])\n",
      "(tensor([0.2600, 0.1527, 0.0840, 0.0818, 0.0564], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' too', ' a'])\n",
      "(tensor([0.8556, 0.0789, 0.0094, 0.0093, 0.0065], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' because', ' so'])\n",
      "(tensor([0.4425, 0.1755, 0.0387, 0.0258, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.2199, 0.0819, 0.0683, 0.0653, 0.0494], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' think', ' was', ' just'])\n",
      "(tensor([0.2009, 0.0942, 0.0825, 0.0712, 0.0671], grad_fn=<ToCopyBackward0>), [' enjoyed', ' like', ' liked', ' thought', ' did'])\n",
      "(tensor([0.4104, 0.3171, 0.1434, 0.0343, 0.0212], grad_fn=<ToCopyBackward0>), [' enjoy', '.', ' like', ' not', ','])\n",
      "(tensor([0.5757, 0.2140, 0.0918, 0.0310, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' watching', ' \"'])\n",
      "(tensor([0.4811, 0.1149, 0.0814, 0.0328, 0.0274], grad_fn=<ToCopyBackward0>), ['.', ',', ' a', ' in', ' more'])\n",
      "(tensor([0.4021, 0.1355, 0.0445, 0.0388, 0.0324], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', 'I', ' So'])\n",
      "(tensor([0.2526, 0.0868, 0.0622, 0.0521, 0.0514], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' just', ' think'])\n",
      "(tensor([0.4794, 0.0713, 0.0650, 0.0593, 0.0342], grad_fn=<ToCopyBackward0>), [' did', ' enjoyed', ' do', ' really', ' thought'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was watching a bad soap opera until I saw this horrid thing in the middle of the road in the middle of the night. I was truly shocked to see this in the middle of a busy road in my hometown of Regina, Canada. I\n",
      "(tensor([0.3834, 0.1723, 0.0903, 0.0770, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2250, 0.1946, 0.1573, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4599, 0.2360, 0.0590, 0.0170, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.5143, 0.1712, 0.0812, 0.0384, 0.0320], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' this', ' some'])\n",
      "(tensor([0.0846, 0.0680, 0.0573, 0.0535, 0.0469], grad_fn=<ToCopyBackward0>), [' good', ' really', ' horror', ' bad', ' movie'])\n",
      "(tensor([0.1895, 0.1225, 0.0917, 0.0593, 0.0405], grad_fn=<ToCopyBackward0>), [' comedy', ' movie', ' soap', ' acid', ' porn'])\n",
      "(tensor([0.9160, 0.0114, 0.0106, 0.0100, 0.0073], grad_fn=<ToCopyBackward0>), [' opera', '-', ' oper', ' movie', ','])\n",
      "(tensor([0.3453, 0.1797, 0.1248, 0.0433, 0.0353], grad_fn=<ToCopyBackward0>), [' when', '.', ',', ' until', ' and'])\n",
      "(tensor([0.8802, 0.0275, 0.0236, 0.0145, 0.0142], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' my'])\n",
      "(tensor([0.2042, 0.1849, 0.1176, 0.0637, 0.0488], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' realized', ' started', ' watched'])\n",
      "(tensor([0.3291, 0.2484, 0.0757, 0.0460, 0.0250], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' that', ' my'])\n",
      "(tensor([0.1683, 0.1150, 0.0761, 0.0408, 0.0385], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' one', ' on', ' hor'])\n",
      "(tensor([9.9195e-01, 1.4938e-03, 5.4124e-04, 5.2939e-04, 3.8777e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['rid', 'sey', 'c', 'r', 'nd'])\n",
      "(tensor([0.2588, 0.1607, 0.0795, 0.0700, 0.0324], grad_fn=<ToCopyBackward0>), [' thing', ' little', ' scene', ',', ' CGI'])\n",
      "(tensor([0.4743, 0.1139, 0.0447, 0.0391, 0.0296], grad_fn=<ToCopyBackward0>), ['.', ' in', ' on', ':', '!'])\n",
      "(tensor([0.5761, 0.3138, 0.0316, 0.0092, 0.0056], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' its', ' this'])\n",
      "(tensor([0.1667, 0.1287, 0.0520, 0.0487, 0.0340], grad_fn=<ToCopyBackward0>), [' middle', ' sky', ' background', ' corner', ' summer'])\n",
      "(tensor([9.8557e-01, 1.3800e-03, 1.2653e-03, 7.0500e-04, 3.3893e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' east', '.', ' ground', ' and'])\n",
      "(tensor([0.7948, 0.0357, 0.0290, 0.0186, 0.0169], grad_fn=<ToCopyBackward0>), [' the', ' nowhere', ' my', ' a', ' it'])\n",
      "(tensor([0.3299, 0.0328, 0.0326, 0.0287, 0.0233], grad_fn=<ToCopyBackward0>), [' road', ' woods', ' screen', ' street', ' movie'])\n",
      "(tensor([0.4743, 0.0505, 0.0468, 0.0334, 0.0307], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' with'])\n",
      "(tensor([0.7984, 0.0377, 0.0321, 0.0225, 0.0068], grad_fn=<ToCopyBackward0>), [' the', ' my', ' front', ' a', ' New'])\n",
      "(tensor([0.8389, 0.0244, 0.0160, 0.0056, 0.0050], grad_fn=<ToCopyBackward0>), [' middle', ' pouring', ' early', ' south', ' South'])\n",
      "(tensor([9.9779e-01, 1.1346e-04, 8.9741e-05, 7.7389e-05, 4.0067e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '-', ' o', ' east', ' and'])\n",
      "(tensor([0.9465, 0.0084, 0.0045, 0.0045, 0.0032], grad_fn=<ToCopyBackward0>), [' the', ' a', ' nowhere', ' my', ' South'])\n",
      "(tensor([0.6718, 0.0776, 0.0162, 0.0156, 0.0154], grad_fn=<ToCopyBackward0>), [' day', ' night', ' middle', ' city', ' country'])\n",
      "(tensor([0.5593, 0.0559, 0.0344, 0.0246, 0.0206], grad_fn=<ToCopyBackward0>), ['.', '...', ',', ' when', ' while'])\n",
      "(tensor([0.4100, 0.1423, 0.0586, 0.0253, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' No', ' A'])\n",
      "(tensor([0.1295, 0.1182, 0.0802, 0.0495, 0.0457], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' have', ' mean'])\n",
      "(tensor([0.1457, 0.0919, 0.0757, 0.0486, 0.0481], grad_fn=<ToCopyBackward0>), [' really', ' so', ' truly', ' actually', ' very'])\n",
      "(tensor([0.2149, 0.1638, 0.1246, 0.0781, 0.0282], grad_fn=<ToCopyBackward0>), [' fre', ' shocked', ' scared', ' surprised', ' in'])\n",
      "(tensor([0.3203, 0.1493, 0.1277, 0.1007, 0.0745], grad_fn=<ToCopyBackward0>), [' to', '.', ' and', ' by', ' when'])\n",
      "(tensor([0.7656, 0.0927, 0.0229, 0.0171, 0.0148], grad_fn=<ToCopyBackward0>), [' see', ' find', ' watch', ' say', ' be'])\n",
      "(tensor([0.3802, 0.1411, 0.0920, 0.0870, 0.0705], grad_fn=<ToCopyBackward0>), [' what', ' it', ' the', ' this', ' that'])\n",
      "(tensor([0.3816, 0.1066, 0.0468, 0.0276, 0.0230], grad_fn=<ToCopyBackward0>), ['.', ' in', ' thing', '!', ' on'])\n",
      "(tensor([0.5271, 0.3560, 0.0222, 0.0193, 0.0053], grad_fn=<ToCopyBackward0>), [' my', ' the', ' a', ' our', ' this'])\n",
      "(tensor([0.6998, 0.0793, 0.0289, 0.0096, 0.0078], grad_fn=<ToCopyBackward0>), [' middle', ' UK', ' daylight', ' road', ' dark'])\n",
      "(tensor([9.9876e-01, 3.9991e-05, 3.0721e-05, 2.7236e-05, 2.4720e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' on', ' in', '.', ' lane'])\n",
      "(tensor([0.8145, 0.0632, 0.0488, 0.0050, 0.0035], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' an', ' our'])\n",
      "(tensor([0.2995, 0.0942, 0.0417, 0.0329, 0.0305], grad_fn=<ToCopyBackward0>), [' busy', ' residential', ' dark', ' public', ' road'])\n",
      "(tensor([0.3580, 0.1142, 0.1121, 0.0948, 0.0415], grad_fn=<ToCopyBackward0>), [' road', ' highway', ' street', ' intersection', ' city'])\n",
      "(tensor([0.3141, 0.2453, 0.0958, 0.0504, 0.0407], grad_fn=<ToCopyBackward0>), [' in', '.', ' and', ',', ' with'])\n",
      "(tensor([0.6804, 0.1111, 0.0540, 0.0322, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' my', ' broad', ' a', ' New'])\n",
      "(tensor([0.3752, 0.1579, 0.0676, 0.0414, 0.0314], grad_fn=<ToCopyBackward0>), [' hometown', ' home', ' own', ' town', ' country'])\n",
      "(tensor([0.3873, 0.1554, 0.0361, 0.0321, 0.0255], grad_fn=<ToCopyBackward0>), ['.', ' of', '.\"', ' town', ','])\n",
      "(tensor([0.2154, 0.0576, 0.0360, 0.0324, 0.0267], grad_fn=<ToCopyBackward0>), [' Regina', ' Chicago', ' Ann', ' Toronto', ' New'])\n",
      "(tensor([0.6612, 0.1592, 0.0223, 0.0196, 0.0195], grad_fn=<ToCopyBackward0>), [',', '.', ' (', ' Saskatchewan', '.\"'])\n",
      "(tensor([0.8879, 0.0547, 0.0098, 0.0092, 0.0061], grad_fn=<ToCopyBackward0>), [' Saskatchewan', ' Canada', ' Manitoba', ' Sask', ' Regina'])\n",
      "(tensor([0.8730, 0.0328, 0.0108, 0.0104, 0.0102], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '.\"', ' ('])\n",
      "(tensor([0.4617, 0.0879, 0.0536, 0.0192, 0.0171], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would never watch it again. I thought it would be the one movie I wouldn't watch. I'm a movie-lover. I'm a movie-lover. But I watched this movie just to see what was in store for the\n",
      "(tensor([0.3840, 0.1715, 0.0898, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2241, 0.1944, 0.1575, 0.0695, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2254, 0.1218, 0.0840, 0.0368, 0.0235], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.3918, 0.0916, 0.0783, 0.0620, 0.0475], grad_fn=<ToCopyBackward0>), [' see', ' get', ' be', ' watch', ' find'])\n",
      "(tensor([0.4014, 0.2581, 0.0892, 0.0498, 0.0225], grad_fn=<ToCopyBackward0>), [' this', ' it', ' anything', ' a', ' any'])\n",
      "(tensor([0.4597, 0.2126, 0.0781, 0.0285, 0.0236], grad_fn=<ToCopyBackward0>), [' again', '.', ',', ' because', ' until'])\n",
      "(tensor([0.6431, 0.1704, 0.0538, 0.0357, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ' after', ',', ' because', ' but'])\n",
      "(tensor([0.4027, 0.1076, 0.0757, 0.0301, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' How'])\n",
      "(tensor([0.1532, 0.0574, 0.0525, 0.0430, 0.0430], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' watched', \"'m\", ' am'])\n",
      "(tensor([0.4324, 0.1771, 0.1231, 0.0649, 0.0421], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' that'])\n",
      "(tensor([0.5239, 0.3597, 0.0271, 0.0156, 0.0141], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' might', ' must'])\n",
      "(tensor([0.6981, 0.0855, 0.0537, 0.0377, 0.0191], grad_fn=<ToCopyBackward0>), [' be', ' never', ' have', ' go', ' make'])\n",
      "(tensor([0.2877, 0.1020, 0.0819, 0.0725, 0.0511], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' too', ' like'])\n",
      "(tensor([0.1918, 0.1615, 0.0855, 0.0782, 0.0734], grad_fn=<ToCopyBackward0>), [' last', ' worst', ' same', ' end', ' one'])\n",
      "(tensor([0.1914, 0.0885, 0.0698, 0.0460, 0.0396], grad_fn=<ToCopyBackward0>), [' movie', ' and', ' time', '.', ' that'])\n",
      "(tensor([0.4864, 0.3870, 0.0277, 0.0275, 0.0157], grad_fn=<ToCopyBackward0>), [' that', ' I', ' i', ' where', ' my'])\n",
      "(tensor([0.5313, 0.1151, 0.0814, 0.0549, 0.0334], grad_fn=<ToCopyBackward0>), [' would', \"'d\", ' could', ' couldn', ' wouldn'])\n",
      "(tensor([9.9541e-01, 1.5765e-03, 3.6455e-04, 2.8853e-04, 2.6109e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.3331, 0.1489, 0.0604, 0.0414, 0.0407], grad_fn=<ToCopyBackward0>), [' watch', ' ever', ' want', ' laugh', ' even'])\n",
      "(tensor([0.5520, 0.1346, 0.0689, 0.0423, 0.0343], grad_fn=<ToCopyBackward0>), [' again', '.', ' at', ' in', ','])\n",
      "(tensor([0.3031, 0.1097, 0.0726, 0.0458, 0.0319], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' Then', ' And'])\n",
      "(tensor([0.1832, 0.0948, 0.0610, 0.0410, 0.0372], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' watched', ' actually'])\n",
      "(tensor([0.1518, 0.1445, 0.1124, 0.0629, 0.0573], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' really', ' actually'])\n",
      "(tensor([0.3262, 0.1398, 0.0594, 0.0315, 0.0184], grad_fn=<ToCopyBackward0>), [' big', ' fan', ' huge', ' movie', ' film'])\n",
      "(tensor([0.2382, 0.0956, 0.0810, 0.0792, 0.0692], grad_fn=<ToCopyBackward0>), ['-', ' guy', ' person', ' buff', ' sn'])\n",
      "(tensor([0.3636, 0.1437, 0.0602, 0.0308, 0.0278], grad_fn=<ToCopyBackward0>), ['l', 'go', 'j', 'buff', 'loving'])\n",
      "(tensor([0.9427, 0.0140, 0.0052, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), ['over', 'oser', 'ifer', 'one', 'ion'])\n",
      "(tensor([0.4345, 0.2269, 0.1119, 0.0494, 0.0158], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' at', ';'])\n",
      "(tensor([0.6660, 0.0728, 0.0527, 0.0195, 0.0146], grad_fn=<ToCopyBackward0>), [' I', ' But', ' And', ' It', ' So'])\n",
      "(tensor([0.1388, 0.0769, 0.0552, 0.0522, 0.0491], grad_fn=<ToCopyBackward0>), [' love', \"'m\", ' like', ' can', ' just'])\n",
      "(tensor([0.4226, 0.1704, 0.0512, 0.0311, 0.0256], grad_fn=<ToCopyBackward0>), [' a', ' not', ' an', ' always', ' the'])\n",
      "(tensor([0.2385, 0.1811, 0.1277, 0.0745, 0.0506], grad_fn=<ToCopyBackward0>), [' movie', ' big', ' fan', ' film', ' huge'])\n",
      "(tensor([0.7980, 0.0276, 0.0171, 0.0168, 0.0160], grad_fn=<ToCopyBackward0>), ['-', ' buff', ' freak', ' fan', ' lover'])\n",
      "(tensor([0.7833, 0.0318, 0.0198, 0.0126, 0.0124], grad_fn=<ToCopyBackward0>), ['l', 'h', 's', 'go', 'w'])\n",
      "(tensor([0.9129, 0.0066, 0.0057, 0.0050, 0.0043], grad_fn=<ToCopyBackward0>), ['over', 'ifer', 'ob', 'oved', 'iver'])\n",
      "(tensor([0.2080, 0.1364, 0.1325, 0.0900, 0.0376], grad_fn=<ToCopyBackward0>), [' to', ' at', '.', ',', ' for'])\n",
      "(tensor([0.4528, 0.1248, 0.0916, 0.0403, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' But', ' And', ' So', ' It'])\n",
      "(tensor([0.3049, 0.2483, 0.0430, 0.0336, 0.0328], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' after', ' the'])\n",
      "(tensor([0.1128, 0.1010, 0.0985, 0.0452, 0.0410], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' just', \"'m\", ' really'])\n",
      "(tensor([0.7210, 0.1652, 0.0482, 0.0102, 0.0045], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' \"', ' that'])\n",
      "(tensor([0.7574, 0.0747, 0.0335, 0.0135, 0.0088], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', ' just', ' because'])\n",
      "(tensor([0.3568, 0.0589, 0.0474, 0.0353, 0.0344], grad_fn=<ToCopyBackward0>), [' just', ' and', ',', '.', ' the'])\n",
      "(tensor([0.3278, 0.2092, 0.2086, 0.0457, 0.0318], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' one', ' because'])\n",
      "(tensor([0.5724, 0.0608, 0.0553, 0.0329, 0.0257], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' watch', ' laugh'])\n",
      "(tensor([0.1543, 0.1378, 0.1198, 0.0914, 0.0703], grad_fn=<ToCopyBackward0>), [' how', ' if', ' what', ' it', ' the'])\n",
      "(tensor([0.2904, 0.1115, 0.0602, 0.0495, 0.0362], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' was', ' they'])\n",
      "(tensor([0.2520, 0.2388, 0.2326, 0.0297, 0.0203], grad_fn=<ToCopyBackward0>), [' in', ' on', ' going', ' at', ' so'])\n",
      "(tensor([0.5408, 0.0973, 0.0892, 0.0660, 0.0617], grad_fn=<ToCopyBackward0>), [' it', ' store', ' this', ' the', ' there'])\n",
      "(tensor([0.8336, 0.1135, 0.0145, 0.0058, 0.0049], grad_fn=<ToCopyBackward0>), [' for', '.', ',', ' and', ' after'])\n",
      "(tensor([0.2021, 0.0702, 0.0638, 0.0218, 0.0216], grad_fn=<ToCopyBackward0>), [' the', ' us', ' me', ' my', ' this'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it could be the worst movie ever made, and it was. The acting was horrible. The plot was ridiculous. It was a complete joke. And that's why I'm a fan of it: because it was so terrible. The plot was ridiculous\n",
      "(tensor([0.3837, 0.1720, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.5409, 0.2984, 0.0181, 0.0169, 0.0167], grad_fn=<ToCopyBackward0>), [' be', ' have', ' only', ' not', ' make'])\n",
      "(tensor([0.4211, 0.1057, 0.1011, 0.0476, 0.0408], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' the', ' an', ' better'])\n",
      "(tensor([0.1704, 0.1415, 0.0383, 0.0364, 0.0338], grad_fn=<ToCopyBackward0>), [' worst', ' movie', ' best', ' film', ' same'])\n",
      "(tensor([0.8054, 0.0750, 0.0168, 0.0110, 0.0103], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' horror', ' comedy', ' Christmas'])\n",
      "(tensor([0.6224, 0.2171, 0.0759, 0.0281, 0.0112], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' i', ' of', ' in'])\n",
      "(tensor([0.6445, 0.1518, 0.0615, 0.0148, 0.0090], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' if', '...'])\n",
      "(tensor([0.5466, 0.2410, 0.0296, 0.0194, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ',', ' if', '!', '...'])\n",
      "(tensor([0.3214, 0.1518, 0.0710, 0.0228, 0.0201], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' the', ' then'])\n",
      "(tensor([0.2952, 0.1935, 0.0710, 0.0634, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' that', ' the'])\n",
      "(tensor([0.2902, 0.2753, 0.0874, 0.0398, 0.0225], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' is', \"'s\", ' turned'])\n",
      "(tensor([0.6324, 0.0683, 0.0331, 0.0228, 0.0146], grad_fn=<ToCopyBackward0>), ['.', ' not', ',', ' so', ' funny'])\n",
      "(tensor([0.2806, 0.1129, 0.0975, 0.0370, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0978, 0.0884, 0.0671, 0.0671, 0.0567], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' movie', ' plot'])\n",
      "(tensor([0.5918, 0.1714, 0.0760, 0.0219, 0.0204], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' and', '?'])\n",
      "(tensor([0.1816, 0.1047, 0.0816, 0.0733, 0.0658], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' so', ' horrible'])\n",
      "(tensor([0.5288, 0.2408, 0.1589, 0.0108, 0.0073], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ';', ' ('])\n",
      "(tensor([0.5557, 0.0908, 0.0666, 0.0313, 0.0131], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.3227, 0.1727, 0.0836, 0.0346, 0.0274], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' only'])\n",
      "(tensor([0.8442, 0.0157, 0.0110, 0.0108, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' had', ' sounded'])\n",
      "(tensor([0.1542, 0.0783, 0.0600, 0.0441, 0.0406], grad_fn=<ToCopyBackward0>), [' ridiculous', ' stupid', ' terrible', ' predictable', ' even'])\n",
      "(tensor([0.6953, 0.1732, 0.0867, 0.0068, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ';'])\n",
      "(tensor([0.5475, 0.0991, 0.0801, 0.0614, 0.0120], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.4398, 0.1241, 0.0619, 0.0360, 0.0356], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' wasn', ' seemed'])\n",
      "(tensor([0.1262, 0.0958, 0.0884, 0.0586, 0.0272], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' like', ' the'])\n",
      "(tensor([0.1251, 0.0831, 0.0613, 0.0431, 0.0343], grad_fn=<ToCopyBackward0>), [' big', ' bad', ' complete', ' total', ' mess'])\n",
      "(tensor([0.3197, 0.1240, 0.0817, 0.0444, 0.0403], grad_fn=<ToCopyBackward0>), [' waste', ' joke', ' disaster', ' mess', ' rip'])\n",
      "(tensor([0.8385, 0.0322, 0.0270, 0.0177, 0.0153], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' from', ' with'])\n",
      "(tensor([0.2037, 0.1379, 0.1208, 0.0369, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', 'I'])\n",
      "(tensor([0.2796, 0.1850, 0.0801, 0.0603, 0.0305], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' that', ','])\n",
      "(tensor([0.3581, 0.2894, 0.1388, 0.0479, 0.0082], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' movie', ' makes'])\n",
      "(tensor([0.2792, 0.1491, 0.0659, 0.0570, 0.0521], grad_fn=<ToCopyBackward0>), [' not', ' why', ' the', ' just', ' saying'])\n",
      "(tensor([0.4660, 0.3218, 0.0409, 0.0250, 0.0160], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' people'])\n",
      "(tensor([0.1147, 0.1056, 0.0644, 0.0372, 0.0343], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' watched', ' stopped', ' didn'])\n",
      "(tensor([0.2395, 0.1056, 0.0712, 0.0654, 0.0591], grad_fn=<ToCopyBackward0>), [' not', ' a', ' giving', ' still', ' really'])\n",
      "(tensor([0.1913, 0.1412, 0.1184, 0.0392, 0.0312], grad_fn=<ToCopyBackward0>), [' movie', ' fan', ' film', ' huge', ' big'])\n",
      "(tensor([0.9394, 0.0116, 0.0092, 0.0078, 0.0065], grad_fn=<ToCopyBackward0>), [' of', ' now', '.', ',', ' for'])\n",
      "(tensor([0.1053, 0.0930, 0.0396, 0.0315, 0.0220], grad_fn=<ToCopyBackward0>), [' the', ' it', ' bad', ' movies', ' low'])\n",
      "(tensor([0.4430, 0.1341, 0.0549, 0.0536, 0.0327], grad_fn=<ToCopyBackward0>), ['.', ',', ':', ' now', ' -'])\n",
      "(tensor([0.2052, 0.1864, 0.1674, 0.1117, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' because', ' It', ' Because', ' it'])\n",
      "(tensor([0.3420, 0.2915, 0.0845, 0.0289, 0.0221], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' of', ' there'])\n",
      "(tensor([0.2870, 0.2812, 0.0810, 0.0356, 0.0199], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' had'])\n",
      "(tensor([0.5403, 0.0507, 0.0450, 0.0345, 0.0259], grad_fn=<ToCopyBackward0>), [' so', ' funny', ' a', ' bad', ' such'])\n",
      "(tensor([0.4597, 0.0747, 0.0563, 0.0482, 0.0289], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' stupid', ' ridiculous', ' funny'])\n",
      "(tensor([0.7431, 0.0734, 0.0526, 0.0264, 0.0193], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' and', ' that'])\n",
      "(tensor([0.1866, 0.1173, 0.0470, 0.0402, 0.0390], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' The'])\n",
      "(tensor([0.1181, 0.0687, 0.0313, 0.0284, 0.0280], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' plot', ' worst', ' people'])\n",
      "(tensor([0.6220, 0.0777, 0.0230, 0.0212, 0.0194], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' had', ' makes'])\n",
      "(tensor([0.1909, 0.1318, 0.1291, 0.0455, 0.0415], grad_fn=<ToCopyBackward0>), [' so', ' ridiculous', ' stupid', ' just', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty funny how much the film was dated in the movie world, and I really enjoyed watching it because it was so dated, I just really enjoyed watching it. But I really don't think I would recommend it. It's not funny,\n",
      "(tensor([0.3844, 0.1713, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1165, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1843, 0.1452, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1139, 0.0954, 0.0748, 0.0613, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.2842, 0.0860, 0.0656, 0.0651, 0.0540], grad_fn=<ToCopyBackward0>), [' when', ' how', ' to', ' watching', ' and'])\n",
      "(tensor([0.1828, 0.1761, 0.0659, 0.0386, 0.0307], grad_fn=<ToCopyBackward0>), [' the', ' many', ' much', ' some', ' a'])\n",
      "(tensor([0.0807, 0.0645, 0.0576, 0.0523, 0.0398], grad_fn=<ToCopyBackward0>), [' of', ' they', ' money', ' the', ' it'])\n",
      "(tensor([0.1684, 0.0936, 0.0605, 0.0602, 0.0376], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' writers', ' DVD', ' script'])\n",
      "(tensor([0.2903, 0.0424, 0.0403, 0.0316, 0.0281], grad_fn=<ToCopyBackward0>), [' was', ' resembled', ' tried', ' is', ' had'])\n",
      "(tensor([0.1584, 0.0391, 0.0369, 0.0315, 0.0282], grad_fn=<ToCopyBackward0>), [' based', ' about', ' dated', ' compared', ' focused'])\n",
      "(tensor([0.1934, 0.1904, 0.1308, 0.1031, 0.0562], grad_fn=<ToCopyBackward0>), [' in', ' and', '.', ',', ' at'])\n",
      "(tensor([0.1522, 0.1106, 0.0657, 0.0355, 0.0349], grad_fn=<ToCopyBackward0>), [' the', ' 2001', ' its', ' terms', ' spots'])\n",
      "(tensor([0.1593, 0.0864, 0.0558, 0.0477, 0.0386], grad_fn=<ToCopyBackward0>), [' first', ' beginning', ' way', ' movie', ' 80'])\n",
      "(tensor([0.1186, 0.0978, 0.0556, 0.0419, 0.0258], grad_fn=<ToCopyBackward0>), ['.', ' industry', ',', ' theater', ' world'])\n",
      "(tensor([0.1695, 0.1241, 0.1051, 0.0889, 0.0794], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' at', ' back'])\n",
      "(tensor([0.1591, 0.1536, 0.0400, 0.0392, 0.0371], grad_fn=<ToCopyBackward0>), [' but', ' and', ' especially', ' how', ' the'])\n",
      "(tensor([0.1608, 0.1408, 0.1161, 0.0751, 0.0678], grad_fn=<ToCopyBackward0>), [' that', ' I', ' how', ' the', ' then'])\n",
      "(tensor([0.1175, 0.1097, 0.1021, 0.0641, 0.0564], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' really', ' just'])\n",
      "(tensor([0.1496, 0.1089, 0.0976, 0.0747, 0.0452], grad_fn=<ToCopyBackward0>), [' enjoyed', ' wanted', ' liked', ' didn', ' don'])\n",
      "(tensor([0.2886, 0.2006, 0.1759, 0.0497, 0.0294], grad_fn=<ToCopyBackward0>), [' the', ' it', ' watching', ' seeing', ' how'])\n",
      "(tensor([0.4384, 0.1369, 0.0174, 0.0171, 0.0128], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' all', ' this'])\n",
      "(tensor([0.3785, 0.1140, 0.0789, 0.0731, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ' in', ' as', ',', ' because'])\n",
      "(tensor([0.2872, 0.2193, 0.1141, 0.1066, 0.0334], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.4886, 0.0805, 0.0473, 0.0384, 0.0289], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' wasn', ' had'])\n",
      "(tensor([0.4200, 0.0699, 0.0460, 0.0430, 0.0332], grad_fn=<ToCopyBackward0>), [' so', ' like', ' a', ' such', ' very'])\n",
      "(tensor([0.1517, 0.1046, 0.0900, 0.0484, 0.0388], grad_fn=<ToCopyBackward0>), [' bad', ' low', ' dated', ' much', ' over'])\n",
      "(tensor([0.4255, 0.1938, 0.1196, 0.0802, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' in', ' and', ',', ' at'])\n",
      "(tensor([0.2809, 0.2764, 0.0428, 0.0380, 0.0245], grad_fn=<ToCopyBackward0>), [' and', ' but', ' I', ' it', ' so'])\n",
      "(tensor([0.1205, 0.1110, 0.1099, 0.1007, 0.0703], grad_fn=<ToCopyBackward0>), [' thought', ' just', ' really', ' mean', ' was'])\n",
      "(tensor([0.3266, 0.0790, 0.0562, 0.0452, 0.0422], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' think', ' couldn', ' didn'])\n",
      "(tensor([0.4154, 0.0972, 0.0784, 0.0492, 0.0427], grad_fn=<ToCopyBackward0>), [' enjoyed', ' liked', ' thought', ' wanted', ' enjoy'])\n",
      "(tensor([0.4461, 0.2322, 0.1235, 0.0466, 0.0227], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' seeing', ' how'])\n",
      "(tensor([0.5959, 0.1255, 0.0191, 0.0112, 0.0106], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' all', ' this'])\n",
      "(tensor([0.5143, 0.1056, 0.0899, 0.0636, 0.0561], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' and', ' in'])\n",
      "(tensor([0.2695, 0.1020, 0.0821, 0.0628, 0.0339], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' The'])\n",
      "(tensor([0.1206, 0.1013, 0.0882, 0.0748, 0.0509], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' when', ' then'])\n",
      "(tensor([0.1213, 0.0944, 0.0739, 0.0432, 0.0394], grad_fn=<ToCopyBackward0>), [' really', ' was', ' just', ' don', ' also'])\n",
      "(tensor([0.1282, 0.0743, 0.0705, 0.0697, 0.0658], grad_fn=<ToCopyBackward0>), [' enjoyed', ' wanted', ' didn', ' don', ' enjoy'])\n",
      "(tensor([9.9632e-01, 1.1528e-03, 5.4706e-04, 2.1709e-04, 1.5447e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3201, 0.1647, 0.1125, 0.1019, 0.0412], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' remember', ' want'])\n",
      "(tensor([0.4339, 0.2745, 0.0458, 0.0422, 0.0354], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' I', ' the'])\n",
      "(tensor([0.1490, 0.1404, 0.0782, 0.0695, 0.0602], grad_fn=<ToCopyBackward0>), [\"'m\", ' would', ' was', ' could', ' can'])\n",
      "(tensor([0.2354, 0.1310, 0.0871, 0.0688, 0.0510], grad_fn=<ToCopyBackward0>), [' have', ' recommend', ' watch', ' ever', ' like'])\n",
      "(tensor([0.5529, 0.1622, 0.0874, 0.0261, 0.0231], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' people', ' watching'])\n",
      "(tensor([0.3178, 0.2413, 0.0813, 0.0762, 0.0575], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' as', ' for'])\n",
      "(tensor([0.3345, 0.1518, 0.0506, 0.0288, 0.0244], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' If'])\n",
      "(tensor([0.4979, 0.1798, 0.0646, 0.0494, 0.0225], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' just', ' has'])\n",
      "(tensor([0.1672, 0.1102, 0.1027, 0.0927, 0.0815], grad_fn=<ToCopyBackward0>), [' just', ' dated', ' not', ' a', ' kind'])\n",
      "(tensor([0.3740, 0.0919, 0.0750, 0.0739, 0.0738], grad_fn=<ToCopyBackward0>), [' really', ' that', ' funny', ' as', ' even'])\n",
      "(tensor([0.3195, 0.2370, 0.0799, 0.0402, 0.0399], grad_fn=<ToCopyBackward0>), [',', '.', ' any', ' in', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I was going to be in for a long time. I thought I was in for a couple of years. It's not the same as it used to be. I'm just happy that I'm still here, that other players have given me\n",
      "(tensor([0.3839, 0.1715, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3315, 0.2364, 0.0584, 0.0561, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2144, 0.1253, 0.1123, 0.0826, 0.0741], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.4713, 0.1363, 0.0312, 0.0291, 0.0225], grad_fn=<ToCopyBackward0>), [' going', ' in', ' so', ' watching', ' a'])\n",
      "(tensor([9.8498e-01, 2.6942e-03, 1.9285e-03, 1.1708e-03, 9.1285e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' through', ' in', ' into'])\n",
      "(tensor([0.2921, 0.2070, 0.1062, 0.0924, 0.0712], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' get', ' see', ' have'])\n",
      "(tensor([0.0818, 0.0703, 0.0493, 0.0423, 0.0415], grad_fn=<ToCopyBackward0>), [' a', ' in', ' very', ' the', ' able'])\n",
      "(tensor([0.2838, 0.2381, 0.0847, 0.0720, 0.0592], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' this', ' trouble'])\n",
      "(tensor([0.7159, 0.0795, 0.0248, 0.0208, 0.0189], grad_fn=<ToCopyBackward0>), [' a', ' an', ' another', ' some', ' the'])\n",
      "(tensor([0.2050, 0.0913, 0.0610, 0.0526, 0.0324], grad_fn=<ToCopyBackward0>), [' long', ' big', ' real', ' movie', ' good'])\n",
      "(tensor([0.2201, 0.1140, 0.0510, 0.0292, 0.0286], grad_fn=<ToCopyBackward0>), [' night', ' time', ' day', ',', ' haul'])\n",
      "(tensor([0.2816, 0.1089, 0.1052, 0.0917, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ',', ' watching', ' with', ' and'])\n",
      "(tensor([0.3780, 0.1134, 0.0846, 0.0415, 0.0241], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2662, 0.1202, 0.0439, 0.0439, 0.0378], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' just', \"'m\", ' had'])\n",
      "(tensor([0.4194, 0.1618, 0.0767, 0.0598, 0.0463], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' this', ' my'])\n",
      "(tensor([0.6219, 0.1395, 0.0545, 0.0424, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', \"'d\", ' could'])\n",
      "(tensor([0.8654, 0.0812, 0.0055, 0.0033, 0.0028], grad_fn=<ToCopyBackward0>), [' going', ' in', ' not', ' gonna', ' getting'])\n",
      "(tensor([0.8422, 0.0362, 0.0279, 0.0273, 0.0147], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' my', ' to'])\n",
      "(tensor([0.4467, 0.0910, 0.0383, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' two', ' the', ' 10', ' one'])\n",
      "(tensor([0.3089, 0.2173, 0.1255, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' long', ' lifetime', ' couple', ' year', ' sequel'])\n",
      "(tensor([9.8679e-01, 6.8076e-03, 1.6777e-03, 6.4761e-04, 3.8786e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' years', ' more', ' months', ' hundred'])\n",
      "(tensor([0.5493, 0.1169, 0.0705, 0.0439, 0.0406], grad_fn=<ToCopyBackward0>), [' years', ' hours', ' days', ' weeks', ' months'])\n",
      "(tensor([0.3397, 0.1284, 0.0932, 0.0481, 0.0362], grad_fn=<ToCopyBackward0>), ['.', ',', ' before', ' at', ' but'])\n",
      "(tensor([0.3758, 0.0786, 0.0647, 0.0516, 0.0387], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' And'])\n",
      "(tensor([0.2859, 0.2770, 0.0566, 0.0524, 0.0326], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' seemed', ' is'])\n",
      "(tensor([0.1990, 0.1935, 0.1517, 0.0313, 0.0305], grad_fn=<ToCopyBackward0>), [' been', ' not', ' a', ' one', ' just'])\n",
      "(tensor([0.1374, 0.1040, 0.1038, 0.0973, 0.0576], grad_fn=<ToCopyBackward0>), [' that', ' even', ' the', ' like', ' a'])\n",
      "(tensor([0.1620, 0.1386, 0.1191, 0.0735, 0.0603], grad_fn=<ToCopyBackward0>), [' case', ' end', ' best', ' worst', ' same'])\n",
      "(tensor([0.4744, 0.0891, 0.0629, 0.0376, 0.0317], grad_fn=<ToCopyBackward0>), [' movie', ' as', ' film', ' story', ' scene'])\n",
      "(tensor([0.1371, 0.0963, 0.0881, 0.0460, 0.0226], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' a', ' my'])\n",
      "(tensor([0.4630, 0.3338, 0.0603, 0.0245, 0.0236], grad_fn=<ToCopyBackward0>), [' used', ' was', ' once', ' is', \"'s\"])\n",
      "(tensor([9.9789e-01, 1.4036e-03, 1.0226e-04, 3.4782e-05, 2.4609e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' be', '-', ' been', ' too'])\n",
      "(tensor([9.9388e-01, 3.3567e-03, 5.8066e-04, 5.3769e-04, 1.3721e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' feel', '.', ' have', ' get'])\n",
      "(tensor([0.5569, 0.1259, 0.0794, 0.0562, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' but'])\n",
      "(tensor([0.3054, 0.2131, 0.1046, 0.0252, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' You'])\n",
      "(tensor([0.1342, 0.0867, 0.0802, 0.0627, 0.0595], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' am', ' can', ' just'])\n",
      "(tensor([0.1949, 0.1159, 0.0728, 0.0480, 0.0432], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' in', ' going'])\n",
      "(tensor([0.0934, 0.0682, 0.0672, 0.0622, 0.0606], grad_fn=<ToCopyBackward0>), [' trying', ' not', ' happy', ' out', ' really'])\n",
      "(tensor([0.4551, 0.2211, 0.2046, 0.0254, 0.0176], grad_fn=<ToCopyBackward0>), [' to', ' that', ' I', ' with', ' it'])\n",
      "(tensor([0.8733, 0.0377, 0.0117, 0.0068, 0.0067], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' we', ' they'])\n",
      "(tensor([0.4538, 0.0556, 0.0553, 0.0497, 0.0409], grad_fn=<ToCopyBackward0>), [\"'m\", ' am', \"'ve\", ' was', ' can'])\n",
      "(tensor([0.2964, 0.1015, 0.0860, 0.0555, 0.0492], grad_fn=<ToCopyBackward0>), [' here', ' still', ' not', ' able', ' in'])\n",
      "(tensor([0.5657, 0.1336, 0.0316, 0.0270, 0.0217], grad_fn=<ToCopyBackward0>), [' here', ' able', ' playing', ' alive', ' around'])\n",
      "(tensor([0.5145, 0.1406, 0.0616, 0.0399, 0.0380], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' with', ' in'])\n",
      "(tensor([0.2797, 0.1259, 0.0944, 0.0906, 0.0549], grad_fn=<ToCopyBackward0>), [' and', ' but', ' that', ' still', ' even'])\n",
      "(tensor([0.8142, 0.0269, 0.0169, 0.0126, 0.0101], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' people', ' other'])\n",
      "(tensor([0.6644, 0.1096, 0.0950, 0.0118, 0.0108], grad_fn=<ToCopyBackward0>), [' people', ' guys', ' kids', ' players', ' young'])\n",
      "(tensor([0.2530, 0.0514, 0.0410, 0.0374, 0.0357], grad_fn=<ToCopyBackward0>), [' are', ' have', ' on', ' like', ' still'])\n",
      "(tensor([0.1182, 0.0985, 0.0587, 0.0416, 0.0347], grad_fn=<ToCopyBackward0>), [' given', ' been', ' made', ' gone', ' come'])\n",
      "(tensor([0.9059, 0.0367, 0.0157, 0.0059, 0.0043], grad_fn=<ToCopyBackward0>), [' me', ' it', ' up', ' the', ' back'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was the worst movie I have ever seen in my life. The plot was ridiculous and the acting was terrible! I really wanted to shoot my agent in the head when I saw this movie. I was actually really disappointed. I thought that after I\n",
      "(tensor([0.3833, 0.1721, 0.0902, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1161, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1835, 0.1464, 0.0506, 0.0452, 0.0435], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.3200, 0.1424, 0.0620, 0.0385, 0.0218], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' best', ' biggest', ' movie'])\n",
      "(tensor([0.7418, 0.0792, 0.0225, 0.0204, 0.0085], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' horror', ' Christmas'])\n",
      "(tensor([0.7029, 0.1339, 0.1136, 0.0098, 0.0074], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.3362, 0.2562, 0.1934, 0.1118, 0.0703], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([9.2145e-01, 7.1587e-02, 1.4959e-03, 8.8932e-04, 6.4594e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9133, 0.0204, 0.0128, 0.0099, 0.0096], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' made'])\n",
      "(tensor([0.5241, 0.1623, 0.1292, 0.0602, 0.0136], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', '!', ' and'])\n",
      "(tensor([9.8826e-01, 4.2716e-03, 3.4804e-03, 1.2302e-03, 5.5441e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' my', ' the', ' all', ' a', ' this'])\n",
      "(tensor([7.5756e-01, 2.1177e-01, 2.2331e-02, 3.0274e-03, 4.0710e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' life', ' entire', ' whole', ' lifetime', ' movie'])\n",
      "(tensor([0.7093, 0.0662, 0.0423, 0.0223, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' until', ' when'])\n",
      "(tensor([0.3455, 0.1358, 0.0892, 0.0258, 0.0234], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.1514, 0.1148, 0.0744, 0.0706, 0.0590], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' story', ' movie'])\n",
      "(tensor([0.7674, 0.0904, 0.0140, 0.0102, 0.0090], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' sounded', ' had'])\n",
      "(tensor([0.1588, 0.1039, 0.0955, 0.0504, 0.0358], grad_fn=<ToCopyBackward0>), [' so', ' stupid', ' ridiculous', ' terrible', ' just'])\n",
      "(tensor([0.3873, 0.2812, 0.2264, 0.0117, 0.0109], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ';', '!'])\n",
      "(tensor([0.4778, 0.0825, 0.0333, 0.0178, 0.0176], grad_fn=<ToCopyBackward0>), [' the', ' predictable', ' I', ' it', ' stupid'])\n",
      "(tensor([0.7376, 0.0421, 0.0263, 0.0184, 0.0153], grad_fn=<ToCopyBackward0>), [' acting', ' dialogue', ' actors', ' movie', ' characters'])\n",
      "(tensor([0.7393, 0.0199, 0.0167, 0.0116, 0.0111], grad_fn=<ToCopyBackward0>), [' was', ',', '...', ' (', ' so'])\n",
      "(tensor([0.0955, 0.0846, 0.0723, 0.0399, 0.0397], grad_fn=<ToCopyBackward0>), [' terrible', ' even', ' so', ' horrible', ' worse'])\n",
      "(tensor([0.8413, 0.0374, 0.0357, 0.0165, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.3114, 0.1867, 0.0839, 0.0182, 0.0178], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' No'])\n",
      "(tensor([0.0734, 0.0554, 0.0505, 0.0448, 0.0419], grad_fn=<ToCopyBackward0>), [' was', ' have', ' don', ' would', ' really'])\n",
      "(tensor([0.1463, 0.0695, 0.0694, 0.0662, 0.0488], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' can', ' wanted', ' feel'])\n",
      "(tensor([0.5565, 0.3120, 0.0442, 0.0282, 0.0125], grad_fn=<ToCopyBackward0>), [' to', ' my', ' the', ' it', ' that'])\n",
      "(tensor([0.1942, 0.0927, 0.0654, 0.0612, 0.0534], grad_fn=<ToCopyBackward0>), [' shoot', ' walk', ' see', ' kill', ' turn'])\n",
      "(tensor([0.3308, 0.1476, 0.1010, 0.1009, 0.0831], grad_fn=<ToCopyBackward0>), [' myself', ' my', ' it', ' the', ' someone'])\n",
      "(tensor([0.1959, 0.1364, 0.1128, 0.0872, 0.0211], grad_fn=<ToCopyBackward0>), [' eye', ' agent', ' eyes', ' mouth', ' ears'])\n",
      "(tensor([0.1593, 0.1575, 0.0619, 0.0337, 0.0288], grad_fn=<ToCopyBackward0>), [' because', ' in', ' so', ' with', ' out'])\n",
      "(tensor([9.7885e-01, 1.1714e-02, 2.5943e-03, 1.0962e-03, 7.2072e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' his', ' her'])\n",
      "(tensor([0.7031, 0.0892, 0.0344, 0.0269, 0.0187], grad_fn=<ToCopyBackward0>), [' head', ' face', ' eye', ' forehead', ' mouth'])\n",
      "(tensor([0.2391, 0.1420, 0.1076, 0.0841, 0.0735], grad_fn=<ToCopyBackward0>), [' when', ' with', '.', ' because', ' and'])\n",
      "(tensor([0.7721, 0.1039, 0.0310, 0.0309, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' he', ' she', ' i', ' this'])\n",
      "(tensor([0.5831, 0.0762, 0.0654, 0.0533, 0.0339], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' found', ' was', ' read'])\n",
      "(tensor([0.4915, 0.1959, 0.0947, 0.0598, 0.0377], grad_fn=<ToCopyBackward0>), [' this', ' the', ' that', ' it', ' how'])\n",
      "(tensor([0.6824, 0.0909, 0.0838, 0.0364, 0.0099], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', '!'])\n",
      "(tensor([0.6370, 0.1045, 0.0553, 0.0341, 0.0195], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' and'])\n",
      "(tensor([0.3818, 0.1183, 0.1071, 0.0336, 0.0155], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.0637, 0.0625, 0.0577, 0.0564, 0.0482], grad_fn=<ToCopyBackward0>), [' really', ' have', ' think', ' was', ' thought'])\n",
      "(tensor([0.1302, 0.0938, 0.0894, 0.0668, 0.0542], grad_fn=<ToCopyBackward0>), [' so', ' really', ' actually', ' in', ' a'])\n",
      "(tensor([0.1183, 0.0968, 0.0723, 0.0384, 0.0349], grad_fn=<ToCopyBackward0>), [' really', ' in', ' very', ' so', ' a'])\n",
      "(tensor([0.1662, 0.1241, 0.0998, 0.0790, 0.0724], grad_fn=<ToCopyBackward0>), [' surprised', ' looking', ' disappointed', ' angry', ' excited'])\n",
      "(tensor([0.2770, 0.2220, 0.1122, 0.1040, 0.0461], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', '.', ' that'])\n",
      "(tensor([0.3944, 0.1145, 0.1088, 0.0470, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.0799, 0.0773, 0.0765, 0.0553, 0.0450], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' have', ' think'])\n",
      "(tensor([0.5118, 0.1307, 0.0812, 0.0802, 0.0601], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' that'])\n",
      "(tensor([0.3352, 0.2146, 0.0929, 0.0778, 0.0186], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' I', ' after'])\n",
      "(tensor([0.1532, 0.1481, 0.0905, 0.0739, 0.0656], grad_fn=<ToCopyBackward0>), [' I', ' this', ' watching', ' the', ' all'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was very well made. There were a number of interesting characters in it and I thought the story was fairly original. However, the acting wasn't very convincing. I thought that it was very difficult for the actors to convincingly portray the\n",
      "(tensor([0.3848, 0.1713, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3311, 0.2372, 0.0583, 0.0558, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4154, 0.1961, 0.1687, 0.0497, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6385, 0.0588, 0.0580, 0.0560, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1008, 0.0717, 0.0698, 0.0440, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.3670, 0.0837, 0.0667, 0.0521, 0.0297], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' much', ' interesting'])\n",
      "(tensor([0.5672, 0.1879, 0.1421, 0.0151, 0.0128], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.3627, 0.2162, 0.1839, 0.0307, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' but', '...'])\n",
      "(tensor([0.2094, 0.2067, 0.1897, 0.0286, 0.0239], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' Unfortunately', ' There'])\n",
      "(tensor([0.3174, 0.2879, 0.1537, 0.1083, 0.0749], grad_fn=<ToCopyBackward0>), [' were', ' was', ' are', ' is', \"'s\"])\n",
      "(tensor([0.2105, 0.2078, 0.0640, 0.0626, 0.0487], grad_fn=<ToCopyBackward0>), [' a', ' some', ' no', ' so', ' very'])\n",
      "(tensor([0.4469, 0.2553, 0.1881, 0.0797, 0.0079], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' number', ' bunch'])\n",
      "(tensor([9.9910e-01, 2.2869e-04, 1.1043e-04, 7.0587e-05, 5.6856e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' that', ',', ' (', ' in'])\n",
      "(tensor([0.0654, 0.0621, 0.0575, 0.0512, 0.0448], grad_fn=<ToCopyBackward0>), [' well', ' interesting', ' good', ' very', ' things'])\n",
      "(tensor([0.1549, 0.0834, 0.0608, 0.0559, 0.0489], grad_fn=<ToCopyBackward0>), [' characters', ' points', ' and', ' character', ' themes'])\n",
      "(tensor([0.2162, 0.1782, 0.1185, 0.1129, 0.0486], grad_fn=<ToCopyBackward0>), [' and', ' in', '.', ',', ' that'])\n",
      "(tensor([0.5383, 0.2156, 0.1917, 0.0173, 0.0157], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' there', ' here'])\n",
      "(tensor([0.4584, 0.2721, 0.1309, 0.0270, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' -'])\n",
      "(tensor([0.4492, 0.1279, 0.0865, 0.0576, 0.0336], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' a', ' some'])\n",
      "(tensor([0.2252, 0.1756, 0.0734, 0.0627, 0.0586], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' enjoyed', ' found'])\n",
      "(tensor([0.3926, 0.2492, 0.2440, 0.0136, 0.0071], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' there'])\n",
      "(tensor([0.1805, 0.1187, 0.0605, 0.0445, 0.0339], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' director', ' film', ' plot'])\n",
      "(tensor([0.5580, 0.0433, 0.0413, 0.0328, 0.0221], grad_fn=<ToCopyBackward0>), [' was', ' itself', ' line', ' did', ' could'])\n",
      "(tensor([0.2352, 0.1357, 0.0847, 0.0652, 0.0584], grad_fn=<ToCopyBackward0>), [' interesting', ' pretty', ' very', ' fairly', ' good'])\n",
      "(tensor([0.1957, 0.0913, 0.0870, 0.0769, 0.0620], grad_fn=<ToCopyBackward0>), [' interesting', ' original', ' well', ' good', ' compelling'])\n",
      "(tensor([0.4931, 0.3427, 0.0441, 0.0212, 0.0184], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' in', ' but'])\n",
      "(tensor([0.1401, 0.1294, 0.1152, 0.0977, 0.0627], grad_fn=<ToCopyBackward0>), [' I', ' The', ' However', ' It', ' Unfortunately'])\n",
      "(tensor([0.7572, 0.0781, 0.0447, 0.0428, 0.0086], grad_fn=<ToCopyBackward0>), [',', ' the', ' I', ' it', ' there'])\n",
      "(tensor([0.2900, 0.2403, 0.1116, 0.0354, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' this', ' there'])\n",
      "(tensor([0.0828, 0.0713, 0.0639, 0.0485, 0.0476], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' story', ' problem', ' main'])\n",
      "(tensor([0.4503, 0.1451, 0.0814, 0.0380, 0.0307], grad_fn=<ToCopyBackward0>), [' was', ' and', ' in', ' of', ' wasn'])\n",
      "(tensor([9.9465e-01, 2.0984e-03, 4.4084e-04, 4.2545e-04, 2.4315e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", ','])\n",
      "(tensor([0.1990, 0.1034, 0.0839, 0.0753, 0.0719], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' as', ' all'])\n",
      "(tensor([0.7355, 0.1590, 0.0294, 0.0098, 0.0072], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' believable', ' well', ' appealing'])\n",
      "(tensor([0.5208, 0.1867, 0.1063, 0.0437, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' to'])\n",
      "(tensor([0.2417, 0.2087, 0.0805, 0.0609, 0.0416], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.1094, 0.1021, 0.0695, 0.0606, 0.0451], grad_fn=<ToCopyBackward0>), [' think', ' thought', ' found', ' don', \"'m\"])\n",
      "(tensor([0.2448, 0.2334, 0.1150, 0.0258, 0.0194], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' they', ' there'])\n",
      "(tensor([0.3648, 0.0704, 0.0413, 0.0292, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' it', ' they', ' there', ' I'])\n",
      "(tensor([0.3607, 0.0732, 0.0564, 0.0414, 0.0400], grad_fn=<ToCopyBackward0>), [' was', ' would', ' needed', ' just', ' could'])\n",
      "(tensor([0.2206, 0.1913, 0.0415, 0.0327, 0.0283], grad_fn=<ToCopyBackward0>), [' a', ' very', ' all', ' pretty', ' just'])\n",
      "(tensor([0.1776, 0.0940, 0.0563, 0.0540, 0.0267], grad_fn=<ToCopyBackward0>), [' hard', ' difficult', ' over', ' disappointing', ' obvious'])\n",
      "(tensor([8.4589e-01, 1.3891e-01, 2.8570e-03, 1.0559e-03, 7.9835e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' and', ',', ' trying'])\n",
      "(tensor([0.6026, 0.0425, 0.0385, 0.0195, 0.0193], grad_fn=<ToCopyBackward0>), [' the', ' some', ' most', ' a', ' them'])\n",
      "(tensor([0.4434, 0.0722, 0.0664, 0.0422, 0.0384], grad_fn=<ToCopyBackward0>), [' actors', ' actor', ' characters', ' leading', ' director'])\n",
      "(tensor([0.9606, 0.0091, 0.0049, 0.0036, 0.0036], grad_fn=<ToCopyBackward0>), [' to', '.', ' in', ',', ' and'])\n",
      "(tensor([0.0892, 0.0748, 0.0716, 0.0620, 0.0546], grad_fn=<ToCopyBackward0>), [' convinc', ' convince', ' portray', ' convey', ' deliver'])\n",
      "(tensor([0.8609, 0.0425, 0.0160, 0.0059, 0.0032], grad_fn=<ToCopyBackward0>), ['ingly', 'er', 'ie', 'e', '.'])\n",
      "(tensor([0.6777, 0.0748, 0.0374, 0.0135, 0.0122], grad_fn=<ToCopyBackward0>), [' portray', ' play', ' deliver', ' give', ' carry'])\n",
      "(tensor([0.5741, 0.0763, 0.0525, 0.0367, 0.0136], grad_fn=<ToCopyBackward0>), [' the', ' their', ' a', ' characters', ' these'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of the worst films I've ever had the misfortune to watch. I can honestly say that I have never been so embarrassed to be a member of the public. I was actually in a theater when I first saw this. My friend and\n",
      "(tensor([0.3822, 0.1731, 0.0907, 0.0773, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7145, 0.1159, 0.0392, 0.0100, 0.0087], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1832, 0.1475, 0.0505, 0.0450, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9172, 0.0123, 0.0065, 0.0065, 0.0040], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7577, 0.1963, 0.0169, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.5698, 0.1209, 0.1093, 0.0263, 0.0262], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' most', ' funn', ' dumb'])\n",
      "(tensor([0.7940, 0.1565, 0.0050, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ',', ' film'])\n",
      "(tensor([0.7825, 0.0621, 0.0524, 0.0439, 0.0123], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.3754, 0.3511, 0.1365, 0.0552, 0.0519], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', \"'d\", ' ever'])\n",
      "(tensor([0.8660, 0.1132, 0.0071, 0.0051, 0.0016], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' been'])\n",
      "(tensor([0.8431, 0.0435, 0.0255, 0.0199, 0.0145], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' made', ' been'])\n",
      "(tensor([0.8355, 0.0723, 0.0221, 0.0172, 0.0107], grad_fn=<ToCopyBackward0>), [' the', ' to', ' a', ' in', ','])\n",
      "(tensor([0.5293, 0.3830, 0.0232, 0.0217, 0.0078], grad_fn=<ToCopyBackward0>), [' misfortune', ' displeasure', ' privilege', ' pleasure', ' fortune'])\n",
      "(tensor([9.3510e-01, 6.3590e-02, 1.0335e-04, 8.5632e-05, 8.0572e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' of', ' in', ' for', ' with'])\n",
      "(tensor([0.4180, 0.2731, 0.1555, 0.0303, 0.0236], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' sit', ' have', ' be'])\n",
      "(tensor([0.6919, 0.1304, 0.0651, 0.0133, 0.0127], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', '!', '...'])\n",
      "(tensor([0.1733, 0.1665, 0.1440, 0.0234, 0.0212], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'The'])\n",
      "(tensor([0.1062, 0.0883, 0.0746, 0.0419, 0.0418], grad_fn=<ToCopyBackward0>), [' was', ' can', \"'m\", ' really', ' don'])\n",
      "(tensor([0.4775, 0.2032, 0.0529, 0.0391, 0.0223], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' only', ' remember', ' say'])\n",
      "(tensor([0.9639, 0.0085, 0.0037, 0.0025, 0.0020], grad_fn=<ToCopyBackward0>), [' say', ' tell', ' remember', ' only', ' not'])\n",
      "(tensor([0.4867, 0.2665, 0.1070, 0.0468, 0.0142], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ' this', ' there'])\n",
      "(tensor([0.3957, 0.1355, 0.1017, 0.0389, 0.0318], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' the', ' even'])\n",
      "(tensor([0.1778, 0.0722, 0.0638, 0.0583, 0.0489], grad_fn=<ToCopyBackward0>), [' have', ' was', \"'m\", \"'ve\", ' am'])\n",
      "(tensor([0.3813, 0.1585, 0.0621, 0.0527, 0.0424], grad_fn=<ToCopyBackward0>), [' never', ' seen', ' not', ' absolutely', ' a'])\n",
      "(tensor([0.3137, 0.1344, 0.0639, 0.0631, 0.0420], grad_fn=<ToCopyBackward0>), [' seen', ' been', ',', ' in', ' had'])\n",
      "(tensor([0.6435, 0.1530, 0.0566, 0.0235, 0.0210], grad_fn=<ToCopyBackward0>), [' so', ' more', ' as', ' a', ' able'])\n",
      "(tensor([0.2865, 0.2110, 0.2104, 0.0440, 0.0347], grad_fn=<ToCopyBackward0>), [' disappointed', ' bored', ' angry', ' embarrassed', ' disgusted'])\n",
      "(tensor([0.5857, 0.1718, 0.0663, 0.0332, 0.0210], grad_fn=<ToCopyBackward0>), [' to', ' in', ' by', ' as', ' for'])\n",
      "(tensor([0.7155, 0.1070, 0.0401, 0.0237, 0.0129], grad_fn=<ToCopyBackward0>), [' be', ' have', ' see', ' watch', ' sit'])\n",
      "(tensor([0.2478, 0.2250, 0.1504, 0.0366, 0.0355], grad_fn=<ToCopyBackward0>), [' a', ' from', ' in', ' on', ' watching'])\n",
      "(tensor([0.3993, 0.2715, 0.0347, 0.0314, 0.0258], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' part', ' member', ' cinema'])\n",
      "(tensor([9.9913e-01, 3.5760e-04, 9.7097e-05, 5.6114e-05, 3.2319e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' in', ' or', ' on', ' and'])\n",
      "(tensor([0.8146, 0.1035, 0.0214, 0.0192, 0.0110], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' this', ' an'])\n",
      "(tensor([0.2249, 0.1523, 0.0210, 0.0170, 0.0164], grad_fn=<ToCopyBackward0>), [' public', ' human', ' film', ' Canadian', ' British'])\n",
      "(tensor([0.3857, 0.1010, 0.0913, 0.0628, 0.0476], grad_fn=<ToCopyBackward0>), ['.', ' as', ' to', ' at', ' in'])\n",
      "(tensor([0.2246, 0.1185, 0.0924, 0.0392, 0.0328], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1200, 0.0925, 0.0764, 0.0562, 0.0532], grad_fn=<ToCopyBackward0>), [' was', ' can', \"'m\", ' actually', ' have'])\n",
      "(tensor([0.1151, 0.0804, 0.0757, 0.0622, 0.0469], grad_fn=<ToCopyBackward0>), [' in', ' actually', ' so', ' embarrassed', ' a'])\n",
      "(tensor([0.1542, 0.0600, 0.0448, 0.0359, 0.0350], grad_fn=<ToCopyBackward0>), [' in', ' embarrassed', ' a', ' surprised', ' so'])\n",
      "(tensor([0.5130, 0.3380, 0.0141, 0.0127, 0.0106], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' such', ' shock'])\n",
      "(tensor([0.0559, 0.0502, 0.0298, 0.0277, 0.0250], grad_fn=<ToCopyBackward0>), [' theater', ' public', ' state', ' dark', ' deep'])\n",
      "(tensor([0.1815, 0.1029, 0.0619, 0.0567, 0.0423], grad_fn=<ToCopyBackward0>), [' when', ' with', ' the', ' in', ' about'])\n",
      "(tensor([0.4737, 0.3313, 0.0762, 0.0555, 0.0109], grad_fn=<ToCopyBackward0>), [' I', ' this', ' the', ' it', ' they'])\n",
      "(tensor([0.1868, 0.1488, 0.1194, 0.0667, 0.0460], grad_fn=<ToCopyBackward0>), [' saw', ' first', ' found', ' watched', ' was'])\n",
      "(tensor([0.6111, 0.0973, 0.0387, 0.0248, 0.0221], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' heard', ' viewed', ' came'])\n",
      "(tensor([0.5237, 0.4200, 0.0361, 0.0048, 0.0025], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "(tensor([0.4209, 0.1286, 0.0939, 0.0807, 0.0654], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ',', ' and'])\n",
      "(tensor([0.4566, 0.0905, 0.0655, 0.0284, 0.0154], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' My'])\n",
      "(tensor([0.2357, 0.1731, 0.0683, 0.0376, 0.0309], grad_fn=<ToCopyBackward0>), [' friend', ' friends', ' girlfriend', ' eyes', ' jaw'])\n",
      "(tensor([0.4013, 0.0673, 0.0408, 0.0314, 0.0274], grad_fn=<ToCopyBackward0>), [' and', ' was', ' who', ' wanted', ' rented'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this is a movie that should be watched by the entire world. I really enjoyed the movie because I think that there is a lot of truth in it. It's not a great movie, but I really enjoyed it. I think that it's\n",
      "(tensor([0.3841, 0.1717, 0.0898, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3313, 0.2367, 0.0582, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4151, 0.1965, 0.1688, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.3749, 0.2203, 0.0978, 0.0616, 0.0314], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' not', ' an'])\n",
      "(tensor([0.3299, 0.0729, 0.0613, 0.0572, 0.0408], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' good', ' sequel', ' film'])\n",
      "(tensor([0.5388, 0.0724, 0.0629, 0.0533, 0.0296], grad_fn=<ToCopyBackward0>), [' that', ' for', ' about', ' where', ' which'])\n",
      "(tensor([0.1164, 0.0797, 0.0478, 0.0421, 0.0402], grad_fn=<ToCopyBackward0>), [' is', ' should', ' could', ' I', ' the'])\n",
      "(tensor([0.5458, 0.1669, 0.1145, 0.0414, 0.0144], grad_fn=<ToCopyBackward0>), [' be', ' not', ' have', ' never', ' stay'])\n",
      "(tensor([0.2781, 0.0994, 0.0811, 0.0438, 0.0397], grad_fn=<ToCopyBackward0>), [' watched', ' in', ' shown', ' on', ' made'])\n",
      "(tensor([0.3190, 0.1602, 0.0848, 0.0463, 0.0395], grad_fn=<ToCopyBackward0>), [' by', ' in', ' on', ' with', ' at'])\n",
      "(tensor([0.1552, 0.1096, 0.0862, 0.0738, 0.0680], grad_fn=<ToCopyBackward0>), [' everyone', ' people', ' the', ' a', ' everybody'])\n",
      "(tensor([0.1123, 0.0728, 0.0722, 0.0427, 0.0377], grad_fn=<ToCopyBackward0>), [' public', ' people', ' entire', ' whole', ' masses'])\n",
      "(tensor([0.3600, 0.2712, 0.0401, 0.0250, 0.0205], grad_fn=<ToCopyBackward0>), [' family', ' world', ' population', ' universe', ' city'])\n",
      "(tensor([0.4169, 0.2394, 0.0930, 0.0282, 0.0265], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' because'])\n",
      "(tensor([0.3109, 0.1288, 0.0675, 0.0573, 0.0452], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' So'])\n",
      "(tensor([0.1317, 0.0985, 0.0762, 0.0541, 0.0387], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' am', \"'m\"])\n",
      "(tensor([0.2304, 0.0569, 0.0510, 0.0503, 0.0434], grad_fn=<ToCopyBackward0>), [' wanted', ' did', ' enjoyed', ' hope', ' thought'])\n",
      "(tensor([0.4221, 0.2810, 0.1164, 0.0193, 0.0116], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' this', ' all'])\n",
      "(tensor([0.2365, 0.1352, 0.1071, 0.0601, 0.0447], grad_fn=<ToCopyBackward0>), [' first', ' story', ' movie', ' fact', ' book'])\n",
      "(tensor([0.2106, 0.1974, 0.0648, 0.0632, 0.0603], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', '...'])\n",
      "(tensor([0.3528, 0.1896, 0.1649, 0.0587, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' there'])\n",
      "(tensor([0.0812, 0.0785, 0.0713, 0.0621, 0.0554], grad_fn=<ToCopyBackward0>), [' was', ' think', ' really', ' am', ' thought'])\n",
      "(tensor([0.4589, 0.1805, 0.0631, 0.0418, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.2474, 0.1108, 0.1012, 0.0500, 0.0335], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.4079, 0.3070, 0.0937, 0.0796, 0.0404], grad_fn=<ToCopyBackward0>), [' is', ' are', \"'s\", ' was', ' were'])\n",
      "(tensor([0.2145, 0.1693, 0.1563, 0.1415, 0.0661], grad_fn=<ToCopyBackward0>), [' something', ' a', ' no', ' nothing', ' not'])\n",
      "(tensor([0.4417, 0.1880, 0.0501, 0.0285, 0.0217], grad_fn=<ToCopyBackward0>), [' lot', ' message', ' very', ' good', ' great'])\n",
      "(tensor([0.4342, 0.3928, 0.0596, 0.0368, 0.0189], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ' in', ' more'])\n",
      "(tensor([0.1372, 0.1071, 0.0548, 0.0426, 0.0389], grad_fn=<ToCopyBackward0>), [' humor', ' truth', ' art', ' value', ' good'])\n",
      "(tensor([0.5304, 0.1661, 0.0956, 0.0624, 0.0218], grad_fn=<ToCopyBackward0>), [' in', ' to', ' behind', ' and', '.'])\n",
      "(tensor([0.6098, 0.2487, 0.0767, 0.0098, 0.0072], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' what', ' there'])\n",
      "(tensor([0.5343, 0.1646, 0.0606, 0.0250, 0.0236], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' that'])\n",
      "(tensor([0.5035, 0.0744, 0.0407, 0.0393, 0.0370], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.5478, 0.1936, 0.0361, 0.0330, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' has', ' touches'])\n",
      "(tensor([0.3517, 0.1142, 0.0921, 0.0746, 0.0237], grad_fn=<ToCopyBackward0>), [' not', ' a', ' like', ' about', ' just'])\n",
      "(tensor([0.1806, 0.1585, 0.1312, 0.1128, 0.0825], grad_fn=<ToCopyBackward0>), [' like', ' the', ' a', ' just', ' that'])\n",
      "(tensor([0.1259, 0.1164, 0.1098, 0.1068, 0.0382], grad_fn=<ToCopyBackward0>), [' great', ' Hollywood', ' movie', ' perfect', ' good'])\n",
      "(tensor([0.7840, 0.1089, 0.0317, 0.0087, 0.0064], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' plot', ' Hollywood'])\n",
      "(tensor([0.3360, 0.1497, 0.0955, 0.0655, 0.0535], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' in', ' for'])\n",
      "(tensor([0.7731, 0.0823, 0.0192, 0.0188, 0.0107], grad_fn=<ToCopyBackward0>), [' but', ' it', ' I', ' and', ' not'])\n",
      "(tensor([0.3494, 0.3109, 0.0301, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' at', ' if', ' the'])\n",
      "(tensor([0.2245, 0.1829, 0.0758, 0.0370, 0.0277], grad_fn=<ToCopyBackward0>), [' really', ' think', ' thought', ' enjoyed', ' can'])\n",
      "(tensor([0.1905, 0.1580, 0.1075, 0.0897, 0.0725], grad_fn=<ToCopyBackward0>), [' enjoyed', ' think', ' liked', ' thought', ' enjoy'])\n",
      "(tensor([0.4871, 0.4118, 0.0485, 0.0102, 0.0062], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' seeing', ' this'])\n",
      "(tensor([0.5445, 0.1392, 0.0872, 0.0681, 0.0305], grad_fn=<ToCopyBackward0>), ['.', ' because', ' and', ',', ' for'])\n",
      "(tensor([0.5591, 0.0898, 0.0441, 0.0304, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' If'])\n",
      "(tensor([0.1576, 0.1325, 0.0611, 0.0441, 0.0378], grad_fn=<ToCopyBackward0>), [' really', ' think', ' was', ' thought', ' don'])\n",
      "(tensor([0.6127, 0.1133, 0.0645, 0.0515, 0.0174], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' the', ' if'])\n",
      "(tensor([0.2485, 0.2209, 0.1007, 0.0395, 0.0315], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' people', ' if'])\n",
      "(tensor([0.4834, 0.1266, 0.0833, 0.0417, 0.0330], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' should', ' has', ' was'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was going to be sick from the whole thing. I actually thought that it might have been something to do with the polio vaccine. But I was wrong. It was actually a brain tumor.\"The tumor was so small, it did not cause any\n",
      "(tensor([0.3833, 0.1720, 0.0900, 0.0772, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2249, 0.1947, 0.1571, 0.0695, 0.0537], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4600, 0.2366, 0.0587, 0.0169, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.9598, 0.0135, 0.0028, 0.0028, 0.0026], grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' for', ' mad', ' blind'])\n",
      "(tensor([0.5421, 0.1264, 0.0628, 0.0560, 0.0285], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' have', ' get', ' see'])\n",
      "(tensor([0.0963, 0.0800, 0.0554, 0.0389, 0.0333], grad_fn=<ToCopyBackward0>), [' in', ' sick', ' a', ' locked', ' watching'])\n",
      "(tensor([0.1477, 0.1077, 0.0986, 0.0836, 0.0735], grad_fn=<ToCopyBackward0>), ['.', ' to', ' when', ',', ' from'])\n",
      "(tensor([0.3125, 0.2954, 0.0765, 0.0557, 0.0309], grad_fn=<ToCopyBackward0>), [' this', ' the', ' watching', ' it', ' all'])\n",
      "(tensor([0.0386, 0.0317, 0.0279, 0.0203, 0.0203], grad_fn=<ToCopyBackward0>), [' idea', ' whole', ' first', ' entire', ' pain'])\n",
      "(tensor([0.0669, 0.0595, 0.0419, 0.0325, 0.0209], grad_fn=<ToCopyBackward0>), [' thing', ' movie', ' stupid', ' episode', ' filming'])\n",
      "(tensor([0.4907, 0.2822, 0.0282, 0.0239, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '!', ' and'])\n",
      "(tensor([0.3821, 0.0754, 0.0703, 0.0283, 0.0212], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' This'])\n",
      "(tensor([0.1660, 0.1316, 0.0950, 0.0496, 0.0411], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', \"'m\", ' actually'])\n",
      "(tensor([0.3067, 0.0771, 0.0317, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' thought', ' felt', ' had', ' started', ' think'])\n",
      "(tensor([0.4951, 0.1444, 0.0654, 0.0440, 0.0348], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' that', ' this'])\n",
      "(tensor([0.3278, 0.2179, 0.0781, 0.0739, 0.0341], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' my'])\n",
      "(tensor([0.5979, 0.1674, 0.1066, 0.0427, 0.0300], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' had', ' could'])\n",
      "(tensor([0.4059, 0.3761, 0.1029, 0.0182, 0.0178], grad_fn=<ToCopyBackward0>), [' have', ' be', ' kill', ' not', ' make'])\n",
      "(tensor([0.5003, 0.1803, 0.0420, 0.0342, 0.0172], grad_fn=<ToCopyBackward0>), [' been', ' something', ' to', ' had', ' come'])\n",
      "(tensor([0.1696, 0.1344, 0.0607, 0.0471, 0.0438], grad_fn=<ToCopyBackward0>), [' the', ' a', ' something', ' better', ' contagious'])\n",
      "(tensor([0.3005, 0.1040, 0.0889, 0.0459, 0.0435], grad_fn=<ToCopyBackward0>), [' to', ' in', ' that', ' more', ' I'])\n",
      "(tensor([0.9369, 0.0189, 0.0085, 0.0017, 0.0012], grad_fn=<ToCopyBackward0>), [' do', ' with', ' be', ' get', ' make'])\n",
      "(tensor([9.9277e-01, 8.1472e-04, 5.9251e-04, 5.3873e-04, 3.9956e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' to', ' or', ' more', ' in'])\n",
      "(tensor([0.4328, 0.0249, 0.0186, 0.0149, 0.0115], grad_fn=<ToCopyBackward0>), [' the', ' my', ' this', ' it', ' a'])\n",
      "(tensor([0.0593, 0.0315, 0.0306, 0.0199, 0.0124], grad_fn=<ToCopyBackward0>), [' fact', ' movie', ' drugs', ' LSD', ' polio'])\n",
      "(tensor([0.5429, 0.0644, 0.0541, 0.0353, 0.0301], grad_fn=<ToCopyBackward0>), [' vaccine', ' virus', ' vaccines', '.', ','])\n",
      "(tensor([0.2946, 0.2918, 0.0907, 0.0489, 0.0428], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' I', ' that'])\n",
      "(tensor([0.3334, 0.0669, 0.0635, 0.0495, 0.0313], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.1887, 0.1522, 0.0771, 0.0705, 0.0501], grad_fn=<ToCopyBackward0>), [' it', ' I', ' after', ' when', ' no'])\n",
      "(tensor([0.2996, 0.0950, 0.0597, 0.0587, 0.0407], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', \"'m\", ' guess'])\n",
      "(tensor([0.4583, 0.0304, 0.0301, 0.0192, 0.0190], grad_fn=<ToCopyBackward0>), [' wrong', ' feeling', ' not', ' so', ' told'])\n",
      "(tensor([0.6993, 0.0809, 0.0418, 0.0324, 0.0263], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' because', ' on'])\n",
      "(tensor([0.2825, 0.1513, 0.1478, 0.0709, 0.0176], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' This', ' There'])\n",
      "(tensor([0.6620, 0.0971, 0.0806, 0.0271, 0.0257], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' had', ' turned'])\n",
      "(tensor([0.1104, 0.1081, 0.0789, 0.0531, 0.0464], grad_fn=<ToCopyBackward0>), [' not', ' the', ' actually', ' a', ' really'])\n",
      "(tensor([0.2725, 0.0941, 0.0896, 0.0724, 0.0228], grad_fn=<ToCopyBackward0>), [' the', ' something', ' a', ' from', ' because'])\n",
      "(tensor([0.1471, 0.0806, 0.0243, 0.0181, 0.0180], grad_fn=<ToCopyBackward0>), [' brain', ' virus', ' little', ' bad', ' big'])\n",
      "(tensor([0.1612, 0.1318, 0.0684, 0.0634, 0.0346], grad_fn=<ToCopyBackward0>), [' tumor', ' an', ' infection', ' injury', ' cancer'])\n",
      "(tensor([0.3290, 0.1595, 0.0642, 0.0541, 0.0440], grad_fn=<ToCopyBackward0>), ['.', '.\"', '\"', ' called', ','])\n",
      "(tensor([0.1151, 0.0650, 0.0449, 0.0317, 0.0300], grad_fn=<ToCopyBackward0>), ['The', 'A', 'I', ' The', 'In'])\n",
      "(tensor([0.5183, 0.0596, 0.0421, 0.0155, 0.0120], grad_fn=<ToCopyBackward0>), [' tumor', ' brain', ' cancer', ' doctor', ' tum'])\n",
      "(tensor([0.2324, 0.0773, 0.0601, 0.0430, 0.0213], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' grew'])\n",
      "(tensor([0.2080, 0.0736, 0.0595, 0.0457, 0.0412], grad_fn=<ToCopyBackward0>), [' so', ' removed', ' benign', ' in', ' surg'])\n",
      "(tensor([0.2552, 0.1141, 0.0872, 0.0505, 0.0409], grad_fn=<ToCopyBackward0>), [' large', ' big', ' small', ' bad', ' massive'])\n",
      "(tensor([0.3104, 0.2296, 0.1722, 0.0685, 0.0371], grad_fn=<ToCopyBackward0>), [' that', ',', ' it', ' and', ' ('])\n",
      "(tensor([0.2329, 0.1158, 0.0566, 0.0548, 0.0518], grad_fn=<ToCopyBackward0>), [' it', ' he', ' in', ' that', ' \"'])\n",
      "(tensor([0.2462, 0.1200, 0.1071, 0.0834, 0.0444], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' did', ' could', ' would'])\n",
      "(tensor([0.9525, 0.0220, 0.0021, 0.0020, 0.0018], grad_fn=<ToCopyBackward0>), [' not', ' nothing', ' something', ' NOT', ' no'])\n",
      "(tensor([0.1364, 0.0663, 0.0424, 0.0419, 0.0418], grad_fn=<ToCopyBackward0>), [' cause', ' show', ' require', ' go', ' even'])\n",
      "(tensor([0.3839, 0.1809, 0.1008, 0.0605, 0.0368], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' much', ' life'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was a good movie. It was a really bad premise, but it was good to see some good actors in the movie. I was really surprised at the acting ability of Glover and Glover. They really pulled it all off. I really didn\n",
      "(tensor([0.3836, 0.1722, 0.0900, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5006, 0.0599, 0.0341, 0.0152, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6239, 0.0399, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0596, 0.0527, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1137, 0.0606, 0.0595, 0.0563, 0.0371], grad_fn=<ToCopyBackward0>), [' good', ' very', ' pretty', ' great', ' little'])\n",
      "(tensor([0.5097, 0.1118, 0.0925, 0.0333, 0.0246], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' idea', ' story', ' comedy'])\n",
      "(tensor([0.3205, 0.1456, 0.1100, 0.0728, 0.0374], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', '!'])\n",
      "(tensor([0.3114, 0.1755, 0.0739, 0.0373, 0.0188], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.4288, 0.1223, 0.0817, 0.0345, 0.0339], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' has'])\n",
      "(tensor([0.0963, 0.0860, 0.0836, 0.0606, 0.0423], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' entertaining', ' interesting', ' fun'])\n",
      "(tensor([0.3645, 0.0420, 0.0371, 0.0325, 0.0316], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' very', ' really', ' pretty'])\n",
      "(tensor([0.4547, 0.1473, 0.0599, 0.0497, 0.0240], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' boring', ' funny', ','])\n",
      "(tensor([0.6282, 0.0685, 0.0392, 0.0234, 0.0233], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' premise', ' comedy'])\n",
      "(tensor([0.2821, 0.1623, 0.1620, 0.0864, 0.0857], grad_fn=<ToCopyBackward0>), [' for', ',', ' and', '.', ' to'])\n",
      "(tensor([0.4056, 0.0905, 0.0489, 0.0489, 0.0247], grad_fn=<ToCopyBackward0>), [' but', ' and', ' a', ' the', ' I'])\n",
      "(tensor([0.2259, 0.2216, 0.1345, 0.0337, 0.0269], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' at', ' a'])\n",
      "(tensor([0.4818, 0.1095, 0.0661, 0.0525, 0.0314], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' could', ' seemed'])\n",
      "(tensor([0.1807, 0.1231, 0.0999, 0.0897, 0.0579], grad_fn=<ToCopyBackward0>), [' a', ' good', ' interesting', ' funny', ' entertaining'])\n",
      "(tensor([0.3099, 0.1086, 0.0907, 0.0611, 0.0568], grad_fn=<ToCopyBackward0>), [' in', ' to', ' as', ' for', '.'])\n",
      "(tensor([0.6005, 0.1141, 0.0931, 0.0392, 0.0231], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' be', ' have', ' make'])\n",
      "(tensor([0.0676, 0.0658, 0.0393, 0.0228, 0.0161], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' something', ' some'])\n",
      "(tensor([0.1101, 0.0766, 0.0690, 0.0687, 0.0587], grad_fn=<ToCopyBackward0>), [' real', ' good', ' funny', ' of', ' pretty'])\n",
      "(tensor([0.2401, 0.1124, 0.0612, 0.0385, 0.0362], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' looking', ' performances', ' action'])\n",
      "(tensor([0.7471, 0.0927, 0.0306, 0.0123, 0.0113], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' and', ' on'])\n",
      "(tensor([0.3595, 0.2037, 0.1801, 0.0687, 0.0340], grad_fn=<ToCopyBackward0>), [' it', ' the', ' a', ' there', ' this'])\n",
      "(tensor([0.7283, 0.0385, 0.0232, 0.0158, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' lead', ' cast', ' same'])\n",
      "(tensor([0.7831, 0.1437, 0.0138, 0.0082, 0.0077], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' like'])\n",
      "(tensor([0.2558, 0.1634, 0.0480, 0.0479, 0.0454], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' The'])\n",
      "(tensor([0.1372, 0.1039, 0.0772, 0.0691, 0.0519], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', \"'m\"])\n",
      "(tensor([0.4591, 0.1385, 0.0266, 0.0234, 0.0213], grad_fn=<ToCopyBackward0>), [' really', ' very', ' not', ' just', ' a'])\n",
      "(tensor([0.4133, 0.1651, 0.1042, 0.0277, 0.0270], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' looking', ' excited', ' impressed'])\n",
      "(tensor([0.2180, 0.1530, 0.1299, 0.1250, 0.1010], grad_fn=<ToCopyBackward0>), [' by', ' when', ' that', ' at', ' with'])\n",
      "(tensor([0.6890, 0.1754, 0.0362, 0.0136, 0.0132], grad_fn=<ToCopyBackward0>), [' the', ' how', ' some', ' what', ' that'])\n",
      "(tensor([0.2400, 0.1228, 0.0427, 0.0426, 0.0347], grad_fn=<ToCopyBackward0>), [' acting', ' ending', ' end', ' plot', ' number'])\n",
      "(tensor([0.4156, 0.0906, 0.0486, 0.0418, 0.0361], grad_fn=<ToCopyBackward0>), [' in', ' ability', ' of', ' and', ','])\n",
      "(tensor([0.7791, 0.1185, 0.0231, 0.0125, 0.0116], grad_fn=<ToCopyBackward0>), [' of', ' in', ' on', ' and', ','])\n",
      "(tensor([0.1000, 0.0463, 0.0400, 0.0254, 0.0213], grad_fn=<ToCopyBackward0>), [' the', ' Michael', ' Glover', ' Adam', ' Kevin'])\n",
      "(tensor([0.4530, 0.1587, 0.0984, 0.0709, 0.0635], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' as', ' in'])\n",
      "(tensor([0.3132, 0.0615, 0.0520, 0.0262, 0.0219], grad_fn=<ToCopyBackward0>), [' Glover', ' Mc', ' the', ' I', ' G'])\n",
      "(tensor([0.3503, 0.1753, 0.0995, 0.0770, 0.0310], grad_fn=<ToCopyBackward0>), [' was', ' alone', ' is', '.', ' in'])\n",
      "(tensor([0.3140, 0.1032, 0.0548, 0.0502, 0.0493], grad_fn=<ToCopyBackward0>), [' I', ' It', ' They', ' And', ' The'])\n",
      "(tensor([0.2994, 0.1120, 0.0909, 0.0668, 0.0511], grad_fn=<ToCopyBackward0>), [' were', ' did', ' really', \"'re\", ' just'])\n",
      "(tensor([0.0928, 0.0517, 0.0504, 0.0472, 0.0462], grad_fn=<ToCopyBackward0>), [' did', ' made', ' were', ' pulled', ' do'])\n",
      "(tensor([0.7279, 0.0479, 0.0390, 0.0299, 0.0186], grad_fn=<ToCopyBackward0>), [' it', ' through', ' the', ' off', ' all'])\n",
      "(tensor([0.9071, 0.0450, 0.0158, 0.0136, 0.0126], grad_fn=<ToCopyBackward0>), [' off', ' together', ' all', ' through', ' out'])\n",
      "(tensor([0.6062, 0.3596, 0.0194, 0.0045, 0.0019], grad_fn=<ToCopyBackward0>), [' together', ' off', ' out', ' through', ' of'])\n",
      "(tensor([0.6600, 0.1195, 0.0497, 0.0230, 0.0191], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' well'])\n",
      "(tensor([0.2759, 0.1225, 0.0715, 0.0587, 0.0462], grad_fn=<ToCopyBackward0>), [' I', ' It', 'I', ' The', ' And'])\n",
      "(tensor([0.1703, 0.1253, 0.0888, 0.0703, 0.0671], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', ' think', \"'m\"])\n",
      "(tensor([0.1475, 0.1340, 0.1194, 0.0822, 0.0574], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' like', ' enjoyed', ' didn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie sucked. I thought it was a big waste of time and money. The movie was not scary or interesting or scary or interesting in any way. The characters were not believable. The acting was terrible. I think it was just a bad movie\n",
      "(tensor([0.3829, 0.1723, 0.0904, 0.0773, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2447, 0.1958, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6529, 0.0595, 0.0362, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.2484, 0.0999, 0.0783, 0.0380, 0.0303], grad_fn=<ToCopyBackward0>), [' big', ' so', '.', ' when', ' as'])\n",
      "(tensor([0.2363, 0.1336, 0.1131, 0.0224, 0.0217], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.1029, 0.0853, 0.0813, 0.0754, 0.0484], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.3712, 0.3150, 0.1754, 0.0372, 0.0108], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' I'])\n",
      "(tensor([0.7296, 0.0739, 0.0200, 0.0170, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' had', ' would', \"'s\"])\n",
      "(tensor([0.1462, 0.0534, 0.0521, 0.0507, 0.0499], grad_fn=<ToCopyBackward0>), [' a', ' boring', ' the', ' stupid', ' terrible'])\n",
      "(tensor([0.1765, 0.0729, 0.0599, 0.0410, 0.0296], grad_fn=<ToCopyBackward0>), [' big', ' bad', ' piece', ' waste', ' complete'])\n",
      "(tensor([0.1718, 0.0994, 0.0732, 0.0703, 0.0334], grad_fn=<ToCopyBackward0>), [' joke', ' disappointment', ',', ' waste', ' fat'])\n",
      "(tensor([9.9048e-01, 4.8734e-03, 6.2508e-04, 5.9900e-04, 4.1277e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' to'])\n",
      "(tensor([0.5318, 0.1461, 0.0535, 0.0346, 0.0238], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' a', ' talent'])\n",
      "(tensor([0.4538, 0.3886, 0.0965, 0.0082, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' for'])\n",
      "(tensor([0.4413, 0.1013, 0.0931, 0.0916, 0.0241], grad_fn=<ToCopyBackward0>), [' money', ' effort', ' a', ' I', ' my'])\n",
      "(tensor([0.7789, 0.0841, 0.0465, 0.0313, 0.0107], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' ('])\n",
      "(tensor([0.3552, 0.0909, 0.0745, 0.0335, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.1957, 0.1079, 0.0510, 0.0417, 0.0246], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' special'])\n",
      "(tensor([0.2865, 0.1451, 0.0366, 0.0292, 0.0292], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' starts', ' just'])\n",
      "(tensor([0.1103, 0.0931, 0.0636, 0.0474, 0.0370], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' very', ' about'])\n",
      "(tensor([0.4948, 0.1383, 0.0847, 0.0788, 0.0166], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' entertaining', ' fun'])\n",
      "(tensor([0.2527, 0.1792, 0.1387, 0.1292, 0.0581], grad_fn=<ToCopyBackward0>), [',', ' at', ' or', '.', ' enough'])\n",
      "(tensor([0.2251, 0.1634, 0.1294, 0.0948, 0.0555], grad_fn=<ToCopyBackward0>), [' interesting', ' funny', ' entertaining', ' suspense', ' scary'])\n",
      "(tensor([0.2979, 0.1575, 0.1162, 0.0946, 0.0691], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' at', ' enough'])\n",
      "(tensor([0.1689, 0.1465, 0.1371, 0.0646, 0.0541], grad_fn=<ToCopyBackward0>), [' entertaining', ' interesting', ' funny', ' scary', ' suspense'])\n",
      "(tensor([0.4163, 0.1599, 0.0944, 0.0697, 0.0692], grad_fn=<ToCopyBackward0>), [' or', ' at', ' in', '.', ' enough'])\n",
      "(tensor([0.8241, 0.0348, 0.0291, 0.0193, 0.0190], grad_fn=<ToCopyBackward0>), [' interesting', ' even', ' funny', ' entertaining', ' scary'])\n",
      "(tensor([0.3106, 0.2526, 0.1002, 0.0679, 0.0610], grad_fn=<ToCopyBackward0>), [' at', '.', ' or', ' to', ' in'])\n",
      "(tensor([0.5104, 0.1943, 0.1936, 0.0347, 0.0277], grad_fn=<ToCopyBackward0>), [' a', ' any', ' the', ' an', ' some'])\n",
      "(tensor([0.9458, 0.0301, 0.0025, 0.0018, 0.0016], grad_fn=<ToCopyBackward0>), [' way', ' other', ' manner', ' conceivable', ' of'])\n",
      "(tensor([0.6263, 0.0731, 0.0566, 0.0411, 0.0188], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' whatsoever', '....'])\n",
      "(tensor([0.2458, 0.1966, 0.1884, 0.0255, 0.0169], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' There', 'I'])\n",
      "(tensor([0.3918, 0.0896, 0.0524, 0.0520, 0.0438], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' only', ' movie'])\n",
      "(tensor([0.6534, 0.0485, 0.0484, 0.0328, 0.0265], grad_fn=<ToCopyBackward0>), [' were', ' are', ' in', ' weren', ' had'])\n",
      "(tensor([0.2217, 0.0521, 0.0466, 0.0374, 0.0318], grad_fn=<ToCopyBackward0>), [' not', ' boring', ' so', ' annoying', ' un'])\n",
      "(tensor([0.2594, 0.1136, 0.1027, 0.0922, 0.0652], grad_fn=<ToCopyBackward0>), [' interesting', ' lik', ' even', ' scary', ' believable'])\n",
      "(tensor([0.5898, 0.1158, 0.1124, 0.0883, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', ' in'])\n",
      "(tensor([0.6487, 0.0917, 0.0513, 0.0284, 0.0212], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.3177, 0.1306, 0.0624, 0.0623, 0.0479], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' dialogue', ' actors'])\n",
      "(tensor([0.8872, 0.0277, 0.0126, 0.0076, 0.0065], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ',', ' is'])\n",
      "(tensor([0.2048, 0.1159, 0.0708, 0.0658, 0.0573], grad_fn=<ToCopyBackward0>), [' not', ' terrible', ' horrible', ' bad', ' awful'])\n",
      "(tensor([0.7820, 0.1152, 0.0442, 0.0074, 0.0046], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' in'])\n",
      "(tensor([0.5924, 0.1026, 0.0795, 0.0454, 0.0158], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0805, 0.0668, 0.0647, 0.0599, 0.0429], grad_fn=<ToCopyBackward0>), [' was', ' don', \"'m\", ' think', ' thought'])\n",
      "(tensor([0.3039, 0.1568, 0.1240, 0.0977, 0.0571], grad_fn=<ToCopyBackward0>), [' the', ' that', ' it', ' this', ' I'])\n",
      "(tensor([0.3724, 0.1621, 0.1053, 0.0618, 0.0272], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' would', ' is', ' could'])\n",
      "(tensor([0.2139, 0.0961, 0.0684, 0.0516, 0.0493], grad_fn=<ToCopyBackward0>), [' a', ' shot', ' filmed', ' the', ' just'])\n",
      "(tensor([0.4824, 0.0391, 0.0350, 0.0242, 0.0242], grad_fn=<ToCopyBackward0>), [' a', ' an', ' terrible', ' plain', ' one'])\n",
      "(tensor([0.2104, 0.2040, 0.0746, 0.0276, 0.0267], grad_fn=<ToCopyBackward0>), [' waste', ' big', ' bad', ' bunch', ' stupid'])\n",
      "(tensor([0.5415, 0.1103, 0.0326, 0.0307, 0.0208], grad_fn=<ToCopyBackward0>), [' movie', ' script', ',', ' film', ' copy'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was an opportunity for the entire world to see the film. It was an opportunity to bring together all the people involved in this movie and the world to be able to see the movie in all its beauty. It was a very emotional movie for me\n",
      "(tensor([0.3830, 0.1726, 0.0904, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4368, 0.2455, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4796, 0.1351, 0.1036, 0.0551, 0.0255], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1496, 0.0706, 0.0648, 0.0337, 0.0313], grad_fn=<ToCopyBackward0>), [' awful', ' OK', ' atro', ' insult', ' opportunity'])\n",
      "(tensor([0.8052, 0.1397, 0.0074, 0.0073, 0.0068], grad_fn=<ToCopyBackward0>), [' to', ' for', ' that', '.', '...'])\n",
      "(tensor([0.2372, 0.1658, 0.1656, 0.1001, 0.0321], grad_fn=<ToCopyBackward0>), [' me', ' the', ' us', ' a', ' all'])\n",
      "(tensor([0.0534, 0.0392, 0.0370, 0.0317, 0.0252], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' producers', ' entire', ' creators'])\n",
      "(tensor([0.1447, 0.1045, 0.0414, 0.0386, 0.0348], grad_fn=<ToCopyBackward0>), [' film', ' world', ' community', ' movie', ' universe'])\n",
      "(tensor([0.9591, 0.0094, 0.0044, 0.0038, 0.0036], grad_fn=<ToCopyBackward0>), [' to', \"'s\", ' -', ' of', '.'])\n",
      "(tensor([0.3939, 0.1929, 0.1168, 0.0383, 0.0341], grad_fn=<ToCopyBackward0>), [' see', ' learn', ' get', ' be', ' say'])\n",
      "(tensor([0.0739, 0.0725, 0.0668, 0.0582, 0.0182], grad_fn=<ToCopyBackward0>), [' what', ' this', ' how', ' the', ' Cuba'])\n",
      "(tensor([0.0238, 0.0184, 0.0172, 0.0148, 0.0094], grad_fn=<ToCopyBackward0>), [' real', ' artwork', ' movie', ' film', ' trailer'])\n",
      "(tensor([0.3545, 0.1496, 0.0350, 0.0258, 0.0229], grad_fn=<ToCopyBackward0>), ['.', ' in', ' because', ' without', ' for'])\n",
      "(tensor([0.3270, 0.0991, 0.0810, 0.0463, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' So', ' And'])\n",
      "(tensor([0.4361, 0.1680, 0.1137, 0.0285, 0.0272], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' has'])\n",
      "(tensor([0.1826, 0.1056, 0.0512, 0.0356, 0.0312], grad_fn=<ToCopyBackward0>), [' a', ' like', ' an', ' so', ' the'])\n",
      "(tensor([0.6675, 0.0959, 0.0180, 0.0164, 0.0126], grad_fn=<ToCopyBackward0>), [' opportunity', ' embarrassment', ' insult', ' awful', ' amazing'])\n",
      "(tensor([0.5177, 0.4492, 0.0060, 0.0045, 0.0029], grad_fn=<ToCopyBackward0>), [' to', ' for', ' that', '...', ','])\n",
      "(tensor([0.1954, 0.0722, 0.0565, 0.0541, 0.0455], grad_fn=<ToCopyBackward0>), [' show', ' bring', ' be', ' tell', ' get'])\n",
      "(tensor([0.1914, 0.1591, 0.0750, 0.0719, 0.0545], grad_fn=<ToCopyBackward0>), [' to', ' the', ' out', ' together', ' a'])\n",
      "(tensor([0.2085, 0.1837, 0.1199, 0.0681, 0.0332], grad_fn=<ToCopyBackward0>), [' the', ' all', ' people', ' a', ' great'])\n",
      "(tensor([0.2724, 0.1992, 0.0995, 0.0315, 0.0285], grad_fn=<ToCopyBackward0>), [' the', ' those', ' people', ' these', ' of'])\n",
      "(tensor([0.1354, 0.0613, 0.0342, 0.0185, 0.0156], grad_fn=<ToCopyBackward0>), [' different', ' people', ' great', ' talented', ' world'])\n",
      "(tensor([0.5826, 0.2385, 0.0548, 0.0183, 0.0176], grad_fn=<ToCopyBackward0>), [' involved', ' who', ' in', ' that', ' from'])\n",
      "(tensor([0.7053, 0.0860, 0.0710, 0.0416, 0.0411], grad_fn=<ToCopyBackward0>), [' in', '.', ' with', ',', ' and'])\n",
      "(tensor([0.4383, 0.3114, 0.1281, 0.0208, 0.0101], grad_fn=<ToCopyBackward0>), [' the', ' this', ' making', ' it', ' a'])\n",
      "(tensor([0.3784, 0.1634, 0.1266, 0.0305, 0.0257], grad_fn=<ToCopyBackward0>), [' film', ' project', ' movie', '.', ' and'])\n",
      "(tensor([0.4965, 0.1260, 0.0965, 0.0692, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' to'])\n",
      "(tensor([0.6454, 0.0586, 0.0294, 0.0194, 0.0173], grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' make', ' get'])\n",
      "(tensor([0.0959, 0.0368, 0.0345, 0.0260, 0.0252], grad_fn=<ToCopyBackward0>), [' people', ' entire', ' world', ' rest', ' fact'])\n",
      "(tensor([0.2177, 0.1600, 0.1596, 0.0518, 0.0460], grad_fn=<ToCopyBackward0>), [' to', '.', ' of', ' in', ' at'])\n",
      "(tensor([0.3193, 0.0645, 0.0571, 0.0564, 0.0401], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' watch', ' enjoy'])\n",
      "(tensor([0.4389, 0.0526, 0.0409, 0.0352, 0.0338], grad_fn=<ToCopyBackward0>), [' able', ' together', ' amazed', ' a', ' inspired'])\n",
      "(tensor([9.9745e-01, 1.0900e-03, 3.9776e-04, 5.8682e-05, 5.7716e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' see', ' for', ' really', ' in'])\n",
      "(tensor([0.4183, 0.0676, 0.0474, 0.0339, 0.0333], grad_fn=<ToCopyBackward0>), [' see', ' enjoy', ' say', ' get', ' learn'])\n",
      "(tensor([0.3446, 0.2335, 0.1536, 0.0456, 0.0419], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' what'])\n",
      "(tensor([0.2694, 0.0884, 0.0805, 0.0343, 0.0220], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' result', ' complete', ' trailer'])\n",
      "(tensor([0.2769, 0.1611, 0.1566, 0.0464, 0.0354], grad_fn=<ToCopyBackward0>), [' in', ' together', '.', ' for', ' on'])\n",
      "(tensor([0.1786, 0.1633, 0.0904, 0.0873, 0.0377], grad_fn=<ToCopyBackward0>), [' all', ' a', ' the', ' its', ' an'])\n",
      "(tensor([0.6090, 0.0441, 0.0343, 0.0342, 0.0291], grad_fn=<ToCopyBackward0>), [' its', ' kinds', ' sorts', ' of', ' forms'])\n",
      "(tensor([0.4506, 0.0643, 0.0301, 0.0225, 0.0092], grad_fn=<ToCopyBackward0>), [' glory', ' spl', ' original', ' beauty', ' weird'])\n",
      "(tensor([0.6031, 0.1867, 0.0701, 0.0420, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' -'])\n",
      "(tensor([0.2164, 0.1154, 0.0811, 0.0593, 0.0548], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' So'])\n",
      "(tensor([0.3405, 0.2573, 0.0790, 0.0438, 0.0388], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' seemed', ' wasn'])\n",
      "(tensor([0.3397, 0.3301, 0.0366, 0.0225, 0.0219], grad_fn=<ToCopyBackward0>), [' an', ' a', ' the', ' not', ' like'])\n",
      "(tensor([0.3230, 0.1080, 0.0520, 0.0495, 0.0413], grad_fn=<ToCopyBackward0>), [' chance', ' very', ' way', ' great', ' good'])\n",
      "(tensor([0.1592, 0.0825, 0.0463, 0.0277, 0.0270], grad_fn=<ToCopyBackward0>), [' emotional', ' sad', ' difficult', ' unfortunate', ' emotionally'])\n",
      "(tensor([0.5218, 0.1121, 0.0920, 0.0417, 0.0303], grad_fn=<ToCopyBackward0>), [' movie', ' experience', ' film', ' time', ' day'])\n",
      "(tensor([0.4117, 0.1881, 0.1494, 0.0604, 0.0383], grad_fn=<ToCopyBackward0>), [' for', '.', ' to', ' and', ','])\n",
      "(tensor([0.8910, 0.0334, 0.0211, 0.0117, 0.0085], grad_fn=<ToCopyBackward0>), [' me', ' the', ' all', ' us', ' everyone'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a waste of my time. It was so predictable and predictable, I was actually surprised when I found out that it was not going to be the same movie as the preview I saw. The movie was slow and boring, the plot was\n",
      "(tensor([0.3841, 0.1716, 0.0897, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2433, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6518, 0.0597, 0.0363, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1371, 0.0702, 0.0660, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0613, 0.0530, 0.0426, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.9688, 0.0080, 0.0021, 0.0019, 0.0014], grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' because'])\n",
      "(tensor([0.5765, 0.0886, 0.0884, 0.0432, 0.0165], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' 90', ' 2'])\n",
      "(tensor([0.7810, 0.1434, 0.0491, 0.0054, 0.0042], grad_fn=<ToCopyBackward0>), [' time', ' money', ' life', ' $', ' precious'])\n",
      "(tensor([0.6273, 0.1341, 0.0512, 0.0329, 0.0165], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', '....'])\n",
      "(tensor([0.2251, 0.2047, 0.1471, 0.0253, 0.0204], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.2725, 0.1747, 0.0663, 0.0551, 0.0430], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' wasn'])\n",
      "(tensor([0.1132, 0.0943, 0.0837, 0.0792, 0.0764], grad_fn=<ToCopyBackward0>), [' slow', ' so', ' not', ' boring', ' a'])\n",
      "(tensor([0.3177, 0.1152, 0.0660, 0.0341, 0.0285], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' bad', ' slow', ' stupid'])\n",
      "(tensor([0.4359, 0.1536, 0.1456, 0.0862, 0.0349], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' that', ' I'])\n",
      "(tensor([0.1219, 0.0709, 0.0584, 0.0525, 0.0485], grad_fn=<ToCopyBackward0>), [' predictable', ' cliché', ' the', ' so', ' I'])\n",
      "(tensor([0.1529, 0.1163, 0.0910, 0.0856, 0.0795], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', '.', ' it'])\n",
      "(tensor([0.3079, 0.1449, 0.1240, 0.0648, 0.0636], grad_fn=<ToCopyBackward0>), [' I', ' that', ' and', ' the', ' it'])\n",
      "(tensor([0.1368, 0.1225, 0.0957, 0.0671, 0.0653], grad_fn=<ToCopyBackward0>), [' just', ' was', ' could', ' couldn', ' almost'])\n",
      "(tensor([0.0687, 0.0520, 0.0459, 0.0426, 0.0366], grad_fn=<ToCopyBackward0>), [' almost', ' disappointed', ' really', ' actually', ' getting'])\n",
      "(tensor([0.1330, 0.0802, 0.0798, 0.0661, 0.0388], grad_fn=<ToCopyBackward0>), [' surprised', ' disappointed', ' bored', ' embarrassed', ' looking'])\n",
      "(tensor([0.3050, 0.1628, 0.1392, 0.0829, 0.0680], grad_fn=<ToCopyBackward0>), [' when', ' at', ' that', ' by', ' to'])\n",
      "(tensor([0.5326, 0.2043, 0.1197, 0.0368, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' they', ' this'])\n",
      "(tensor([0.1718, 0.1502, 0.0666, 0.0598, 0.0587], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' was', ' got', ' watched'])\n",
      "(tensor([0.7774, 0.0600, 0.0593, 0.0408, 0.0259], grad_fn=<ToCopyBackward0>), [' out', ' that', ' it', ' myself', ' the'])\n",
      "(tensor([0.3597, 0.3228, 0.0737, 0.0350, 0.0343], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' how', ' the'])\n",
      "(tensor([0.2565, 0.1515, 0.1428, 0.1207, 0.0508], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.6645, 0.1206, 0.0396, 0.0230, 0.0219], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' had', \"'s\", ' would'])\n",
      "(tensor([0.3038, 0.0938, 0.0815, 0.0505, 0.0470], grad_fn=<ToCopyBackward0>), [' a', ' based', ' not', ' about', ' supposed'])\n",
      "(tensor([0.1802, 0.1173, 0.0963, 0.0769, 0.0756], grad_fn=<ToCopyBackward0>), [' a', ' even', ' only', ' based', ' going'])\n",
      "(tensor([9.8531e-01, 1.9698e-03, 1.0393e-03, 9.9717e-04, 9.3773e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' straight', ' in', ' anywhere'])\n",
      "(tensor([0.5306, 0.3210, 0.0470, 0.0172, 0.0124], grad_fn=<ToCopyBackward0>), [' be', ' end', ' get', ' have', ' make'])\n",
      "(tensor([0.2334, 0.0967, 0.0631, 0.0481, 0.0450], grad_fn=<ToCopyBackward0>), [' a', ' rated', ' released', ' the', ' in'])\n",
      "(tensor([0.1056, 0.1018, 0.0795, 0.0780, 0.0662], grad_fn=<ToCopyBackward0>), [' last', ' worst', ' same', ' best', ' \"'])\n",
      "(tensor([0.2564, 0.1385, 0.1020, 0.0620, 0.0319], grad_fn=<ToCopyBackward0>), [' as', '.', ' movie', ' ending', ' in'])\n",
      "(tensor([0.4740, 0.0989, 0.0930, 0.0554, 0.0324], grad_fn=<ToCopyBackward0>), ['.', ' I', ' as', ' at', ' in'])\n",
      "(tensor([0.5233, 0.1455, 0.0360, 0.0246, 0.0241], grad_fn=<ToCopyBackward0>), [' the', ' in', ' before', ' it', ' \"'])\n",
      "(tensor([0.3187, 0.1766, 0.0748, 0.0654, 0.0436], grad_fn=<ToCopyBackward0>), [' previews', ' preview', ' one', ' trailer', ' book'])\n",
      "(tensor([0.3795, 0.1664, 0.0526, 0.0379, 0.0378], grad_fn=<ToCopyBackward0>), ['.', ' I', ',', ' reel', ' showed'])\n",
      "(tensor([0.6419, 0.1853, 0.0385, 0.0141, 0.0121], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' had', ' viewed', ' was'])\n",
      "(tensor([0.7594, 0.0578, 0.0316, 0.0190, 0.0144], grad_fn=<ToCopyBackward0>), ['.', ' at', ' in', ' back', ' on'])\n",
      "(tensor([0.2109, 0.1837, 0.1055, 0.0440, 0.0219], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'The'])\n",
      "(tensor([0.1230, 0.1222, 0.0691, 0.0396, 0.0380], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' acting', ' plot', ' first'])\n",
      "(tensor([0.2879, 0.1131, 0.0577, 0.0444, 0.0434], grad_fn=<ToCopyBackward0>), [' was', ' is', ' starts', ' started', ' had'])\n",
      "(tensor([0.1004, 0.0748, 0.0712, 0.0546, 0.0489], grad_fn=<ToCopyBackward0>), [' so', ' not', ' about', ' slow', ' predictable'])\n",
      "(tensor([0.4299, 0.2991, 0.1306, 0.0516, 0.0142], grad_fn=<ToCopyBackward0>), [' and', ' moving', ',', ' paced', ' going'])\n",
      "(tensor([0.3217, 0.1281, 0.0567, 0.0323, 0.0289], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' had', ' was', ' the'])\n",
      "(tensor([0.3538, 0.2887, 0.1993, 0.0320, 0.0196], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' with'])\n",
      "(tensor([0.2258, 0.1824, 0.0900, 0.0870, 0.0322], grad_fn=<ToCopyBackward0>), [' and', ' the', ' I', ' with', ' it'])\n",
      "(tensor([0.5502, 0.0491, 0.0484, 0.0373, 0.0306], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' plot', ' story'])\n",
      "(tensor([0.8093, 0.0187, 0.0098, 0.0091, 0.0080], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' seemed', ' wasn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this was a really dumb movie. It's not even that I think it's dumb. It's just dumb in a way that makes no sense, that has nothing to do with what's going to happen in the story. There's a reason\n",
      "(tensor([0.3834, 0.1723, 0.0903, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3325, 0.2358, 0.0582, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4146, 0.1972, 0.1689, 0.0494, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.4996, 0.1496, 0.0852, 0.0601, 0.0126], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' probably'])\n",
      "(tensor([0.1224, 0.0881, 0.0844, 0.0760, 0.0756], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' really', ' pretty', ' sequel'])\n",
      "(tensor([0.3680, 0.1323, 0.0573, 0.0366, 0.0229], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' stupid', ' cheesy', ' good'])\n",
      "(tensor([0.8759, 0.0411, 0.0138, 0.0094, 0.0070], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' story', ','])\n",
      "(tensor([0.6346, 0.0805, 0.0349, 0.0335, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', '!', ' when'])\n",
      "(tensor([0.2260, 0.1299, 0.0868, 0.0229, 0.0219], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.3113, 0.2136, 0.0516, 0.0408, 0.0365], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' seemed', ' had', ' is'])\n",
      "(tensor([0.2884, 0.0886, 0.0792, 0.0744, 0.0629], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' really', ' just'])\n",
      "(tensor([0.5607, 0.0942, 0.0666, 0.0471, 0.0298], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' as', ' that', ' a'])\n",
      "(tensor([0.4015, 0.1233, 0.0469, 0.0466, 0.0284], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' good', ' that', ' the'])\n",
      "(tensor([0.3763, 0.1177, 0.1014, 0.0784, 0.0782], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' bad', ' I', ' it'])\n",
      "(tensor([0.1673, 0.1355, 0.1205, 0.0669, 0.0641], grad_fn=<ToCopyBackward0>), [' don', \"'m\", ' think', ' didn', ' hate'])\n",
      "(tensor([0.5513, 0.1169, 0.0616, 0.0438, 0.0261], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' they'])\n",
      "(tensor([0.6749, 0.1165, 0.0961, 0.0133, 0.0118], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' sucks'])\n",
      "(tensor([0.2645, 0.2257, 0.1232, 0.0651, 0.0278], grad_fn=<ToCopyBackward0>), [' a', ' dumb', ' bad', ' stupid', ' good'])\n",
      "(tensor([0.4139, 0.2195, 0.0998, 0.0294, 0.0242], grad_fn=<ToCopyBackward0>), [',', '.', ';', ' --', ' to'])\n",
      "(tensor([0.4056, 0.2668, 0.0436, 0.0256, 0.0188], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.8336, 0.1105, 0.0178, 0.0097, 0.0049], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' is', ' was', ' has'])\n",
      "(tensor([0.6881, 0.0866, 0.0775, 0.0244, 0.0148], grad_fn=<ToCopyBackward0>), [' just', ' that', ' not', ' dumb', ' the'])\n",
      "(tensor([0.2391, 0.2278, 0.1040, 0.0521, 0.0369], grad_fn=<ToCopyBackward0>), [' that', ' dumb', ' stupid', ' really', ' not'])\n",
      "(tensor([0.3213, 0.2503, 0.1065, 0.0631, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ' in', ' that', ' as', ' for'])\n",
      "(tensor([0.3958, 0.3468, 0.0370, 0.0326, 0.0315], grad_fn=<ToCopyBackward0>), [' a', ' the', ' that', ' an', ' its'])\n",
      "(tensor([0.1315, 0.1134, 0.0829, 0.0798, 0.0491], grad_fn=<ToCopyBackward0>), [' way', ' really', ' different', ' very', ' movie'])\n",
      "(tensor([0.9010, 0.0400, 0.0127, 0.0092, 0.0020], grad_fn=<ToCopyBackward0>), [' that', ' I', ' you', ' where', ' thats'])\n",
      "(tensor([0.1514, 0.1443, 0.0834, 0.0624, 0.0539], grad_fn=<ToCopyBackward0>), [' is', ' makes', ' I', \"'s\", ' doesn'])\n",
      "(tensor([0.2911, 0.2513, 0.1799, 0.1128, 0.0173], grad_fn=<ToCopyBackward0>), [' you', ' no', ' it', ' me', ' the'])\n",
      "(tensor([0.8830, 0.0242, 0.0109, 0.0051, 0.0037], grad_fn=<ToCopyBackward0>), [' sense', ' damn', ' logical', ' goddamn', ' obvious'])\n",
      "(tensor([0.6111, 0.1002, 0.0893, 0.0713, 0.0304], grad_fn=<ToCopyBackward0>), ['.', ' whatsoever', ',', ' at', ' to'])\n",
      "(tensor([0.1808, 0.0739, 0.0560, 0.0470, 0.0410], grad_fn=<ToCopyBackward0>), [' and', ' like', ' even', ' except', ' that'])\n",
      "(tensor([0.2600, 0.1419, 0.1074, 0.0889, 0.0640], grad_fn=<ToCopyBackward0>), [' makes', \"'s\", ' is', ' doesn', ' has'])\n",
      "(tensor([0.6658, 0.2310, 0.0137, 0.0133, 0.0104], grad_fn=<ToCopyBackward0>), [' no', ' nothing', ' absolutely', ' been', ' a'])\n",
      "(tensor([0.9457, 0.0149, 0.0074, 0.0064, 0.0025], grad_fn=<ToCopyBackward0>), [' to', ' at', ' in', ' even', ' but'])\n",
      "(tensor([0.9279, 0.0169, 0.0111, 0.0109, 0.0043], grad_fn=<ToCopyBackward0>), [' do', ' say', ' recommend', ' offer', ' contribute'])\n",
      "(tensor([0.9742, 0.0068, 0.0035, 0.0018, 0.0011], grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' the', ' other', ' even'])\n",
      "(tensor([0.6771, 0.0723, 0.0270, 0.0241, 0.0230], grad_fn=<ToCopyBackward0>), [' the', ' any', ' anything', ' what', ' reality'])\n",
      "(tensor([0.2453, 0.2237, 0.0764, 0.0724, 0.0694], grad_fn=<ToCopyBackward0>), [' the', ' it', ' is', ' you', \"'s\"])\n",
      "(tensor([0.5848, 0.0972, 0.0727, 0.0503, 0.0376], grad_fn=<ToCopyBackward0>), [' going', ' happening', ' actually', ' supposed', ' on'])\n",
      "(tensor([9.8944e-01, 4.3143e-03, 2.1990e-03, 7.6308e-04, 5.7327e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' on', ' to', ' in', ' through', ' down'])\n",
      "(tensor([0.8962, 0.0384, 0.0195, 0.0063, 0.0049], grad_fn=<ToCopyBackward0>), [' happen', ' be', ' come', ' make', ' actually'])\n",
      "(tensor([0.4009, 0.2458, 0.0889, 0.0533, 0.0482], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' to', ' next'])\n",
      "(tensor([0.7764, 0.0823, 0.0599, 0.0169, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' any', ' a'])\n",
      "(tensor([0.6158, 0.1291, 0.0476, 0.0219, 0.0182], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' end', ' film', ' future'])\n",
      "(tensor([0.5903, 0.2297, 0.0525, 0.0380, 0.0131], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', ' at'])\n",
      "(tensor([0.1588, 0.1274, 0.1253, 0.0487, 0.0279], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.4129, 0.2121, 0.1953, 0.0877, 0.0240], grad_fn=<ToCopyBackward0>), [\"'s\", ' are', ' is', ' was', ' were'])\n",
      "(tensor([0.4259, 0.2229, 0.0693, 0.0374, 0.0344], grad_fn=<ToCopyBackward0>), [' nothing', ' no', ' a', ' not', ' just'])\n",
      "(tensor([0.1248, 0.0937, 0.0735, 0.0599, 0.0465], grad_fn=<ToCopyBackward0>), [' guy', ' really', ' lot', ' plot', ' reason'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was going to watch this movie just because it's a good movie. I had a pretty good time and it was well made. I don't think I was expecting much, but it was pretty funny too. I really don't like to be\n",
      "(tensor([0.3838, 0.1715, 0.0896, 0.0771, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2234, 0.1943, 0.1577, 0.0694, 0.0541], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4603, 0.2361, 0.0590, 0.0169, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.9597, 0.0135, 0.0028, 0.0028, 0.0027], grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' for', ' mad', ' blind'])\n",
      "(tensor([0.5428, 0.1262, 0.0626, 0.0561, 0.0284], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' have', ' get', ' see'])\n",
      "(tensor([0.5129, 0.1303, 0.1064, 0.0797, 0.0164], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' paint'])\n",
      "(tensor([0.5605, 0.1600, 0.0330, 0.0154, 0.0148], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' because', ' one', ' for'])\n",
      "(tensor([0.1312, 0.1176, 0.1099, 0.0805, 0.0551], grad_fn=<ToCopyBackward0>), [' all', ' just', ' for', ' in', ' and'])\n",
      "(tensor([0.5996, 0.1270, 0.1042, 0.0256, 0.0235], grad_fn=<ToCopyBackward0>), [' to', ' for', ' because', ' the', ' a'])\n",
      "(tensor([0.3071, 0.2041, 0.1212, 0.0694, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' of', ' its'])\n",
      "(tensor([0.4787, 0.1985, 0.0762, 0.0286, 0.0255], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' is', ' stars'])\n",
      "(tensor([0.2103, 0.1098, 0.0698, 0.0326, 0.0259], grad_fn=<ToCopyBackward0>), [' a', ' so', ' the', ' fun', ' about'])\n",
      "(tensor([0.1998, 0.1789, 0.1110, 0.0433, 0.0398], grad_fn=<ToCopyBackward0>), [' remake', ' good', ' really', ' pretty', ' classic'])\n",
      "(tensor([0.7560, 0.0302, 0.0250, 0.0248, 0.0167], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' comedy', ' horror', ' thriller'])\n",
      "(tensor([0.6662, 0.1708, 0.0328, 0.0151, 0.0129], grad_fn=<ToCopyBackward0>), ['.', ',', '...', '....', ' and'])\n",
      "(tensor([0.3613, 0.1099, 0.0874, 0.0481, 0.0411], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' And', ' Then'])\n",
      "(tensor([0.2217, 0.0626, 0.0553, 0.0478, 0.0329], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' didn', ' thought', ' had'])\n",
      "(tensor([0.1565, 0.1178, 0.0989, 0.0749, 0.0468], grad_fn=<ToCopyBackward0>), [' a', ' no', ' to', ' very', ' absolutely'])\n",
      "(tensor([0.3200, 0.1848, 0.0715, 0.0631, 0.0475], grad_fn=<ToCopyBackward0>), [' feeling', ' really', ' pretty', ' great', ' few'])\n",
      "(tensor([0.8883, 0.0189, 0.0076, 0.0056, 0.0042], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' high', ' decent', ' easy'])\n",
      "(tensor([0.3737, 0.3160, 0.0563, 0.0388, 0.0237], grad_fn=<ToCopyBackward0>), [' time', ' idea', ' laugh', ' feeling', ' imagination'])\n",
      "(tensor([0.3678, 0.2051, 0.1097, 0.0314, 0.0220], grad_fn=<ToCopyBackward0>), [' watching', '.', ',', ' and', ' with'])\n",
      "(tensor([0.4583, 0.0936, 0.0371, 0.0335, 0.0254], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' was'])\n",
      "(tensor([0.4381, 0.1360, 0.0398, 0.0319, 0.0291], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' wasn', ' had'])\n",
      "(tensor([0.1431, 0.0863, 0.0655, 0.0643, 0.0375], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' kind', ' funny', ' well'])\n",
      "(tensor([0.5275, 0.1425, 0.0982, 0.0941, 0.0264], grad_fn=<ToCopyBackward0>), [' acted', ' made', '-', ' done', ' directed'])\n",
      "(tensor([0.5510, 0.1099, 0.0932, 0.0831, 0.0337], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.2159, 0.1506, 0.1056, 0.0926, 0.0263], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' So'])\n",
      "(tensor([0.1032, 0.0867, 0.0773, 0.0601, 0.0427], grad_fn=<ToCopyBackward0>), [' was', ' just', ' really', \"'m\", ' don'])\n",
      "(tensor([9.9645e-01, 9.3083e-04, 5.8273e-04, 2.2626e-04, 1.6898e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2740, 0.2340, 0.1850, 0.0465, 0.0282], grad_fn=<ToCopyBackward0>), [' know', ' think', ' really', ' mind', ' like'])\n",
      "(tensor([0.2678, 0.2239, 0.0959, 0.0667, 0.0629], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' there', ' anyone'])\n",
      "(tensor([0.1228, 0.1214, 0.0746, 0.0743, 0.0607], grad_fn=<ToCopyBackward0>), [\"'m\", \"'ll\", ' would', ' was', \"'ve\"])\n",
      "(tensor([0.4698, 0.0507, 0.0406, 0.0377, 0.0306], grad_fn=<ToCopyBackward0>), [' expecting', ' looking', ' watching', ' disappointed', ' really'])\n",
      "(tensor([0.3810, 0.1643, 0.0997, 0.0939, 0.0280], grad_fn=<ToCopyBackward0>), [' much', ' a', ' to', ' it', ' anything'])\n",
      "(tensor([0.2108, 0.1990, 0.1643, 0.1260, 0.0778], grad_fn=<ToCopyBackward0>), [',', '.', ' from', ' more', ' in'])\n",
      "(tensor([0.8228, 0.0218, 0.0161, 0.0143, 0.0113], grad_fn=<ToCopyBackward0>), [' but', ' except', ' though', ' I', ' and'])\n",
      "(tensor([0.4350, 0.2142, 0.0348, 0.0308, 0.0216], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' the', ' what'])\n",
      "(tensor([0.4695, 0.0518, 0.0340, 0.0319, 0.0319], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' exceeded', ' didn', ' turned'])\n",
      "(tensor([0.2030, 0.0887, 0.0518, 0.0452, 0.0372], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' a', ' well', ' funny'])\n",
      "(tensor([0.5251, 0.0785, 0.0770, 0.0582, 0.0382], grad_fn=<ToCopyBackward0>), [' funny', ' cool', ' good', ' entertaining', ' boring'])\n",
      "(tensor([0.4664, 0.3543, 0.0563, 0.0134, 0.0121], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' too', '!'])\n",
      "(tensor([0.8990, 0.0374, 0.0121, 0.0106, 0.0092], grad_fn=<ToCopyBackward0>), ['.', ',', ' so', '!', ' and'])\n",
      "(tensor([0.3409, 0.1470, 0.1289, 0.0254, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'I', 'The'])\n",
      "(tensor([0.0878, 0.0839, 0.0640, 0.0576, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' really', ' don', \"'m\", ' think'])\n",
      "(tensor([0.1453, 0.1337, 0.0734, 0.0562, 0.0552], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' enjoyed', ' liked', ' wanted'])\n",
      "(tensor([9.9809e-01, 2.6486e-04, 2.4119e-04, 1.6472e-04, 8.4986e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3648, 0.2364, 0.0834, 0.0524, 0.0515], grad_fn=<ToCopyBackward0>), [' think', ' know', ' understand', ' like', ' mind'])\n",
      "(tensor([0.1292, 0.1236, 0.1119, 0.0463, 0.0405], grad_fn=<ToCopyBackward0>), [' to', ' this', ' the', ' it', ' movies'])\n",
      "(tensor([0.1623, 0.1195, 0.0965, 0.0723, 0.0543], grad_fn=<ToCopyBackward0>), [' be', ' see', ' watch', ' give', ' think'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty boring. I mean, I've seen a few bad bad movies before, but this was just boring. It wasn't funny, it wasn't entertaining, the plot was stupid, and the acting was terrible. I mean, how\n",
      "(tensor([0.3838, 0.1721, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2443, 0.1964, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7769, 0.0502, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1266, 0.1052, 0.0741, 0.0604], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.3245, 0.2565, 0.1515, 0.0371, 0.0155], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' at'])\n",
      "(tensor([0.1947, 0.1705, 0.1468, 0.0341, 0.0178], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.0962, 0.0702, 0.0618, 0.0559, 0.0475], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', ' really', \"'m\"])\n",
      "(tensor([0.3914, 0.1454, 0.0984, 0.0431, 0.0221], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' this'])\n",
      "(tensor([0.1754, 0.1474, 0.1263, 0.0264, 0.0229], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' there', ' this'])\n",
      "(tensor([0.0896, 0.0599, 0.0529, 0.0453, 0.0441], grad_fn=<ToCopyBackward0>), [\"'m\", ' like', \"'ve\", ' was', ' didn'])\n",
      "(tensor([0.4643, 0.1149, 0.0646, 0.0609, 0.0213], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' watched', ' never', ' got'])\n",
      "(tensor([0.1043, 0.0776, 0.0516, 0.0413, 0.0391], grad_fn=<ToCopyBackward0>), [' some', ' lots', ' a', ' pretty', ' more'])\n",
      "(tensor([0.8063, 0.0550, 0.0417, 0.0210, 0.0154], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' couple', ' bunch', ' ton'])\n",
      "(tensor([0.1447, 0.0964, 0.0732, 0.0507, 0.0375], grad_fn=<ToCopyBackward0>), [' bad', ' other', ' films', ' movies', ' more'])\n",
      "(tensor([0.3159, 0.1607, 0.1246, 0.0531, 0.0259], grad_fn=<ToCopyBackward0>), [' movies', ' horror', ' films', ' bad', ' ones'])\n",
      "(tensor([0.6875, 0.0739, 0.0669, 0.0450, 0.0114], grad_fn=<ToCopyBackward0>), [' movies', ' bad', ' films', ' horror', ' movie'])\n",
      "(tensor([0.4783, 0.2503, 0.0732, 0.0394, 0.0385], grad_fn=<ToCopyBackward0>), [' in', ',', '.', ' before', ' and'])\n",
      "(tensor([0.5854, 0.0915, 0.0795, 0.0712, 0.0336], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', '...'])\n",
      "(tensor([0.7266, 0.1107, 0.0242, 0.0175, 0.0145], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' I', ' like'])\n",
      "(tensor([0.6231, 0.1408, 0.0701, 0.0091, 0.0090], grad_fn=<ToCopyBackward0>), [' this', ' I', ' not', ' they', ' it'])\n",
      "(tensor([0.5097, 0.2067, 0.0675, 0.0349, 0.0246], grad_fn=<ToCopyBackward0>), [' one', ' was', ' is', ' wasn', ' movie'])\n",
      "(tensor([0.3059, 0.0669, 0.0654, 0.0609, 0.0576], grad_fn=<ToCopyBackward0>), [' just', ' one', ' really', ' the', ' pretty'])\n",
      "(tensor([0.2657, 0.0475, 0.0399, 0.0387, 0.0379], grad_fn=<ToCopyBackward0>), [' boring', ' bad', ' a', ' plain', ' lame'])\n",
      "(tensor([0.4701, 0.2543, 0.0608, 0.0359, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' to'])\n",
      "(tensor([0.1983, 0.1834, 0.1386, 0.0376, 0.0252], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2879, 0.1205, 0.1091, 0.0803, 0.0778], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' wasn', ' didn'])\n",
      "(tensor([9.9769e-01, 6.3103e-04, 2.9513e-04, 1.3166e-04, 1.2387e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.4173, 0.2790, 0.0971, 0.0347, 0.0206], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' even', ' bad', ' that'])\n",
      "(tensor([0.4189, 0.1629, 0.1242, 0.1043, 0.0504], grad_fn=<ToCopyBackward0>), [',', '.', ' at', ' or', ' and'])\n",
      "(tensor([0.3235, 0.0836, 0.0829, 0.0746, 0.0582], grad_fn=<ToCopyBackward0>), [' it', ' or', ' and', ' the', ' I'])\n",
      "(tensor([0.7300, 0.1345, 0.0962, 0.0129, 0.0064], grad_fn=<ToCopyBackward0>), [' wasn', ' didn', ' was', ' had', ' just'])\n",
      "(tensor([9.9786e-01, 4.1826e-04, 3.1893e-04, 1.6936e-04, 1.0430e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '.'])\n",
      "(tensor([0.4437, 0.1345, 0.0509, 0.0508, 0.0436], grad_fn=<ToCopyBackward0>), [' interesting', ' scary', ' exciting', ' entertaining', ' suspense'])\n",
      "(tensor([0.5704, 0.2109, 0.0616, 0.0450, 0.0139], grad_fn=<ToCopyBackward0>), [',', '.', '...', ' and', ' or'])\n",
      "(tensor([0.6652, 0.1572, 0.0426, 0.0230, 0.0214], grad_fn=<ToCopyBackward0>), [' it', ' and', ' the', ' I', ' there'])\n",
      "(tensor([0.4567, 0.0651, 0.0562, 0.0538, 0.0269], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' plot', ' actors', ' story'])\n",
      "(tensor([0.8029, 0.0406, 0.0158, 0.0116, 0.0107], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' didn', ' had', ' made'])\n",
      "(tensor([0.0979, 0.0769, 0.0625, 0.0482, 0.0427], grad_fn=<ToCopyBackward0>), [' predictable', ' stupid', ' boring', ' so', ' weak'])\n",
      "(tensor([0.3984, 0.3353, 0.1420, 0.0404, 0.0107], grad_fn=<ToCopyBackward0>), [' and', ',', '.', '...', ' as'])\n",
      "(tensor([0.4356, 0.3420, 0.0578, 0.0253, 0.0230], grad_fn=<ToCopyBackward0>), [' and', ' the', ' it', ' I', ' there'])\n",
      "(tensor([0.5496, 0.1175, 0.0978, 0.0363, 0.0113], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' even'])\n",
      "(tensor([0.7286, 0.0458, 0.0353, 0.0321, 0.0120], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' ending', ' movie'])\n",
      "(tensor([0.7525, 0.0968, 0.0436, 0.0150, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', '...', ' just'])\n",
      "(tensor([0.0891, 0.0763, 0.0425, 0.0375, 0.0368], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' even', ' horrible', ' worse'])\n",
      "(tensor([0.8823, 0.0281, 0.0159, 0.0155, 0.0089], grad_fn=<ToCopyBackward0>), ['.', '!', '...', ',', ' ('])\n",
      "(tensor([0.1906, 0.1733, 0.1000, 0.0341, 0.0226], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1008, 0.0782, 0.0635, 0.0555, 0.0495], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' don', ' really', ' mean'])\n",
      "(tensor([0.8010, 0.0312, 0.0216, 0.0166, 0.0104], grad_fn=<ToCopyBackward0>), [',', ' the', ' I', ' it', ' this'])\n",
      "(tensor([0.2188, 0.0573, 0.0560, 0.0314, 0.0220], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' how', ' there'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the worst movie I have ever seen! It is not even funny! I am a Christian and a Christian has no business making a movie like this. If you want to see a movie that is FUNNY you should see The Naked Gun with\n",
      "(tensor([0.3834, 0.1721, 0.0904, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4368, 0.2450, 0.1961, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4797, 0.1353, 0.1035, 0.0552, 0.0255], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7418, 0.0314, 0.0260, 0.0243, 0.0177], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.6909, 0.0867, 0.0160, 0.0158, 0.0132], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' Christmas', ' horror'])\n",
      "(tensor([0.6251, 0.2458, 0.0754, 0.0089, 0.0069], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.4412, 0.2257, 0.1737, 0.0719, 0.0536], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([9.2860e-01, 6.2850e-02, 2.0397e-03, 1.6488e-03, 9.0628e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' EVER', ' watched'])\n",
      "(tensor([0.8896, 0.0225, 0.0169, 0.0107, 0.0106], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' wasted', ' made'])\n",
      "(tensor([0.5637, 0.1221, 0.1023, 0.0780, 0.0212], grad_fn=<ToCopyBackward0>), ['.', '!', ' in', ',', '...'])\n",
      "(tensor([0.3399, 0.1364, 0.1149, 0.0301, 0.0201], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' No'])\n",
      "(tensor([0.4375, 0.1171, 0.0820, 0.0376, 0.0338], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' had', ' made'])\n",
      "(tensor([0.1811, 0.1624, 0.0938, 0.0508, 0.0338], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' the', ' just'])\n",
      "(tensor([0.6197, 0.2367, 0.0128, 0.0125, 0.0118], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' a', ' as', ' only'])\n",
      "(tensor([0.5610, 0.0643, 0.0451, 0.0434, 0.0396], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' worthy', ' a', ' worth'])\n",
      "(tensor([0.2785, 0.2502, 0.1081, 0.0787, 0.0404], grad_fn=<ToCopyBackward0>), ['!', '.', ',', ' to', ' at'])\n",
      "(tensor([0.2609, 0.1575, 0.1521, 0.0186, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' No'])\n",
      "(tensor([0.0863, 0.0710, 0.0594, 0.0561, 0.0457], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', ' don', ' would'])\n",
      "(tensor([0.1958, 0.1008, 0.0652, 0.0617, 0.0354], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' speech', ' surprised'])\n",
      "(tensor([0.0914, 0.0838, 0.0376, 0.0376, 0.0338], grad_fn=<ToCopyBackward0>), [' Christian', ' big', ' family', ' huge', ' fan'])\n",
      "(tensor([0.3647, 0.1158, 0.0431, 0.0305, 0.0219], grad_fn=<ToCopyBackward0>), [' and', ',', ' &', ' I', '!'])\n",
      "(tensor([0.5247, 0.0667, 0.0395, 0.0329, 0.0303], grad_fn=<ToCopyBackward0>), [' I', ' this', ' a', ' my', ' am'])\n",
      "(tensor([0.3024, 0.0851, 0.0184, 0.0148, 0.0147], grad_fn=<ToCopyBackward0>), [' Christian', ' family', ' Catholic', ' Family', ' very'])\n",
      "(tensor([0.0689, 0.0482, 0.0366, 0.0308, 0.0263], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' does', ' doesn', ' has'])\n",
      "(tensor([0.1509, 0.0764, 0.0700, 0.0678, 0.0624], grad_fn=<ToCopyBackward0>), [' a', ' no', ' always', ' never', ' to'])\n",
      "(tensor([0.3190, 0.1138, 0.0702, 0.0427, 0.0302], grad_fn=<ToCopyBackward0>), [' right', ' sense', ' business', ' taste', ' part'])\n",
      "(tensor([0.4466, 0.0497, 0.0431, 0.0395, 0.0295], grad_fn=<ToCopyBackward0>), [' making', ' saying', ' being', ' telling', ' ever'])\n",
      "(tensor([0.4763, 0.2654, 0.0814, 0.0465, 0.0108], grad_fn=<ToCopyBackward0>), [' a', ' movies', ' such', ' this', ' an'])\n",
      "(tensor([0.7535, 0.1247, 0.0325, 0.0085, 0.0055], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' mockery', ' stupid'])\n",
      "(tensor([0.8260, 0.0711, 0.0183, 0.0174, 0.0164], grad_fn=<ToCopyBackward0>), [' like', ' that', ' about', ' this', ' with'])\n",
      "(tensor([9.7466e-01, 2.2093e-02, 7.8636e-04, 4.4078e-04, 3.8438e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' this', ' that', ' the', ' \"', ' it'])\n",
      "(tensor([0.3209, 0.2831, 0.0705, 0.0606, 0.0590], grad_fn=<ToCopyBackward0>), ['.', '!', '!\"', ' one', ','])\n",
      "(tensor([0.3181, 0.0997, 0.0946, 0.0246, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' They'])\n",
      "(tensor([0.4186, 0.1740, 0.1162, 0.0445, 0.0372], grad_fn=<ToCopyBackward0>), [' you', ' I', ' they', ' this', ' it'])\n",
      "(tensor([0.2526, 0.2004, 0.0653, 0.0407, 0.0354], grad_fn=<ToCopyBackward0>), [' want', ' are', ' have', ' do', ' like'])\n",
      "(tensor([0.8618, 0.0951, 0.0067, 0.0053, 0.0031], grad_fn=<ToCopyBackward0>), [' to', ' a', ' an', ' comedy', ' the'])\n",
      "(tensor([0.2983, 0.1816, 0.0809, 0.0657, 0.0500], grad_fn=<ToCopyBackward0>), [' see', ' make', ' be', ' watch', ' go'])\n",
      "(tensor([0.6165, 0.0491, 0.0446, 0.0338, 0.0256], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' more', ' how'])\n",
      "(tensor([0.3593, 0.1586, 0.1428, 0.0918, 0.0386], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' comedy', ' laugh', ' funny'])\n",
      "(tensor([0.5758, 0.1229, 0.0856, 0.0295, 0.0216], grad_fn=<ToCopyBackward0>), [' that', ' about', ' like', ' with', ' you'])\n",
      "(tensor([0.2238, 0.1060, 0.0535, 0.0274, 0.0267], grad_fn=<ToCopyBackward0>), [' is', ' will', ' has', ' teaches', \"'s\"])\n",
      "(tensor([0.5012, 0.1006, 0.0867, 0.0584, 0.0321], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' FUN', ' not', ' fun'])\n",
      "(tensor([9.7089e-01, 6.1290e-03, 5.6744e-03, 4.8085e-03, 8.1983e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['NY', 'N', 'ny', 'D', '-'])\n",
      "(tensor([0.3898, 0.1358, 0.0831, 0.0593, 0.0349], grad_fn=<ToCopyBackward0>), [' then', ' you', ',', ' and', ' this'])\n",
      "(tensor([0.2337, 0.1733, 0.1063, 0.0884, 0.0692], grad_fn=<ToCopyBackward0>), [' should', ' can', ' have', ' go', ' will'])\n",
      "(tensor([0.3955, 0.3530, 0.0780, 0.0213, 0.0144], grad_fn=<ToCopyBackward0>), [' go', ' see', ' watch', ' rent', ' get'])\n",
      "(tensor([0.0307, 0.0254, 0.0168, 0.0114, 0.0110], grad_fn=<ToCopyBackward0>), [' the', ' The', ' Dumb', ' \"', ' Billy'])\n",
      "(tensor([0.3987, 0.0271, 0.0230, 0.0203, 0.0163], grad_fn=<ToCopyBackward0>), [' Naked', ' Simpsons', ' Nut', ' Incredible', ' Ring'])\n",
      "(tensor([0.8582, 0.0382, 0.0141, 0.0069, 0.0040], grad_fn=<ToCopyBackward0>), [' Gun', ' Brothers', ' Man', ' Brother', ' Mile'])\n",
      "(tensor([0.1880, 0.1075, 0.0744, 0.0592, 0.0531], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' with', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd seen it all. I was wrong. This movie is really funny. It's not as bad as I thought it could be. It's not as bad as I thought it could be. I was really looking forward to seeing it. This\n",
      "(tensor([0.3834, 0.1722, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2252, 0.1946, 0.1574, 0.0695, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2081, 0.1049, 0.0618, 0.0395, 0.0298], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.4483, 0.0926, 0.0914, 0.0736, 0.0531], grad_fn=<ToCopyBackward0>), [' it', ' everything', ' the', ' all', ' a'])\n",
      "(tensor([9.9687e-01, 5.3489e-04, 3.8403e-04, 2.1002e-04, 1.7362e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', '.', ',', ' done', ' in'])\n",
      "(tensor([0.2495, 0.2374, 0.1108, 0.0868, 0.0486], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', ' in', ' before'])\n",
      "(tensor([0.4638, 0.0460, 0.0439, 0.0386, 0.0208], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', ' But'])\n",
      "(tensor([0.1886, 0.1414, 0.0792, 0.0623, 0.0295], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', \"'m\", \"'ve\"])\n",
      "(tensor([0.8307, 0.0477, 0.0147, 0.0082, 0.0058], grad_fn=<ToCopyBackward0>), [' wrong', ' so', ' really', ' in', ' a'])\n",
      "(tensor([0.8309, 0.0975, 0.0101, 0.0099, 0.0068], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' on', '!!'])\n",
      "(tensor([0.3055, 0.2609, 0.0551, 0.0472, 0.0385], grad_fn=<ToCopyBackward0>), [' This', ' I', ' The', ' It', 'This'])\n",
      "(tensor([0.3767, 0.1608, 0.0961, 0.0861, 0.0415], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.4154, 0.1871, 0.0378, 0.0359, 0.0319], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' really'])\n",
      "(tensor([0.0927, 0.0614, 0.0559, 0.0457, 0.0412], grad_fn=<ToCopyBackward0>), [' just', ' about', ' so', ' really', ' not'])\n",
      "(tensor([0.2096, 0.1168, 0.0829, 0.0709, 0.0487], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ',', ' funny', ' something'])\n",
      "(tensor([0.5458, 0.1687, 0.1187, 0.0611, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' as'])\n",
      "(tensor([0.2225, 0.2065, 0.1217, 0.0218, 0.0196], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.4573, 0.1644, 0.1432, 0.0216, 0.0193], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' really'])\n",
      "(tensor([0.1980, 0.0816, 0.0597, 0.0546, 0.0398], grad_fn=<ToCopyBackward0>), [' not', ' really', ' got', ' like', ' funny'])\n",
      "(tensor([0.1114, 0.1046, 0.0918, 0.0648, 0.0597], grad_fn=<ToCopyBackward0>), [' even', ' as', ' just', ' a', ' that'])\n",
      "(tensor([0.2531, 0.0522, 0.0367, 0.0297, 0.0206], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' dumb', ' bad', ' much'])\n",
      "(tensor([0.9722, 0.0102, 0.0027, 0.0017, 0.0014], grad_fn=<ToCopyBackward0>), [' as', ' a', ',', ' or', ' for'])\n",
      "(tensor([0.4552, 0.0722, 0.0673, 0.0424, 0.0372], grad_fn=<ToCopyBackward0>), [' I', ' \"', ' it', ' you', ' the'])\n",
      "(tensor([0.4993, 0.0920, 0.0866, 0.0405, 0.0356], grad_fn=<ToCopyBackward0>), [' thought', ' expected', ' was', ' think', ' first'])\n",
      "(tensor([9.8347e-01, 5.6156e-03, 4.2148e-03, 9.1651e-04, 6.0574e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' I', ' the'])\n",
      "(tensor([0.6812, 0.2497, 0.0361, 0.0162, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', \"'d\", ' might'])\n",
      "(tensor([0.9217, 0.0342, 0.0149, 0.0063, 0.0043], grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', ' or', ' possibly'])\n",
      "(tensor([0.5176, 0.3562, 0.0177, 0.0130, 0.0094], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.2348, 0.2061, 0.1224, 0.0223, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.5983, 0.1164, 0.0561, 0.0211, 0.0166], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' really'])\n",
      "(tensor([0.2385, 0.1490, 0.0616, 0.0383, 0.0358], grad_fn=<ToCopyBackward0>), [' not', ' funny', ' just', ' a', ' really'])\n",
      "(tensor([0.7490, 0.0910, 0.0240, 0.0156, 0.0136], grad_fn=<ToCopyBackward0>), [' as', ' even', ' that', ' funny', ' the'])\n",
      "(tensor([0.3582, 0.2635, 0.0712, 0.0411, 0.0127], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' good', ' boring', ' scary'])\n",
      "(tensor([0.9755, 0.0049, 0.0046, 0.0022, 0.0011], grad_fn=<ToCopyBackward0>), [' as', ',', ' a', ' or', ' it'])\n",
      "(tensor([0.8397, 0.0182, 0.0105, 0.0089, 0.0083], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' it', ' you'])\n",
      "(tensor([0.8584, 0.0396, 0.0284, 0.0227, 0.0032], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' expected', ' would'])\n",
      "(tensor([0.6622, 0.1293, 0.0597, 0.0260, 0.0094], grad_fn=<ToCopyBackward0>), [' it', ' I', '.', ' the', ','])\n",
      "(tensor([0.8906, 0.0384, 0.0343, 0.0071, 0.0044], grad_fn=<ToCopyBackward0>), [' could', ' would', ' was', ' should', ' might'])\n",
      "(tensor([9.9225e-01, 1.8182e-03, 1.4155e-03, 4.1104e-04, 3.6075e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', '.', ' become'])\n",
      "(tensor([0.8265, 0.0677, 0.0202, 0.0165, 0.0076], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' because', ' either'])\n",
      "(tensor([0.3619, 0.2088, 0.1067, 0.0284, 0.0239], grad_fn=<ToCopyBackward0>), [' It', ' I', ' This', ' The', ' You'])\n",
      "(tensor([0.1361, 0.1058, 0.1018, 0.0744, 0.0376], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' think'])\n",
      "(tensor([0.4432, 0.1290, 0.0635, 0.0579, 0.0395], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' so', ' very', ' actually'])\n",
      "(tensor([0.2566, 0.1647, 0.1084, 0.0829, 0.0432], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' surprised', ' wrong', ','])\n",
      "(tensor([9.9557e-01, 2.8094e-03, 5.6005e-04, 3.5118e-04, 2.3215e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' at', ' forwards'])\n",
      "(tensor([9.9206e-01, 1.8473e-03, 1.1525e-03, 1.0936e-03, 7.0513e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', '.', ','])\n",
      "(tensor([0.3173, 0.2053, 0.1913, 0.1337, 0.0286], grad_fn=<ToCopyBackward0>), [' this', ' watching', ' seeing', ' it', ' the'])\n",
      "(tensor([0.5159, 0.2295, 0.0274, 0.0206, 0.0115], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' some', ' a'])\n",
      "(tensor([0.2753, 0.2594, 0.0852, 0.0702, 0.0587], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' and', ' but'])\n",
      "(tensor([0.5408, 0.1355, 0.0374, 0.0244, 0.0241], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' This', ' The'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a good movie. It's not the best movie I've ever seen, but it's not the worst movie I've ever seen. I thought it was pretty cool and I really wanted to like it. The problem I had with this\n",
      "(tensor([0.3844, 0.1717, 0.0898, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2437, 0.1962, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6521, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0661, 0.0547, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0615, 0.0527, 0.0428, 0.0406], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.3100, 0.1901, 0.0865, 0.0350, 0.0293], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' story', ' premise'])\n",
      "(tensor([0.3249, 0.1267, 0.0883, 0.0762, 0.0577], grad_fn=<ToCopyBackward0>), ['.', '...', '....', '!', ','])\n",
      "(tensor([0.3263, 0.2303, 0.0792, 0.0279, 0.0153], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.3805, 0.1241, 0.1186, 0.0543, 0.0362], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' has', ' wasn'])\n",
      "(tensor([0.1638, 0.1083, 0.0621, 0.0547, 0.0532], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' one', ' got'])\n",
      "(tensor([0.1194, 0.0989, 0.0939, 0.0782, 0.0691], grad_fn=<ToCopyBackward0>), [' as', ' even', ' great', ' that', ' the'])\n",
      "(tensor([0.5115, 0.3804, 0.0435, 0.0132, 0.0042], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' most', ' first'])\n",
      "(tensor([0.4744, 0.0762, 0.0647, 0.0352, 0.0279], grad_fn=<ToCopyBackward0>), [' movie', ' I', ' picture', ' comedy', ' film'])\n",
      "(tensor([0.7010, 0.1190, 0.0320, 0.0266, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ',', ' in'])\n",
      "(tensor([0.7021, 0.2336, 0.0267, 0.0077, 0.0076], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' think', ' saw'])\n",
      "(tensor([0.7489, 0.2309, 0.0046, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' made'])\n",
      "(tensor([0.7672, 0.1278, 0.0238, 0.0205, 0.0166], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' watched', ' had', ' been'])\n",
      "(tensor([0.7132, 0.0813, 0.0660, 0.0552, 0.0115], grad_fn=<ToCopyBackward0>), [',', '.', ' in', ' but', ' ('])\n",
      "(tensor([0.8433, 0.0286, 0.0228, 0.0157, 0.0097], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' not', ' I'])\n",
      "(tensor([0.6709, 0.0929, 0.0145, 0.0139, 0.0132], grad_fn=<ToCopyBackward0>), [' it', ' I', ' for', ' its', ' if'])\n",
      "(tensor([0.5882, 0.2051, 0.0386, 0.0225, 0.0215], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.4788, 0.0868, 0.0754, 0.0460, 0.0306], grad_fn=<ToCopyBackward0>), [' not', ' good', ' a', ' worth', ' pretty'])\n",
      "(tensor([0.6972, 0.0812, 0.0511, 0.0299, 0.0171], grad_fn=<ToCopyBackward0>), [' the', ' a', ' as', ' bad', ' one'])\n",
      "(tensor([9.8363e-01, 4.2912e-03, 1.1746e-03, 8.3616e-04, 8.0905e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' worst', ' worse', ' bad', ' second', ' most'])\n",
      "(tensor([0.3258, 0.3116, 0.1535, 0.0695, 0.0429], grad_fn=<ToCopyBackward0>), [' either', ' movie', ' one', ',', '.'])\n",
      "(tensor([0.9497, 0.0157, 0.0127, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' either', '.', ','])\n",
      "(tensor([9.7937e-01, 1.3476e-02, 2.7501e-03, 7.8985e-04, 7.0895e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' think', \"'m\"])\n",
      "(tensor([9.6931e-01, 2.7118e-02, 1.1365e-03, 4.0801e-04, 3.8085e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' even', ' EVER'])\n",
      "(tensor([0.9290, 0.0521, 0.0039, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' been', ' had', ' made'])\n",
      "(tensor([0.4102, 0.4083, 0.1003, 0.0251, 0.0062], grad_fn=<ToCopyBackward0>), [',', '.', ' either', ' and', ' as'])\n",
      "(tensor([0.2905, 0.2722, 0.0675, 0.0339, 0.0194], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'It'])\n",
      "(tensor([0.0768, 0.0719, 0.0663, 0.0569, 0.0448], grad_fn=<ToCopyBackward0>), [\"'m\", ' thought', ' think', ' just', ' don'])\n",
      "(tensor([0.5461, 0.1687, 0.1337, 0.0495, 0.0116], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.7773, 0.0458, 0.0209, 0.0207, 0.0130], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' did'])\n",
      "(tensor([0.2009, 0.1233, 0.0590, 0.0563, 0.0546], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' watch', ' interesting', ' funny'])\n",
      "(tensor([0.2956, 0.2564, 0.1447, 0.0355, 0.0270], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' entertaining', ' cool', ' interesting'])\n",
      "(tensor([0.3407, 0.2319, 0.1780, 0.0363, 0.0170], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' when'])\n",
      "(tensor([0.2909, 0.1302, 0.0686, 0.0577, 0.0426], grad_fn=<ToCopyBackward0>), [' I', ' interesting', ' the', ' pretty', ' well'])\n",
      "(tensor([0.1737, 0.1196, 0.1065, 0.0634, 0.0506], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' really', ' enjoyed', ' was'])\n",
      "(tensor([0.2483, 0.2109, 0.1023, 0.0582, 0.0578], grad_fn=<ToCopyBackward0>), [' liked', ' enjoyed', ' wanted', ' like', ' thought'])\n",
      "(tensor([0.9357, 0.0242, 0.0111, 0.0086, 0.0050], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' my', ' a'])\n",
      "(tensor([0.5812, 0.2714, 0.0722, 0.0077, 0.0063], grad_fn=<ToCopyBackward0>), [' see', ' like', ' watch', ' be', ' go'])\n",
      "(tensor([0.8788, 0.0732, 0.0280, 0.0025, 0.0014], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' some'])\n",
      "(tensor([0.5487, 0.2721, 0.0168, 0.0167, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' more', ' a', ' but'])\n",
      "(tensor([0.2504, 0.1233, 0.0940, 0.0725, 0.0352], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', 'I'])\n",
      "(tensor([0.2437, 0.1207, 0.0784, 0.0588, 0.0548], grad_fn=<ToCopyBackward0>), [' problem', ' only', ' acting', ' main', ' movie'])\n",
      "(tensor([0.6431, 0.2365, 0.0437, 0.0396, 0.0066], grad_fn=<ToCopyBackward0>), [' is', ' was', ' I', ' with', ','])\n",
      "(tensor([0.5313, 0.3638, 0.0215, 0.0116, 0.0072], grad_fn=<ToCopyBackward0>), [' have', ' had', ' found', ' think', ' really'])\n",
      "(tensor([0.6511, 0.2034, 0.0760, 0.0171, 0.0053], grad_fn=<ToCopyBackward0>), [' was', ' with', ' is', ',', ' in'])\n",
      "(tensor([0.5265, 0.3908, 0.0705, 0.0021, 0.0009], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie had very little to do with New York. I think the city has an enormous amount to do with it, but this movie was more about the city than the city was about the movie. So I really didn't think it could be a\n",
      "(tensor([0.3836, 0.1720, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5002, 0.0601, 0.0341, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6239, 0.0400, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2579, 0.0922, 0.0640, 0.0438, 0.0428], grad_fn=<ToCopyBackward0>), [' a', ' to', ' some', ' the', ' very'])\n",
      "(tensor([0.3123, 0.2931, 0.0420, 0.0280, 0.0204], grad_fn=<ToCopyBackward0>), [' little', ' good', ' few', ' well', ' interesting'])\n",
      "(tensor([0.4918, 0.1670, 0.0240, 0.0216, 0.0190], grad_fn=<ToCopyBackward0>), [' to', ' plot', ' humor', ' in', ','])\n",
      "(tensor([0.3500, 0.2445, 0.1909, 0.0744, 0.0535], grad_fn=<ToCopyBackward0>), [' do', ' recommend', ' say', ' offer', ' comment'])\n",
      "(tensor([0.8996, 0.0346, 0.0088, 0.0044, 0.0042], grad_fn=<ToCopyBackward0>), [' with', ' about', ' in', ' other', ','])\n",
      "(tensor([0.2603, 0.0343, 0.0291, 0.0234, 0.0220], grad_fn=<ToCopyBackward0>), [' the', ' Little', ' this', ' New', ' any'])\n",
      "(tensor([0.7220, 0.0445, 0.0389, 0.0341, 0.0187], grad_fn=<ToCopyBackward0>), [' York', ' Zealand', ' Zeal', ' Age', ' Jersey'])\n",
      "(tensor([0.4524, 0.2682, 0.1022, 0.0254, 0.0212], grad_fn=<ToCopyBackward0>), [' City', '.', ',', ' and', ' in'])\n",
      "(tensor([0.1619, 0.1454, 0.1341, 0.0365, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.1489, 0.0981, 0.0713, 0.0595, 0.0503], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' mean', \"'m\"])\n",
      "(tensor([0.3492, 0.1443, 0.1238, 0.0537, 0.0330], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' New'])\n",
      "(tensor([0.1029, 0.0855, 0.0607, 0.0371, 0.0361], grad_fn=<ToCopyBackward0>), [' movie', ' city', ' director', ' only', ' New'])\n",
      "(tensor([0.3081, 0.1576, 0.0869, 0.0796, 0.0434], grad_fn=<ToCopyBackward0>), [' is', ' was', ' itself', ' of', ' has'])\n",
      "(tensor([0.2364, 0.1358, 0.1029, 0.0590, 0.0377], grad_fn=<ToCopyBackward0>), [' a', ' to', ' been', ' always', ' an'])\n",
      "(tensor([0.1268, 0.0772, 0.0486, 0.0366, 0.0261], grad_fn=<ToCopyBackward0>), [' incredible', ' enormous', ' amazing', ' important', ' incredibly'])\n",
      "(tensor([0.4083, 0.0620, 0.0360, 0.0272, 0.0270], grad_fn=<ToCopyBackward0>), [' amount', ' impact', ' cultural', ' role', ' part'])\n",
      "(tensor([0.8750, 0.0990, 0.0063, 0.0045, 0.0027], grad_fn=<ToCopyBackward0>), [' to', ' of', ' in', ' more', ' going'])\n",
      "(tensor([0.4099, 0.3440, 0.0637, 0.0457, 0.0399], grad_fn=<ToCopyBackward0>), [' say', ' do', ' tell', ' offer', ' be'])\n",
      "(tensor([9.8107e-01, 2.7768e-03, 1.1748e-03, 1.0938e-03, 7.7522e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' in', ' not', ' as', ' and'])\n",
      "(tensor([0.5058, 0.2429, 0.0464, 0.0278, 0.0249], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' why', ' New'])\n",
      "(tensor([0.4355, 0.3471, 0.0267, 0.0251, 0.0222], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' but', ' in'])\n",
      "(tensor([0.6785, 0.0731, 0.0355, 0.0175, 0.0145], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' as', ' especially'])\n",
      "(tensor([0.2419, 0.1313, 0.1065, 0.0529, 0.0328], grad_fn=<ToCopyBackward0>), [' the', ' this', ' I', ' it', ' that'])\n",
      "(tensor([0.4430, 0.1911, 0.1087, 0.0373, 0.0325], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.1499, 0.1443, 0.1057, 0.0998, 0.0633], grad_fn=<ToCopyBackward0>), [' was', ' is', ' has', ' had', ' seemed'])\n",
      "(tensor([0.1067, 0.0806, 0.0586, 0.0578, 0.0537], grad_fn=<ToCopyBackward0>), [' about', ' more', ' just', ' very', ' really'])\n",
      "(tensor([0.6478, 0.0986, 0.0401, 0.0248, 0.0173], grad_fn=<ToCopyBackward0>), [' about', ' like', ' a', ' of', ' along'])\n",
      "(tensor([0.3448, 0.1865, 0.0395, 0.0189, 0.0169], grad_fn=<ToCopyBackward0>), [' the', ' New', ' Manhattan', ' a', ' how'])\n",
      "(tensor([0.0964, 0.0378, 0.0359, 0.0289, 0.0281], grad_fn=<ToCopyBackward0>), [' city', ' idea', ' people', ' story', ' relationship'])\n",
      "(tensor([0.1767, 0.1436, 0.1421, 0.1359, 0.0912], grad_fn=<ToCopyBackward0>), [' than', ' in', '.', ' as', ' of'])\n",
      "(tensor([0.5686, 0.1031, 0.0957, 0.0738, 0.0376], grad_fn=<ToCopyBackward0>), [' the', ' New', ' about', ' it', ' any'])\n",
      "(tensor([0.4618, 0.0721, 0.0421, 0.0317, 0.0194], grad_fn=<ToCopyBackward0>), [' city', ' rest', ' people', ' characters', ' borough'])\n",
      "(tensor([0.3328, 0.2496, 0.1390, 0.0756, 0.0328], grad_fn=<ToCopyBackward0>), [' was', '.', ' had', ' did', ' itself'])\n",
      "(tensor([0.8311, 0.0270, 0.0204, 0.0187, 0.0087], grad_fn=<ToCopyBackward0>), [' about', '.', ' in', ' the', ' with'])\n",
      "(tensor([0.9696, 0.0092, 0.0072, 0.0025, 0.0014], grad_fn=<ToCopyBackward0>), [' the', ' it', ' New', ' this', ' its'])\n",
      "(tensor([0.2022, 0.1838, 0.1151, 0.0631, 0.0592], grad_fn=<ToCopyBackward0>), [' characters', ' movie', ' city', ' character', ' people'])\n",
      "(tensor([0.9673, 0.0055, 0.0049, 0.0032, 0.0029], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', '....'])\n",
      "(tensor([0.1578, 0.0989, 0.0965, 0.0425, 0.0401], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' So', ' There'])\n",
      "(tensor([0.2344, 0.1368, 0.0869, 0.0807, 0.0544], grad_fn=<ToCopyBackward0>), [' I', ',', ' it', ' the', ' when'])\n",
      "(tensor([0.1467, 0.0929, 0.0647, 0.0580, 0.0560], grad_fn=<ToCopyBackward0>), [' was', ' think', \"'m\", ' really', ' don'])\n",
      "(tensor([0.1370, 0.0882, 0.0739, 0.0651, 0.0633], grad_fn=<ToCopyBackward0>), [' wanted', ' tried', ' don', ' was', ' didn'])\n",
      "(tensor([9.9630e-01, 1.2156e-03, 4.7450e-04, 2.8445e-04, 1.7229e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2788, 0.1010, 0.0932, 0.0654, 0.0467], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' have', ' see'])\n",
      "(tensor([0.2241, 0.1639, 0.1498, 0.1323, 0.0426], grad_fn=<ToCopyBackward0>), [' that', ' it', ' much', ' the', ' there'])\n",
      "(tensor([0.4410, 0.4293, 0.0423, 0.0266, 0.0089], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', \"'d\"])\n",
      "(tensor([0.6993, 0.0608, 0.0290, 0.0277, 0.0257], grad_fn=<ToCopyBackward0>), [' be', ' work', ' possibly', ' get', ' have'])\n",
      "(tensor([0.2707, 0.1042, 0.0446, 0.0431, 0.0422], grad_fn=<ToCopyBackward0>), [' a', ' as', ' very', ' funny', ' much'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a sequel to this movie. I can't even describe how bad that movie was, but this movie, I could not watch after it. The only part of that movie that was entertaining was the guy that plays a doctor that gets bitten by\n",
      "(tensor([0.3847, 0.1712, 0.0895, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1165, 0.0399, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1844, 0.1450, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1649, 0.1162, 0.0572, 0.0492, 0.0452], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8524, 0.0311, 0.0139, 0.0121, 0.0085], grad_fn=<ToCopyBackward0>), [' to', ' of', ',', '...', ' that'])\n",
      "(tensor([0.1700, 0.0655, 0.0591, 0.0368, 0.0355], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.7994, 0.1092, 0.0049, 0.0037, 0.0026], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' movies', ','])\n",
      "(tensor([0.7300, 0.0735, 0.0192, 0.0153, 0.0136], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' because', ' and'])\n",
      "(tensor([0.3158, 0.0944, 0.0636, 0.0300, 0.0265], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1478, 0.1220, 0.0558, 0.0521, 0.0453], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' can', ' really'])\n",
      "(tensor([0.6263, 0.0736, 0.0483, 0.0435, 0.0301], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' only', ' tell', ' remember'])\n",
      "(tensor([0.2940, 0.2651, 0.1040, 0.0712, 0.0344], grad_fn=<ToCopyBackward0>), [' remember', ' believe', ' even', ' really', ' say'])\n",
      "(tensor([0.4470, 0.1006, 0.0909, 0.0719, 0.0436], grad_fn=<ToCopyBackward0>), [' remember', ' describe', ' believe', ' tell', ' explain'])\n",
      "(tensor([0.4927, 0.2466, 0.0865, 0.0490, 0.0346], grad_fn=<ToCopyBackward0>), [' it', ' how', ' the', ' to', ' this'])\n",
      "(tensor([0.2302, 0.1709, 0.1292, 0.0596, 0.0593], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' awful', ' horrible', ' terrible'])\n",
      "(tensor([0.6927, 0.2170, 0.0406, 0.0236, 0.0153], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' that'])\n",
      "(tensor([0.6049, 0.1787, 0.0407, 0.0334, 0.0245], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' film', ' one'])\n",
      "(tensor([0.7836, 0.1625, 0.0218, 0.0075, 0.0019], grad_fn=<ToCopyBackward0>), [' was', ' is', ' really', ' actually', ' got'])\n",
      "(tensor([0.7469, 0.1264, 0.0352, 0.0153, 0.0072], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', ' but'])\n",
      "(tensor([0.3820, 0.0782, 0.0648, 0.0518, 0.0356], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' I', ' it'])\n",
      "(tensor([0.3944, 0.1338, 0.0998, 0.0376, 0.0326], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' the', ' at'])\n",
      "(tensor([0.3894, 0.1507, 0.1266, 0.1035, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' one', ' just'])\n",
      "(tensor([0.3232, 0.2174, 0.0618, 0.0524, 0.0331], grad_fn=<ToCopyBackward0>), [' is', ' was', '?', ',', ' just'])\n",
      "(tensor([0.2539, 0.1725, 0.0798, 0.0543, 0.0300], grad_fn=<ToCopyBackward0>), [' I', ' it', ' oh', ' well', ' not'])\n",
      "(tensor([0.1465, 0.1314, 0.0709, 0.0609, 0.0537], grad_fn=<ToCopyBackward0>), [' can', ' thought', ' think', ' mean', ' could'])\n",
      "(tensor([0.4413, 0.1420, 0.0655, 0.0324, 0.0319], grad_fn=<ToCopyBackward0>), [' not', ' barely', ' see', ' describe', ' watch'])\n",
      "(tensor([0.2830, 0.1239, 0.1232, 0.0750, 0.0412], grad_fn=<ToCopyBackward0>), [' even', ' stop', ' believe', ' get', ' watch'])\n",
      "(tensor([0.5944, 0.0536, 0.0529, 0.0352, 0.0273], grad_fn=<ToCopyBackward0>), [' it', '.', ' for', ' the', ' after'])\n",
      "(tensor([0.1633, 0.1127, 0.0967, 0.0769, 0.0748], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' it', ' watching', ' I'])\n",
      "(tensor([0.3386, 0.2336, 0.0551, 0.0430, 0.0235], grad_fn=<ToCopyBackward0>), ['.', ' was', ',', ' even', ' finished'])\n",
      "(tensor([0.3817, 0.1597, 0.1164, 0.0238, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.2399, 0.1741, 0.0576, 0.0480, 0.0390], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' movie', ' story'])\n",
      "(tensor([0.3273, 0.1516, 0.1167, 0.0471, 0.0348], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' part', ' way'])\n",
      "(tensor([0.4372, 0.2440, 0.1942, 0.0325, 0.0104], grad_fn=<ToCopyBackward0>), [' that', ' I', ' of', ' where', ' about'])\n",
      "(tensor([0.4649, 0.3114, 0.1983, 0.0179, 0.0031], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' that', ' my'])\n",
      "(tensor([9.8548e-01, 5.5043e-03, 1.8111e-03, 1.0708e-03, 5.2016e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' film', ' that', ' I', ' stupid'])\n",
      "(tensor([0.8833, 0.0905, 0.0048, 0.0033, 0.0021], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' which', ' was'])\n",
      "(tensor([0.5024, 0.1664, 0.0338, 0.0337, 0.0284], grad_fn=<ToCopyBackward0>), [' was', ' I', ' made', ' kept', ' could'])\n",
      "(tensor([0.2737, 0.1786, 0.1758, 0.0478, 0.0344], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' entertaining', ' toler', ' enjoyable'])\n",
      "(tensor([0.7961, 0.0622, 0.0346, 0.0153, 0.0144], grad_fn=<ToCopyBackward0>), [' was', ' were', ' to', ' for', ','])\n",
      "(tensor([0.5947, 0.1735, 0.0186, 0.0068, 0.0062], grad_fn=<ToCopyBackward0>), [' the', ' when', ' that', ' a', ' seeing'])\n",
      "(tensor([0.0497, 0.0492, 0.0327, 0.0283, 0.0267], grad_fn=<ToCopyBackward0>), [' ending', ' car', ' last', ' bit', ' guy'])\n",
      "(tensor([0.1395, 0.1361, 0.1015, 0.0700, 0.0583], grad_fn=<ToCopyBackward0>), [' that', ' with', ' who', ' falling', ' getting'])\n",
      "(tensor([0.3548, 0.1034, 0.0604, 0.0460, 0.0321], grad_fn=<ToCopyBackward0>), [' played', ' plays', ' was', ' did', ' got'])\n",
      "(tensor([0.4822, 0.0417, 0.0122, 0.0085, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' a', ' his', ' that', ' Mr'])\n",
      "(tensor([0.0781, 0.0579, 0.0309, 0.0260, 0.0212], grad_fn=<ToCopyBackward0>), [' cop', ' guy', ' doctor', ' little', ' really'])\n",
      "(tensor([0.2247, 0.2124, 0.0795, 0.0689, 0.0382], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' in', ' who'])\n",
      "(tensor([0.0677, 0.0348, 0.0343, 0.0330, 0.0322], grad_fn=<ToCopyBackward0>), [' gets', ' is', ' was', ' plays', ' has'])\n",
      "(tensor([0.0861, 0.0764, 0.0673, 0.0620, 0.0591], grad_fn=<ToCopyBackward0>), [' the', ' bit', ' bitten', ' to', ' his'])\n",
      "(tensor([0.8676, 0.0389, 0.0260, 0.0177, 0.0116], grad_fn=<ToCopyBackward0>), [' by', ' in', ' and', ' on', '.'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch it in a theater because the premise was so ridiculous and ridiculous, but I was wrong. I was really wrong. I actually found this to be a pretty enjoyable film with a pretty good cast. I was actually surprised by how well\n",
      "(tensor([0.3848, 0.1712, 0.0896, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2244, 0.1943, 0.1573, 0.0694, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6388, 0.1358, 0.0519, 0.0220, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2275, 0.1012, 0.0992, 0.0795, 0.0620], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6470, 0.1892, 0.0580, 0.0090, 0.0080], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.1713, 0.1294, 0.1087, 0.0934, 0.0925], grad_fn=<ToCopyBackward0>), [' because', ' all', ',', ' in', ' to'])\n",
      "(tensor([0.1654, 0.1384, 0.0754, 0.0570, 0.0374], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' order', ' Japanese'])\n",
      "(tensor([0.2793, 0.0366, 0.0291, 0.0270, 0.0247], grad_fn=<ToCopyBackward0>), [' theater', ' certain', ' theatre', ' language', ' special'])\n",
      "(tensor([0.1767, 0.1169, 0.0856, 0.0707, 0.0549], grad_fn=<ToCopyBackward0>), [' because', '.', ' so', ' like', ' to'])\n",
      "(tensor([0.4483, 0.1711, 0.0522, 0.0274, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' of'])\n",
      "(tensor([0.1176, 0.0705, 0.0302, 0.0261, 0.0242], grad_fn=<ToCopyBackward0>), [' movie', ' premise', ' sound', ' cover', ' story'])\n",
      "(tensor([0.3833, 0.2281, 0.1187, 0.0928, 0.0476], grad_fn=<ToCopyBackward0>), [' was', ' is', ' sounded', ' of', ' seemed'])\n",
      "(tensor([0.6501, 0.0233, 0.0182, 0.0156, 0.0153], grad_fn=<ToCopyBackward0>), [' so', ' pretty', ' ridiculous', ' terrible', ' bad'])\n",
      "(tensor([0.1482, 0.0997, 0.0504, 0.0408, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' ridiculous', ' terrible', ' bizarre', ' absurd'])\n",
      "(tensor([0.4179, 0.1485, 0.1105, 0.1040, 0.0241], grad_fn=<ToCopyBackward0>), ['.', ' and', ' that', ',', ' it'])\n",
      "(tensor([0.1342, 0.0662, 0.0478, 0.0401, 0.0272], grad_fn=<ToCopyBackward0>), [' the', ' I', ' predictable', ' so', ' ridiculous'])\n",
      "(tensor([0.2088, 0.1718, 0.1454, 0.1062, 0.0463], grad_fn=<ToCopyBackward0>), [' and', ' that', '.', ',', ' it'])\n",
      "(tensor([0.4853, 0.1601, 0.0525, 0.0466, 0.0375], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' the', ' that'])\n",
      "(tensor([0.3251, 0.1895, 0.0681, 0.0577, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' after'])\n",
      "(tensor([0.1021, 0.0841, 0.0779, 0.0767, 0.0598], grad_fn=<ToCopyBackward0>), [' actually', ' really', ' just', ' was', ' rented'])\n",
      "(tensor([0.2230, 0.1376, 0.1323, 0.0659, 0.0398], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' actually', ' so', ' pleasantly'])\n",
      "(tensor([0.8673, 0.0531, 0.0135, 0.0101, 0.0090], grad_fn=<ToCopyBackward0>), ['.', '!', ' because', ' on', ','])\n",
      "(tensor([0.4405, 0.1518, 0.0943, 0.0643, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.0956, 0.0695, 0.0691, 0.0649, 0.0539], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' really', ' actually', ' just'])\n",
      "(tensor([0.3114, 0.1485, 0.0730, 0.0717, 0.0476], grad_fn=<ToCopyBackward0>), [' wrong', ' really', ' actually', ' very', ' so'])\n",
      "(tensor([0.2406, 0.2155, 0.1440, 0.0800, 0.0415], grad_fn=<ToCopyBackward0>), [' disappointed', ' wrong', ' bored', ' looking', ' surprised'])\n",
      "(tensor([0.7940, 0.0408, 0.0319, 0.0260, 0.0232], grad_fn=<ToCopyBackward0>), ['.', ' about', ' in', ' on', ' because'])\n",
      "(tensor([0.4002, 0.1505, 0.0869, 0.0612, 0.0268], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1029, 0.0937, 0.0689, 0.0569, 0.0423], grad_fn=<ToCopyBackward0>), [' was', ' really', ' actually', \"'m\", ' watched'])\n",
      "(tensor([0.1850, 0.0804, 0.0764, 0.0542, 0.0332], grad_fn=<ToCopyBackward0>), [' found', ' rented', ' really', ' thought', ' enjoyed'])\n",
      "(tensor([0.3610, 0.2300, 0.0985, 0.0793, 0.0679], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' myself', ' watching'])\n",
      "(tensor([0.2604, 0.2261, 0.0753, 0.0526, 0.0378], grad_fn=<ToCopyBackward0>), [' to', ' movie', ' film', ' on', ' one'])\n",
      "(tensor([0.9760, 0.0050, 0.0037, 0.0019, 0.0017], grad_fn=<ToCopyBackward0>), [' be', ' a', ' actually', ' really', ' have'])\n",
      "(tensor([0.2303, 0.1186, 0.0929, 0.0770, 0.0657], grad_fn=<ToCopyBackward0>), [' a', ' one', ' pretty', ' very', ' an'])\n",
      "(tensor([0.3878, 0.2312, 0.1218, 0.0183, 0.0163], grad_fn=<ToCopyBackward0>), [' very', ' pretty', ' really', ' fairly', ' great'])\n",
      "(tensor([0.1871, 0.1269, 0.0854, 0.0543, 0.0392], grad_fn=<ToCopyBackward0>), [' entertaining', ' enjoyable', ' interesting', ' accurate', ' funny'])\n",
      "(tensor([0.5066, 0.3178, 0.0368, 0.0095, 0.0089], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' watch', ' 90', ','])\n",
      "(tensor([0.7274, 0.0791, 0.0290, 0.0248, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' with'])\n",
      "(tensor([0.3159, 0.0495, 0.0450, 0.0420, 0.0383], grad_fn=<ToCopyBackward0>), [' a', ' some', ' very', ' an', ' pretty'])\n",
      "(tensor([0.1179, 0.1153, 0.0795, 0.0631, 0.0629], grad_fn=<ToCopyBackward0>), [' very', ' few', ' pretty', ' good', ' lot'])\n",
      "(tensor([0.3297, 0.0882, 0.0866, 0.0295, 0.0288], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' entertaining', ' funny', ' decent'])\n",
      "(tensor([0.2092, 0.1418, 0.0902, 0.0752, 0.0700], grad_fn=<ToCopyBackward0>), [' cast', ' plot', ' budget', ' acting', ' supporting'])\n",
      "(tensor([0.6368, 0.1565, 0.0933, 0.0147, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' of', '.\"'])\n",
      "(tensor([0.1969, 0.1371, 0.1246, 0.0384, 0.0357], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.0877, 0.0864, 0.0532, 0.0491, 0.0485], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', \"'m\", ' just'])\n",
      "(tensor([0.4854, 0.1079, 0.0414, 0.0214, 0.0163], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' very', ' surprised', ' not'])\n",
      "(tensor([0.4752, 0.0689, 0.0684, 0.0528, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' very', ' pretty', ' looking', ' surprised'])\n",
      "(tensor([0.2947, 0.2283, 0.1677, 0.0700, 0.0648], grad_fn=<ToCopyBackward0>), [' by', ' at', ' that', ' to', ' how'])\n",
      "(tensor([0.4877, 0.1704, 0.0799, 0.0673, 0.0368], grad_fn=<ToCopyBackward0>), [' the', ' how', ' this', ' it', ' some'])\n",
      "(tensor([0.1735, 0.1232, 0.0965, 0.0724, 0.0677], grad_fn=<ToCopyBackward0>), [' much', ' good', ' bad', ' many', ' well'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch this because I was really disappointed with this movie. I'm not even gonna waste my time telling anyone about it because it's not funny...not even a little bit. I'm not even kidding...it's not even even funny\n",
      "(tensor([0.3836, 0.1721, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2256, 0.1946, 0.1571, 0.0694, 0.0535], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6388, 0.1358, 0.0516, 0.0221, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2255, 0.1012, 0.0998, 0.0803, 0.0619], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6463, 0.1897, 0.0579, 0.0089, 0.0081], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3172, 0.2491, 0.0943, 0.0425, 0.0238], grad_fn=<ToCopyBackward0>), [' movie', ' because', ' film', ' one', ' so'])\n",
      "(tensor([0.3611, 0.1971, 0.0618, 0.0535, 0.0213], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' i'])\n",
      "(tensor([0.1337, 0.1295, 0.1167, 0.0980, 0.0473], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', \"'m\", ' have'])\n",
      "(tensor([0.1674, 0.1248, 0.1205, 0.0420, 0.0406], grad_fn=<ToCopyBackward0>), [' so', ' really', ' a', ' in', ' going'])\n",
      "(tensor([0.3061, 0.1541, 0.0755, 0.0484, 0.0244], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' bored', ' interested', ' hoping'])\n",
      "(tensor([0.3101, 0.2897, 0.2593, 0.0235, 0.0196], grad_fn=<ToCopyBackward0>), [' in', ' with', ' by', '.', ' when'])\n",
      "(tensor([0.5692, 0.1476, 0.0206, 0.0190, 0.0150], grad_fn=<ToCopyBackward0>), [' this', ' the', ' how', ' it', ' \"'])\n",
      "(tensor([0.6199, 0.1961, 0.0350, 0.0093, 0.0090], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' episode', ' sequel', ' show'])\n",
      "(tensor([0.7872, 0.0507, 0.0216, 0.0181, 0.0135], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' in'])\n",
      "(tensor([0.2433, 0.1769, 0.1353, 0.0318, 0.0269], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1192, 0.0869, 0.0608, 0.0590, 0.0589], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' have', ' thought'])\n",
      "(tensor([0.3126, 0.2496, 0.0476, 0.0290, 0.0230], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' an', ' sure'])\n",
      "(tensor([0.1899, 0.1432, 0.1351, 0.1094, 0.0953], grad_fn=<ToCopyBackward0>), [' a', ' sure', ' even', ' one', ' going'])\n",
      "(tensor([0.3505, 0.2493, 0.1040, 0.0379, 0.0267], grad_fn=<ToCopyBackward0>), [' going', ' gonna', ' sure', ' talking', ' kidding'])\n",
      "(tensor([0.0862, 0.0826, 0.0825, 0.0772, 0.0552], grad_fn=<ToCopyBackward0>), [' call', ' waste', ' go', ' talk', ' bother'])\n",
      "(tensor([0.2966, 0.2441, 0.0845, 0.0738, 0.0659], grad_fn=<ToCopyBackward0>), [' my', ' time', ' more', ' any', ' the'])\n",
      "(tensor([0.9021, 0.0467, 0.0058, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' time', ' money', ' words', ' precious', ' 10'])\n",
      "(tensor([0.2961, 0.1595, 0.1346, 0.0472, 0.0430], grad_fn=<ToCopyBackward0>), [' commenting', ' writing', ' talking', ' telling', ' describing'])\n",
      "(tensor([0.6997, 0.0618, 0.0441, 0.0389, 0.0307], grad_fn=<ToCopyBackward0>), [' you', ' anyone', ' the', ' others', ' people'])\n",
      "(tensor([0.1817, 0.1147, 0.1004, 0.0949, 0.0921], grad_fn=<ToCopyBackward0>), [' about', ' else', ' how', ' to', ' what'])\n",
      "(tensor([0.5582, 0.3360, 0.0477, 0.0282, 0.0105], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' how', ' that'])\n",
      "(tensor([0.4888, 0.1798, 0.1383, 0.0141, 0.0112], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' or', ' but'])\n",
      "(tensor([0.3841, 0.2102, 0.0721, 0.0653, 0.0283], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' there', ' this'])\n",
      "(tensor([0.5210, 0.0868, 0.0681, 0.0657, 0.0379], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' doesn'])\n",
      "(tensor([0.5065, 0.1032, 0.0570, 0.0336, 0.0291], grad_fn=<ToCopyBackward0>), [' not', ' really', ' so', ' just', ' a'])\n",
      "(tensor([0.3681, 0.2592, 0.1130, 0.0841, 0.0317], grad_fn=<ToCopyBackward0>), [' worth', ' even', ' funny', ' really', ' worthy'])\n",
      "(tensor([0.3242, 0.1982, 0.1485, 0.0990, 0.0570], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' or', '...'])\n",
      "(tensor([0.0948, 0.0776, 0.0691, 0.0662, 0.0437], grad_fn=<ToCopyBackward0>), ['not', 'it', 'so', 'and', ' It'])\n",
      "(tensor([0.5857, 0.1565, 0.0638, 0.0336, 0.0294], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' really', ' in', ' at'])\n",
      "(tensor([0.1678, 0.1374, 0.0916, 0.0543, 0.0405], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' close', ' the', ' in'])\n",
      "(tensor([0.9449, 0.0235, 0.0065, 0.0049, 0.0011], grad_fn=<ToCopyBackward0>), [' little', ' bit', ' tiny', ' tad', ' few'])\n",
      "(tensor([0.9439, 0.0343, 0.0061, 0.0043, 0.0015], grad_fn=<ToCopyBackward0>), [' bit', '.', '...', ' funny', ' little'])\n",
      "(tensor([0.6997, 0.1208, 0.0486, 0.0236, 0.0133], grad_fn=<ToCopyBackward0>), ['.', '...', '!', '....', ','])\n",
      "(tensor([0.1927, 0.1903, 0.1310, 0.0421, 0.0288], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2077, 0.0662, 0.0650, 0.0403, 0.0370], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' was', ' can', ' really'])\n",
      "(tensor([0.3464, 0.0618, 0.0479, 0.0384, 0.0381], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' just', ' a', ' gonna'])\n",
      "(tensor([0.6334, 0.0646, 0.0502, 0.0464, 0.0394], grad_fn=<ToCopyBackward0>), [' even', ' gonna', ' one', ' sure', ' going'])\n",
      "(tensor([0.9482, 0.0247, 0.0080, 0.0029, 0.0018], grad_fn=<ToCopyBackward0>), [' gonna', ' going', ' kidding', ' sure', ' even'])\n",
      "(tensor([0.4731, 0.2211, 0.0779, 0.0400, 0.0324], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', '...', '!'])\n",
      "(tensor([0.1837, 0.1830, 0.1390, 0.1109, 0.0495], grad_fn=<ToCopyBackward0>), ['it', 'I', 'this', 'not', 'the'])\n",
      "(tensor([0.8237, 0.0370, 0.0216, 0.0164, 0.0137], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' wasn', ' is', ' really'])\n",
      "(tensor([0.6238, 0.0884, 0.0315, 0.0213, 0.0192], grad_fn=<ToCopyBackward0>), [' not', ' just', ' so', ' the', ' a'])\n",
      "(tensor([0.6782, 0.2934, 0.0070, 0.0054, 0.0016], grad_fn=<ToCopyBackward0>), [' even', ' funny', '.', ' a', ' amusing'])\n",
      "(tensor([0.6446, 0.1099, 0.0500, 0.0175, 0.0139], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' worth', ' entertaining', ' even'])\n",
      "(tensor([0.6554, 0.1026, 0.0411, 0.0203, 0.0136], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' worth', ' entertaining', ' amusing'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a good movie. I think that the idea of the two families is a pretty good one. And the movie is pretty well-acted, too. The movie is pretty well-acted. It's not a great movie, but I think\n",
      "(tensor([0.3841, 0.1713, 0.0897, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1165, 0.0398, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1451, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1648, 0.1161, 0.0572, 0.0491, 0.0452], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8846, 0.0467, 0.0214, 0.0081, 0.0078], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' idea', ' premise', ' sequel'])\n",
      "(tensor([0.3984, 0.1174, 0.0714, 0.0431, 0.0330], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' for'])\n",
      "(tensor([0.3423, 0.1963, 0.0491, 0.0222, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1429, 0.1428, 0.0653, 0.0406, 0.0394], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' liked'])\n",
      "(tensor([0.4278, 0.1287, 0.0867, 0.0375, 0.0280], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.2660, 0.1055, 0.0697, 0.0329, 0.0219], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' if', ' there'])\n",
      "(tensor([0.1002, 0.0693, 0.0575, 0.0550, 0.0490], grad_fn=<ToCopyBackward0>), [' movie', ' director', ' acting', ' story', ' idea'])\n",
      "(tensor([0.5093, 0.1263, 0.0955, 0.0880, 0.0811], grad_fn=<ToCopyBackward0>), [' of', ' was', ' that', ' behind', ' is'])\n",
      "(tensor([0.1566, 0.1298, 0.0849, 0.0476, 0.0291], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' a', ' having', ' being'])\n",
      "(tensor([0.0233, 0.0176, 0.0173, 0.0171, 0.0144], grad_fn=<ToCopyBackward0>), [' family', ' two', ' movie', ' guy', ' \"'])\n",
      "(tensor([0.3209, 0.0403, 0.0340, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' main', ' families', ' brothers', ' of', ' sides'])\n",
      "(tensor([0.1663, 0.1062, 0.0586, 0.0546, 0.0435], grad_fn=<ToCopyBackward0>), [' is', ' being', ' (', ' in', ','])\n",
      "(tensor([0.2350, 0.1218, 0.0981, 0.0761, 0.0336], grad_fn=<ToCopyBackward0>), [' a', ' very', ' interesting', ' good', ' pretty'])\n",
      "(tensor([0.6368, 0.0667, 0.0591, 0.0510, 0.0208], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' very', ' really', ' great'])\n",
      "(tensor([0.3563, 0.2085, 0.0348, 0.0303, 0.0248], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' original', ' strong', ' intriguing'])\n",
      "(tensor([0.4013, 0.2100, 0.0634, 0.0573, 0.0367], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' plot', ' idea', ' story'])\n",
      "(tensor([0.4572, 0.2492, 0.1196, 0.0326, 0.0176], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' for', ' to'])\n",
      "(tensor([0.1613, 0.1138, 0.0948, 0.0678, 0.0605], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' And'])\n",
      "(tensor([0.2683, 0.1736, 0.0836, 0.0434, 0.0362], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ',', ' that'])\n",
      "(tensor([0.1783, 0.1190, 0.0956, 0.0314, 0.0204], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' movie', ' fact', ' story'])\n",
      "(tensor([0.3240, 0.1174, 0.0701, 0.0620, 0.0340], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' does', ' did'])\n",
      "(tensor([0.3355, 0.0647, 0.0439, 0.0425, 0.0268], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' good', ' a', ' very'])\n",
      "(tensor([0.3029, 0.0877, 0.0767, 0.0751, 0.0729], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' entertaining', ' well', ' funny'])\n",
      "(tensor([0.6500, 0.1397, 0.0999, 0.0350, 0.0085], grad_fn=<ToCopyBackward0>), [' acted', ' done', '-', ' made', ' scripted'])\n",
      "(tensor([0.7526, 0.0895, 0.0577, 0.0318, 0.0194], grad_fn=<ToCopyBackward0>), ['acted', 'made', 'written', 'crafted', 'directed'])\n",
      "(tensor([0.4055, 0.2376, 0.1236, 0.0550, 0.0240], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' too', ' as'])\n",
      "(tensor([0.2342, 0.1308, 0.0818, 0.0729, 0.0609], grad_fn=<ToCopyBackward0>), [' too', ' but', ' I', ' and', ' although'])\n",
      "(tensor([0.6995, 0.2388, 0.0113, 0.0092, 0.0072], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' (', ';'])\n",
      "(tensor([0.1720, 0.1325, 0.1232, 0.0613, 0.0451], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' So', ' The'])\n",
      "(tensor([0.0942, 0.0835, 0.0791, 0.0558, 0.0256], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' problem', ' only', ' acting'])\n",
      "(tensor([0.4526, 0.0960, 0.0611, 0.0430, 0.0377], grad_fn=<ToCopyBackward0>), [' is', ' has', ' was', ' just', ' does'])\n",
      "(tensor([0.2660, 0.0426, 0.0393, 0.0356, 0.0313], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' a', ' good', ' really'])\n",
      "(tensor([0.2397, 0.1902, 0.1018, 0.0875, 0.0754], grad_fn=<ToCopyBackward0>), [' well', ' good', ' entertaining', ' funny', ' interesting'])\n",
      "(tensor([0.6581, 0.2732, 0.0233, 0.0118, 0.0046], grad_fn=<ToCopyBackward0>), ['-', ' acted', ' done', ' made', ' directed'])\n",
      "(tensor([0.6916, 0.0895, 0.0781, 0.0708, 0.0211], grad_fn=<ToCopyBackward0>), ['acted', 'made', 'written', 'directed', 'crafted'])\n",
      "(tensor([0.3338, 0.2828, 0.1061, 0.0498, 0.0231], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' in', ' by'])\n",
      "(tensor([0.1496, 0.1216, 0.1129, 0.0730, 0.0702], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' And', ' The'])\n",
      "(tensor([0.5738, 0.1000, 0.0497, 0.0468, 0.0374], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' just', ' is', ' was'])\n",
      "(tensor([0.1629, 0.1495, 0.0621, 0.0539, 0.0405], grad_fn=<ToCopyBackward0>), [' a', ' not', ' good', ' pretty', ' one'])\n",
      "(tensor([0.1085, 0.1013, 0.0706, 0.0693, 0.0691], grad_fn=<ToCopyBackward0>), [' a', ' great', ' very', ' one', ' the'])\n",
      "(tensor([0.3793, 0.0999, 0.0781, 0.0422, 0.0414], grad_fn=<ToCopyBackward0>), [' great', ' big', ' good', ' movie', ' very'])\n",
      "(tensor([0.7684, 0.0535, 0.0286, 0.0232, 0.0188], grad_fn=<ToCopyBackward0>), [' movie', ' picture', ' film', ' one', ' acting'])\n",
      "(tensor([0.4969, 0.2992, 0.0538, 0.0212, 0.0205], grad_fn=<ToCopyBackward0>), [',', '.', ' by', ' in', ' but'])\n",
      "(tensor([0.8796, 0.0211, 0.0140, 0.0110, 0.0096], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' it', ' by'])\n",
      "(tensor([0.5821, 0.1406, 0.0437, 0.0220, 0.0141], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' that', ' then'])\n",
      "(tensor([0.3081, 0.1248, 0.0528, 0.0517, 0.0364], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' liked', ' really', \"'m\"])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a good movie. I thought it was a good movie. But I was wrong. I was so wrong. I thought it was a good movie.I think that the first one is better than the second one, but it's not as\n",
      "(tensor([0.3834, 0.1720, 0.0902, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1838, 0.1463, 0.0507, 0.0451, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1641, 0.1159, 0.0580, 0.0491, 0.0446], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8839, 0.0469, 0.0218, 0.0081, 0.0078], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' idea', ' premise', ' sequel'])\n",
      "(tensor([0.3974, 0.1174, 0.0722, 0.0432, 0.0331], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' for'])\n",
      "(tensor([0.3431, 0.1962, 0.0490, 0.0222, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1432, 0.1430, 0.0653, 0.0404, 0.0395], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' think', ' liked'])\n",
      "(tensor([0.7154, 0.1126, 0.0338, 0.0170, 0.0153], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.8197, 0.0260, 0.0181, 0.0150, 0.0105], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' represented'])\n",
      "(tensor([0.2276, 0.0864, 0.0769, 0.0574, 0.0520], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' pretty', ' interesting', ' entertaining'])\n",
      "(tensor([0.4335, 0.1036, 0.0634, 0.0415, 0.0356], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' bad', ' pretty'])\n",
      "(tensor([0.9155, 0.0207, 0.0204, 0.0047, 0.0034], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' idea', ' acting'])\n",
      "(tensor([0.7024, 0.0423, 0.0320, 0.0283, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' for', ' when'])\n",
      "(tensor([0.3076, 0.0863, 0.0728, 0.0319, 0.0304], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' This', ' And'])\n",
      "(tensor([0.1487, 0.1237, 0.0902, 0.0900, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' the', ' what'])\n",
      "(tensor([0.1110, 0.0690, 0.0658, 0.0602, 0.0582], grad_fn=<ToCopyBackward0>), [' was', ' can', ' really', ' didn', ' don'])\n",
      "(tensor([0.1674, 0.0853, 0.0838, 0.0604, 0.0560], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' not', ' so', ' disappointed'])\n",
      "(tensor([0.5825, 0.0910, 0.0606, 0.0369, 0.0353], grad_fn=<ToCopyBackward0>), ['.', ' on', ' in', ',', '!'])\n",
      "(tensor([0.2357, 0.1437, 0.0789, 0.0519, 0.0447], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', 'I'])\n",
      "(tensor([0.4080, 0.0767, 0.0447, 0.0332, 0.0315], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' should', ' am'])\n",
      "(tensor([0.6850, 0.0745, 0.0708, 0.0233, 0.0108], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' so', ' really', ' not'])\n",
      "(tensor([0.9453, 0.0107, 0.0037, 0.0023, 0.0022], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ',', ' mistaken', ' far'])\n",
      "(tensor([0.4469, 0.1037, 0.0788, 0.0725, 0.0498], grad_fn=<ToCopyBackward0>), ['.', ' in', ' about', ' I', '!'])\n",
      "(tensor([0.3527, 0.1052, 0.0740, 0.0635, 0.0420], grad_fn=<ToCopyBackward0>), [' I', ' It', 'I', ' This', ' The'])\n",
      "(tensor([0.2875, 0.0701, 0.0646, 0.0553, 0.0397], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' thought', ' can'])\n",
      "(tensor([0.6316, 0.1359, 0.0509, 0.0420, 0.0317], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' that'])\n",
      "(tensor([0.9152, 0.0350, 0.0069, 0.0051, 0.0041], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', \"'s\", ' could'])\n",
      "(tensor([0.4144, 0.1010, 0.0483, 0.0395, 0.0246], grad_fn=<ToCopyBackward0>), [' a', ' so', ' funny', ' boring', ' terrible'])\n",
      "(tensor([0.6749, 0.0949, 0.0743, 0.0284, 0.0137], grad_fn=<ToCopyBackward0>), [' good', ' great', ' really', ' very', ' bad'])\n",
      "(tensor([9.8789e-01, 2.0343e-03, 1.8766e-03, 1.3441e-03, 8.8246e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' story', ','])\n",
      "(tensor([0.6868, 0.0954, 0.0484, 0.0201, 0.0163], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '!', ' but'])\n",
      "(tensor([0.5496, 0.0667, 0.0631, 0.0352, 0.0314], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', 'I', ' And'])\n",
      "(tensor([0.1380, 0.1376, 0.0699, 0.0669, 0.0568], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', \"'m\", ' think'])\n",
      "(tensor([0.2541, 0.1808, 0.0986, 0.0804, 0.0789], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' that', ' the'])\n",
      "(tensor([0.1607, 0.1270, 0.1245, 0.0423, 0.0397], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' if', \"'s\"])\n",
      "(tensor([0.0582, 0.0579, 0.0558, 0.0499, 0.0497], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' first', ' biggest', ' acting'])\n",
      "(tensor([0.1214, 0.0545, 0.0455, 0.0353, 0.0315], grad_fn=<ToCopyBackward0>), [' one', ' time', ' movie', ' 10', ' half'])\n",
      "(tensor([0.5275, 0.1284, 0.0275, 0.0208, 0.0156], grad_fn=<ToCopyBackward0>), [' was', ' is', ' that', ' I', ' had'])\n",
      "(tensor([0.1790, 0.0846, 0.0653, 0.0562, 0.0544], grad_fn=<ToCopyBackward0>), [' a', ' better', ' the', ' more', ' probably'])\n",
      "(tensor([0.4978, 0.3383, 0.0756, 0.0161, 0.0120], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' because', ' but'])\n",
      "(tensor([0.7837, 0.1453, 0.0078, 0.0061, 0.0052], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' The', ' I'])\n",
      "(tensor([0.9018, 0.0156, 0.0156, 0.0129, 0.0129], grad_fn=<ToCopyBackward0>), [' second', ' sequel', ' last', ' third', ' first'])\n",
      "(tensor([0.8879, 0.0572, 0.0185, 0.0135, 0.0049], grad_fn=<ToCopyBackward0>), [' one', '.', ' movie', ',', ' and'])\n",
      "(tensor([0.6060, 0.2101, 0.0560, 0.0192, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', '...', ' in'])\n",
      "(tensor([0.5359, 0.0642, 0.0540, 0.0275, 0.0267], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' so', ' because'])\n",
      "(tensor([0.3558, 0.1339, 0.1253, 0.0640, 0.0370], grad_fn=<ToCopyBackward0>), [' I', ' this', ' the', ' it', ' then'])\n",
      "(tensor([0.3869, 0.0924, 0.0605, 0.0567, 0.0308], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.3674, 0.1069, 0.0671, 0.0376, 0.0375], grad_fn=<ToCopyBackward0>), [' not', ' a', ' all', ' funny', ' still'])\n",
      "(tensor([0.2866, 0.0795, 0.0668, 0.0653, 0.0632], grad_fn=<ToCopyBackward0>), [' as', ' even', ' the', '.', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad it could not be worse.The acting is bad, the script is bad.I'm not saying this because I'm a fan of the actor.I'm saying this because the script is bad.The acting is bad.\n",
      "(tensor([0.3842, 0.1718, 0.0899, 0.0769, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2435, 0.1968, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6527, 0.0596, 0.0360, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1371, 0.0701, 0.0661, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5192, 0.0474, 0.0456, 0.0419, 0.0297], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2383, 0.2246, 0.2054, 0.0698, 0.0469], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.6809, 0.0473, 0.0332, 0.0264, 0.0222], grad_fn=<ToCopyBackward0>), [' was', ' could', ' had', ' would', ' should'])\n",
      "(tensor([0.5128, 0.1678, 0.1119, 0.0563, 0.0469], grad_fn=<ToCopyBackward0>), [' not', ' have', ' only', ' be', ' never'])\n",
      "(tensor([0.4398, 0.1177, 0.0930, 0.0891, 0.0495], grad_fn=<ToCopyBackward0>), [' be', ' possibly', ' even', ' have', ' get'])\n",
      "(tensor([0.2926, 0.2058, 0.1235, 0.0225, 0.0221], grad_fn=<ToCopyBackward0>), [' funny', ' worse', ' good', ' less', ' more'])\n",
      "(tensor([0.7945, 0.0852, 0.0383, 0.0190, 0.0119], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', '!', '...'])\n",
      "(tensor([0.2039, 0.1968, 0.1169, 0.0353, 0.0197], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.1981, 0.1341, 0.0664, 0.0588, 0.0549], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' story', ' movie'])\n",
      "(tensor([0.7102, 0.1395, 0.0253, 0.0233, 0.0174], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' and', ' in'])\n",
      "(tensor([0.1453, 0.0902, 0.0721, 0.0501, 0.0501], grad_fn=<ToCopyBackward0>), [' so', ' terrible', ' bad', ' horrible', ' awful'])\n",
      "(tensor([0.3283, 0.1784, 0.1652, 0.0387, 0.0252], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' at'])\n",
      "(tensor([0.4927, 0.1552, 0.0707, 0.0488, 0.0245], grad_fn=<ToCopyBackward0>), [' the', 'the', ' and', ' but', 'and'])\n",
      "(tensor([0.2666, 0.2009, 0.1190, 0.0296, 0.0246], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' directing', ' dialog'])\n",
      "(tensor([0.7435, 0.1108, 0.0288, 0.0267, 0.0070], grad_fn=<ToCopyBackward0>), [' is', ' was', ' and', ',', '...'])\n",
      "(tensor([0.5194, 0.0813, 0.0403, 0.0325, 0.0276], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' worse', ' horrible'])\n",
      "(tensor([0.5465, 0.2516, 0.1274, 0.0109, 0.0066], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.2061, 0.1484, 0.0497, 0.0488, 0.0393], grad_fn=<ToCopyBackward0>), [' The', 'The', ' I', 'I', ' It'])\n",
      "(tensor([0.0593, 0.0571, 0.0533, 0.0526, 0.0493], grad_fn=<ToCopyBackward0>), [' think', ' don', ' have', \"'m\", ' am'])\n",
      "(tensor([0.3367, 0.0973, 0.0479, 0.0429, 0.0362], grad_fn=<ToCopyBackward0>), [' not', ' a', ' giving', ' sure', ' so'])\n",
      "(tensor([0.2599, 0.2141, 0.0993, 0.0793, 0.0438], grad_fn=<ToCopyBackward0>), [' sure', ' even', ' going', ' a', ' saying'])\n",
      "(tensor([0.3198, 0.1697, 0.1627, 0.1488, 0.0463], grad_fn=<ToCopyBackward0>), [' that', ' this', ' it', ' the', ' I'])\n",
      "(tensor([0.3321, 0.2919, 0.2494, 0.0153, 0.0119], grad_fn=<ToCopyBackward0>), [' because', ' movie', ' is', ' was', ' for'])\n",
      "(tensor([0.5286, 0.1110, 0.0815, 0.0804, 0.0443], grad_fn=<ToCopyBackward0>), [' I', ' the', ' of', ' i', ' it'])\n",
      "(tensor([0.4840, 0.0604, 0.0539, 0.0440, 0.0403], grad_fn=<ToCopyBackward0>), [\"'m\", ' think', ' have', ' like', ' don'])\n",
      "(tensor([0.5993, 0.0749, 0.0365, 0.0336, 0.0217], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' some', ' in'])\n",
      "(tensor([0.1724, 0.0875, 0.0582, 0.0508, 0.0270], grad_fn=<ToCopyBackward0>), [' fan', ' movie', ' big', ' critic', ' huge'])\n",
      "(tensor([0.8399, 0.0516, 0.0364, 0.0273, 0.0121], grad_fn=<ToCopyBackward0>), [' of', ',', ' or', '.', ' but'])\n",
      "(tensor([0.2291, 0.2259, 0.0812, 0.0104, 0.0095], grad_fn=<ToCopyBackward0>), [' the', ' this', ' any', ' John', ' it'])\n",
      "(tensor([0.1944, 0.1303, 0.0718, 0.0312, 0.0270], grad_fn=<ToCopyBackward0>), [' actors', ' movie', ' actor', ' book', ' original'])\n",
      "(tensor([0.3107, 0.2393, 0.1768, 0.0349, 0.0218], grad_fn=<ToCopyBackward0>), [',', ' or', '.', ' in', '/'])\n",
      "(tensor([0.2699, 0.2279, 0.0460, 0.0399, 0.0388], grad_fn=<ToCopyBackward0>), [' I', 'I', ' It', 'This', 'The'])\n",
      "(tensor([0.7638, 0.0281, 0.0250, 0.0192, 0.0184], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' don', ' am', ' think'])\n",
      "(tensor([0.7328, 0.0948, 0.0731, 0.0268, 0.0064], grad_fn=<ToCopyBackward0>), [' saying', ' not', ' just', ' a', ' only'])\n",
      "(tensor([0.5864, 0.3675, 0.0230, 0.0095, 0.0025], grad_fn=<ToCopyBackward0>), [' this', ' it', ' because', ' that', ' the'])\n",
      "(tensor([0.9501, 0.0072, 0.0059, 0.0056, 0.0053], grad_fn=<ToCopyBackward0>), [' because', ' for', ' as', ' so', ' to'])\n",
      "(tensor([0.2690, 0.2237, 0.1363, 0.0524, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' there'])\n",
      "(tensor([0.1921, 0.1871, 0.0429, 0.0407, 0.0349], grad_fn=<ToCopyBackward0>), [' movie', ' script', ' only', ' plot', ' story'])\n",
      "(tensor([0.4155, 0.1552, 0.0797, 0.0368, 0.0319], grad_fn=<ToCopyBackward0>), [' is', ' was', ' and', ' sucks', ' makes'])\n",
      "(tensor([0.6527, 0.1148, 0.0169, 0.0158, 0.0153], grad_fn=<ToCopyBackward0>), [' so', ' bad', ' just', ' not', ' terrible'])\n",
      "(tensor([0.6988, 0.1175, 0.1081, 0.0144, 0.0061], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2198, 0.1460, 0.0371, 0.0336, 0.0316], grad_fn=<ToCopyBackward0>), ['The', 'I', 'It', ' The', 'This'])\n",
      "(tensor([0.2229, 0.0738, 0.0668, 0.0397, 0.0381], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' actors', ' plot'])\n",
      "(tensor([0.5489, 0.1117, 0.0516, 0.0476, 0.0392], grad_fn=<ToCopyBackward0>), [' is', ' was', '.', ',', ' in'])\n",
      "(tensor([0.5896, 0.0463, 0.0354, 0.0225, 0.0201], grad_fn=<ToCopyBackward0>), [' bad', ' so', ' not', ' terrible', ' poor'])\n",
      "(tensor([0.4279, 0.3503, 0.0507, 0.0486, 0.0131], grad_fn=<ToCopyBackward0>), [',', '.', ' because', ' and', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was very boring to me. It was very disappointing. The acting is very bad. The script was not very good. The story was not interesting. The editing was not good. I was very disappointed.I was very disappointed with that movie\n",
      "(tensor([0.3831, 0.1723, 0.0903, 0.0773, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2450, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7767, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1275, 0.0770, 0.0660, 0.0556, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.3631, 0.1015, 0.0651, 0.0282, 0.0276], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' much', ' poorly'])\n",
      "(tensor([0.4531, 0.2327, 0.1050, 0.0393, 0.0141], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' as'])\n",
      "(tensor([0.7539, 0.0509, 0.0288, 0.0209, 0.0168], grad_fn=<ToCopyBackward0>), [' watch', ' the', ' be', ' me', ' sit'])\n",
      "(tensor([0.6854, 0.0786, 0.0399, 0.0272, 0.0252], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' at'])\n",
      "(tensor([0.2298, 0.1764, 0.1569, 0.0282, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.2713, 0.1609, 0.1176, 0.0762, 0.0377], grad_fn=<ToCopyBackward0>), [' was', ' seemed', \"'s\", ' had', ' didn'])\n",
      "(tensor([0.1868, 0.1021, 0.0719, 0.0649, 0.0460], grad_fn=<ToCopyBackward0>), [' boring', ' very', ' just', ' not', ' so'])\n",
      "(tensor([0.2466, 0.0551, 0.0530, 0.0384, 0.0222], grad_fn=<ToCopyBackward0>), [' boring', ' preach', ' predictable', ' disappointing', ' hard'])\n",
      "(tensor([0.5633, 0.1312, 0.0924, 0.0543, 0.0415], grad_fn=<ToCopyBackward0>), ['.', ' to', ' and', ' because', ','])\n",
      "(tensor([0.3043, 0.1696, 0.1069, 0.0336, 0.0263], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1065, 0.0940, 0.0619, 0.0444, 0.0442], grad_fn=<ToCopyBackward0>), [' story', ' acting', ' only', ' first', ' movie'])\n",
      "(tensor([0.8023, 0.0457, 0.0451, 0.0155, 0.0123], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.2071, 0.1185, 0.0500, 0.0410, 0.0350], grad_fn=<ToCopyBackward0>), [' not', ' very', ' terrible', ' so', ' poor'])\n",
      "(tensor([0.1465, 0.1022, 0.0545, 0.0506, 0.0498], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' weak', ' poor', ','])\n",
      "(tensor([0.3444, 0.2333, 0.2222, 0.0583, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.2884, 0.1640, 0.1563, 0.0375, 0.0227], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2370, 0.2129, 0.1418, 0.0375, 0.0286], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' only'])\n",
      "(tensor([0.5126, 0.3767, 0.0128, 0.0123, 0.0066], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' and', ' had'])\n",
      "(tensor([0.3472, 0.0766, 0.0689, 0.0457, 0.0332], grad_fn=<ToCopyBackward0>), [' very', ' not', ' bad', ' weak', ' terrible'])\n",
      "(tensor([0.2282, 0.1925, 0.0866, 0.0704, 0.0432], grad_fn=<ToCopyBackward0>), [' good', ' very', ' funny', ' interesting', ' even'])\n",
      "(tensor([0.7658, 0.0343, 0.0324, 0.0309, 0.0207], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' interesting', ' well', ' entertaining'])\n",
      "(tensor([0.4843, 0.1585, 0.0864, 0.0724, 0.0515], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' either'])\n",
      "(tensor([0.3360, 0.1681, 0.1378, 0.0385, 0.0293], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.1049, 0.0562, 0.0519, 0.0491, 0.0469], grad_fn=<ToCopyBackward0>), [' story', ' cinem', ' acting', ' director', ' plot'])\n",
      "(tensor([0.6246, 0.1970, 0.0337, 0.0099, 0.0098], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' itself', ' just'])\n",
      "(tensor([0.4292, 0.2245, 0.0545, 0.0221, 0.0206], grad_fn=<ToCopyBackward0>), [' not', ' very', ' weak', ' stupid', ' boring'])\n",
      "(tensor([0.5563, 0.0808, 0.0718, 0.0640, 0.0399], grad_fn=<ToCopyBackward0>), [' very', ' interesting', ' really', ' that', ' good'])\n",
      "(tensor([0.3102, 0.2981, 0.1300, 0.0536, 0.0493], grad_fn=<ToCopyBackward0>), ['.', ' to', ' at', ',', ' and'])\n",
      "(tensor([0.3720, 0.1669, 0.1552, 0.0322, 0.0261], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0938, 0.0586, 0.0494, 0.0464, 0.0333], grad_fn=<ToCopyBackward0>), [' acting', ' editing', ' cinem', ' movie', ' story'])\n",
      "(tensor([0.7975, 0.0905, 0.0182, 0.0139, 0.0133], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' of', ' wasn'])\n",
      "(tensor([0.4001, 0.2455, 0.0408, 0.0292, 0.0254], grad_fn=<ToCopyBackward0>), [' not', ' very', ' poor', ' terrible', ' bad'])\n",
      "(tensor([0.3851, 0.3268, 0.0534, 0.0232, 0.0183], grad_fn=<ToCopyBackward0>), [' very', ' good', ' interesting', ' convincing', ' well'])\n",
      "(tensor([0.7721, 0.0686, 0.0588, 0.0177, 0.0098], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', '...'])\n",
      "(tensor([0.4206, 0.1688, 0.1125, 0.0426, 0.0224], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0911, 0.0826, 0.0778, 0.0714, 0.0642], grad_fn=<ToCopyBackward0>), [' really', ' was', ' think', ' just', ' don'])\n",
      "(tensor([0.5652, 0.1245, 0.0634, 0.0498, 0.0216], grad_fn=<ToCopyBackward0>), [' very', ' not', ' expecting', ' really', ' so'])\n",
      "(tensor([0.9213, 0.0164, 0.0123, 0.0053, 0.0038], grad_fn=<ToCopyBackward0>), [' disappointed', ',', ' surprised', ' dis', ' critical'])\n",
      "(tensor([0.4449, 0.2757, 0.1183, 0.0297, 0.0244], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' and'])\n",
      "(tensor([0.4654, 0.1238, 0.0794, 0.0537, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0942, 0.0914, 0.0794, 0.0634, 0.0517], grad_fn=<ToCopyBackward0>), [' was', ' really', ' think', ' have', \"'m\"])\n",
      "(tensor([0.4115, 0.1619, 0.1067, 0.0610, 0.0244], grad_fn=<ToCopyBackward0>), [' very', ' expecting', ' really', ' not', ' hoping'])\n",
      "(tensor([0.9439, 0.0174, 0.0037, 0.0026, 0.0023], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ',', ' dis', ' much'])\n",
      "(tensor([0.4053, 0.2046, 0.1180, 0.0555, 0.0499], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' because', ' when'])\n",
      "(tensor([0.7443, 0.1446, 0.0898, 0.0062, 0.0013], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "(tensor([0.6119, 0.2576, 0.0233, 0.0197, 0.0100], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ' one', ' because'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was a great way to get the word out about the film festival. I'm really excited to be in it. I was very disappointed with the film in the end. I really wanted to like it, because I'm a fan of the\n",
      "(tensor([0.3839, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4377, 0.2435, 0.1962, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7760, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1274, 0.0769, 0.0660, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.0785, 0.0620, 0.0525, 0.0453, 0.0446], grad_fn=<ToCopyBackward0>), [' very', ' waste', ' good', ' great', ' really'])\n",
      "(tensor([0.1965, 0.1800, 0.0849, 0.0515, 0.0262], grad_fn=<ToCopyBackward0>), [' idea', ' way', ' concept', ' example', ' film'])\n",
      "(tensor([9.3412e-01, 4.2374e-02, 1.5473e-02, 1.0977e-03, 6.5257e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' of', ' in', ' into'])\n",
      "(tensor([0.1653, 0.1429, 0.0414, 0.0359, 0.0318], grad_fn=<ToCopyBackward0>), [' bring', ' get', ' introduce', ' see', ' be'])\n",
      "(tensor([0.3420, 0.1178, 0.0625, 0.0555, 0.0495], grad_fn=<ToCopyBackward0>), [' the', ' into', ' to', ' a', ' my'])\n",
      "(tensor([0.1282, 0.0643, 0.0386, 0.0329, 0.0306], grad_fn=<ToCopyBackward0>), [' word', ' fans', ' kids', ' people', ' message'])\n",
      "(tensor([0.9833, 0.0030, 0.0021, 0.0021, 0.0019], grad_fn=<ToCopyBackward0>), [' out', '-', ' about', ' across', ' to'])\n",
      "(tensor([0.4441, 0.1607, 0.0989, 0.0729, 0.0417], grad_fn=<ToCopyBackward0>), [' about', ' on', ' to', '.', ' in'])\n",
      "(tensor([0.4247, 0.0646, 0.0569, 0.0236, 0.0146], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' what', ' our'])\n",
      "(tensor([0.0960, 0.0802, 0.0248, 0.0178, 0.0174], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' Nightmare', ' festival', ' films'])\n",
      "(tensor([0.5358, 0.2359, 0.0430, 0.0268, 0.0098], grad_fn=<ToCopyBackward0>), [' festival', '.', ' industry', ' festivals', ' because'])\n",
      "(tensor([0.5758, 0.2069, 0.0647, 0.0174, 0.0111], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' that'])\n",
      "(tensor([0.3932, 0.1173, 0.0823, 0.0546, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' We', ' So'])\n",
      "(tensor([0.1553, 0.0767, 0.0653, 0.0534, 0.0494], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' think', ' had'])\n",
      "(tensor([0.1917, 0.1209, 0.0802, 0.0684, 0.0585], grad_fn=<ToCopyBackward0>), [' a', ' really', ' sure', ' very', ' not'])\n",
      "(tensor([0.3750, 0.1173, 0.0788, 0.0426, 0.0412], grad_fn=<ToCopyBackward0>), [' looking', ' excited', ' surprised', ' disappointed', ','])\n",
      "(tensor([0.4706, 0.2756, 0.0654, 0.0511, 0.0285], grad_fn=<ToCopyBackward0>), [' to', ' for', ' by', ' about', ' that'])\n",
      "(tensor([0.5180, 0.1699, 0.0925, 0.0265, 0.0227], grad_fn=<ToCopyBackward0>), [' see', ' be', ' have', ' get', ' start'])\n",
      "(tensor([0.1341, 0.1205, 0.0781, 0.0676, 0.0619], grad_fn=<ToCopyBackward0>), [' a', ' part', ' able', ' in', ' involved'])\n",
      "(tensor([0.1463, 0.0725, 0.0543, 0.0343, 0.0281], grad_fn=<ToCopyBackward0>), [' the', ' it', ' a', ' front', ' New'])\n",
      "(tensor([0.1942, 0.1636, 0.1619, 0.1008, 0.0731], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' as'])\n",
      "(tensor([0.5615, 0.1587, 0.0443, 0.0198, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' And'])\n",
      "(tensor([0.1436, 0.0887, 0.0798, 0.0657, 0.0618], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' think', ' really'])\n",
      "(tensor([0.4464, 0.0893, 0.0282, 0.0264, 0.0250], grad_fn=<ToCopyBackward0>), [' really', ' very', ' a', ' so', ' actually'])\n",
      "(tensor([0.2794, 0.0862, 0.0704, 0.0611, 0.0571], grad_fn=<ToCopyBackward0>), [' excited', ' skeptical', ' surprised', ' disappointed', ' much'])\n",
      "(tensor([0.4814, 0.1935, 0.0785, 0.0653, 0.0596], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', ' that', ' by'])\n",
      "(tensor([0.3789, 0.2200, 0.0512, 0.0269, 0.0232], grad_fn=<ToCopyBackward0>), [' the', ' this', ' \"', ' how', ' my'])\n",
      "(tensor([0.1489, 0.1400, 0.0462, 0.0421, 0.0400], grad_fn=<ToCopyBackward0>), [' film', ' original', ' first', ' movie', ' last'])\n",
      "(tensor([0.0862, 0.0730, 0.0672, 0.0593, 0.0522], grad_fn=<ToCopyBackward0>), [' last', ' festival', ' in', ' I', ' that'])\n",
      "(tensor([0.3917, 0.0510, 0.0495, 0.0383, 0.0364], grad_fn=<ToCopyBackward0>), [' the', ' its', ' my', ' some', ' terms'])\n",
      "(tensor([0.1694, 0.0971, 0.0825, 0.0758, 0.0521], grad_fn=<ToCopyBackward0>), [' first', ' end', ' beginning', ' theater', ' U'])\n",
      "(tensor([0.4418, 0.2014, 0.0910, 0.0881, 0.0402], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' because', ' and'])\n",
      "(tensor([0.3355, 0.1677, 0.1158, 0.0261, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.1808, 0.0898, 0.0838, 0.0790, 0.0666], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', ' think', \"'m\"])\n",
      "(tensor([0.1640, 0.1148, 0.0673, 0.0536, 0.0500], grad_fn=<ToCopyBackward0>), [' wanted', ' thought', ' didn', ' was', ' don'])\n",
      "(tensor([0.8968, 0.0604, 0.0196, 0.0058, 0.0029], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' a', ' my'])\n",
      "(tensor([0.8427, 0.0441, 0.0270, 0.0081, 0.0068], grad_fn=<ToCopyBackward0>), [' like', ' see', ' be', ' give', ' get'])\n",
      "(tensor([0.8469, 0.0803, 0.0426, 0.0045, 0.0014], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' what'])\n",
      "(tensor([0.3441, 0.2518, 0.1931, 0.0411, 0.0359], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' more', ' a'])\n",
      "(tensor([0.8537, 0.0244, 0.0238, 0.0167, 0.0107], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' because', ' so'])\n",
      "(tensor([0.4950, 0.1546, 0.1060, 0.0230, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' as', ' when'])\n",
      "(tensor([0.1477, 0.1436, 0.1354, 0.1130, 0.0947], grad_fn=<ToCopyBackward0>), [' really', ' think', \"'m\", ' thought', ' like'])\n",
      "(tensor([0.6591, 0.0789, 0.0410, 0.0293, 0.0218], grad_fn=<ToCopyBackward0>), [' a', ' an', ' from', ' not', ' such'])\n",
      "(tensor([0.4186, 0.2027, 0.0988, 0.0506, 0.0312], grad_fn=<ToCopyBackward0>), [' fan', ' big', ' film', ' huge', ' movie'])\n",
      "(tensor([0.9673, 0.0127, 0.0068, 0.0029, 0.0013], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' and', ' but'])\n",
      "(tensor([0.1540, 0.0269, 0.0227, 0.0176, 0.0148], grad_fn=<ToCopyBackward0>), [' the', ' low', ' \"', ' all', ' independent'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the most boring movie I have ever seen. It was not even funny, not even interesting. It was boring. It was just boring and stupid. I was very disappointed. This movie is a big disappointment. It was a big waste of\n",
      "(tensor([0.3846, 0.1712, 0.0896, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4386, 0.2429, 0.1959, 0.0166, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4800, 0.1358, 0.1037, 0.0555, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7397, 0.0315, 0.0258, 0.0250, 0.0178], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.3615, 0.0476, 0.0350, 0.0334, 0.0330], grad_fn=<ToCopyBackward0>), [' boring', ' god', ' atro', ' cl', ' pointless'])\n",
      "(tensor([0.6474, 0.2181, 0.0185, 0.0158, 0.0123], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' documentary', ' and', ','])\n",
      "(tensor([0.6466, 0.1980, 0.0677, 0.0168, 0.0143], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' in', ' of'])\n",
      "(tensor([0.4355, 0.2992, 0.1242, 0.0669, 0.0418], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([0.9360, 0.0492, 0.0038, 0.0027, 0.0014], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' EVER'])\n",
      "(tensor([0.7284, 0.1010, 0.0348, 0.0210, 0.0192], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' wasted'])\n",
      "(tensor([0.5013, 0.1793, 0.0683, 0.0632, 0.0594], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', '...', '!'])\n",
      "(tensor([0.2046, 0.1693, 0.1313, 0.0281, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' Not'])\n",
      "(tensor([0.2714, 0.0981, 0.0764, 0.0709, 0.0566], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' is', ' seemed'])\n",
      "(tensor([0.1438, 0.1385, 0.1284, 0.0860, 0.0322], grad_fn=<ToCopyBackward0>), [' so', ' not', ' slow', ' boring', ' like'])\n",
      "(tensor([0.6167, 0.0626, 0.0618, 0.0368, 0.0323], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' entertaining', ' boring', ' scary'])\n",
      "(tensor([0.4243, 0.1960, 0.0545, 0.0258, 0.0246], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' interesting', ' scary', ' suspense'])\n",
      "(tensor([0.5101, 0.0708, 0.0656, 0.0608, 0.0267], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' to', ' in'])\n",
      "(tensor([0.2141, 0.1146, 0.0814, 0.0757, 0.0596], grad_fn=<ToCopyBackward0>), [' it', ' the', ' not', ' just', ' but'])\n",
      "(tensor([0.5758, 0.1502, 0.0560, 0.0318, 0.0289], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' interesting', ' entertaining', ' one'])\n",
      "(tensor([0.3973, 0.0888, 0.0759, 0.0348, 0.0317], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' scary', ' suspense', ' funny'])\n",
      "(tensor([0.3733, 0.3220, 0.0501, 0.0436, 0.0357], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' to'])\n",
      "(tensor([0.2223, 0.2026, 0.1725, 0.0221, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.5157, 0.0706, 0.0634, 0.0393, 0.0311], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', \"'s\", ' wasn'])\n",
      "(tensor([0.4035, 0.1447, 0.0604, 0.0322, 0.0294], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' so', ' simply'])\n",
      "(tensor([0.3261, 0.2870, 0.1258, 0.0706, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' as'])\n",
      "(tensor([0.1809, 0.1517, 0.1294, 0.0401, 0.0255], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.5118, 0.0886, 0.0415, 0.0398, 0.0324], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' wasn', ' didn'])\n",
      "(tensor([0.1548, 0.1292, 0.0790, 0.0522, 0.0468], grad_fn=<ToCopyBackward0>), [' boring', ' not', ' just', ' so', ' stupid'])\n",
      "(tensor([0.3156, 0.1149, 0.0769, 0.0397, 0.0359], grad_fn=<ToCopyBackward0>), [' boring', ' a', ' plain', ' stupid', ' one'])\n",
      "(tensor([0.7783, 0.0564, 0.0254, 0.0230, 0.0219], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.1701, 0.0517, 0.0463, 0.0455, 0.0311], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' I', ' not', ' boring'])\n",
      "(tensor([0.6793, 0.2088, 0.0333, 0.0097, 0.0092], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.2460, 0.1236, 0.1154, 0.0344, 0.0264], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.0724, 0.0664, 0.0541, 0.0437, 0.0390], grad_fn=<ToCopyBackward0>), [' was', ' don', ' have', ' really', \"'m\"])\n",
      "(tensor([0.1452, 0.0678, 0.0478, 0.0423, 0.0399], grad_fn=<ToCopyBackward0>), [' not', ' so', ' very', ' actually', ' really'])\n",
      "(tensor([0.8418, 0.0189, 0.0176, 0.0109, 0.0082], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' angry', ' confused', ' dis'])\n",
      "(tensor([0.5616, 0.1588, 0.1207, 0.0351, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' and'])\n",
      "(tensor([0.3319, 0.1155, 0.1035, 0.0420, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.4806, 0.2765, 0.1239, 0.0217, 0.0163], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' has'])\n",
      "(tensor([0.2774, 0.1119, 0.0751, 0.0679, 0.0443], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' made', ' has'])\n",
      "(tensor([0.2724, 0.0904, 0.0858, 0.0671, 0.0511], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' only'])\n",
      "(tensor([0.2317, 0.1332, 0.0610, 0.0447, 0.0445], grad_fn=<ToCopyBackward0>), [' big', ' complete', ' total', ' waste', ' perfect'])\n",
      "(tensor([0.1943, 0.1734, 0.0742, 0.0574, 0.0264], grad_fn=<ToCopyBackward0>), [' joke', ' disappointment', ' waste', ' insult', ' disaster'])\n",
      "(tensor([0.4329, 0.1386, 0.1077, 0.0623, 0.0582], grad_fn=<ToCopyBackward0>), ['.', ' to', ' for', ' because', ' and'])\n",
      "(tensor([0.2414, 0.1457, 0.1197, 0.0312, 0.0303], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' This'])\n",
      "(tensor([0.2423, 0.1785, 0.1473, 0.0613, 0.0404], grad_fn=<ToCopyBackward0>), [' is', ' was', \"'s\", ' has', ' really'])\n",
      "(tensor([0.2816, 0.1433, 0.0611, 0.0572, 0.0505], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' very'])\n",
      "(tensor([0.5605, 0.1247, 0.0327, 0.0294, 0.0172], grad_fn=<ToCopyBackward0>), [' big', ' waste', ' huge', ' complete', ' real'])\n",
      "(tensor([0.4300, 0.2582, 0.0311, 0.0305, 0.0220], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' let', ',', ' joke'])\n",
      "(tensor([9.9384e-01, 3.7865e-03, 7.8365e-04, 2.3617e-04, 1.6310e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' to', ' and'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was a sequel, not a remake. The first one was very good, but it's just not the same. The first one was a very good movie. The original movie was very good. This is a remake that is not the same as\n",
      "(tensor([0.3838, 0.1719, 0.0899, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2440, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4798, 0.1356, 0.1036, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1341, 0.1095, 0.0993, 0.0923, 0.0466], grad_fn=<ToCopyBackward0>), [' really', ' sequel', ' good', ' great', ' movie'])\n",
      "(tensor([0.7063, 0.0560, 0.0492, 0.0204, 0.0171], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ',', ' in'])\n",
      "(tensor([0.2275, 0.2080, 0.0467, 0.0458, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' but', ' and', ' I', ' then'])\n",
      "(tensor([0.8808, 0.0317, 0.0256, 0.0229, 0.0054], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' another', ' even'])\n",
      "(tensor([0.5868, 0.1769, 0.0529, 0.0391, 0.0322], grad_fn=<ToCopyBackward0>), [' remake', ' pre', ' re', ' sequel', ' rem'])\n",
      "(tensor([0.7945, 0.0574, 0.0409, 0.0172, 0.0126], grad_fn=<ToCopyBackward0>), ['.', ' of', ',', '!', ' ('])\n",
      "(tensor([0.2404, 0.1765, 0.0503, 0.0466, 0.0185], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.1746, 0.1080, 0.0580, 0.0504, 0.0337], grad_fn=<ToCopyBackward0>), [' original', ' first', ' movie', ' acting', ' only'])\n",
      "(tensor([0.3690, 0.1282, 0.0746, 0.0341, 0.0286], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' film', ' sequel', ' was'])\n",
      "(tensor([0.7065, 0.0574, 0.0523, 0.0093, 0.0092], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' I'])\n",
      "(tensor([0.2250, 0.0733, 0.0549, 0.0514, 0.0499], grad_fn=<ToCopyBackward0>), [' great', ' good', ' very', ' a', ' pretty'])\n",
      "(tensor([0.4885, 0.0570, 0.0560, 0.0469, 0.0391], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' well', ' much', ' interesting'])\n",
      "(tensor([0.3461, 0.3089, 0.1599, 0.0377, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' in'])\n",
      "(tensor([0.2639, 0.1541, 0.0933, 0.0799, 0.0700], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' so', ' very'])\n",
      "(tensor([0.6470, 0.0598, 0.0562, 0.0456, 0.0136], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' not'])\n",
      "(tensor([0.4153, 0.0841, 0.0716, 0.0533, 0.0504], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' had', ' didn'])\n",
      "(tensor([0.2902, 0.1213, 0.0733, 0.0413, 0.0404], grad_fn=<ToCopyBackward0>), [' not', ' a', ' been', ' hard', ' just'])\n",
      "(tensor([0.3311, 0.2439, 0.0429, 0.0355, 0.0311], grad_fn=<ToCopyBackward0>), [' not', ' a', ' the', ' been', ' too'])\n",
      "(tensor([0.2649, 0.2061, 0.0581, 0.0439, 0.0387], grad_fn=<ToCopyBackward0>), [' as', ' the', ' funny', ' a', ' that'])\n",
      "(tensor([0.9782, 0.0047, 0.0034, 0.0016, 0.0010], grad_fn=<ToCopyBackward0>), [' same', ' movie', ' story', ' sequel', ' original'])\n",
      "(tensor([0.4873, 0.1343, 0.0799, 0.0697, 0.0219], grad_fn=<ToCopyBackward0>), ['.', ' as', ' movie', ' story', ','])\n",
      "(tensor([0.2632, 0.1751, 0.0757, 0.0651, 0.0230], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'I'])\n",
      "(tensor([0.2694, 0.0865, 0.0604, 0.0536, 0.0408], grad_fn=<ToCopyBackward0>), [' first', ' original', ' story', ' second', ' acting'])\n",
      "(tensor([0.8536, 0.0315, 0.0161, 0.0078, 0.0075], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' was', ' time', ' film'])\n",
      "(tensor([0.7195, 0.1180, 0.0412, 0.0107, 0.0099], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' has'])\n",
      "(tensor([0.1262, 0.1184, 0.1067, 0.0493, 0.0345], grad_fn=<ToCopyBackward0>), [' a', ' very', ' more', ' good', ' interesting'])\n",
      "(tensor([0.3078, 0.0988, 0.0625, 0.0523, 0.0419], grad_fn=<ToCopyBackward0>), [' good', ' great', ' very', ' remake', ' little'])\n",
      "(tensor([0.5743, 0.0724, 0.0586, 0.0157, 0.0108], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' interesting', ' creepy', ' clever'])\n",
      "(tensor([0.5026, 0.1297, 0.0617, 0.0616, 0.0496], grad_fn=<ToCopyBackward0>), [' movie', ' action', ' comedy', ' film', ' story'])\n",
      "(tensor([0.3753, 0.3287, 0.0438, 0.0373, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' in', ' with', ' but'])\n",
      "(tensor([0.2629, 0.1588, 0.1081, 0.1049, 0.0991], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' But', ' It'])\n",
      "(tensor([0.3767, 0.3262, 0.0336, 0.0221, 0.0217], grad_fn=<ToCopyBackward0>), [' second', ' first', ' original', ' remake', ' only'])\n",
      "(tensor([0.3484, 0.1912, 0.0985, 0.0274, 0.0237], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' was', ' \"', ' is'])\n",
      "(tensor([0.6223, 0.1257, 0.0389, 0.0251, 0.0108], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ',', ' has'])\n",
      "(tensor([0.2075, 0.1401, 0.0995, 0.0503, 0.0434], grad_fn=<ToCopyBackward0>), [' very', ' great', ' a', ' good', ' pretty'])\n",
      "(tensor([0.6845, 0.0622, 0.0564, 0.0243, 0.0181], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' entertaining', ' interesting', ' well'])\n",
      "(tensor([0.4261, 0.3810, 0.0342, 0.0200, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' too'])\n",
      "(tensor([0.2213, 0.1822, 0.1428, 0.0753, 0.0604], grad_fn=<ToCopyBackward0>), [' I', ' The', ' But', ' This', ' It'])\n",
      "(tensor([0.3500, 0.2435, 0.1870, 0.0649, 0.0169], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' is', ' was', ' sequel'])\n",
      "(tensor([0.3704, 0.2666, 0.1148, 0.0258, 0.0168], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' an', ' the'])\n",
      "(tensor([0.1927, 0.0964, 0.0649, 0.0549, 0.0435], grad_fn=<ToCopyBackward0>), [' remake', ' very', ' good', ' bad', ' totally'])\n",
      "(tensor([0.3125, 0.2285, 0.2044, 0.0600, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ' of', ',', ' that', ' and'])\n",
      "(tensor([0.1881, 0.1385, 0.0861, 0.0689, 0.0453], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' doesn', ' was', ' tries'])\n",
      "(tensor([0.3454, 0.1034, 0.0635, 0.0634, 0.0312], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' very', ' trying'])\n",
      "(tensor([0.1898, 0.1565, 0.1361, 0.0759, 0.0740], grad_fn=<ToCopyBackward0>), [' good', ' very', ' as', ' really', ' the'])\n",
      "(tensor([0.9246, 0.0589, 0.0022, 0.0017, 0.0015], grad_fn=<ToCopyBackward0>), [' same', ' original', ' first', ' quality', ' remake'])\n",
      "(tensor([0.4086, 0.1819, 0.1773, 0.0472, 0.0283], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' as', ' at', ','])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. The acting was pretty bad. The script was just terrible. I mean it's a comedy. It's like a comedy. It's not a drama. It's a comedy. It's like a comedy. It's\n",
      "(tensor([0.3833, 0.1721, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4373, 0.2447, 0.1959, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7767, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0660, 0.0557, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1267, 0.1048, 0.0741, 0.0606], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5981, 0.0807, 0.0745, 0.0361, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2067, 0.1827, 0.1379, 0.0233, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.1615, 0.0781, 0.0753, 0.0724, 0.0317], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' only', ' script'])\n",
      "(tensor([0.7844, 0.0461, 0.0400, 0.0236, 0.0191], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ',', ' and'])\n",
      "(tensor([0.1184, 0.0918, 0.0767, 0.0738, 0.0545], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' pretty', ' horrible'])\n",
      "(tensor([0.2892, 0.0982, 0.0863, 0.0734, 0.0484], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' poor', ' terrible', ' lousy'])\n",
      "(tensor([0.2886, 0.2791, 0.1493, 0.1236, 0.0496], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' too', ' as'])\n",
      "(tensor([0.4702, 0.1028, 0.0861, 0.0320, 0.0242], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.2854, 0.1650, 0.1546, 0.0484, 0.0257], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' cinem', ' storyline'])\n",
      "(tensor([0.7548, 0.0743, 0.0196, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' is', ' wasn', ','])\n",
      "(tensor([0.3429, 0.0657, 0.0565, 0.0427, 0.0412], grad_fn=<ToCopyBackward0>), [' pretty', ' bad', ' really', ' just', ' terrible'])\n",
      "(tensor([0.1366, 0.1184, 0.0968, 0.0570, 0.0544], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' stupid', ' horrible', ' a'])\n",
      "(tensor([0.8222, 0.0831, 0.0395, 0.0091, 0.0040], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' -'])\n",
      "(tensor([0.3266, 0.1416, 0.0964, 0.0524, 0.0253], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0741, 0.0727, 0.0684, 0.0680, 0.0621], grad_fn=<ToCopyBackward0>), [' thought', ' don', ' was', ' mean', ' think'])\n",
      "(tensor([0.3794, 0.1691, 0.0962, 0.0678, 0.0344], grad_fn=<ToCopyBackward0>), [',', ' the', ' it', ' I', ' this'])\n",
      "(tensor([0.4202, 0.1779, 0.0527, 0.0404, 0.0338], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' could'])\n",
      "(tensor([0.2053, 0.1517, 0.0302, 0.0243, 0.0234], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' pretty', ' like'])\n",
      "(tensor([0.1377, 0.1043, 0.0628, 0.0499, 0.0365], grad_fn=<ToCopyBackward0>), [' comedy', ' really', ' good', ' movie', ' great'])\n",
      "(tensor([0.2008, 0.1640, 0.1347, 0.1272, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' and', ' so'])\n",
      "(tensor([0.2456, 0.1211, 0.0598, 0.0542, 0.0359], grad_fn=<ToCopyBackward0>), [' It', ' I', ' You', ' The', ' But'])\n",
      "(tensor([0.6926, 0.0781, 0.0361, 0.0221, 0.0201], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' was', ' doesn', ' has'])\n",
      "(tensor([0.2651, 0.2096, 0.1727, 0.0598, 0.0282], grad_fn=<ToCopyBackward0>), [' supposed', ' not', ' a', ' like', ' funny'])\n",
      "(tensor([0.2649, 0.1707, 0.0660, 0.0568, 0.0266], grad_fn=<ToCopyBackward0>), [' a', ' watching', ',', ' the', ' you'])\n",
      "(tensor([0.1575, 0.0846, 0.0530, 0.0442, 0.0404], grad_fn=<ToCopyBackward0>), [' comedy', ' bad', ' really', ' sitcom', ' drama'])\n",
      "(tensor([0.4452, 0.0693, 0.0434, 0.0373, 0.0355], grad_fn=<ToCopyBackward0>), ['.', ' with', ' that', '-', '!'])\n",
      "(tensor([0.2306, 0.1248, 0.0700, 0.0491, 0.0373], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' You', ' But'])\n",
      "(tensor([0.6252, 0.0740, 0.0337, 0.0326, 0.0315], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' has', ' doesn', ' was'])\n",
      "(tensor([0.2859, 0.2234, 0.1465, 0.0634, 0.0451], grad_fn=<ToCopyBackward0>), [' like', ' not', ' supposed', ' a', ' funny'])\n",
      "(tensor([0.1912, 0.1483, 0.1459, 0.0779, 0.0397], grad_fn=<ToCopyBackward0>), [' supposed', ' a', ' like', ' even', ' really'])\n",
      "(tensor([0.4211, 0.2453, 0.0276, 0.0258, 0.0256], grad_fn=<ToCopyBackward0>), [' drama', ' serious', ' tragedy', ' really', ' dramatic'])\n",
      "(tensor([0.6262, 0.0780, 0.0735, 0.0488, 0.0156], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', '!', ' at'])\n",
      "(tensor([0.2910, 0.1417, 0.0750, 0.0627, 0.0363], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' So'])\n",
      "(tensor([0.7538, 0.0526, 0.0230, 0.0229, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' doesn', ' just', ' has'])\n",
      "(tensor([0.3353, 0.1903, 0.1498, 0.0826, 0.0297], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' just', ' supposed'])\n",
      "(tensor([0.6419, 0.0644, 0.0388, 0.0265, 0.0136], grad_fn=<ToCopyBackward0>), [' comedy', ' stupid', ' silly', ' funny', ' very'])\n",
      "(tensor([0.7301, 0.0539, 0.0404, 0.0232, 0.0230], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' with', ' that'])\n",
      "(tensor([0.2157, 0.1302, 0.0690, 0.0642, 0.0435], grad_fn=<ToCopyBackward0>), [' It', ' I', ' And', ' The', ' But'])\n",
      "(tensor([0.6860, 0.0499, 0.0393, 0.0323, 0.0249], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' has', ' was', ' just'])\n",
      "(tensor([0.2880, 0.2159, 0.0877, 0.0658, 0.0653], grad_fn=<ToCopyBackward0>), [' like', ' not', ' a', ' funny', ' supposed'])\n",
      "(tensor([0.4844, 0.0637, 0.0452, 0.0432, 0.0368], grad_fn=<ToCopyBackward0>), [' a', ',', ' watching', ' the', ' an'])\n",
      "(tensor([0.6670, 0.0264, 0.0193, 0.0177, 0.0158], grad_fn=<ToCopyBackward0>), [' comedy', ' drama', ' really', ' sitcom', ' funny'])\n",
      "(tensor([0.8093, 0.0261, 0.0205, 0.0195, 0.0138], grad_fn=<ToCopyBackward0>), ['.', '!', ' with', ',', ' that'])\n",
      "(tensor([0.2659, 0.0994, 0.0572, 0.0504, 0.0446], grad_fn=<ToCopyBackward0>), [' It', ' I', ' And', ' The', ' But'])\n",
      "(tensor([0.8669, 0.0189, 0.0157, 0.0136, 0.0118], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' is', ' just'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch this movie just to find out where the money came from to make such a terrible movie. I was so disappointed that I just turned it off after 30 minutes. I can't even believe that this movie is not in the IMDB\n",
      "(tensor([0.3843, 0.1718, 0.0899, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2251, 0.1947, 0.1573, 0.0695, 0.0535], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6381, 0.1364, 0.0516, 0.0222, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2257, 0.1009, 0.1000, 0.0802, 0.0619], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6466, 0.1896, 0.0578, 0.0089, 0.0081], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3176, 0.2490, 0.0943, 0.0425, 0.0238], grad_fn=<ToCopyBackward0>), [' movie', ' because', ' film', ' one', ' so'])\n",
      "(tensor([0.4171, 0.0955, 0.0559, 0.0463, 0.0427], grad_fn=<ToCopyBackward0>), [' just', ' because', '.', ' to', ','])\n",
      "(tensor([0.7761, 0.0735, 0.0664, 0.0414, 0.0169], grad_fn=<ToCopyBackward0>), [' to', ' because', ' for', ' so', ' in'])\n",
      "(tensor([0.5381, 0.0910, 0.0825, 0.0371, 0.0205], grad_fn=<ToCopyBackward0>), [' see', ' get', ' be', ' find', ' understand'])\n",
      "(tensor([0.9666, 0.0088, 0.0039, 0.0025, 0.0020], grad_fn=<ToCopyBackward0>), [' out', ' the', ' a', ' that', ' something'])\n",
      "(tensor([0.4212, 0.2156, 0.1227, 0.0512, 0.0464], grad_fn=<ToCopyBackward0>), [' what', ' how', ' if', ' why', ' where'])\n",
      "(tensor([0.3907, 0.3219, 0.0740, 0.0366, 0.0229], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' they', ' to'])\n",
      "(tensor([0.0843, 0.0607, 0.0589, 0.0395, 0.0393], grad_fn=<ToCopyBackward0>), [' hell', ' money', ' title', ' ending', ' movie'])\n",
      "(tensor([0.3110, 0.1858, 0.1271, 0.0611, 0.0525], grad_fn=<ToCopyBackward0>), [' went', ' was', ' came', ' is', ' goes'])\n",
      "(tensor([0.8763, 0.0330, 0.0301, 0.0181, 0.0056], grad_fn=<ToCopyBackward0>), [' from', ' up', ' out', ' in', ' into'])\n",
      "(tensor([0.4492, 0.1620, 0.1236, 0.0573, 0.0474], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' for', ' and'])\n",
      "(tensor([0.6909, 0.0561, 0.0463, 0.0434, 0.0296], grad_fn=<ToCopyBackward0>), [' make', ' get', ' pay', ' create', ' buy'])\n",
      "(tensor([0.6291, 0.2157, 0.0503, 0.0322, 0.0194], grad_fn=<ToCopyBackward0>), [' it', ' this', ' such', ' the', ' a'])\n",
      "(tensor([0.9322, 0.0330, 0.0024, 0.0022, 0.0016], grad_fn=<ToCopyBackward0>), [' a', ' an', ' bad', ' terrible', ' great'])\n",
      "(tensor([0.1505, 0.1492, 0.0951, 0.0770, 0.0375], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' crappy', ' lousy', ' horrible'])\n",
      "(tensor([0.8611, 0.0874, 0.0185, 0.0055, 0.0045], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ',', ' sequel'])\n",
      "(tensor([0.8835, 0.0288, 0.0246, 0.0123, 0.0085], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', '..'])\n",
      "(tensor([0.2822, 0.1824, 0.0851, 0.0405, 0.0223], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.1529, 0.0760, 0.0666, 0.0447, 0.0318], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' have', ' can'])\n",
      "(tensor([0.2080, 0.1318, 0.0822, 0.0602, 0.0493], grad_fn=<ToCopyBackward0>), [' really', ' so', ' very', ' actually', ' wrong'])\n",
      "(tensor([0.6667, 0.0368, 0.0253, 0.0200, 0.0197], grad_fn=<ToCopyBackward0>), [' disappointed', ' wrong', ' surprised', ' upset', ' annoyed'])\n",
      "(tensor([0.2427, 0.1355, 0.0903, 0.0839, 0.0775], grad_fn=<ToCopyBackward0>), ['.', ' in', ' when', ' with', ' that'])\n",
      "(tensor([0.4235, 0.1206, 0.0885, 0.0593, 0.0527], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' they', ' it'])\n",
      "(tensor([0.1573, 0.1069, 0.0747, 0.0544, 0.0490], grad_fn=<ToCopyBackward0>), [' actually', ' had', ' could', ' just', ' couldn'])\n",
      "(tensor([0.3051, 0.0585, 0.0558, 0.0498, 0.0492], grad_fn=<ToCopyBackward0>), [' couldn', ' wasted', ' could', ' turned', ' had'])\n",
      "(tensor([0.5418, 0.3620, 0.0314, 0.0245, 0.0072], grad_fn=<ToCopyBackward0>), [' it', ' the', ' off', ' this', ' to'])\n",
      "(tensor([9.9747e-01, 6.1253e-04, 4.4795e-04, 2.9794e-04, 1.8187e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' off', ' on', ' OFF', ' over', ' down'])\n",
      "(tensor([0.5574, 0.1067, 0.0739, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' after', '.', ' halfway', ' in', ' and'])\n",
      "(tensor([0.1540, 0.0800, 0.0648, 0.0587, 0.0514], grad_fn=<ToCopyBackward0>), [' the', ' 10', ' 45', ' 30', ' 40'])\n",
      "(tensor([9.8023e-01, 9.5613e-03, 4.9778e-03, 1.3362e-03, 5.4700e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' minutes', ' seconds', ' mins', ' min', 'min'])\n",
      "(tensor([0.6706, 0.0620, 0.0555, 0.0420, 0.0347], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' thinking'])\n",
      "(tensor([0.3087, 0.2120, 0.0735, 0.0355, 0.0171], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.0931, 0.0708, 0.0689, 0.0684, 0.0680], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' can', ' would'])\n",
      "(tensor([0.6652, 0.0559, 0.0403, 0.0365, 0.0363], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' see', ' understand', ' honestly'])\n",
      "(tensor([0.5994, 0.1058, 0.0890, 0.0431, 0.0236], grad_fn=<ToCopyBackward0>), [' believe', ' imagine', ' even', ' understand', ' say'])\n",
      "(tensor([0.1530, 0.1036, 0.1017, 0.0784, 0.0644], grad_fn=<ToCopyBackward0>), [' describe', ' tell', ' believe', ' imagine', ' begin'])\n",
      "(tensor([0.5975, 0.1106, 0.0390, 0.0357, 0.0299], grad_fn=<ToCopyBackward0>), [' that', ' this', ' I', ' the', ' they'])\n",
      "(tensor([0.1327, 0.1080, 0.1007, 0.0932, 0.0767], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' a', ' so'])\n",
      "(tensor([0.8639, 0.0362, 0.0205, 0.0150, 0.0141], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' kind'])\n",
      "(tensor([0.4251, 0.1567, 0.0569, 0.0516, 0.0512], grad_fn=<ToCopyBackward0>), [' was', ' is', ' even', ' could', ' got'])\n",
      "(tensor([0.1836, 0.1203, 0.0842, 0.0736, 0.0731], grad_fn=<ToCopyBackward0>), [' actually', ' in', ' being', ' even', ' not'])\n",
      "(tensor([0.1670, 0.0895, 0.0793, 0.0668, 0.0540], grad_fn=<ToCopyBackward0>), [' in', ' a', ' even', ' at', ' more'])\n",
      "(tensor([0.7058, 0.0540, 0.0452, 0.0114, 0.0088], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' any', ' some'])\n",
      "(tensor([0.4225, 0.3351, 0.0163, 0.0107, 0.0102], grad_fn=<ToCopyBackward0>), [' IM', ' bottom', ' top', ' worst', ' Bottom'])\n",
      "(tensor([8.8444e-01, 1.0763e-01, 4.9939e-03, 5.3864e-04, 4.0474e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['DB', 'Db', 'db', 'BD', 'AX'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the ending was pretty cool too. I thought it was pretty violent. I thought the movie would have been better with no ending and only one or two more minutes of the movie. I thought the movie would have been better with no ending and only one\n",
      "(tensor([0.3834, 0.1720, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5010, 0.0600, 0.0340, 0.0152, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.5513, 0.0548, 0.0542, 0.0335, 0.0287], grad_fn=<ToCopyBackward0>), [' was', ' made', ' of', ' to', ' sucked'])\n",
      "(tensor([0.3874, 0.0509, 0.0385, 0.0363, 0.0336], grad_fn=<ToCopyBackward0>), [' pretty', ' really', ' very', ' weak', ' a'])\n",
      "(tensor([0.1634, 0.0785, 0.0572, 0.0493, 0.0354], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' cool', ' decent', ' weak'])\n",
      "(tensor([0.4667, 0.1455, 0.1344, 0.0591, 0.0289], grad_fn=<ToCopyBackward0>), [' too', ',', '.', ' and', ' as'])\n",
      "(tensor([0.7144, 0.1364, 0.0240, 0.0175, 0.0124], grad_fn=<ToCopyBackward0>), ['.', ',', '..', '!', ' but'])\n",
      "(tensor([0.2959, 0.0747, 0.0605, 0.0320, 0.0160], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Too', 'I'])\n",
      "(tensor([0.1313, 0.1037, 0.0907, 0.0608, 0.0396], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' think', ' mean'])\n",
      "(tensor([0.2998, 0.2213, 0.0402, 0.0289, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' I'])\n",
      "(tensor([0.5796, 0.0983, 0.0472, 0.0212, 0.0212], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' really', ' had'])\n",
      "(tensor([0.4781, 0.1048, 0.0371, 0.0341, 0.0212], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' the', ' really', ' very'])\n",
      "(tensor([0.1330, 0.0667, 0.0434, 0.0321, 0.0302], grad_fn=<ToCopyBackward0>), [' original', ' good', ' violent', ' well', ' intense'])\n",
      "(tensor([0.3423, 0.2174, 0.1248, 0.0613, 0.0471], grad_fn=<ToCopyBackward0>), [' too', ' and', ',', '.', ' but'])\n",
      "(tensor([0.4310, 0.0547, 0.0529, 0.0501, 0.0439], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.1900, 0.0896, 0.0794, 0.0692, 0.0547], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' mean', ' really'])\n",
      "(tensor([0.4377, 0.2454, 0.0386, 0.0367, 0.0225], grad_fn=<ToCopyBackward0>), [' it', ' the', ' there', ' that', ' I'])\n",
      "(tensor([0.3601, 0.0822, 0.0412, 0.0208, 0.0186], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' violence', ' film', ' ending'])\n",
      "(tensor([0.5681, 0.0379, 0.0373, 0.0271, 0.0256], grad_fn=<ToCopyBackward0>), [' was', ' did', ' had', ' could', ' would'])\n",
      "(tensor([0.5492, 0.2095, 0.0303, 0.0195, 0.0175], grad_fn=<ToCopyBackward0>), [' be', ' have', ' get', ' end', ' make'])\n",
      "(tensor([0.4564, 0.1936, 0.0614, 0.0288, 0.0276], grad_fn=<ToCopyBackward0>), [' been', ' a', ' more', ' to', ' ended'])\n",
      "(tensor([0.4741, 0.1638, 0.1209, 0.0256, 0.0211], grad_fn=<ToCopyBackward0>), [' better', ' more', ' a', ' much', ' really'])\n",
      "(tensor([0.6530, 0.0769, 0.0696, 0.0496, 0.0303], grad_fn=<ToCopyBackward0>), [' if', ' with', ' without', ' had', '.'])\n",
      "(tensor([0.2552, 0.1304, 0.0811, 0.0576, 0.0530], grad_fn=<ToCopyBackward0>), [' a', ' the', ' more', ' some', ' no'])\n",
      "(tensor([0.1774, 0.0872, 0.0448, 0.0357, 0.0226], grad_fn=<ToCopyBackward0>), [' ending', ' violence', ' plot', ' resolution', ' real'])\n",
      "(tensor([0.4074, 0.1930, 0.1848, 0.0317, 0.0254], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' because', ' and'])\n",
      "(tensor([0.1335, 0.1187, 0.0823, 0.0727, 0.0478], grad_fn=<ToCopyBackward0>), [' no', ' just', ' only', ' it', ' maybe'])\n",
      "(tensor([0.2513, 0.1873, 0.1085, 0.1006, 0.0760], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' some', ' two'])\n",
      "(tensor([0.0708, 0.0696, 0.0695, 0.0405, 0.0291], grad_fn=<ToCopyBackward0>), [' or', ' part', ' scene', ' more', ' person'])\n",
      "(tensor([0.8822, 0.0663, 0.0141, 0.0042, 0.0034], grad_fn=<ToCopyBackward0>), [' two', ' maybe', ' no', ' a', ' the'])\n",
      "(tensor([0.5275, 0.0498, 0.0408, 0.0218, 0.0114], grad_fn=<ToCopyBackward0>), [' more', ' people', ' of', ' scenes', ' parts'])\n",
      "(tensor([0.1342, 0.0957, 0.0573, 0.0336, 0.0267], grad_fn=<ToCopyBackward0>), [' minutes', ' scenes', ' people', ' hours', ' things'])\n",
      "(tensor([0.3557, 0.2225, 0.1557, 0.0637, 0.0293], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' left', ' to'])\n",
      "(tensor([0.3919, 0.0726, 0.0384, 0.0359, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' it', ' story', ' footage', ' people'])\n",
      "(tensor([0.4569, 0.0555, 0.0328, 0.0164, 0.0160], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' guy', ' same'])\n",
      "(tensor([0.5158, 0.2277, 0.0354, 0.0313, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' that', ' and'])\n",
      "(tensor([0.2589, 0.1440, 0.0637, 0.0541, 0.0369], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' That'])\n",
      "(tensor([0.1827, 0.0920, 0.0904, 0.0729, 0.0505], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' really', ' just'])\n",
      "(tensor([0.4084, 0.2537, 0.0962, 0.0351, 0.0208], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' there', ' this'])\n",
      "(tensor([0.5643, 0.0277, 0.0251, 0.0198, 0.0155], grad_fn=<ToCopyBackward0>), [' movie', ' ending', ' acting', ' story', ' violence'])\n",
      "(tensor([0.3123, 0.2820, 0.0819, 0.0454, 0.0228], grad_fn=<ToCopyBackward0>), [' would', ' was', ' could', ' had', ' should'])\n",
      "(tensor([0.7250, 0.2107, 0.0096, 0.0086, 0.0067], grad_fn=<ToCopyBackward0>), [' have', ' be', ' not', ' of', ' make'])\n",
      "(tensor([0.8729, 0.0316, 0.0176, 0.0140, 0.0084], grad_fn=<ToCopyBackward0>), [' been', ' made', ' ended', ' had', ' worked'])\n",
      "(tensor([0.6376, 0.1421, 0.0458, 0.0279, 0.0237], grad_fn=<ToCopyBackward0>), [' better', ' more', ' a', ' much', ' even'])\n",
      "(tensor([0.3891, 0.3780, 0.1580, 0.0137, 0.0092], grad_fn=<ToCopyBackward0>), [' if', ' with', ' without', ' had', '.'])\n",
      "(tensor([0.3899, 0.1216, 0.0680, 0.0646, 0.0618], grad_fn=<ToCopyBackward0>), [' no', ' a', ' the', ' only', ' one'])\n",
      "(tensor([0.8264, 0.0327, 0.0117, 0.0103, 0.0094], grad_fn=<ToCopyBackward0>), [' ending', ' end', ' beginning', ' one', ' violence'])\n",
      "(tensor([0.7515, 0.0740, 0.0668, 0.0202, 0.0143], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' at', ' but'])\n",
      "(tensor([0.9658, 0.0094, 0.0044, 0.0032, 0.0019], grad_fn=<ToCopyBackward0>), [' only', ' just', ' no', ' one', ' the'])\n",
      "(tensor([0.9183, 0.0442, 0.0117, 0.0080, 0.0018], grad_fn=<ToCopyBackward0>), [' one', ' two', ' three', ' a', ' five'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty boring. The plot was weak and I didn't care one bit about any of the actors in this movie. The acting was bad as was the story. The movie was not funny. I don't see why they made it in\n",
      "(tensor([0.3831, 0.1724, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4996, 0.0602, 0.0341, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6238, 0.0399, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0595, 0.0528, 0.0428, 0.0412], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1970, 0.1240, 0.1045, 0.0785, 0.0679], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3189, 0.2552, 0.2075, 0.0207, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' but'])\n",
      "(tensor([0.1796, 0.1512, 0.1356, 0.0298, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' Not'])\n",
      "(tensor([0.1918, 0.0887, 0.0816, 0.0536, 0.0417], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' story', ' plot'])\n",
      "(tensor([0.7251, 0.0570, 0.0207, 0.0147, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' had', ' seemed'])\n",
      "(tensor([0.1200, 0.1158, 0.1128, 0.0570, 0.0451], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' pretty', ' not', ' very'])\n",
      "(tensor([0.5405, 0.2286, 0.1045, 0.0195, 0.0082], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' at', ' as'])\n",
      "(tensor([0.3853, 0.1681, 0.0259, 0.0238, 0.0172], grad_fn=<ToCopyBackward0>), [' the', ' predictable', ' was', ' I', ' seemed'])\n",
      "(tensor([0.1752, 0.1353, 0.0762, 0.0594, 0.0587], grad_fn=<ToCopyBackward0>), [' didn', ' was', ' just', ' really', ' found'])\n",
      "(tensor([9.9574e-01, 1.7313e-03, 6.2601e-04, 3.9112e-04, 1.4406e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2654, 0.2113, 0.1468, 0.0684, 0.0614], grad_fn=<ToCopyBackward0>), [' care', ' really', ' understand', ' get', ' enjoy'])\n",
      "(tensor([0.3628, 0.2962, 0.0625, 0.0497, 0.0373], grad_fn=<ToCopyBackward0>), [' for', ' about', ' one', ' at', ' who'])\n",
      "(tensor([0.9347, 0.0270, 0.0197, 0.0026, 0.0017], grad_fn=<ToCopyBackward0>), [' bit', ' way', ' little', ' dime', ' i'])\n",
      "(tensor([0.7863, 0.1023, 0.0241, 0.0179, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' for', ' what', '.', ' when'])\n",
      "(tensor([0.5547, 0.2923, 0.0354, 0.0124, 0.0092], grad_fn=<ToCopyBackward0>), [' any', ' the', ' it', ' what', ' anyone'])\n",
      "(tensor([0.9261, 0.0265, 0.0148, 0.0100, 0.0025], grad_fn=<ToCopyBackward0>), [' of', ' character', ' one', ' characters', ' part'])\n",
      "(tensor([0.9787, 0.0069, 0.0041, 0.0035, 0.0035], grad_fn=<ToCopyBackward0>), [' the', ' these', ' it', ' those', ' them'])\n",
      "(tensor([9.7924e-01, 1.3323e-02, 1.6588e-03, 6.1666e-04, 4.9950e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' characters', ' actors', ' character', ' main', ' people'])\n",
      "(tensor([0.5727, 0.1264, 0.0853, 0.0824, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ' or', ' in', ',', ' that'])\n",
      "(tensor([0.4257, 0.2949, 0.2351, 0.0155, 0.0057], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' any', ' that'])\n",
      "(tensor([0.6769, 0.1018, 0.0871, 0.0200, 0.0190], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', ' flick', '.'])\n",
      "(tensor([0.7186, 0.1014, 0.0170, 0.0166, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ',', ' except', ' (', '...'])\n",
      "(tensor([0.2484, 0.2182, 0.1019, 0.0321, 0.0264], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1675, 0.1453, 0.0924, 0.0640, 0.0427], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' acting', ' plot', ' story'])\n",
      "(tensor([0.7288, 0.0626, 0.0435, 0.0282, 0.0148], grad_fn=<ToCopyBackward0>), [' was', ' in', ' wasn', ' is', ' and'])\n",
      "(tensor([0.0818, 0.0632, 0.0438, 0.0435, 0.0435], grad_fn=<ToCopyBackward0>), [' so', ' terrible', ' awful', ' pretty', ' bad'])\n",
      "(tensor([0.3924, 0.1480, 0.1039, 0.0707, 0.0668], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' too', ' as'])\n",
      "(tensor([0.5701, 0.0632, 0.0351, 0.0304, 0.0220], grad_fn=<ToCopyBackward0>), [' well', ' was', ' it', ' they', ' usual'])\n",
      "(tensor([0.8662, 0.0338, 0.0174, 0.0110, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' all', ' everything', ' most', ' acting'])\n",
      "(tensor([0.5632, 0.1466, 0.0468, 0.0323, 0.0229], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' directing'])\n",
      "(tensor([0.3457, 0.3131, 0.1048, 0.0466, 0.0357], grad_fn=<ToCopyBackward0>), [' line', '.', ' telling', ' and', ','])\n",
      "(tensor([0.2738, 0.2155, 0.0658, 0.0536, 0.0206], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.2176, 0.1904, 0.0618, 0.0387, 0.0250], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' plot', ' whole', ' story'])\n",
      "(tensor([0.2726, 0.0878, 0.0634, 0.0506, 0.0359], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' is', ' didn'])\n",
      "(tensor([0.1219, 0.0943, 0.0814, 0.0691, 0.0614], grad_fn=<ToCopyBackward0>), [' not', ' just', ' about', ' so', ' a'])\n",
      "(tensor([0.2464, 0.2287, 0.0776, 0.0738, 0.0664], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' even', ' worth', ' entertaining'])\n",
      "(tensor([0.3538, 0.1471, 0.1011, 0.0958, 0.0632], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' and', ' or'])\n",
      "(tensor([0.2878, 0.2238, 0.1210, 0.0267, 0.0239], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.0842, 0.0675, 0.0608, 0.0571, 0.0570], grad_fn=<ToCopyBackward0>), [' don', ' think', ' really', ' was', \"'m\"])\n",
      "(tensor([9.9418e-01, 1.7918e-03, 1.1144e-03, 5.1999e-04, 3.1525e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3292, 0.1849, 0.1174, 0.0962, 0.0567], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' see'])\n",
      "(tensor([0.5254, 0.2862, 0.0414, 0.0320, 0.0283], grad_fn=<ToCopyBackward0>), [' how', ' why', ' the', ' any', ' what'])\n",
      "(tensor([0.1802, 0.1582, 0.1167, 0.0786, 0.0692], grad_fn=<ToCopyBackward0>), [' people', ' they', ' anyone', ' this', ' it'])\n",
      "(tensor([0.2605, 0.1060, 0.0936, 0.0508, 0.0351], grad_fn=<ToCopyBackward0>), [' made', ' even', ' would', ' had', ' put'])\n",
      "(tensor([0.3806, 0.3091, 0.1276, 0.1040, 0.0572], grad_fn=<ToCopyBackward0>), [' it', ' this', ' a', ' the', ' such'])\n",
      "(tensor([0.1634, 0.0986, 0.0739, 0.0526, 0.0507], grad_fn=<ToCopyBackward0>), ['.', ' a', ' so', ' in', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film would be a real disappointment to me. It wasn't. It was a total waste of time. I was very disappointed. The story had potential. The acting was very bad and the story was not interesting to begin with. The plot was\n",
      "(tensor([0.3835, 0.1719, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2443, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7766, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.8312, 0.0781, 0.0099, 0.0060, 0.0057], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' get', ' never'])\n",
      "(tensor([0.1771, 0.1268, 0.1050, 0.0669, 0.0600], grad_fn=<ToCopyBackward0>), [' a', ' more', ' interesting', ' great', ' better'])\n",
      "(tensor([0.3916, 0.1457, 0.0485, 0.0477, 0.0141], grad_fn=<ToCopyBackward0>), [' good', ' great', ' lot', ' real', ' big'])\n",
      "(tensor([0.2457, 0.0336, 0.0335, 0.0310, 0.0275], grad_fn=<ToCopyBackward0>), [' disappointment', ' hit', ' challenge', ' waste', ' chore'])\n",
      "(tensor([0.4516, 0.1511, 0.0810, 0.0430, 0.0367], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' after', ' for'])\n",
      "(tensor([0.2676, 0.2024, 0.0863, 0.0764, 0.0398], grad_fn=<ToCopyBackward0>), [' the', ' me', ' see', ' those', ' everyone'])\n",
      "(tensor([0.6207, 0.0974, 0.0520, 0.0318, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' when', ' if'])\n",
      "(tensor([0.2263, 0.1775, 0.1383, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' Not'])\n",
      "(tensor([0.2142, 0.1402, 0.0828, 0.0602, 0.0488], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' seemed', ' wasn'])\n",
      "(tensor([9.9631e-01, 1.2934e-03, 3.8290e-04, 2.5885e-04, 1.7858e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3257, 0.1189, 0.0970, 0.0584, 0.0358], grad_fn=<ToCopyBackward0>), ['.', ' even', ' at', ' as', ' that'])\n",
      "(tensor([0.2800, 0.2389, 0.1015, 0.0449, 0.0362], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' This', 'I'])\n",
      "(tensor([0.7112, 0.0418, 0.0352, 0.0269, 0.0121], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' really'])\n",
      "(tensor([0.1312, 0.0984, 0.0691, 0.0390, 0.0379], grad_fn=<ToCopyBackward0>), [' a', ' actually', ' boring', ' just', ' pretty'])\n",
      "(tensor([0.1726, 0.0741, 0.0735, 0.0525, 0.0480], grad_fn=<ToCopyBackward0>), [' real', ' disappointment', ' very', ' complete', ' total'])\n",
      "(tensor([0.4780, 0.2878, 0.0145, 0.0117, 0.0106], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' hit', ' bomb', ' blast'])\n",
      "(tensor([9.9482e-01, 3.4314e-03, 2.6195e-04, 2.1596e-04, 1.4832e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', '!', ' and', ','])\n",
      "(tensor([0.6273, 0.1490, 0.0388, 0.0198, 0.0186], grad_fn=<ToCopyBackward0>), [' time', ' my', ' 90', ' money', ' 2'])\n",
      "(tensor([0.5714, 0.2558, 0.0915, 0.0110, 0.0080], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', '....'])\n",
      "(tensor([0.2758, 0.2002, 0.1187, 0.0241, 0.0217], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' This'])\n",
      "(tensor([0.1058, 0.0727, 0.0449, 0.0388, 0.0374], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' really', ' don'])\n",
      "(tensor([0.2002, 0.1141, 0.0614, 0.0474, 0.0345], grad_fn=<ToCopyBackward0>), [' very', ' really', ' so', ' actually', ' extremely'])\n",
      "(tensor([0.9002, 0.0116, 0.0088, 0.0071, 0.0060], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' skeptical', ' much', ' dis'])\n",
      "(tensor([0.3952, 0.2544, 0.1556, 0.0390, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' when'])\n",
      "(tensor([0.2764, 0.1627, 0.1558, 0.0262, 0.0199], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1590, 0.0966, 0.0729, 0.0433, 0.0411], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' actors', ' plot'])\n",
      "(tensor([0.5112, 0.1550, 0.0713, 0.0192, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' just', ' had'])\n",
      "(tensor([0.1351, 0.1006, 0.0921, 0.0831, 0.0716], grad_fn=<ToCopyBackward0>), [' so', ' no', ' a', ' potential', ' some'])\n",
      "(tensor([0.5851, 0.1531, 0.0810, 0.0594, 0.0404], grad_fn=<ToCopyBackward0>), [',', '.', ' to', ' and', ' but'])\n",
      "(tensor([0.3767, 0.1684, 0.1072, 0.0603, 0.0282], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' But', ' There'])\n",
      "(tensor([0.3310, 0.0749, 0.0381, 0.0287, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' idea', ' characters', ' main'])\n",
      "(tensor([0.8046, 0.0398, 0.0250, 0.0182, 0.0177], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' and', ' is', ','])\n",
      "(tensor([0.0802, 0.0636, 0.0513, 0.0434, 0.0425], grad_fn=<ToCopyBackward0>), [' good', ' okay', ' mediocre', ' very', ' not'])\n",
      "(tensor([0.5819, 0.0441, 0.0375, 0.0269, 0.0170], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' weak', ' poor', ' well'])\n",
      "(tensor([0.6289, 0.1597, 0.1209, 0.0131, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' but', ' in'])\n",
      "(tensor([0.5678, 0.1041, 0.0323, 0.0248, 0.0193], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' very'])\n",
      "(tensor([0.3261, 0.1557, 0.1011, 0.0289, 0.0234], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' storyline'])\n",
      "(tensor([0.5558, 0.0472, 0.0274, 0.0228, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', ' just', ' line'])\n",
      "(tensor([0.1148, 0.0848, 0.0617, 0.0367, 0.0364], grad_fn=<ToCopyBackward0>), [' very', ' not', ' weak', ' predictable', ' just'])\n",
      "(tensor([0.0995, 0.0953, 0.0885, 0.0514, 0.0443], grad_fn=<ToCopyBackward0>), [' interesting', ' very', ' even', ' convincing', ' really'])\n",
      "(tensor([0.4380, 0.2086, 0.1009, 0.0720, 0.0582], grad_fn=<ToCopyBackward0>), ['.', ' at', ' to', ' enough', ' or'])\n",
      "(tensor([0.3200, 0.2243, 0.1111, 0.0752, 0.0320], grad_fn=<ToCopyBackward0>), [' me', ' say', ' begin', ' watch', ' tell'])\n",
      "(tensor([9.9247e-01, 3.4191e-03, 6.4950e-04, 4.7071e-04, 3.9782e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', '.', ' to', ',', ' and'])\n",
      "(tensor([0.9371, 0.0207, 0.0049, 0.0040, 0.0028], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' (', '!'])\n",
      "(tensor([0.2455, 0.2372, 0.1134, 0.0243, 0.0229], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1148, 0.0741, 0.0731, 0.0453, 0.0401], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' movie', ' plot', ' actors'])\n",
      "(tensor([0.6887, 0.0414, 0.0387, 0.0165, 0.0092], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' made', ' wasn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty boring. It was not funny or interesting at all. It seemed like it was just a bunch of people running around. And that is not at all scary. It seemed like it was more of a drama than an horror movie.\n",
      "(tensor([0.3840, 0.1715, 0.0896, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2431, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6514, 0.0598, 0.0363, 0.0355, 0.0264], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0660, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1760, 0.1519, 0.1083, 0.0953, 0.0896], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.3315, 0.2171, 0.1737, 0.0373, 0.0158], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' compared'])\n",
      "(tensor([0.1853, 0.1705, 0.1501, 0.0311, 0.0179], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.2595, 0.1409, 0.1160, 0.0920, 0.0480], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' had', ' wasn'])\n",
      "(tensor([0.1292, 0.0917, 0.0496, 0.0485, 0.0460], grad_fn=<ToCopyBackward0>), [' boring', ' not', ' slow', ' just', ' more'])\n",
      "(tensor([0.4357, 0.0837, 0.0780, 0.0436, 0.0369], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' entertaining', ' as'])\n",
      "(tensor([0.3969, 0.1991, 0.1338, 0.0504, 0.0503], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' or', ' in'])\n",
      "(tensor([0.4691, 0.1962, 0.0334, 0.0332, 0.0247], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' funny', ' exciting', ' dramatic'])\n",
      "(tensor([0.2743, 0.2568, 0.0970, 0.0917, 0.0662], grad_fn=<ToCopyBackward0>), ['.', ' at', ' to', ',', ' or'])\n",
      "(tensor([0.9860, 0.0053, 0.0030, 0.0013, 0.0012], grad_fn=<ToCopyBackward0>), [' all', ' any', ' least', ' the', ' ALL'])\n",
      "(tensor([0.7413, 0.1121, 0.0425, 0.0147, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' to'])\n",
      "(tensor([0.2559, 0.1838, 0.1404, 0.0306, 0.0268], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', 'The', ' There'])\n",
      "(tensor([0.4229, 0.1004, 0.0852, 0.0507, 0.0333], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' had', \"'s\", ' wasn'])\n",
      "(tensor([0.3584, 0.2981, 0.1505, 0.0678, 0.0282], grad_fn=<ToCopyBackward0>), [' like', ' to', ' more', ' as', ' that'])\n",
      "(tensor([0.3573, 0.2436, 0.1995, 0.0772, 0.0150], grad_fn=<ToCopyBackward0>), [' it', ' they', ' a', ' the', ' an'])\n",
      "(tensor([0.5866, 0.1036, 0.0560, 0.0369, 0.0287], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' just', ' could'])\n",
      "(tensor([0.3001, 0.1220, 0.0780, 0.0446, 0.0273], grad_fn=<ToCopyBackward0>), [' trying', ' made', ' just', ' written', ' a'])\n",
      "(tensor([0.3404, 0.0778, 0.0708, 0.0408, 0.0311], grad_fn=<ToCopyBackward0>), [' a', ' made', ' trying', ' an', ' showing'])\n",
      "(tensor([0.3831, 0.0217, 0.0144, 0.0133, 0.0120], grad_fn=<ToCopyBackward0>), [' bunch', ' movie', ' remake', ' big', ' commercial'])\n",
      "(tensor([9.9119e-01, 1.0040e-03, 5.0070e-04, 3.7770e-04, 3.7692e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' people', ' guys', ' o', ' more'])\n",
      "(tensor([0.1689, 0.1144, 0.0673, 0.0503, 0.0422], grad_fn=<ToCopyBackward0>), [' guys', ' people', ' kids', ' random', ' scenes'])\n",
      "(tensor([0.0635, 0.0532, 0.0452, 0.0384, 0.0374], grad_fn=<ToCopyBackward0>), [' in', ' running', ' sitting', ' that', ' trying'])\n",
      "(tensor([0.7970, 0.0715, 0.0299, 0.0185, 0.0130], grad_fn=<ToCopyBackward0>), [' around', ' through', ' in', ' from', ' into'])\n",
      "(tensor([0.3123, 0.1409, 0.1027, 0.0525, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ' and', ' in', ',', ' trying'])\n",
      "(tensor([0.1872, 0.1601, 0.1173, 0.0799, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' And'])\n",
      "(tensor([0.1278, 0.1061, 0.1051, 0.0809, 0.0756], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' that', ' when'])\n",
      "(tensor([0.4905, 0.2369, 0.1322, 0.0103, 0.0064], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' would'])\n",
      "(tensor([0.5251, 0.0649, 0.0566, 0.0272, 0.0230], grad_fn=<ToCopyBackward0>), [' not', ' NOT', ' the', ' a', ' what'])\n",
      "(tensor([0.5246, 0.1305, 0.0270, 0.0269, 0.0243], grad_fn=<ToCopyBackward0>), [' a', ' the', ' at', ' an', ' fun'])\n",
      "(tensor([9.9639e-01, 1.2583e-03, 8.5548e-04, 4.7020e-04, 2.1735e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' the', ' least', 'yp'])\n",
      "(tensor([0.3067, 0.1378, 0.1303, 0.0907, 0.0832], grad_fn=<ToCopyBackward0>), [' a', ' the', ' what', ' entertaining', ' scary'])\n",
      "(tensor([0.4224, 0.2162, 0.1566, 0.0315, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' to', '!'])\n",
      "(tensor([0.2602, 0.1532, 0.0500, 0.0392, 0.0343], grad_fn=<ToCopyBackward0>), [' I', ' It', ' That', ' The', ' And'])\n",
      "(tensor([0.2578, 0.1991, 0.1541, 0.0796, 0.0621], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' just', ' seemed'])\n",
      "(tensor([0.4936, 0.2804, 0.0530, 0.0271, 0.0179], grad_fn=<ToCopyBackward0>), [' like', ' more', ' to', ' as', ' very'])\n",
      "(tensor([0.4227, 0.2191, 0.1145, 0.0510, 0.0234], grad_fn=<ToCopyBackward0>), [' it', ' they', ' a', ' the', ' people'])\n",
      "(tensor([0.7638, 0.0649, 0.0463, 0.0174, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' would', ' just', ' could', ' had'])\n",
      "(tensor([0.3585, 0.1064, 0.0841, 0.0242, 0.0242], grad_fn=<ToCopyBackward0>), [' just', ' more', ' a', ' all', ' trying'])\n",
      "(tensor([0.7186, 0.0938, 0.0330, 0.0204, 0.0170], grad_fn=<ToCopyBackward0>), [' like', ' of', ' for', ' a', ' boring'])\n",
      "(tensor([0.9000, 0.0540, 0.0248, 0.0057, 0.0034], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' just', ' like'])\n",
      "(tensor([0.1003, 0.0987, 0.0602, 0.0519, 0.0425], grad_fn=<ToCopyBackward0>), [' drama', ' comedy', ' chore', ' \"', ' boring'])\n",
      "(tensor([0.4074, 0.1494, 0.1017, 0.0440, 0.0310], grad_fn=<ToCopyBackward0>), [' than', '.', ' about', '/', ' with'])\n",
      "(tensor([0.5218, 0.2485, 0.0880, 0.0231, 0.0188], grad_fn=<ToCopyBackward0>), [' a', ' anything', ' it', ' horror', ' an'])\n",
      "(tensor([0.3052, 0.0922, 0.0886, 0.0563, 0.0332], grad_fn=<ToCopyBackward0>), [' action', ' actual', ' adventure', ' horror', ' thriller'])\n",
      "(tensor([0.7813, 0.1040, 0.0828, 0.0121, 0.0025], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ' flick', ','])\n",
      "(tensor([0.8663, 0.0316, 0.0289, 0.0241, 0.0049], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', ' at'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty funny, but the idea that a movie like this has a 5.2 rating on this site, it's pretty ridiculous that someone actually thinks that that movie is funny. I'm a huge Adam Sandler fan, I even like\n",
      "(tensor([0.3843, 0.1714, 0.0898, 0.0769, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5015, 0.0597, 0.0340, 0.0150, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6234, 0.0401, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2578, 0.0595, 0.0526, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1975, 0.1232, 0.1053, 0.0782, 0.0676], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3150, 0.1799, 0.1288, 0.0504, 0.0371], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' in'])\n",
      "(tensor([0.4151, 0.0505, 0.0410, 0.0382, 0.0347], grad_fn=<ToCopyBackward0>), [' but', ' especially', ' and', ' so', ' the'])\n",
      "(tensor([0.1366, 0.1269, 0.1008, 0.0824, 0.0431], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' not'])\n",
      "(tensor([0.1646, 0.1428, 0.0621, 0.0597, 0.0498], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' plot', ' idea', ' acting'])\n",
      "(tensor([0.5256, 0.2047, 0.0921, 0.0451, 0.0279], grad_fn=<ToCopyBackward0>), [' of', ' that', ' was', ' behind', ' is'])\n",
      "(tensor([0.1464, 0.0969, 0.0802, 0.0522, 0.0428], grad_fn=<ToCopyBackward0>), [' it', ' the', ' a', ' this', ' they'])\n",
      "(tensor([0.4508, 0.0571, 0.0453, 0.0149, 0.0115], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' bunch', ' comedy', ' group'])\n",
      "(tensor([0.1417, 0.1189, 0.0810, 0.0614, 0.0553], grad_fn=<ToCopyBackward0>), [' could', ' with', ' is', ' like', ' has'])\n",
      "(tensor([0.7340, 0.1062, 0.0366, 0.0185, 0.0063], grad_fn=<ToCopyBackward0>), [' this', ' that', ' \"', ' it', ' The'])\n",
      "(tensor([0.3007, 0.1829, 0.1060, 0.0658, 0.0601], grad_fn=<ToCopyBackward0>), [' is', ' could', ' can', ' has', ' was'])\n",
      "(tensor([0.4603, 0.1364, 0.1157, 0.0906, 0.0174], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' been', ' so'])\n",
      "(tensor([0.0497, 0.0481, 0.0463, 0.0422, 0.0334], grad_fn=<ToCopyBackward0>), [' 1', ' 5', ' $', ' 2', ' 6'])\n",
      "(tensor([0.3135, 0.2709, 0.1054, 0.0291, 0.0281], grad_fn=<ToCopyBackward0>), [' star', '.', '-', ',', ' rating'])\n",
      "(tensor([0.4456, 0.2416, 0.0665, 0.0623, 0.0525], grad_fn=<ToCopyBackward0>), ['1', '2', '0', '5', '7'])\n",
      "(tensor([0.5120, 0.0485, 0.0198, 0.0188, 0.0084], grad_fn=<ToCopyBackward0>), [' rating', ' score', 'M', ' IM', ' here'])\n",
      "(tensor([0.2243, 0.1782, 0.1468, 0.1435, 0.0493], grad_fn=<ToCopyBackward0>), ['?', ' on', ' is', ',', ' in'])\n",
      "(tensor([0.4517, 0.2663, 0.0730, 0.0365, 0.0193], grad_fn=<ToCopyBackward0>), [' IM', ' this', ' R', ' the', ' here'])\n",
      "(tensor([0.5675, 0.2024, 0.0109, 0.0106, 0.0089], grad_fn=<ToCopyBackward0>), [' site', ' website', ' movie', ' page', ' board'])\n",
      "(tensor([0.4543, 0.0892, 0.0670, 0.0367, 0.0276], grad_fn=<ToCopyBackward0>), [' is', ',', ' was', '?', ' makes'])\n",
      "(tensor([0.1843, 0.1432, 0.0819, 0.0707, 0.0665], grad_fn=<ToCopyBackward0>), [' I', ' is', ' that', ' and', ' it'])\n",
      "(tensor([0.6957, 0.0373, 0.0300, 0.0276, 0.0238], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' just', ' makes', ' seems'])\n",
      "(tensor([0.1741, 0.1224, 0.0953, 0.0734, 0.0650], grad_fn=<ToCopyBackward0>), [' not', ' a', ' ridiculous', ' kind', ' pretty'])\n",
      "(tensor([0.1411, 0.1007, 0.0684, 0.0583, 0.0528], grad_fn=<ToCopyBackward0>), [' ridiculous', ' crazy', ' hard', ' scary', ' funny'])\n",
      "(tensor([0.4448, 0.0929, 0.0850, 0.0846, 0.0630], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' that', ' and'])\n",
      "(tensor([0.2393, 0.1387, 0.0663, 0.0655, 0.0649], grad_fn=<ToCopyBackward0>), [' they', ' a', ' something', ' someone', ' that'])\n",
      "(tensor([0.3489, 0.1352, 0.0799, 0.0430, 0.0251], grad_fn=<ToCopyBackward0>), [' rated', ' actually', ' thought', ' would', ' could'])\n",
      "(tensor([0.3946, 0.0828, 0.0331, 0.0291, 0.0182], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' thinks', ' rated', ' put'])\n",
      "(tensor([0.4318, 0.3424, 0.0977, 0.0572, 0.0148], grad_fn=<ToCopyBackward0>), [' that', ' this', ' it', ' they', ' like'])\n",
      "(tensor([0.1841, 0.1375, 0.1346, 0.1190, 0.0990], grad_fn=<ToCopyBackward0>), [' this', ' it', '.', ' that', ' a'])\n",
      "(tensor([0.2964, 0.1280, 0.1007, 0.0473, 0.0408], grad_fn=<ToCopyBackward0>), [' movie', \"'s\", ' is', ' kind', ' sort'])\n",
      "(tensor([0.6830, 0.0643, 0.0294, 0.0277, 0.0273], grad_fn=<ToCopyBackward0>), [' is', ' was', ' deserves', \"'s\", ' should'])\n",
      "(tensor([0.2469, 0.1413, 0.0630, 0.0598, 0.0462], grad_fn=<ToCopyBackward0>), [' funny', ' worth', ' a', ' good', ' that'])\n",
      "(tensor([0.8131, 0.0340, 0.0323, 0.0260, 0.0213], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '!', '....'])\n",
      "(tensor([0.3101, 0.0815, 0.0609, 0.0321, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1699, 0.0867, 0.0668, 0.0652, 0.0484], grad_fn=<ToCopyBackward0>), [' mean', ' was', ' don', \"'m\", ' can'])\n",
      "(tensor([0.3552, 0.1091, 0.0729, 0.0600, 0.0280], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' really'])\n",
      "(tensor([0.2341, 0.0781, 0.0756, 0.0341, 0.0329], grad_fn=<ToCopyBackward0>), [' big', ' huge', ' movie', ' fan', ' little'])\n",
      "(tensor([0.2766, 0.1158, 0.0351, 0.0297, 0.0151], grad_fn=<ToCopyBackward0>), [' Adam', ' Van', ' fan', ' Michael', ' Conan'])\n",
      "(tensor([0.9638, 0.0060, 0.0036, 0.0036, 0.0021], grad_fn=<ToCopyBackward0>), [' Sand', ' sand', ' \"', ' West', ' S'])\n",
      "(tensor([9.9627e-01, 1.1917e-03, 3.8068e-04, 3.7768e-04, 1.4575e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ler', 'lers', 'lin', 'ra', 'erson'])\n",
      "(tensor([9.9301e-01, 2.2343e-03, 8.0861e-04, 6.4420e-04, 5.9018e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' fan', ' fans', ' movie', ' guy', ' Fan'])\n",
      "(tensor([0.7116, 0.1170, 0.0473, 0.0217, 0.0140], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', ' so'])\n",
      "(tensor([0.4004, 0.3288, 0.1231, 0.0657, 0.0094], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' so', ' he'])\n",
      "(tensor([0.1405, 0.1034, 0.0922, 0.0840, 0.0768], grad_fn=<ToCopyBackward0>), [' even', ' think', ' love', \"'m\", \"'ve\"])\n",
      "(tensor([0.0617, 0.0572, 0.0570, 0.0537, 0.0469], grad_fn=<ToCopyBackward0>), [' like', ' have', ' made', ' thought', ' liked'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would watch this movie, since it's a remake of a great movie I loved growing up. The original was about a boy and his dog that get lost in a forest for a month, and then he meets a talking bear. It was pretty\n",
      "(tensor([0.3844, 0.1717, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2246, 0.1945, 0.1575, 0.0694, 0.0537], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2255, 0.1212, 0.0844, 0.0370, 0.0234], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.7081, 0.0898, 0.0597, 0.0410, 0.0110], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.5523, 0.1864, 0.0686, 0.0169, 0.0090], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' because', ' one', ' with'])\n",
      "(tensor([0.5507, 0.0745, 0.0373, 0.0258, 0.0238], grad_fn=<ToCopyBackward0>), [' just', ' because', ' for', ' as', ','])\n",
      "(tensor([0.1466, 0.1459, 0.0724, 0.0565, 0.0344], grad_fn=<ToCopyBackward0>), [' since', ' but', ' because', ' and', ' after'])\n",
      "(tensor([0.3019, 0.2567, 0.0599, 0.0269, 0.0237], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' its', ' i'])\n",
      "(tensor([0.2344, 0.1581, 0.1156, 0.1103, 0.0404], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' stars', ' seemed'])\n",
      "(tensor([0.1639, 0.1400, 0.0527, 0.0526, 0.0518], grad_fn=<ToCopyBackward0>), [' a', ' been', ' about', ' the', ' rated'])\n",
      "(tensor([0.2077, 0.2073, 0.0977, 0.0463, 0.0323], grad_fn=<ToCopyBackward0>), [' remake', ' good', ' classic', ' really', ' great'])\n",
      "(tensor([0.8299, 0.0612, 0.0216, 0.0176, 0.0140], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' (', ' and'])\n",
      "(tensor([0.4135, 0.1839, 0.0651, 0.0529, 0.0180], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' one', \" '\"])\n",
      "(tensor([0.3892, 0.1169, 0.0720, 0.0614, 0.0584], grad_fn=<ToCopyBackward0>), [' classic', ' great', ' really', ' good', ' movie'])\n",
      "(tensor([0.4204, 0.0513, 0.0413, 0.0312, 0.0196], grad_fn=<ToCopyBackward0>), [' movie', ' film', ',', ' classic', ' story'])\n",
      "(tensor([0.3703, 0.1944, 0.1155, 0.0582, 0.0311], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', ' I', ' that'])\n",
      "(tensor([0.1038, 0.0993, 0.0875, 0.0798, 0.0691], grad_fn=<ToCopyBackward0>), [' loved', \"'ve\", ' saw', ' watched', ' used'])\n",
      "(tensor([0.2345, 0.1370, 0.0831, 0.0780, 0.0733], grad_fn=<ToCopyBackward0>), [' as', ' years', '.', ' when', ' growing'])\n",
      "(tensor([9.9977e-01, 4.7357e-05, 2.3040e-05, 1.7642e-05, 1.6753e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' up', '-', ' Up', ' out', 'up'])\n",
      "(tensor([0.6562, 0.1132, 0.0428, 0.0283, 0.0150], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ' in', ' and'])\n",
      "(tensor([0.2217, 0.1085, 0.0913, 0.0752, 0.0371], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' Unfortunately'])\n",
      "(tensor([0.1932, 0.0985, 0.0928, 0.0541, 0.0434], grad_fn=<ToCopyBackward0>), [' original', ' remake', ' movie', ' first', ' story'])\n",
      "(tensor([0.2746, 0.1084, 0.0984, 0.0792, 0.0375], grad_fn=<ToCopyBackward0>), [' \"', ' movie', ' was', \" '\", ' film'])\n",
      "(tensor([0.1450, 0.1148, 0.0499, 0.0435, 0.0356], grad_fn=<ToCopyBackward0>), [' a', ' great', ' about', ' very', ' good'])\n",
      "(tensor([0.5981, 0.0708, 0.0534, 0.0190, 0.0164], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' two', ' this'])\n",
      "(tensor([0.0786, 0.0749, 0.0680, 0.0631, 0.0273], grad_fn=<ToCopyBackward0>), [' guy', ' little', ' group', ' man', ' boy'])\n",
      "(tensor([0.3954, 0.0758, 0.0529, 0.0509, 0.0491], grad_fn=<ToCopyBackward0>), [' who', ' (', ' and', ',', ' with'])\n",
      "(tensor([0.8219, 0.1207, 0.0298, 0.0075, 0.0032], grad_fn=<ToCopyBackward0>), [' his', ' a', ' girl', ' the', ' an'])\n",
      "(tensor([0.2597, 0.0668, 0.0574, 0.0383, 0.0359], grad_fn=<ToCopyBackward0>), [' dog', ' robot', ' father', ' dad', ' mother'])\n",
      "(tensor([0.2479, 0.2459, 0.1736, 0.1694, 0.0256], grad_fn=<ToCopyBackward0>), [',', ' who', '.', ' that', ' and'])\n",
      "(tensor([0.0903, 0.0617, 0.0598, 0.0430, 0.0409], grad_fn=<ToCopyBackward0>), [' move', ' get', ' go', ' fight', ' have'])\n",
      "(tensor([0.1061, 0.0829, 0.0483, 0.0412, 0.0378], grad_fn=<ToCopyBackward0>), [' caught', ' stranded', ' lost', ' mixed', ' into'])\n",
      "(tensor([0.7444, 0.0920, 0.0287, 0.0163, 0.0158], grad_fn=<ToCopyBackward0>), [' in', ' on', ' and', '.', ','])\n",
      "(tensor([0.6869, 0.1640, 0.0200, 0.0148, 0.0120], grad_fn=<ToCopyBackward0>), [' the', ' a', ' some', ' an', ' space'])\n",
      "(tensor([0.1271, 0.0554, 0.0356, 0.0333, 0.0272], grad_fn=<ToCopyBackward0>), [' cave', ' forest', ' maze', ' tunnel', ' small'])\n",
      "(tensor([0.2773, 0.2519, 0.1015, 0.0282, 0.0253], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' where'])\n",
      "(tensor([0.2590, 0.0976, 0.0555, 0.0510, 0.0411], grad_fn=<ToCopyBackward0>), [' a', ' days', ' over', ' weeks', ' years'])\n",
      "(tensor([0.3336, 0.2643, 0.1020, 0.0748, 0.0619], grad_fn=<ToCopyBackward0>), [' week', ' while', ' day', ' month', ' few'])\n",
      "(tensor([0.3644, 0.3021, 0.2009, 0.0179, 0.0090], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' while', ' with'])\n",
      "(tensor([0.4867, 0.0947, 0.0402, 0.0248, 0.0212], grad_fn=<ToCopyBackward0>), [' and', ' then', ' but', ' only', ' surviving'])\n",
      "(tensor([0.3195, 0.0424, 0.0387, 0.0327, 0.0325], grad_fn=<ToCopyBackward0>), [' then', ' he', ' the', ' when', ' while'])\n",
      "(tensor([0.1496, 0.1111, 0.0892, 0.0515, 0.0486], grad_fn=<ToCopyBackward0>), [' he', ' find', ' they', ' the', ' get'])\n",
      "(tensor([0.1842, 0.1127, 0.0699, 0.0450, 0.0418], grad_fn=<ToCopyBackward0>), [' meets', ' finds', ' gets', ' and', ' comes'])\n",
      "(tensor([0.6482, 0.0567, 0.0475, 0.0437, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' the', ' his', ' this', ' an'])\n",
      "(tensor([0.1513, 0.0712, 0.0597, 0.0412, 0.0285], grad_fn=<ToCopyBackward0>), [' girl', ' talking', ' little', ' friendly', ' dog'])\n",
      "(tensor([0.2333, 0.1770, 0.1231, 0.0683, 0.0258], grad_fn=<ToCopyBackward0>), [' dog', ' bear', ' dinosaur', ' wolf', ' tree'])\n",
      "(tensor([0.5689, 0.1266, 0.0632, 0.0546, 0.0265], grad_fn=<ToCopyBackward0>), ['.', ' that', ' who', ',', ' named'])\n",
      "(tensor([0.2413, 0.2228, 0.0729, 0.0484, 0.0341], grad_fn=<ToCopyBackward0>), [' The', ' I', ' This', ' It', ' That'])\n",
      "(tensor([0.4220, 0.3927, 0.0729, 0.0122, 0.0105], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' had', ' has'])\n",
      "(tensor([0.0903, 0.0844, 0.0727, 0.0460, 0.0440], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' pretty', ' very'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty awful. The story was pretty awful. The acting wasn't even good. The script was even worse. I can see why the studio went with this movie. I mean, it was pretty much a perfect movie to make. It\n",
      "(tensor([0.3824, 0.1731, 0.0906, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4366, 0.2456, 0.1960, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6539, 0.0593, 0.0361, 0.0355, 0.0261], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1378, 0.0699, 0.0666, 0.0547, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1750, 0.1515, 0.1089, 0.0938, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.5987, 0.0756, 0.0562, 0.0512, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' when'])\n",
      "(tensor([0.2088, 0.1740, 0.1364, 0.0208, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.2133, 0.0823, 0.0630, 0.0625, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' story', ' movie'])\n",
      "(tensor([0.5354, 0.1347, 0.0406, 0.0251, 0.0215], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' had', ' sucked'])\n",
      "(tensor([0.1051, 0.0608, 0.0553, 0.0486, 0.0379], grad_fn=<ToCopyBackward0>), [' pretty', ' weak', ' stupid', ' so', ' just'])\n",
      "(tensor([0.1295, 0.0569, 0.0554, 0.0444, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' awful', ' boring', ' ridiculous'])\n",
      "(tensor([0.4885, 0.1568, 0.1263, 0.1101, 0.0574], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' and', ' as'])\n",
      "(tensor([0.5614, 0.0912, 0.0673, 0.0339, 0.0220], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' But'])\n",
      "(tensor([0.7266, 0.0355, 0.0211, 0.0164, 0.0143], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' movie', ' characters', ' special'])\n",
      "(tensor([0.8566, 0.0404, 0.0178, 0.0174, 0.0126], grad_fn=<ToCopyBackward0>), [' was', ' wasn', '...', ' pretty', ','])\n",
      "(tensor([9.9614e-01, 1.5514e-03, 3.6072e-04, 3.0056e-04, 1.7085e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.1602, 0.1009, 0.0821, 0.0819, 0.0674], grad_fn=<ToCopyBackward0>), [' even', ' very', ' that', ' good', ' too'])\n",
      "(tensor([0.2585, 0.1721, 0.0708, 0.0497, 0.0419], grad_fn=<ToCopyBackward0>), [' good', ' that', ' bad', ' close', ' all'])\n",
      "(tensor([0.3737, 0.3524, 0.0958, 0.0176, 0.0174], grad_fn=<ToCopyBackward0>), [' enough', '.', ',', ' and', '...'])\n",
      "(tensor([0.3196, 0.1405, 0.0991, 0.0416, 0.0313], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.1581, 0.0880, 0.0703, 0.0688, 0.0547], grad_fn=<ToCopyBackward0>), [' plot', ' special', ' only', ' script', ' cinem'])\n",
      "(tensor([0.6514, 0.1017, 0.0972, 0.0179, 0.0179], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' didn', ','])\n",
      "(tensor([0.2535, 0.0849, 0.0662, 0.0588, 0.0479], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' just', ' even', ' bad'])\n",
      "(tensor([0.8601, 0.0549, 0.0319, 0.0082, 0.0029], grad_fn=<ToCopyBackward0>), [' worse', ' more', ' less', ' worst', ' u'])\n",
      "(tensor([0.9005, 0.0347, 0.0273, 0.0107, 0.0064], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', '!', ' and'])\n",
      "(tensor([0.2713, 0.1444, 0.0650, 0.0480, 0.0391], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' But'])\n",
      "(tensor([0.0979, 0.0944, 0.0590, 0.0519, 0.0514], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' can', ' really', ' don'])\n",
      "(tensor([0.5209, 0.1428, 0.0614, 0.0356, 0.0271], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' see', ' tell', ' only'])\n",
      "(tensor([0.3475, 0.1210, 0.0903, 0.0695, 0.0678], grad_fn=<ToCopyBackward0>), [' why', ' how', ' the', ' where', ' that'])\n",
      "(tensor([0.1733, 0.1479, 0.1455, 0.1446, 0.1196], grad_fn=<ToCopyBackward0>), [' people', ' they', ' it', ' the', ' this'])\n",
      "(tensor([0.2386, 0.1340, 0.0784, 0.0421, 0.0412], grad_fn=<ToCopyBackward0>), [' movie', ' people', ' studio', ' guy', ' director'])\n",
      "(tensor([0.2000, 0.0880, 0.0835, 0.0499, 0.0452], grad_fn=<ToCopyBackward0>), [' made', ' gave', ' was', ' put', ' went'])\n",
      "(tensor([0.1136, 0.1033, 0.0995, 0.0834, 0.0822], grad_fn=<ToCopyBackward0>), [' for', ' down', ' with', ' to', ' back'])\n",
      "(tensor([0.5110, 0.2728, 0.0527, 0.0182, 0.0179], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' such'])\n",
      "(tensor([0.2011, 0.1998, 0.1195, 0.0814, 0.0340], grad_fn=<ToCopyBackward0>), [' one', ' script', ' movie', '.', ','])\n",
      "(tensor([0.4814, 0.1916, 0.0836, 0.0386, 0.0279], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' to', ' and'])\n",
      "(tensor([0.2235, 0.2162, 0.1130, 0.0466, 0.0424], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' They'])\n",
      "(tensor([0.2902, 0.1226, 0.0640, 0.0513, 0.0449], grad_fn=<ToCopyBackward0>), [' can', ' mean', ' just', ' was', ' really'])\n",
      "(tensor([0.4913, 0.1069, 0.0974, 0.0257, 0.0189], grad_fn=<ToCopyBackward0>), [',', ' the', ' it', ' this', ' I'])\n",
      "(tensor([0.1875, 0.1739, 0.0743, 0.0374, 0.0308], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' they', ' what'])\n",
      "(tensor([0.3886, 0.2018, 0.0731, 0.0341, 0.0328], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' has', ' is'])\n",
      "(tensor([0.1526, 0.0736, 0.0532, 0.0507, 0.0387], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' directed', ' made', ' about'])\n",
      "(tensor([0.1660, 0.0578, 0.0480, 0.0444, 0.0361], grad_fn=<ToCopyBackward0>), [' much', ' funny', ' bad', ' cool', '.'])\n",
      "(tensor([0.3708, 0.1413, 0.0603, 0.0599, 0.0365], grad_fn=<ToCopyBackward0>), [' a', ' the', ' all', ' just', ' like'])\n",
      "(tensor([0.0877, 0.0375, 0.0350, 0.0231, 0.0222], grad_fn=<ToCopyBackward0>), [' complete', ' big', ' perfect', ' remake', ' re'])\n",
      "(tensor([0.3987, 0.0996, 0.0811, 0.0215, 0.0182], grad_fn=<ToCopyBackward0>), [' movie', ' script', ' story', ' storm', ' choice'])\n",
      "(tensor([0.3724, 0.0815, 0.0713, 0.0707, 0.0481], grad_fn=<ToCopyBackward0>), ['.', ' for', ' to', ',', ' in'])\n",
      "(tensor([0.1955, 0.1866, 0.0367, 0.0260, 0.0258], grad_fn=<ToCopyBackward0>), [' be', ' make', ' promote', ' go', ' get'])\n",
      "(tensor([0.3973, 0.1622, 0.0615, 0.0328, 0.0280], grad_fn=<ToCopyBackward0>), [' a', '.', ',', ' it', ' for'])\n",
      "(tensor([0.1157, 0.1093, 0.1036, 0.0813, 0.0234], grad_fn=<ToCopyBackward0>), [' It', ' But', ' The', ' I', ' You'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd never see another movie as bad as this one. The story is a joke, the acting is awful, the editing is so sloppy, and there is no plot. The only reason I watched this movie was because the box looked cool. I\n",
      "(tensor([0.3843, 0.1712, 0.0897, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2245, 0.1943, 0.1574, 0.0694, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2082, 0.1041, 0.0617, 0.0395, 0.0300], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.5491, 0.0457, 0.0369, 0.0366, 0.0281], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' find', ' seen'])\n",
      "(tensor([0.6783, 0.0966, 0.0656, 0.0337, 0.0211], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' another', ' a'])\n",
      "(tensor([0.6361, 0.0763, 0.0225, 0.0186, 0.0097], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' horror', ' version', ' Freddy'])\n",
      "(tensor([0.4054, 0.1573, 0.0510, 0.0464, 0.0396], grad_fn=<ToCopyBackward0>), [' as', ' with', ' like', ' where', ' in'])\n",
      "(tensor([0.8531, 0.0333, 0.0227, 0.0105, 0.0082], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' horrible', ' boring'])\n",
      "(tensor([0.9746, 0.0075, 0.0042, 0.0030, 0.0025], grad_fn=<ToCopyBackward0>), [' as', ' in', ' or', ',', ' and'])\n",
      "(tensor([0.8915, 0.0255, 0.0182, 0.0099, 0.0098], grad_fn=<ToCopyBackward0>), [' this', ' that', ' \"', ' it', ' the'])\n",
      "(tensor([0.5515, 0.2454, 0.0675, 0.0399, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ' one', ' again', ',', '...'])\n",
      "(tensor([0.9086, 0.0264, 0.0115, 0.0078, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' in', '..'])\n",
      "(tensor([0.1876, 0.1528, 0.0914, 0.0539, 0.0206], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' Not'])\n",
      "(tensor([0.1375, 0.0990, 0.0886, 0.0749, 0.0440], grad_fn=<ToCopyBackward0>), [' only', ' plot', ' story', ' acting', ' script'])\n",
      "(tensor([0.3620, 0.2852, 0.0286, 0.0282, 0.0199], grad_fn=<ToCopyBackward0>), [' is', ' was', ' line', ' has', ' had'])\n",
      "(tensor([0.1155, 0.0484, 0.0440, 0.0377, 0.0325], grad_fn=<ToCopyBackward0>), [' so', ' about', ' predictable', ' a', ' ridiculous'])\n",
      "(tensor([0.1361, 0.0531, 0.0509, 0.0485, 0.0456], grad_fn=<ToCopyBackward0>), [' joke', ' big', ' cliché', ' mess', ' bad'])\n",
      "(tensor([0.2999, 0.2071, 0.1850, 0.0361, 0.0304], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' from', ' with'])\n",
      "(tensor([0.7539, 0.0904, 0.0154, 0.0125, 0.0072], grad_fn=<ToCopyBackward0>), [' the', ' and', ' acting', ' with', ' it'])\n",
      "(tensor([0.6216, 0.0497, 0.0440, 0.0283, 0.0177], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' actors', ' dialogue', ' casting'])\n",
      "(tensor([0.6048, 0.0302, 0.0249, 0.0248, 0.0236], grad_fn=<ToCopyBackward0>), [' is', ' awful', ' (', ',', ' a'])\n",
      "(tensor([0.1104, 0.0920, 0.0904, 0.0461, 0.0317], grad_fn=<ToCopyBackward0>), [' laughable', ' terrible', ' awful', ' bad', ' a'])\n",
      "(tensor([0.6057, 0.2243, 0.0646, 0.0538, 0.0077], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.4692, 0.4362, 0.0161, 0.0134, 0.0115], grad_fn=<ToCopyBackward0>), [' the', ' and', ' there', ' it', ' even'])\n",
      "(tensor([0.1152, 0.1008, 0.0862, 0.0386, 0.0323], grad_fn=<ToCopyBackward0>), [' special', ' script', ' plot', ' effects', ' editing'])\n",
      "(tensor([0.7559, 0.0418, 0.0265, 0.0109, 0.0091], grad_fn=<ToCopyBackward0>), [' is', ' and', ',', ' was', ' ('])\n",
      "(tensor([0.0590, 0.0569, 0.0539, 0.0515, 0.0436], grad_fn=<ToCopyBackward0>), [' even', ' so', ' awful', ' terrible', ' ridiculous'])\n",
      "(tensor([0.1691, 0.0713, 0.0467, 0.0456, 0.0433], grad_fn=<ToCopyBackward0>), [' bad', ' sloppy', ' awful', ' cho', ' poor'])\n",
      "(tensor([0.2923, 0.1563, 0.1498, 0.1237, 0.0973], grad_fn=<ToCopyBackward0>), [' that', ' it', ' you', ',', ' and'])\n",
      "(tensor([0.4479, 0.3693, 0.0326, 0.0194, 0.0155], grad_fn=<ToCopyBackward0>), [' the', ' and', ' it', ' that', ' even'])\n",
      "(tensor([0.7634, 0.0364, 0.0289, 0.0282, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' I', ' even'])\n",
      "(tensor([0.4395, 0.2378, 0.2086, 0.0248, 0.0219], grad_fn=<ToCopyBackward0>), [\"'s\", ' are', ' is', ' was', ' isn'])\n",
      "(tensor([0.4079, 0.1387, 0.0774, 0.0652, 0.0348], grad_fn=<ToCopyBackward0>), [' no', ' absolutely', ' nothing', ' so', ' just'])\n",
      "(tensor([0.2542, 0.0785, 0.0687, 0.0406, 0.0370], grad_fn=<ToCopyBackward0>), [' plot', ' point', ' real', ' logic', ' reason'])\n",
      "(tensor([0.6858, 0.0766, 0.0403, 0.0361, 0.0319], grad_fn=<ToCopyBackward0>), ['.', ' to', ' -', ' at', ','])\n",
      "(tensor([0.2329, 0.0983, 0.0877, 0.0351, 0.0264], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.3554, 0.0713, 0.0277, 0.0274, 0.0198], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' plot', ' whole', ' acting'])\n",
      "(tensor([0.2718, 0.0931, 0.0753, 0.0421, 0.0393], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' mystery', ' redeem'])\n",
      "(tensor([0.3458, 0.2657, 0.1220, 0.0432, 0.0362], grad_fn=<ToCopyBackward0>), [' I', ' to', ' this', ' for', ' anyone'])\n",
      "(tensor([0.1693, 0.1071, 0.0838, 0.0740, 0.0610], grad_fn=<ToCopyBackward0>), [' gave', ' watched', ' even', \"'m\", ' didn'])\n",
      "(tensor([0.4272, 0.4235, 0.1275, 0.0050, 0.0020], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' all'])\n",
      "(tensor([0.6115, 0.0938, 0.0830, 0.0546, 0.0261], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.5394, 0.3125, 0.0272, 0.0225, 0.0078], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' in', ' for'])\n",
      "(tensor([0.6634, 0.1491, 0.0364, 0.0354, 0.0263], grad_fn=<ToCopyBackward0>), [' because', ' to', ' so', ' for', ' that'])\n",
      "(tensor([0.4564, 0.1071, 0.0856, 0.0608, 0.0570], grad_fn=<ToCopyBackward0>), [' I', ' the', ' of', ' it', ' my'])\n",
      "(tensor([0.2140, 0.1159, 0.0559, 0.0249, 0.0215], grad_fn=<ToCopyBackward0>), [' cover', ' box', ' trailer', ' movie', ' cast'])\n",
      "(tensor([0.0690, 0.0607, 0.0542, 0.0476, 0.0441], grad_fn=<ToCopyBackward0>), [' looked', ' of', ' art', ' said', ' set'])\n",
      "(tensor([0.1768, 0.1214, 0.1168, 0.1166, 0.0773], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' cool', ' so', ' kinda'])\n",
      "(tensor([0.5574, 0.2086, 0.0843, 0.0304, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' on', ' so'])\n",
      "(tensor([0.2863, 0.1474, 0.0607, 0.0533, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' If', ' This'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of the worst movies I have ever seen. The plot was stupid, I don't think it would have made a good movie. The acting was terrible. The plot was stupid, it was so stupid I could not even describe it.\n",
      "(tensor([0.3849, 0.1711, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1167, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1845, 0.1451, 0.0510, 0.0454, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9167, 0.0124, 0.0066, 0.0065, 0.0040], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7612, 0.1932, 0.0167, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.5680, 0.1212, 0.1092, 0.0266, 0.0260], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' most', ' dumb', ' funn'])\n",
      "(tensor([0.7935, 0.1556, 0.0051, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ',', ' film'])\n",
      "(tensor([0.7384, 0.0886, 0.0755, 0.0330, 0.0126], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' that'])\n",
      "(tensor([0.3616, 0.3547, 0.1208, 0.0773, 0.0568], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' had', ' ever', \"'d\"])\n",
      "(tensor([9.1591e-01, 7.6439e-02, 1.6304e-03, 1.6197e-03, 4.4365e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9145, 0.0250, 0.0141, 0.0074, 0.0044], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' wasted'])\n",
      "(tensor([0.5943, 0.1393, 0.0928, 0.0542, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', '!', '...'])\n",
      "(tensor([0.2659, 0.1401, 0.1377, 0.0241, 0.0229], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.1311, 0.1121, 0.1081, 0.0994, 0.0410], grad_fn=<ToCopyBackward0>), [' only', ' story', ' acting', ' plot', ' movie'])\n",
      "(tensor([0.7369, 0.1022, 0.0117, 0.0113, 0.0095], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' sounded', ','])\n",
      "(tensor([0.1135, 0.1039, 0.1011, 0.0383, 0.0368], grad_fn=<ToCopyBackward0>), [' so', ' stupid', ' ridiculous', ' terrible', ' just'])\n",
      "(tensor([0.5186, 0.2596, 0.1532, 0.0107, 0.0067], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' as', ';'])\n",
      "(tensor([0.5512, 0.1444, 0.0619, 0.0207, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' and', ' acting', ' I', ' it'])\n",
      "(tensor([0.1723, 0.0665, 0.0653, 0.0610, 0.0417], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' think', ' couldn', ' mean'])\n",
      "(tensor([9.9080e-01, 3.7906e-03, 1.4428e-03, 9.5714e-04, 4.0188e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.2730, 0.1735, 0.1366, 0.0855, 0.0583], grad_fn=<ToCopyBackward0>), [' know', ' think', ' even', ' understand', ' get'])\n",
      "(tensor([0.2615, 0.1778, 0.0892, 0.0773, 0.0728], grad_fn=<ToCopyBackward0>), [' it', ' the', ' even', ' I', ' any'])\n",
      "(tensor([0.3305, 0.1266, 0.1033, 0.0874, 0.0417], grad_fn=<ToCopyBackward0>), [' was', ' had', ' could', ' would', ' even'])\n",
      "(tensor([0.4331, 0.1264, 0.0668, 0.0662, 0.0599], grad_fn=<ToCopyBackward0>), [' have', ' even', ' be', ' work', ' appeal'])\n",
      "(tensor([0.4682, 0.2311, 0.1043, 0.0323, 0.0155], grad_fn=<ToCopyBackward0>), [' worked', ' made', ' been', ' even', ' lasted'])\n",
      "(tensor([0.5128, 0.1491, 0.1225, 0.0450, 0.0285], grad_fn=<ToCopyBackward0>), [' a', ' any', ' it', ' much', ' an'])\n",
      "(tensor([0.7464, 0.0306, 0.0253, 0.0183, 0.0158], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' great', ' decent', ' very'])\n",
      "(tensor([0.6766, 0.1742, 0.0327, 0.0197, 0.0091], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' story', ' comedy', ' family'])\n",
      "(tensor([0.3504, 0.3418, 0.0339, 0.0314, 0.0280], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' even'])\n",
      "(tensor([0.2642, 0.1484, 0.1005, 0.0318, 0.0310], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' And'])\n",
      "(tensor([0.5788, 0.0451, 0.0423, 0.0318, 0.0229], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' only', ' movie', ' characters'])\n",
      "(tensor([0.8406, 0.0415, 0.0131, 0.0126, 0.0104], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ',', ' sucked', ' and'])\n",
      "(tensor([0.1672, 0.1615, 0.0896, 0.0639, 0.0635], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' horrible', ' awful', ' not'])\n",
      "(tensor([0.3613, 0.2451, 0.2401, 0.0203, 0.0179], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' too'])\n",
      "(tensor([0.3143, 0.1992, 0.0910, 0.0579, 0.0198], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.1626, 0.1050, 0.0711, 0.0509, 0.0401], grad_fn=<ToCopyBackward0>), [' plot', ' only', ' script', ' story', ' movie'])\n",
      "(tensor([0.7811, 0.0215, 0.0177, 0.0154, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ' made', ' is', ' had', ','])\n",
      "(tensor([0.4319, 0.0649, 0.0472, 0.0467, 0.0409], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' just', ' not', ' ridiculous'])\n",
      "(tensor([0.3201, 0.3118, 0.2581, 0.0091, 0.0091], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' too'])\n",
      "(tensor([0.2780, 0.2290, 0.1065, 0.0577, 0.0365], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' it'])\n",
      "(tensor([0.2577, 0.1259, 0.0961, 0.0723, 0.0655], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' would', ' wasn', \"'s\"])\n",
      "(tensor([0.1743, 0.1170, 0.0974, 0.0580, 0.0537], grad_fn=<ToCopyBackward0>), [' not', ' just', ' stupid', ' a', ' so'])\n",
      "(tensor([0.3246, 0.2586, 0.0517, 0.0479, 0.0241], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' dumb', ' bad', ' ridiculous'])\n",
      "(tensor([0.2156, 0.1696, 0.1458, 0.1414, 0.1016], grad_fn=<ToCopyBackward0>), [' that', ' I', '.', ' it', ','])\n",
      "(tensor([0.1740, 0.1127, 0.1000, 0.0716, 0.0610], grad_fn=<ToCopyBackward0>), [' don', ' could', ' can', ' couldn', ' was'])\n",
      "(tensor([0.6924, 0.0409, 0.0394, 0.0366, 0.0284], grad_fn=<ToCopyBackward0>), [' not', ' see', ' only', ' have', ' barely'])\n",
      "(tensor([0.3175, 0.1896, 0.1216, 0.0614, 0.0274], grad_fn=<ToCopyBackward0>), [' even', ' believe', ' understand', ' follow', ' tell'])\n",
      "(tensor([0.2176, 0.1072, 0.0731, 0.0687, 0.0613], grad_fn=<ToCopyBackward0>), [' tell', ' describe', ' believe', ' explain', ' remember'])\n",
      "(tensor([0.8442, 0.0656, 0.0291, 0.0171, 0.0090], grad_fn=<ToCopyBackward0>), [' it', ' the', ' to', ' how', '.'])\n",
      "(tensor([0.6293, 0.1324, 0.0663, 0.0500, 0.0137], grad_fn=<ToCopyBackward0>), ['.', ' to', ' in', ',', ' properly'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very boring. The acting was terrible, and the story seemed pretty trite and obvious in its point. The acting was so bad that it seemed like the actors were reading from cue-cards, and they were not very good at it\n",
      "(tensor([0.3838, 0.1718, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4374, 0.2440, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6524, 0.0597, 0.0361, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5074, 0.0403, 0.0344, 0.0339, 0.0323], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' funny', ','])\n",
      "(tensor([0.4164, 0.2417, 0.1165, 0.0408, 0.0139], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '!'])\n",
      "(tensor([0.1943, 0.1660, 0.1552, 0.0295, 0.0184], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.2563, 0.0652, 0.0595, 0.0574, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' movie', ' actors'])\n",
      "(tensor([0.8309, 0.0453, 0.0400, 0.0117, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.0970, 0.0756, 0.0752, 0.0641, 0.0442], grad_fn=<ToCopyBackward0>), [' terrible', ' not', ' bad', ' very', ' awful'])\n",
      "(tensor([0.3136, 0.3107, 0.2519, 0.0114, 0.0105], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' as', '!'])\n",
      "(tensor([0.4345, 0.1754, 0.0434, 0.0330, 0.0246], grad_fn=<ToCopyBackward0>), [' the', ' and', ' but', ' especially', ' there'])\n",
      "(tensor([0.5765, 0.1257, 0.0416, 0.0384, 0.0235], grad_fn=<ToCopyBackward0>), [' the', ' I', ' there', ' it', ' even'])\n",
      "(tensor([0.3605, 0.2162, 0.0994, 0.0733, 0.0425], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' movie'])\n",
      "(tensor([0.7034, 0.0296, 0.0281, 0.0243, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' line', ' seemed', ' just'])\n",
      "(tensor([0.5772, 0.0660, 0.0515, 0.0357, 0.0306], grad_fn=<ToCopyBackward0>), [' to', ' pretty', ' like', ' very', ' really'])\n",
      "(tensor([0.0713, 0.0495, 0.0439, 0.0398, 0.0391], grad_fn=<ToCopyBackward0>), [' weak', ' much', ' boring', ' stupid', ' tr'])\n",
      "(tensor([9.9883e-01, 1.2815e-04, 9.7740e-05, 7.7654e-05, 6.1930e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['ite', 'udging', 'ully', 'ites', 'ashed'])\n",
      "(tensor([0.4151, 0.3606, 0.0781, 0.0522, 0.0132], grad_fn=<ToCopyBackward0>), ['.', ' and', ' to', ',', ' at'])\n",
      "(tensor([0.0786, 0.0686, 0.0538, 0.0457, 0.0394], grad_fn=<ToCopyBackward0>), [' predictable', ' tr', ' obvious', ' un', ' old'])\n",
      "(tensor([0.6219, 0.0698, 0.0660, 0.0450, 0.0245], grad_fn=<ToCopyBackward0>), ['.', ' to', ' at', ',', ' in'])\n",
      "(tensor([0.1590, 0.1267, 0.0979, 0.0614, 0.0578], grad_fn=<ToCopyBackward0>), [' places', ' the', ' its', ' hindsight', ' retrospect'])\n",
      "(tensor([0.1148, 0.0729, 0.0305, 0.0197, 0.0178], grad_fn=<ToCopyBackward0>), [' message', ' point', ' plot', ' storyline', ' intentions'])\n",
      "(tensor([0.8913, 0.0275, 0.0175, 0.0139, 0.0129], grad_fn=<ToCopyBackward0>), ['.', ' of', '-', ' and', ','])\n",
      "(tensor([0.1788, 0.1708, 0.0764, 0.0296, 0.0276], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' This'])\n",
      "(tensor([0.1561, 0.1005, 0.0467, 0.0441, 0.0353], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' plot', ' acting', ' whole'])\n",
      "(tensor([0.6165, 0.0635, 0.0511, 0.0411, 0.0335], grad_fn=<ToCopyBackward0>), [' was', ' in', ' and', ' is', ','])\n",
      "(tensor([0.1305, 0.1259, 0.0716, 0.0476, 0.0378], grad_fn=<ToCopyBackward0>), [' so', ' bad', ' terrible', ' TER', ' very'])\n",
      "(tensor([0.7211, 0.0216, 0.0205, 0.0187, 0.0172], grad_fn=<ToCopyBackward0>), [' bad', ' wooden', '-', ' poor', ' terrible'])\n",
      "(tensor([0.5330, 0.1818, 0.1263, 0.0353, 0.0300], grad_fn=<ToCopyBackward0>), [' that', ',', ' I', ' it', ' in'])\n",
      "(tensor([0.4330, 0.1378, 0.0728, 0.0702, 0.0521], grad_fn=<ToCopyBackward0>), [' I', ' it', ' even', ' the', ' i'])\n",
      "(tensor([0.3313, 0.1133, 0.0908, 0.0640, 0.0290], grad_fn=<ToCopyBackward0>), [' was', ' made', ' seemed', \"'s\", ' wasn'])\n",
      "(tensor([0.5136, 0.1078, 0.0715, 0.0656, 0.0566], grad_fn=<ToCopyBackward0>), [' like', ' as', ' to', ' more', ' almost'])\n",
      "(tensor([0.2755, 0.2750, 0.1651, 0.0550, 0.0343], grad_fn=<ToCopyBackward0>), [' the', ' they', ' it', ' I', ' there'])\n",
      "(tensor([0.1800, 0.1780, 0.0771, 0.0599, 0.0491], grad_fn=<ToCopyBackward0>), [' director', ' actors', ' producers', ' writers', ' movie'])\n",
      "(tensor([0.6073, 0.0978, 0.0492, 0.0224, 0.0208], grad_fn=<ToCopyBackward0>), [' were', ' had', ' just', ' didn', ' weren'])\n",
      "(tensor([0.1183, 0.1104, 0.0996, 0.0798, 0.0322], grad_fn=<ToCopyBackward0>), [' just', ' actually', ' reading', ' trying', ' having'])\n",
      "(tensor([0.2696, 0.1799, 0.1787, 0.1329, 0.0489], grad_fn=<ToCopyBackward0>), [' their', ' from', ' the', ' cue', ' off'])\n",
      "(tensor([0.6832, 0.2007, 0.0608, 0.0101, 0.0059], grad_fn=<ToCopyBackward0>), [' cue', ' a', ' the', ' tele', ' an'])\n",
      "(tensor([9.7621e-01, 9.4137e-03, 3.6440e-03, 2.9430e-03, 7.1263e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' cards', 'cards', ' sheets', '-', ' card'])\n",
      "(tensor([0.6881, 0.0314, 0.0287, 0.0186, 0.0186], grad_fn=<ToCopyBackward0>), ['cards', 'card', 'rec', 'balls', 'm'])\n",
      "(tensor([0.1219, 0.0817, 0.0757, 0.0574, 0.0565], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' with', ' as'])\n",
      "(tensor([0.4389, 0.0661, 0.0614, 0.0540, 0.0488], grad_fn=<ToCopyBackward0>), [' and', ' not', ' which', ' rather', ' instead'])\n",
      "(tensor([0.2972, 0.0738, 0.0609, 0.0464, 0.0455], grad_fn=<ToCopyBackward0>), [' the', ' they', ' it', ' that', ' not'])\n",
      "(tensor([0.1904, 0.0748, 0.0725, 0.0702, 0.0670], grad_fn=<ToCopyBackward0>), [' were', ' had', ' just', ' didn', ' couldn'])\n",
      "(tensor([0.1652, 0.0962, 0.0509, 0.0463, 0.0288], grad_fn=<ToCopyBackward0>), [' all', ' reading', ' just', ' not', ' doing'])\n",
      "(tensor([0.1904, 0.0743, 0.0487, 0.0447, 0.0416], grad_fn=<ToCopyBackward0>), [' even', ' very', ' trying', ' good', ' doing'])\n",
      "(tensor([0.6173, 0.1989, 0.0187, 0.0161, 0.0154], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' attractive', ' believable', ' adept'])\n",
      "(tensor([0.7716, 0.1151, 0.0220, 0.0132, 0.0113], grad_fn=<ToCopyBackward0>), [' at', '.', ' actors', ' cue', ' in'])\n",
      "(tensor([0.7371, 0.0896, 0.0268, 0.0260, 0.0139], grad_fn=<ToCopyBackward0>), [' it', ' that', ' their', ' this', ' reading'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad that I actually felt compelled to register here.I was really looking forward to watching this movie because I thought it was going to be a funny comedy. The jokes were funny, the acting wasn't. The plot was predictable.\n",
      "(tensor([0.3840, 0.1720, 0.0900, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2439, 0.1964, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6530, 0.0596, 0.0360, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0661, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5191, 0.0473, 0.0457, 0.0419, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2378, 0.2248, 0.2056, 0.0699, 0.0469], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.4868, 0.1500, 0.1345, 0.0379, 0.0304], grad_fn=<ToCopyBackward0>), [' I', ' i', ' it', ' the', ' even'])\n",
      "(tensor([0.1725, 0.0621, 0.0589, 0.0403, 0.0340], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' had', ' couldn'])\n",
      "(tensor([0.1815, 0.1059, 0.0682, 0.0431, 0.0321], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' started', ' felt'])\n",
      "(tensor([0.1018, 0.0851, 0.0810, 0.0739, 0.0394], grad_fn=<ToCopyBackward0>), [' embarrassed', ' sorry', ' bad', ' compelled', ' sick'])\n",
      "(tensor([9.9807e-01, 2.6105e-04, 2.5682e-04, 1.9687e-04, 1.2933e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' by', ' (', ',', ' in'])\n",
      "(tensor([0.2130, 0.1909, 0.1008, 0.0814, 0.0390], grad_fn=<ToCopyBackward0>), [' register', ' write', ' comment', ' watch', ' give'])\n",
      "(tensor([0.1526, 0.0989, 0.0925, 0.0873, 0.0761], grad_fn=<ToCopyBackward0>), [' here', ' just', ' for', ' as', ' a'])\n",
      "(tensor([0.3093, 0.1483, 0.1094, 0.0668, 0.0584], grad_fn=<ToCopyBackward0>), ['.', ' on', ' to', ' just', ','])\n",
      "(tensor([0.3026, 0.1286, 0.0726, 0.0316, 0.0315], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'I', ' This'])\n",
      "(tensor([0.1211, 0.0753, 0.0567, 0.0449, 0.0416], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' am', ' rented', ' have'])\n",
      "(tensor([0.3209, 0.1396, 0.0447, 0.0300, 0.0205], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' so', ' very', ' in'])\n",
      "(tensor([0.4208, 0.1123, 0.0667, 0.0386, 0.0378], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ',', ' really', ' surprised'])\n",
      "(tensor([9.8691e-01, 1.0353e-02, 1.6447e-03, 2.7100e-04, 2.3108e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' at'])\n",
      "(tensor([9.9201e-01, 3.9046e-03, 8.2903e-04, 6.0997e-04, 3.2462e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' this', ' on'])\n",
      "(tensor([0.5046, 0.1924, 0.0877, 0.0761, 0.0136], grad_fn=<ToCopyBackward0>), [' this', ' seeing', ' the', ' watching', ' renting'])\n",
      "(tensor([0.8206, 0.0889, 0.0280, 0.0061, 0.0038], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' The'])\n",
      "(tensor([0.6860, 0.0821, 0.0790, 0.0342, 0.0201], grad_fn=<ToCopyBackward0>), [' movie', ',', ' film', ' because', '.'])\n",
      "(tensor([0.2153, 0.1065, 0.0828, 0.0679, 0.0539], grad_fn=<ToCopyBackward0>), [',', ' because', ' and', '.', ' as'])\n",
      "(tensor([0.4135, 0.1486, 0.1212, 0.1065, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' i'])\n",
      "(tensor([0.1271, 0.1167, 0.0985, 0.0926, 0.0819], grad_fn=<ToCopyBackward0>), [' really', ' thought', \"'m\", ' like', ' am'])\n",
      "(tensor([0.5494, 0.1209, 0.0735, 0.0551, 0.0429], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ' this'])\n",
      "(tensor([0.4330, 0.3130, 0.0609, 0.0521, 0.0209], grad_fn=<ToCopyBackward0>), [' would', ' was', ' might', ' had', ' could'])\n",
      "(tensor([0.3907, 0.1079, 0.0438, 0.0398, 0.0378], grad_fn=<ToCopyBackward0>), [' going', ' a', ' gonna', ' so', ' pretty'])\n",
      "(tensor([9.9424e-01, 3.3225e-03, 2.3494e-04, 2.1219e-04, 1.5522e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' be', ' make', ' really', ' in'])\n",
      "(tensor([9.6905e-01, 6.5184e-03, 4.7499e-03, 1.5755e-03, 7.9938e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' get', ' provide'])\n",
      "(tensor([0.1658, 0.1251, 0.1245, 0.1161, 0.0492], grad_fn=<ToCopyBackward0>), [' a', ' really', ' pretty', ' funny', ' great'])\n",
      "(tensor([0.2073, 0.1620, 0.0764, 0.0707, 0.0491], grad_fn=<ToCopyBackward0>), [' good', ' really', ' great', ' funny', ' pretty'])\n",
      "(tensor([0.4936, 0.0931, 0.0814, 0.0246, 0.0200], grad_fn=<ToCopyBackward0>), [' movie', ',', ' comedy', ' film', ' little'])\n",
      "(tensor([0.5325, 0.1231, 0.0792, 0.0616, 0.0303], grad_fn=<ToCopyBackward0>), ['.', ',', ' about', ' with', ' that'])\n",
      "(tensor([0.2029, 0.1272, 0.1053, 0.0959, 0.0240], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' What'])\n",
      "(tensor([0.1150, 0.0935, 0.0636, 0.0601, 0.0579], grad_fn=<ToCopyBackward0>), [' plot', ' jokes', ' only', ' acting', ' movie'])\n",
      "(tensor([0.4343, 0.1008, 0.0905, 0.0723, 0.0318], grad_fn=<ToCopyBackward0>), [' were', ' weren', ' in', ' are', ' seemed'])\n",
      "(tensor([0.1810, 0.1276, 0.1115, 0.0820, 0.0302], grad_fn=<ToCopyBackward0>), [' funny', ' not', ' stupid', ' supposed', ' predictable'])\n",
      "(tensor([0.2590, 0.2069, 0.1377, 0.0686, 0.0578], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' but', ' in'])\n",
      "(tensor([0.3885, 0.2673, 0.0529, 0.0232, 0.0218], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' especially', ' I'])\n",
      "(tensor([0.4172, 0.0975, 0.0520, 0.0513, 0.0373], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' jokes', ' characters', ' story'])\n",
      "(tensor([0.8873, 0.0588, 0.0049, 0.0047, 0.0027], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' pretty', ' and', ' really'])\n",
      "(tensor([9.9639e-01, 1.0148e-03, 3.8504e-04, 2.6138e-04, 1.9379e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.2397, 0.1627, 0.1215, 0.0618, 0.0432], grad_fn=<ToCopyBackward0>), ['.', ' bad', ',', ' too', ' funny'])\n",
      "(tensor([0.3352, 0.1140, 0.0911, 0.0595, 0.0373], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' And'])\n",
      "(tensor([0.1504, 0.1470, 0.1261, 0.0745, 0.0595], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' jokes', ' story', ' only'])\n",
      "(tensor([0.7487, 0.0380, 0.0221, 0.0194, 0.0090], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' had', ' made'])\n",
      "(tensor([0.1361, 0.0886, 0.0499, 0.0464, 0.0432], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' ridiculous', ' not', ' so'])\n",
      "(tensor([0.3904, 0.2721, 0.2518, 0.0147, 0.0074], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' from', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd never get that rated a movie. It's a really bad movie, but not as bad as this movie. I'm not sure that I've ever seen a movie with so many plot holes and so much bad acting in it. It's\n",
      "(tensor([0.3844, 0.1713, 0.0896, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2246, 0.1944, 0.1573, 0.0695, 0.0539], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2087, 0.1040, 0.0616, 0.0394, 0.0300], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.5495, 0.0456, 0.0369, 0.0365, 0.0282], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' find', ' seen'])\n",
      "(tensor([0.1280, 0.1220, 0.1010, 0.0685, 0.0363], grad_fn=<ToCopyBackward0>), [' through', ' to', ' that', ' out', ' there'])\n",
      "(tensor([0.0594, 0.0507, 0.0479, 0.0240, 0.0190], grad_fn=<ToCopyBackward0>), [' far', ' bored', ' chance', ' tired', ' rated'])\n",
      "(tensor([0.1722, 0.1123, 0.0854, 0.0730, 0.0274], grad_fn=<ToCopyBackward0>), [' a', ' as', '.', ' by', ' but'])\n",
      "(tensor([0.3611, 0.1372, 0.0551, 0.0530, 0.0461], grad_fn=<ToCopyBackward0>), [' 1', ' movie', ' 10', ' negative', ' one'])\n",
      "(tensor([0.4857, 0.0841, 0.0693, 0.0476, 0.0333], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' in', ' again'])\n",
      "(tensor([0.2572, 0.1296, 0.0532, 0.0382, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' But'])\n",
      "(tensor([0.4069, 0.2794, 0.0391, 0.0281, 0.0188], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' just'])\n",
      "(tensor([0.1552, 0.1169, 0.1107, 0.0693, 0.0661], grad_fn=<ToCopyBackward0>), [' not', ' so', ' a', ' just', ' one'])\n",
      "(tensor([0.2230, 0.1194, 0.0938, 0.0443, 0.0363], grad_fn=<ToCopyBackward0>), [' really', ' very', ' real', ' good', ' total'])\n",
      "(tensor([0.2870, 0.2029, 0.1049, 0.0452, 0.0367], grad_fn=<ToCopyBackward0>), [' bad', ' boring', ' cheesy', ' funny', ' dumb'])\n",
      "(tensor([0.7150, 0.0431, 0.0323, 0.0287, 0.0230], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' script', ','])\n",
      "(tensor([0.6923, 0.1029, 0.0540, 0.0195, 0.0137], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' with', ' in'])\n",
      "(tensor([0.3531, 0.0772, 0.0635, 0.0387, 0.0324], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' the', ' a'])\n",
      "(tensor([0.2720, 0.2458, 0.0617, 0.0331, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' at', ' not'])\n",
      "(tensor([0.4593, 0.0757, 0.0638, 0.0523, 0.0425], grad_fn=<ToCopyBackward0>), [' as', ' in', ' a', ' funny', ' that'])\n",
      "(tensor([0.9625, 0.0044, 0.0040, 0.0034, 0.0028], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' stupid', ' much', ' awful'])\n",
      "(tensor([0.9722, 0.0121, 0.0029, 0.0020, 0.0010], grad_fn=<ToCopyBackward0>), [' as', ' a', ',', ' or', ' if'])\n",
      "(tensor([0.2093, 0.1837, 0.1061, 0.0902, 0.0677], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' the', ' \"'])\n",
      "(tensor([0.3906, 0.2445, 0.1238, 0.0162, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' one', '!', ' is'])\n",
      "(tensor([0.7781, 0.1214, 0.0151, 0.0121, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ' is', '!', ' was', '...'])\n",
      "(tensor([0.2326, 0.1841, 0.0840, 0.0676, 0.0192], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.0960, 0.0811, 0.0655, 0.0597, 0.0542], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' can', ' really'])\n",
      "(tensor([0.2059, 0.1396, 0.0918, 0.0603, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' just', ' very'])\n",
      "(tensor([0.2372, 0.1590, 0.1367, 0.1066, 0.0942], grad_fn=<ToCopyBackward0>), [' even', ' a', ' sure', ' going', ' one'])\n",
      "(tensor([0.3339, 0.1368, 0.1307, 0.1080, 0.0710], grad_fn=<ToCopyBackward0>), [' if', ' how', ' why', ' what', ' that'])\n",
      "(tensor([0.2038, 0.1070, 0.0901, 0.0887, 0.0660], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' the', ' any'])\n",
      "(tensor([0.1614, 0.1583, 0.1298, 0.0885, 0.0717], grad_fn=<ToCopyBackward0>), [' could', ' can', \"'ve\", ' would', \"'m\"])\n",
      "(tensor([0.6999, 0.2224, 0.0163, 0.0061, 0.0061], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' been', ' made', ' really'])\n",
      "(tensor([0.4169, 0.1374, 0.0645, 0.0389, 0.0322], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' rated', ' watched', ' had'])\n",
      "(tensor([0.7349, 0.0541, 0.0416, 0.0300, 0.0202], grad_fn=<ToCopyBackward0>), [' a', ' such', ' so', ' anything', ' an'])\n",
      "(tensor([0.5370, 0.2187, 0.0649, 0.0320, 0.0252], grad_fn=<ToCopyBackward0>), [' movie', ' worse', ' bad', ' really', ' more'])\n",
      "(tensor([0.1879, 0.1780, 0.1328, 0.1067, 0.1062], grad_fn=<ToCopyBackward0>), [' as', ' that', ' where', ' with', ' so'])\n",
      "(tensor([0.4713, 0.0960, 0.0824, 0.0794, 0.0671], grad_fn=<ToCopyBackward0>), [' so', ' such', ' more', ' as', ' a'])\n",
      "(tensor([0.5485, 0.3236, 0.0849, 0.0198, 0.0042], grad_fn=<ToCopyBackward0>), [' many', ' much', ' little', ' few', ' bad'])\n",
      "(tensor([0.1674, 0.0695, 0.0457, 0.0193, 0.0171], grad_fn=<ToCopyBackward0>), [' plot', ' bad', ' actors', ' big', ' impl'])\n",
      "(tensor([0.9679, 0.0049, 0.0047, 0.0031, 0.0027], grad_fn=<ToCopyBackward0>), [' holes', ' flaws', '-', ' points', ' twists'])\n",
      "(tensor([0.2165, 0.1830, 0.1770, 0.1612, 0.1044], grad_fn=<ToCopyBackward0>), [' as', ' in', ' and', '.', ','])\n",
      "(tensor([0.3228, 0.0318, 0.0308, 0.0247, 0.0180], grad_fn=<ToCopyBackward0>), [' so', ' plot', ' stupid', ' such', ' continuity'])\n",
      "(tensor([0.5687, 0.1719, 0.1713, 0.0525, 0.0032], grad_fn=<ToCopyBackward0>), [' many', ' little', ' much', ' few', ' bad'])\n",
      "(tensor([0.0895, 0.0473, 0.0450, 0.0308, 0.0277], grad_fn=<ToCopyBackward0>), [' stupidity', ' bad', ' that', ' wrong', ' inconsistency'])\n",
      "(tensor([0.8786, 0.0333, 0.0192, 0.0074, 0.0038], grad_fn=<ToCopyBackward0>), [' acting', ' taste', 'ness', ' dialogue', ' film'])\n",
      "(tensor([0.6330, 0.1621, 0.0876, 0.0433, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', ' as'])\n",
      "(tensor([0.3739, 0.3367, 0.1289, 0.0455, 0.0434], grad_fn=<ToCopyBackward0>), [' it', ' one', ' a', ' the', ' such'])\n",
      "(tensor([0.7321, 0.1185, 0.0413, 0.0358, 0.0114], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' that', ' before'])\n",
      "(tensor([0.1942, 0.1508, 0.1337, 0.0379, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.5716, 0.0607, 0.0482, 0.0249, 0.0244], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' makes'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty funny. But then I got this movie with the same name. I think it's a good movie, but it's just not as good as the other movie I saw by the same name. It had a similar plot and it\n",
      "(tensor([0.3840, 0.1716, 0.0900, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2440, 0.1958, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6523, 0.0596, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0661, 0.0548, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1753, 0.1518, 0.1087, 0.0951, 0.0896], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.4494, 0.1295, 0.0921, 0.0257, 0.0246], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', ' in'])\n",
      "(tensor([0.2603, 0.1845, 0.0998, 0.0193, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.1212, 0.1051, 0.0638, 0.0624, 0.0616], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' then'])\n",
      "(tensor([0.5191, 0.0961, 0.0499, 0.0487, 0.0393], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' when', ','])\n",
      "(tensor([0.2076, 0.1172, 0.0641, 0.0610, 0.0531], grad_fn=<ToCopyBackward0>), [' found', ' watched', ' thought', ' got', ' saw'])\n",
      "(tensor([0.2361, 0.2138, 0.0762, 0.0625, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' the', ' to', ' this', ' really'])\n",
      "(tensor([0.3657, 0.2138, 0.0271, 0.0214, 0.0129], grad_fn=<ToCopyBackward0>), [' movie', ' DVD', ' feeling', ' really', ' film'])\n",
      "(tensor([0.1982, 0.1318, 0.0974, 0.0574, 0.0532], grad_fn=<ToCopyBackward0>), [' called', ' from', ' and', ' with', ' where'])\n",
      "(tensor([0.1176, 0.0785, 0.0341, 0.0264, 0.0199], grad_fn=<ToCopyBackward0>), [' the', ' this', ' Sha', ' Christopher', ' a'])\n",
      "(tensor([0.3175, 0.1410, 0.0309, 0.0296, 0.0202], grad_fn=<ToCopyBackward0>), [' same', ' guy', ' name', ' really', ' kid'])\n",
      "(tensor([0.6415, 0.1199, 0.0568, 0.0354, 0.0119], grad_fn=<ToCopyBackward0>), [' name', ' director', ' title', ' guy', ' plot'])\n",
      "(tensor([0.5914, 0.1351, 0.0860, 0.0243, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ' from'])\n",
      "(tensor([0.3676, 0.1116, 0.0894, 0.0658, 0.0477], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' This'])\n",
      "(tensor([0.1777, 0.1152, 0.0726, 0.0541, 0.0521], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' think', ' really'])\n",
      "(tensor([0.2395, 0.2032, 0.1624, 0.0748, 0.0741], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' that'])\n",
      "(tensor([0.5947, 0.2105, 0.0453, 0.0199, 0.0154], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' might', ' has'])\n",
      "(tensor([0.1767, 0.1115, 0.0827, 0.0628, 0.0585], grad_fn=<ToCopyBackward0>), [' a', ' more', ' just', ' the', ' pretty'])\n",
      "(tensor([0.2425, 0.1704, 0.1156, 0.0816, 0.0525], grad_fn=<ToCopyBackward0>), [' good', ' really', ' lot', ' better', ' little'])\n",
      "(tensor([0.8985, 0.0551, 0.0269, 0.0037, 0.0012], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' idea', ' satire'])\n",
      "(tensor([0.3074, 0.2755, 0.0974, 0.0328, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' but', ' as'])\n",
      "(tensor([0.6731, 0.1308, 0.0510, 0.0299, 0.0184], grad_fn=<ToCopyBackward0>), [' but', ' too', ' I', ' so', ' though'])\n",
      "(tensor([0.2282, 0.1966, 0.0843, 0.0793, 0.0644], grad_fn=<ToCopyBackward0>), [' it', ' I', ' not', ' the', ' this'])\n",
      "(tensor([0.5498, 0.0977, 0.0566, 0.0398, 0.0342], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' doesn', ' is'])\n",
      "(tensor([0.4972, 0.0829, 0.0364, 0.0264, 0.0238], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' really', ' also'])\n",
      "(tensor([0.3655, 0.2150, 0.0386, 0.0372, 0.0357], grad_fn=<ToCopyBackward0>), [' not', ' a', ' too', ' the', ' so'])\n",
      "(tensor([0.6568, 0.2238, 0.0252, 0.0082, 0.0068], grad_fn=<ToCopyBackward0>), [' funny', ' as', ' the', ' that', ' very'])\n",
      "(tensor([0.9461, 0.0268, 0.0058, 0.0027, 0.0018], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' hilarious', ' fun', ' amusing'])\n",
      "(tensor([0.8803, 0.0726, 0.0068, 0.0064, 0.0041], grad_fn=<ToCopyBackward0>), [' as', '.', ' a', ',', ' because'])\n",
      "(tensor([0.2089, 0.1872, 0.0650, 0.0507, 0.0188], grad_fn=<ToCopyBackward0>), [' the', ' this', ' The', ' \"', ' I'])\n",
      "(tensor([0.2358, 0.1993, 0.1144, 0.0931, 0.0179], grad_fn=<ToCopyBackward0>), [' first', ' other', ' one', ' original', ' movie'])\n",
      "(tensor([0.8119, 0.0641, 0.0329, 0.0183, 0.0051], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' movies', ' film', '.'])\n",
      "(tensor([0.4659, 0.2669, 0.0789, 0.0559, 0.0164], grad_fn=<ToCopyBackward0>), [' I', '.', ' with', ' that', ' in'])\n",
      "(tensor([0.2826, 0.1018, 0.0946, 0.0850, 0.0669], grad_fn=<ToCopyBackward0>), [' saw', \"'ve\", ' have', ' just', ' watched'])\n",
      "(tensor([0.3834, 0.1556, 0.0463, 0.0370, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ' with', ' (', ' last', ' by'])\n",
      "(tensor([0.6331, 0.1150, 0.0810, 0.0064, 0.0051], grad_fn=<ToCopyBackward0>), [' the', ' that', ' this', ' The', ' them'])\n",
      "(tensor([0.9664, 0.0145, 0.0030, 0.0026, 0.0019], grad_fn=<ToCopyBackward0>), [' same', ' name', ' guy', ' director', ' title'])\n",
      "(tensor([0.6718, 0.1963, 0.0916, 0.0082, 0.0072], grad_fn=<ToCopyBackward0>), [' name', ' title', ' director', ' guy', ' person'])\n",
      "(tensor([0.9420, 0.0107, 0.0060, 0.0058, 0.0042], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ' I', '!'])\n",
      "(tensor([0.3126, 0.0883, 0.0799, 0.0668, 0.0297], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' So'])\n",
      "(tensor([0.6138, 0.1331, 0.0356, 0.0215, 0.0214], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' is', ' had'])\n",
      "(tensor([0.2897, 0.1668, 0.0772, 0.0655, 0.0386], grad_fn=<ToCopyBackward0>), [' a', ' the', ' more', ' some', ' so'])\n",
      "(tensor([0.2013, 0.1450, 0.0717, 0.0682, 0.0632], grad_fn=<ToCopyBackward0>), [' lot', ' really', ' very', ' similar', ' pretty'])\n",
      "(tensor([0.6869, 0.1689, 0.0454, 0.0230, 0.0140], grad_fn=<ToCopyBackward0>), [' plot', ' storyline', ' theme', ' story', ' premise'])\n",
      "(tensor([0.3572, 0.2720, 0.1229, 0.0672, 0.0262], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', '.', ' with'])\n",
      "(tensor([0.2736, 0.0769, 0.0611, 0.0457, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' was', ' the', ' it', ' plot'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. The acting is awful. The plot is ridiculous. The acting. The plot. The acting. The plot was stupid. The plot was stupid. The acting. The plot. The plot was stupid. The plot was stupid\n",
      "(tensor([0.3845, 0.1715, 0.0896, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2431, 0.1963, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7761, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1273, 0.0769, 0.0659, 0.0556, 0.0420], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1302, 0.1259, 0.1059, 0.0740, 0.0608], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5995, 0.0804, 0.0745, 0.0360, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2067, 0.1826, 0.1377, 0.0234, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.1622, 0.0780, 0.0751, 0.0721, 0.0316], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' only', ' script'])\n",
      "(tensor([0.7843, 0.0463, 0.0401, 0.0236, 0.0189], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ',', ' and'])\n",
      "(tensor([0.0902, 0.0750, 0.0631, 0.0588, 0.0576], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' pretty', ' so'])\n",
      "(tensor([0.3659, 0.3247, 0.1867, 0.0115, 0.0096], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ';'])\n",
      "(tensor([0.6458, 0.0679, 0.0619, 0.0177, 0.0172], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2785, 0.2062, 0.1277, 0.0493, 0.0242], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' directing'])\n",
      "(tensor([0.5438, 0.2283, 0.0212, 0.0144, 0.0139], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' -', ' makes'])\n",
      "(tensor([0.1525, 0.1091, 0.0609, 0.0531, 0.0383], grad_fn=<ToCopyBackward0>), [' awful', ' not', ' even', ' ridiculous', ' terrible'])\n",
      "(tensor([0.6754, 0.2082, 0.0538, 0.0073, 0.0069], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' ('])\n",
      "(tensor([0.6004, 0.0844, 0.0799, 0.0307, 0.0164], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0871, 0.0725, 0.0617, 0.0559, 0.0432], grad_fn=<ToCopyBackward0>), [' special', ' editing', ' acting', ' ending', ' cinem'])\n",
      "(tensor([0.5560, 0.1467, 0.0725, 0.0444, 0.0407], grad_fn=<ToCopyBackward0>), [' is', '.', ' was', ',', ' and'])\n",
      "(tensor([0.4979, 0.0810, 0.0682, 0.0230, 0.0193], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.5641, 0.0817, 0.0226, 0.0205, 0.0199], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' directing', ' writing'])\n",
      "(tensor([0.7044, 0.0954, 0.0817, 0.0185, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', ',', ' and'])\n",
      "(tensor([0.4753, 0.0887, 0.0610, 0.0349, 0.0222], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' This'])\n",
      "(tensor([0.7964, 0.0353, 0.0119, 0.0108, 0.0097], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' casting', ' only', ' actors'])\n",
      "(tensor([0.9355, 0.0088, 0.0081, 0.0064, 0.0062], grad_fn=<ToCopyBackward0>), ['.', ' was', '...', ' is', '!'])\n",
      "(tensor([0.5894, 0.0746, 0.0433, 0.0291, 0.0230], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' This'])\n",
      "(tensor([0.7415, 0.0223, 0.0126, 0.0110, 0.0107], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' script', ' only', ' actors'])\n",
      "(tensor([0.8885, 0.0207, 0.0192, 0.0093, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ' is', ' was', '!', '?'])\n",
      "(tensor([0.2828, 0.0902, 0.0527, 0.0381, 0.0329], grad_fn=<ToCopyBackward0>), [' ridiculous', ' stupid', ' bad', ' silly', ' terrible'])\n",
      "(tensor([0.5092, 0.2115, 0.0756, 0.0474, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' as'])\n",
      "(tensor([0.4821, 0.0900, 0.0394, 0.0323, 0.0269], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3818, 0.2343, 0.0431, 0.0341, 0.0145], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' actors', ' script'])\n",
      "(tensor([0.7190, 0.0740, 0.0515, 0.0402, 0.0100], grad_fn=<ToCopyBackward0>), [' was', '.', ' sucked', ' is', ' had'])\n",
      "(tensor([0.6301, 0.0473, 0.0364, 0.0259, 0.0187], grad_fn=<ToCopyBackward0>), [' stupid', ' ridiculous', ' dumb', ' silly', ' not'])\n",
      "(tensor([0.8150, 0.0361, 0.0340, 0.0227, 0.0090], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', '!!'])\n",
      "(tensor([0.7084, 0.0524, 0.0229, 0.0189, 0.0141], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' And'])\n",
      "(tensor([0.4998, 0.4016, 0.0095, 0.0057, 0.0054], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' actors', ' only', ' script'])\n",
      "(tensor([0.6355, 0.2246, 0.0599, 0.0166, 0.0089], grad_fn=<ToCopyBackward0>), [' was', '.', ' is', '...', ' sucked'])\n",
      "(tensor([0.8703, 0.0243, 0.0112, 0.0101, 0.0081], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.9409, 0.0165, 0.0041, 0.0038, 0.0019], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' script', ' plotting', ' actors'])\n",
      "(tensor([0.6409, 0.2907, 0.0221, 0.0056, 0.0047], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', '?', ','])\n",
      "(tensor([0.8803, 0.0243, 0.0096, 0.0092, 0.0077], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.5046, 0.4145, 0.0070, 0.0039, 0.0039], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' plotting', ' script', ' actors'])\n",
      "(tensor([0.5436, 0.3897, 0.0165, 0.0076, 0.0047], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', ' sucked', '?'])\n",
      "(tensor([0.7748, 0.0301, 0.0212, 0.0203, 0.0117], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' ridiculous', ' silly', ' bad'])\n",
      "(tensor([0.9611, 0.0064, 0.0061, 0.0041, 0.0033], grad_fn=<ToCopyBackward0>), ['.', '!', '...', ',', ' and'])\n",
      "(tensor([0.8453, 0.0338, 0.0138, 0.0134, 0.0094], grad_fn=<ToCopyBackward0>), [' The', ' I', 'The', ' This', ' It'])\n",
      "(tensor([0.5765, 0.3458, 0.0072, 0.0052, 0.0044], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' actors', ' plotting', ' script'])\n",
      "(tensor([0.7713, 0.1810, 0.0142, 0.0053, 0.0032], grad_fn=<ToCopyBackward0>), [' was', '.', ' is', ' sucked', ' wasn'])\n",
      "(tensor([0.9414, 0.0099, 0.0041, 0.0032, 0.0021], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' silly', ' ridiculous', ' pointless'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would watch a good one, since it was rated as the best horror movie of all time. The special effects were pretty good and the story was pretty good too. It is not the best film I have ever seen, it is not even the\n",
      "(tensor([0.3833, 0.1721, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2250, 0.1948, 0.1572, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2276, 0.1205, 0.0843, 0.0368, 0.0233], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.7078, 0.0901, 0.0597, 0.0411, 0.0111], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.2565, 0.1036, 0.0933, 0.0652, 0.0495], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' good', ' scary', ' documentary'])\n",
      "(tensor([0.3691, 0.1582, 0.1381, 0.0532, 0.0263], grad_fn=<ToCopyBackward0>), [' movie', ' horror', ' film', ' one', ' documentary'])\n",
      "(tensor([0.1279, 0.1122, 0.1076, 0.0631, 0.0388], grad_fn=<ToCopyBackward0>), ['.', ' first', ',', ' and', ' to'])\n",
      "(tensor([0.1853, 0.1220, 0.1156, 0.0630, 0.0277], grad_fn=<ToCopyBackward0>), [' so', ' but', ' and', ' since', ' a'])\n",
      "(tensor([0.3770, 0.2577, 0.0644, 0.0389, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' the', ' i'])\n",
      "(tensor([0.3616, 0.1621, 0.0873, 0.0440, 0.0399], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' seemed'])\n",
      "(tensor([0.3470, 0.0894, 0.0572, 0.0382, 0.0320], grad_fn=<ToCopyBackward0>), [' on', ' rated', ' in', ' so', ' only'])\n",
      "(tensor([0.2035, 0.1399, 0.0820, 0.0785, 0.0475], grad_fn=<ToCopyBackward0>), [' as', ' a', ' at', ' so', ' by'])\n",
      "(tensor([0.3611, 0.0974, 0.0773, 0.0681, 0.0605], grad_fn=<ToCopyBackward0>), [' a', ' one', \" '\", ' \"', ' the'])\n",
      "(tensor([0.3083, 0.1907, 0.0680, 0.0656, 0.0481], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' \"', \" '\", ' #'])\n",
      "(tensor([0.2815, 0.1861, 0.0916, 0.0601, 0.0416], grad_fn=<ToCopyBackward0>), [' horror', ' film', ' one', ' movie', ' comedy'])\n",
      "(tensor([0.6246, 0.2592, 0.0102, 0.0093, 0.0088], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' on', ' in', '/'])\n",
      "(tensor([0.3041, 0.1656, 0.1327, 0.0848, 0.0542], grad_fn=<ToCopyBackward0>), [' of', ' in', ' ever', ' on', ' I'])\n",
      "(tensor([0.6045, 0.0750, 0.0705, 0.0241, 0.0219], grad_fn=<ToCopyBackward0>), [' all', ' 2001', ' the', ' 2002', ' 2006'])\n",
      "(tensor([9.9101e-01, 4.8782e-03, 2.2015e-03, 4.5589e-04, 3.1114e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' the', ' Time'])\n",
      "(tensor([0.6910, 0.1037, 0.0696, 0.0216, 0.0214], grad_fn=<ToCopyBackward0>), ['.', ' by', ',', ' in', ' on'])\n",
      "(tensor([0.3331, 0.1006, 0.0611, 0.0360, 0.0299], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' This'])\n",
      "(tensor([0.1188, 0.0732, 0.0408, 0.0386, 0.0320], grad_fn=<ToCopyBackward0>), [' acting', ' original', ' first', ' special', ' only'])\n",
      "(tensor([9.7318e-01, 2.0598e-02, 1.6782e-03, 1.6756e-03, 4.2081e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' effects', ' effect', ' features', ' affects', ' Effects'])\n",
      "(tensor([0.5219, 0.2761, 0.0263, 0.0241, 0.0151], grad_fn=<ToCopyBackward0>), [' were', ' are', ' and', ' in', ' make'])\n",
      "(tensor([0.1623, 0.0632, 0.0559, 0.0484, 0.0460], grad_fn=<ToCopyBackward0>), [' pretty', ' great', ' good', ' really', ' so'])\n",
      "(tensor([0.3897, 0.1256, 0.0907, 0.0710, 0.0176], grad_fn=<ToCopyBackward0>), [' good', ' cool', ' bad', ' impressive', ' cheesy'])\n",
      "(tensor([0.3574, 0.1346, 0.1304, 0.0797, 0.0785], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' too', ' for'])\n",
      "(tensor([0.6426, 0.0915, 0.0452, 0.0401, 0.0067], grad_fn=<ToCopyBackward0>), [' the', ' I', ' there', ' it', ' very'])\n",
      "(tensor([0.1922, 0.1618, 0.1139, 0.0534, 0.0494], grad_fn=<ToCopyBackward0>), [' story', ' acting', ' plot', ' cast', ' special'])\n",
      "(tensor([0.7089, 0.0283, 0.0281, 0.0187, 0.0149], grad_fn=<ToCopyBackward0>), [' was', ' had', ' itself', ' wasn', ' did'])\n",
      "(tensor([0.3042, 0.1163, 0.1113, 0.0488, 0.0286], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' good', ' decent', ' fairly'])\n",
      "(tensor([0.4159, 0.1130, 0.0684, 0.0546, 0.0539], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' decent', ' creepy', ' scary'])\n",
      "(tensor([0.3293, 0.2501, 0.1936, 0.0815, 0.0182], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' as', ' but'])\n",
      "(tensor([0.6067, 0.2141, 0.0373, 0.0194, 0.0191], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' but', ' so'])\n",
      "(tensor([0.2542, 0.1343, 0.0872, 0.0795, 0.0392], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' However'])\n",
      "(tensor([0.3707, 0.1091, 0.0766, 0.0632, 0.0591], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' is', ' wasn'])\n",
      "(tensor([0.1867, 0.0521, 0.0509, 0.0395, 0.0366], grad_fn=<ToCopyBackward0>), [' a', ' not', ' pretty', ' very', ' one'])\n",
      "(tensor([0.2835, 0.1262, 0.0896, 0.0719, 0.0427], grad_fn=<ToCopyBackward0>), [' a', ' the', ' really', ' very', ' scary'])\n",
      "(tensor([0.4160, 0.3538, 0.0599, 0.0376, 0.0123], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' most', ' horror'])\n",
      "(tensor([0.2118, 0.1288, 0.0636, 0.0636, 0.0581], grad_fn=<ToCopyBackward0>), [' movie', ' horror', ' film', ',', ' of'])\n",
      "(tensor([0.2944, 0.2881, 0.0670, 0.0590, 0.0573], grad_fn=<ToCopyBackward0>), [' I', ' of', ',', ' in', ' but'])\n",
      "(tensor([0.8414, 0.0850, 0.0518, 0.0031, 0.0031], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' ever', ' saw', ' had'])\n",
      "(tensor([7.7996e-01, 1.9597e-01, 1.7195e-02, 7.5933e-04, 4.1631e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' see'])\n",
      "(tensor([0.9436, 0.0429, 0.0027, 0.0021, 0.0010], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' viewed', ' saw'])\n",
      "(tensor([0.6666, 0.1505, 0.0305, 0.0276, 0.0173], grad_fn=<ToCopyBackward0>), [',', ' but', ' in', '.', ' by'])\n",
      "(tensor([0.8063, 0.0366, 0.0145, 0.0133, 0.0116], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' since', ' or'])\n",
      "(tensor([0.4747, 0.1031, 0.0730, 0.0512, 0.0290], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' was', ' has', ' just'])\n",
      "(tensor([0.1933, 0.0729, 0.0665, 0.0572, 0.0458], grad_fn=<ToCopyBackward0>), [' not', ' just', ' good', ' a', ' pretty'])\n",
      "(tensor([0.3124, 0.2888, 0.0328, 0.0286, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' even', ' a', ' one', ' as'])\n",
      "(tensor([0.3598, 0.1289, 0.1235, 0.0803, 0.0385], grad_fn=<ToCopyBackward0>), [' the', ' in', ' good', ' close', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie started well enough. It started off well enough. And then I just started to see the same thing over and over again. You get a little bit into the story, then you see something else. You get a little bit into the story\n",
      "(tensor([0.3841, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5015, 0.0598, 0.0340, 0.0151, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6234, 0.0400, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.4244, 0.3174, 0.0989, 0.0110, 0.0109], grad_fn=<ToCopyBackward0>), [' out', ' off', ' well', ' very', ' slow'])\n",
      "(tensor([0.4286, 0.0994, 0.0815, 0.0752, 0.0749], grad_fn=<ToCopyBackward0>), [' enough', ',', ' but', '...', '.'])\n",
      "(tensor([0.3443, 0.2038, 0.0711, 0.0701, 0.0659], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' with', ' but'])\n",
      "(tensor([0.1969, 0.1556, 0.1315, 0.0460, 0.0320], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' But', ' Then'])\n",
      "(tensor([0.2263, 0.0947, 0.0889, 0.0888, 0.0671], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' started', ' had', \"'s\"])\n",
      "(tensor([0.2994, 0.2675, 0.1698, 0.0278, 0.0179], grad_fn=<ToCopyBackward0>), [' well', ' off', ' out', ' with', ' very'])\n",
      "(tensor([0.2668, 0.0685, 0.0618, 0.0614, 0.0447], grad_fn=<ToCopyBackward0>), [' well', ' with', ' pretty', ' fine', ' very'])\n",
      "(tensor([0.9164, 0.0192, 0.0111, 0.0095, 0.0083], grad_fn=<ToCopyBackward0>), [' enough', ',', '.', ' but', ' in'])\n",
      "(tensor([0.2397, 0.1716, 0.1482, 0.1123, 0.0666], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', ' but', ' in'])\n",
      "(tensor([0.1411, 0.1317, 0.1055, 0.0965, 0.0579], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' And'])\n",
      "(tensor([0.2917, 0.1875, 0.1409, 0.0670, 0.0233], grad_fn=<ToCopyBackward0>), [' I', ' then', ' the', ' it', ','])\n",
      "(tensor([0.2869, 0.2269, 0.1407, 0.0357, 0.0291], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ',', ' you'])\n",
      "(tensor([0.0950, 0.0862, 0.0783, 0.0745, 0.0628], grad_fn=<ToCopyBackward0>), [' watched', ' just', ' thought', ' found', ' got'])\n",
      "(tensor([0.1536, 0.1186, 0.0741, 0.0628, 0.0476], grad_fn=<ToCopyBackward0>), [' watched', ' started', ' got', ' thought', ' couldn'])\n",
      "(tensor([0.2741, 0.2669, 0.0494, 0.0375, 0.0316], grad_fn=<ToCopyBackward0>), [' watching', ' to', ' getting', ' wondering', ' looking'])\n",
      "(tensor([0.1692, 0.1073, 0.1035, 0.0660, 0.0622], grad_fn=<ToCopyBackward0>), [' watch', ' feel', ' get', ' think', ' see'])\n",
      "(tensor([0.3473, 0.0872, 0.0684, 0.0428, 0.0397], grad_fn=<ToCopyBackward0>), [' the', ' more', ' it', ' that', ' a'])\n",
      "(tensor([0.0466, 0.0247, 0.0231, 0.0223, 0.0205], grad_fn=<ToCopyBackward0>), [' movie', ' changes', ' same', ' ending', ' teeth'])\n",
      "(tensor([0.1461, 0.0460, 0.0413, 0.0360, 0.0282], grad_fn=<ToCopyBackward0>), [' thing', ' scenes', ' things', ' old', ' acting'])\n",
      "(tensor([0.2692, 0.2358, 0.1085, 0.0997, 0.0404], grad_fn=<ToCopyBackward0>), [' over', ' again', '.', ' in', ','])\n",
      "(tensor([9.2566e-01, 6.8498e-02, 1.6779e-03, 8.3949e-04, 5.0333e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' and', ' again', ' the', ' &', ' in'])\n",
      "(tensor([9.9631e-01, 2.6969e-03, 3.9764e-04, 2.0183e-04, 5.7832e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' over', ' OVER', ' again', ' Over', 'over'])\n",
      "(tensor([0.8159, 0.1144, 0.0287, 0.0130, 0.0073], grad_fn=<ToCopyBackward0>), [' again', ' and', '.', ' in', ','])\n",
      "(tensor([0.3891, 0.2500, 0.1247, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ':'])\n",
      "(tensor([0.2475, 0.1737, 0.0615, 0.0523, 0.0344], grad_fn=<ToCopyBackward0>), [' I', ' And', ' It', ' The', ' You'])\n",
      "(tensor([0.4379, 0.0649, 0.0560, 0.0451, 0.0357], grad_fn=<ToCopyBackward0>), [' know', ' have', ' can', ' just', ' get'])\n",
      "(tensor([0.3011, 0.2000, 0.0561, 0.0425, 0.0275], grad_fn=<ToCopyBackward0>), [' a', ' the', ' to', ' so', ' into'])\n",
      "(tensor([0.1200, 0.1031, 0.0676, 0.0484, 0.0433], grad_fn=<ToCopyBackward0>), [' little', ' bunch', ' guy', ' really', ' group'])\n",
      "(tensor([0.5540, 0.0684, 0.0387, 0.0229, 0.0162], grad_fn=<ToCopyBackward0>), [' bit', ' bored', ' tired', ' more', ' too'])\n",
      "(tensor([0.2656, 0.1162, 0.0531, 0.0497, 0.0431], grad_fn=<ToCopyBackward0>), [' of', ' more', ' tired', ' bored', ' into'])\n",
      "(tensor([0.4623, 0.1139, 0.1041, 0.0952, 0.0384], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' it', ' your'])\n",
      "(tensor([0.2701, 0.1763, 0.0701, 0.0699, 0.0327], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' movie', ' script', ' storyline'])\n",
      "(tensor([0.3998, 0.2079, 0.0213, 0.0194, 0.0149], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' -', ' a'])\n",
      "(tensor([0.3067, 0.2722, 0.1172, 0.0925, 0.0178], grad_fn=<ToCopyBackward0>), [' and', ' you', ' then', ' but', ' the'])\n",
      "(tensor([0.5474, 0.1281, 0.0675, 0.0321, 0.0230], grad_fn=<ToCopyBackward0>), [' you', ' it', ' the', ' there', ' suddenly'])\n",
      "(tensor([0.2651, 0.1104, 0.1011, 0.1007, 0.0541], grad_fn=<ToCopyBackward0>), [' get', ' see', ' realize', ' just', ' have'])\n",
      "(tensor([0.3974, 0.1182, 0.0977, 0.0717, 0.0343], grad_fn=<ToCopyBackward0>), [' the', ' something', ' it', ' a', ' that'])\n",
      "(tensor([0.1310, 0.0879, 0.0826, 0.0677, 0.0612], grad_fn=<ToCopyBackward0>), [' else', ' like', ' different', ' that', ' and'])\n",
      "(tensor([0.2812, 0.2146, 0.1716, 0.0652, 0.0281], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ' on'])\n",
      "(tensor([0.2656, 0.1913, 0.0647, 0.0642, 0.0584], grad_fn=<ToCopyBackward0>), [' You', ' And', ' Then', ' I', ' It'])\n",
      "(tensor([0.2660, 0.1460, 0.0866, 0.0706, 0.0379], grad_fn=<ToCopyBackward0>), [' get', ' see', ' start', ' just', ' don'])\n",
      "(tensor([0.8251, 0.0360, 0.0201, 0.0132, 0.0124], grad_fn=<ToCopyBackward0>), [' a', ' another', ' the', ' into', ' one'])\n",
      "(tensor([0.9661, 0.0019, 0.0017, 0.0017, 0.0017], grad_fn=<ToCopyBackward0>), [' little', ' bit', ' really', ' big', ' few'])\n",
      "(tensor([0.9747, 0.0053, 0.0037, 0.0020, 0.0017], grad_fn=<ToCopyBackward0>), [' bit', ' more', ' something', ' into', ' further'])\n",
      "(tensor([0.9277, 0.0364, 0.0097, 0.0068, 0.0043], grad_fn=<ToCopyBackward0>), [' into', ' in', ' more', ' further', ' deeper'])\n",
      "(tensor([0.9007, 0.0411, 0.0161, 0.0087, 0.0063], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' a', ' that'])\n",
      "(tensor([0.8801, 0.0453, 0.0140, 0.0109, 0.0093], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' character', ' plot', ' storyline'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was a really bad movie. I thought this movie was so bad I actually paid money to see it. It was so terrible. It is one of those movies where you think \"Oh wow, that was so stupid\" and you just watch it\n",
      "(tensor([0.3841, 0.1716, 0.0899, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2440, 0.1957, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4800, 0.1355, 0.1036, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1340, 0.1093, 0.0992, 0.0921, 0.0467], grad_fn=<ToCopyBackward0>), [' really', ' sequel', ' good', ' great', ' movie'])\n",
      "(tensor([0.4369, 0.1023, 0.0498, 0.0395, 0.0183], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8394, 0.0656, 0.0136, 0.0114, 0.0040], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' script', ' horror'])\n",
      "(tensor([0.7276, 0.0473, 0.0455, 0.0231, 0.0145], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', ' when'])\n",
      "(tensor([0.3172, 0.1461, 0.0990, 0.0267, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', ' This'])\n",
      "(tensor([0.1380, 0.0950, 0.0820, 0.0555, 0.0504], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.5645, 0.1621, 0.1216, 0.0568, 0.0190], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' I'])\n",
      "(tensor([0.6075, 0.2104, 0.1083, 0.0087, 0.0058], grad_fn=<ToCopyBackward0>), [' was', ' movie', ' is', ' one', ' film'])\n",
      "(tensor([0.5956, 0.0537, 0.0395, 0.0283, 0.0275], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' sucked'])\n",
      "(tensor([0.1380, 0.1211, 0.0584, 0.0558, 0.0406], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' just', ' the'])\n",
      "(tensor([0.3391, 0.0676, 0.0482, 0.0299, 0.0298], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ','])\n",
      "(tensor([0.3827, 0.1591, 0.1528, 0.0943, 0.0840], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', '.'])\n",
      "(tensor([0.2307, 0.1095, 0.0808, 0.0597, 0.0383], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' could', ' would'])\n",
      "(tensor([0.1099, 0.0652, 0.0589, 0.0583, 0.0525], grad_fn=<ToCopyBackward0>), [' thought', ' made', ' paid', ' started', ' felt'])\n",
      "(tensor([0.1999, 0.1764, 0.0835, 0.0827, 0.0778], grad_fn=<ToCopyBackward0>), [' to', ' $', ' a', ' money', ' for'])\n",
      "(tensor([0.7925, 0.1627, 0.0039, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' to', ' for', ',', ' just', ' and'])\n",
      "(tensor([0.6870, 0.0792, 0.0783, 0.0585, 0.0153], grad_fn=<ToCopyBackward0>), [' see', ' rent', ' watch', ' get', ' go'])\n",
      "(tensor([0.7510, 0.1700, 0.0224, 0.0166, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' something'])\n",
      "(tensor([0.5951, 0.0707, 0.0657, 0.0633, 0.0394], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' in', ' just'])\n",
      "(tensor([0.4067, 0.0757, 0.0403, 0.0399, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.3397, 0.3053, 0.0772, 0.0214, 0.0200], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' looks'])\n",
      "(tensor([0.3902, 0.0874, 0.0543, 0.0400, 0.0345], grad_fn=<ToCopyBackward0>), [' so', ' like', ' a', ' not', ' just'])\n",
      "(tensor([0.7835, 0.0417, 0.0247, 0.0219, 0.0168], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' horrible', ' stupid', ' awful'])\n",
      "(tensor([0.3308, 0.1959, 0.1372, 0.1009, 0.0817], grad_fn=<ToCopyBackward0>), [' I', ' that', '.', ' it', ','])\n",
      "(tensor([0.3550, 0.1516, 0.0624, 0.0521, 0.0265], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', 'I'])\n",
      "(tensor([0.5275, 0.1418, 0.0451, 0.0236, 0.0231], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' had'])\n",
      "(tensor([0.4114, 0.0901, 0.0759, 0.0579, 0.0420], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' just', ' one'])\n",
      "(tensor([0.9225, 0.0191, 0.0089, 0.0085, 0.0041], grad_fn=<ToCopyBackward0>), [' of', ' bad', ' the', ' big', ' thing'])\n",
      "(tensor([0.5629, 0.3930, 0.0293, 0.0043, 0.0012], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' worst'])\n",
      "(tensor([9.5087e-01, 2.5294e-02, 3.5318e-03, 9.5185e-04, 9.3845e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' really', ' rare'])\n",
      "(tensor([0.5545, 0.2209, 0.0689, 0.0523, 0.0154], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', '.'])\n",
      "(tensor([0.2862, 0.2723, 0.1149, 0.0556, 0.0542], grad_fn=<ToCopyBackward0>), [' you', ' I', ' the', ' if', ' it'])\n",
      "(tensor([0.0755, 0.0750, 0.0619, 0.0475, 0.0465], grad_fn=<ToCopyBackward0>), [' can', ' think', ' just', ' are', \"'re\"])\n",
      "(tensor([0.3404, 0.1596, 0.0858, 0.0448, 0.0446], grad_fn=<ToCopyBackward0>), [' it', ',', ' you', ' \"', ' that'])\n",
      "(tensor([0.1207, 0.1128, 0.0709, 0.0523, 0.0492], grad_fn=<ToCopyBackward0>), ['Oh', 'oh', 'ok', 'OK', 'this'])\n",
      "(tensor([0.3296, 0.1025, 0.0832, 0.0655, 0.0553], grad_fn=<ToCopyBackward0>), [' my', ' God', ',', ' yeah', ' wow'])\n",
      "(tensor([0.3642, 0.1734, 0.1539, 0.0494, 0.0355], grad_fn=<ToCopyBackward0>), [',', ' this', '!', ' that', ' I'])\n",
      "(tensor([0.4193, 0.1581, 0.0992, 0.0377, 0.0361], grad_fn=<ToCopyBackward0>), [' I', ' this', ' that', ' what', ' they'])\n",
      "(tensor([0.3416, 0.2430, 0.1649, 0.0544, 0.0397], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' movie'])\n",
      "(tensor([0.6568, 0.0631, 0.0315, 0.0213, 0.0212], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' pretty', ' funny'])\n",
      "(tensor([0.3980, 0.1316, 0.1076, 0.0819, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' stupid', ' terrible', ' awful'])\n",
      "(tensor([0.1881, 0.1585, 0.1408, 0.0712, 0.0702], grad_fn=<ToCopyBackward0>), ['.\"', '\".', '\"', '.', ','])\n",
      "(tensor([0.3133, 0.2753, 0.0425, 0.0310, 0.0296], grad_fn=<ToCopyBackward0>), [' but', ' and', ' -', 'and', ' then'])\n",
      "(tensor([0.6937, 0.1311, 0.0355, 0.0272, 0.0218], grad_fn=<ToCopyBackward0>), [' then', ' you', ' it', ' \"', ' the'])\n",
      "(tensor([0.0976, 0.0737, 0.0720, 0.0587, 0.0523], grad_fn=<ToCopyBackward0>), [' actually', ' think', \"'re\", ' watch', ' just'])\n",
      "(tensor([0.2031, 0.1329, 0.1224, 0.0565, 0.0385], grad_fn=<ToCopyBackward0>), [' laugh', ' keep', ' watch', ' want', ' look'])\n",
      "(tensor([0.5580, 0.3349, 0.0164, 0.0111, 0.0080], grad_fn=<ToCopyBackward0>), [' the', ' it', ' a', ' and', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie was very well acted. The movie was funny, too, but the plot wasn't funny. I was expecting a funny movie with funny plot, but that was not it. I was expecting a funny movie with funny plot. It was\n",
      "(tensor([0.3832, 0.1720, 0.0903, 0.0773, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3315, 0.2364, 0.0583, 0.0562, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2624, 0.0540, 0.0272, 0.0220, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5314, 0.0603, 0.0539, 0.0231, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.1799, 0.0810, 0.0520, 0.0427, 0.0358], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' a', ' so', ' terrible'])\n",
      "(tensor([0.4255, 0.0824, 0.0428, 0.0316, 0.0266], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ',', ' funny'])\n",
      "(tensor([0.5640, 0.2573, 0.0836, 0.0228, 0.0175], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.3237, 0.2061, 0.1855, 0.0461, 0.0290], grad_fn=<ToCopyBackward0>), [' and', ',', '.', '...', ' by'])\n",
      "(tensor([0.1585, 0.1583, 0.1259, 0.0379, 0.0358], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' But'])\n",
      "(tensor([0.2306, 0.1719, 0.0524, 0.0407, 0.0256], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' movie', ' acting', ' characters'])\n",
      "(tensor([0.3447, 0.2228, 0.0583, 0.0367, 0.0319], grad_fn=<ToCopyBackward0>), [' was', ' is', ' has', ' had', ' just'])\n",
      "(tensor([0.1412, 0.0813, 0.0460, 0.0414, 0.0399], grad_fn=<ToCopyBackward0>), [' very', ' pretty', ' good', ' funny', ' really'])\n",
      "(tensor([0.3552, 0.1737, 0.1228, 0.0592, 0.0486], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' too', ' in'])\n",
      "(tensor([0.2659, 0.1777, 0.0527, 0.0473, 0.0405], grad_fn=<ToCopyBackward0>), [' but', ' the', ' too', ' it', ' I'])\n",
      "(tensor([0.6852, 0.2267, 0.0108, 0.0084, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ';', ' -'])\n",
      "(tensor([0.5109, 0.1022, 0.0429, 0.0368, 0.0224], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' so', ' although'])\n",
      "(tensor([0.2145, 0.1537, 0.1311, 0.0751, 0.0636], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' not', ' that'])\n",
      "(tensor([0.2091, 0.1462, 0.0890, 0.0646, 0.0555], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' plot', ' acting', ' funny'])\n",
      "(tensor([0.8559, 0.0313, 0.0215, 0.0127, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' itself', ' had'])\n",
      "(tensor([9.9619e-01, 1.4111e-03, 4.2526e-04, 2.5777e-04, 1.8022e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2207, 0.1157, 0.1050, 0.0678, 0.0628], grad_fn=<ToCopyBackward0>), [' that', ' too', ' very', ' funny', ' as'])\n",
      "(tensor([0.5210, 0.1369, 0.0988, 0.0486, 0.0263], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' in', ' and'])\n",
      "(tensor([0.2450, 0.1894, 0.1603, 0.0314, 0.0284], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' That', ' There'])\n",
      "(tensor([0.1065, 0.0784, 0.0778, 0.0720, 0.0636], grad_fn=<ToCopyBackward0>), [' thought', ' don', ' think', ' just', ' was'])\n",
      "(tensor([0.1916, 0.1329, 0.0880, 0.0686, 0.0452], grad_fn=<ToCopyBackward0>), [' very', ' really', ' expecting', ' not', ' looking'])\n",
      "(tensor([0.2818, 0.1313, 0.1047, 0.0730, 0.0598], grad_fn=<ToCopyBackward0>), [' a', ' it', ' more', ' something', ' to'])\n",
      "(tensor([0.2974, 0.1833, 0.0555, 0.0468, 0.0324], grad_fn=<ToCopyBackward0>), [' lot', ' movie', ' comedy', ' funny', ' very'])\n",
      "(tensor([0.6471, 0.2721, 0.0339, 0.0101, 0.0041], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ',', ' film'])\n",
      "(tensor([0.3735, 0.2548, 0.0950, 0.0384, 0.0292], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', ' because', ' to'])\n",
      "(tensor([0.5161, 0.1176, 0.0881, 0.0237, 0.0139], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' stupid', ' lots', ' bad'])\n",
      "(tensor([0.2839, 0.2067, 0.0899, 0.0640, 0.0436], grad_fn=<ToCopyBackward0>), [' plot', ' plots', ' characters', ' jokes', ' people'])\n",
      "(tensor([0.5923, 0.1939, 0.0450, 0.0232, 0.0169], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' because'])\n",
      "(tensor([0.5972, 0.0872, 0.0751, 0.0714, 0.0318], grad_fn=<ToCopyBackward0>), [' but', ' and', ' not', ' like', ' instead'])\n",
      "(tensor([0.2891, 0.2091, 0.1062, 0.0922, 0.0395], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' I', ' that'])\n",
      "(tensor([0.3510, 0.2186, 0.1448, 0.1281, 0.0680], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' didn', \"'s\", ' plot'])\n",
      "(tensor([0.9103, 0.0129, 0.0095, 0.0065, 0.0064], grad_fn=<ToCopyBackward0>), [' not', ' just', ' the', ' a', ' nowhere'])\n",
      "(tensor([0.5398, 0.1557, 0.0848, 0.0691, 0.0388], grad_fn=<ToCopyBackward0>), [' the', ' it', ' to', ' what', ' at'])\n",
      "(tensor([0.8954, 0.0452, 0.0141, 0.0136, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', '...', '!'])\n",
      "(tensor([0.2285, 0.1997, 0.1916, 0.0294, 0.0275], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1422, 0.0895, 0.0699, 0.0580, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' think', ' don', ' thought', ' really'])\n",
      "(tensor([0.7116, 0.0518, 0.0403, 0.0302, 0.0214], grad_fn=<ToCopyBackward0>), [' expecting', ' very', ' not', ' really', ' hoping'])\n",
      "(tensor([0.5156, 0.0652, 0.0550, 0.0460, 0.0457], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' something', ' some', ' it'])\n",
      "(tensor([0.3259, 0.2854, 0.0830, 0.0458, 0.0157], grad_fn=<ToCopyBackward0>), [' funny', ' movie', ' comedy', ' good', ' really'])\n",
      "(tensor([0.8299, 0.0653, 0.0291, 0.0175, 0.0066], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ' comedy', ','])\n",
      "(tensor([0.5304, 0.1607, 0.0859, 0.0729, 0.0182], grad_fn=<ToCopyBackward0>), [' with', ',', '.', ' that', ' about'])\n",
      "(tensor([0.8643, 0.0507, 0.0355, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' funny', ' stupid', ' a', ' good', ' some'])\n",
      "(tensor([0.8404, 0.0483, 0.0276, 0.0101, 0.0077], grad_fn=<ToCopyBackward0>), [' plot', ' plots', ' characters', ' story', ' actors'])\n",
      "(tensor([0.6931, 0.1157, 0.0454, 0.0265, 0.0235], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' with'])\n",
      "(tensor([0.1794, 0.1457, 0.1230, 0.1023, 0.0699], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' That', ' But'])\n",
      "(tensor([0.3787, 0.2773, 0.1017, 0.0600, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' didn', \"'s\", ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was so atrocious it was funny. I was actually really impressed with the fact that the film was so awful it was funny. It's one of those movies that you can sit through and enjoy. I'm sure it will be a classic\n",
      "(tensor([0.3845, 0.1715, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2432, 0.1963, 0.0167, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7759, 0.0505, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1273, 0.0769, 0.0659, 0.0556, 0.0420], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4445, 0.0487, 0.0405, 0.0301, 0.0287], grad_fn=<ToCopyBackward0>), [' bad', ' atro', ' awful', ' boring', ' terrible'])\n",
      "(tensor([9.9990e-01, 6.4446e-06, 6.0951e-06, 3.7859e-06, 3.6254e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'ct', 'etic', 'per', 'cul'])\n",
      "(tensor([0.3432, 0.1034, 0.1026, 0.0758, 0.0721], grad_fn=<ToCopyBackward0>), [' that', ' it', '.', ',', ' I'])\n",
      "(tensor([0.5417, 0.0787, 0.0516, 0.0412, 0.0329], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' should', ' must'])\n",
      "(tensor([0.5671, 0.0780, 0.0322, 0.0231, 0.0227], grad_fn=<ToCopyBackward0>), [' funny', ' almost', ' actually', ' hilarious', ' good'])\n",
      "(tensor([0.6847, 0.0745, 0.0438, 0.0195, 0.0177], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' to', '...'])\n",
      "(tensor([0.2294, 0.1389, 0.1027, 0.0259, 0.0162], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' Not'])\n",
      "(tensor([0.1093, 0.0615, 0.0550, 0.0517, 0.0432], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' can', ' really'])\n",
      "(tensor([0.1796, 0.1361, 0.0849, 0.0644, 0.0361], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' so', ' in', ' very'])\n",
      "(tensor([0.2011, 0.0778, 0.0672, 0.0518, 0.0460], grad_fn=<ToCopyBackward0>), [' really', ' looking', ' very', ' in', ' surprised'])\n",
      "(tensor([0.4430, 0.1172, 0.0703, 0.0265, 0.0264], grad_fn=<ToCopyBackward0>), [' looking', ' surprised', ' disappointed', ' interested', ' impressed'])\n",
      "(tensor([0.5666, 0.1539, 0.0669, 0.0440, 0.0430], grad_fn=<ToCopyBackward0>), [' by', ' with', ' at', '.', ' that'])\n",
      "(tensor([0.4707, 0.2466, 0.1008, 0.0368, 0.0129], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' some'])\n",
      "(tensor([0.0548, 0.0478, 0.0472, 0.0452, 0.0368], grad_fn=<ToCopyBackward0>), [' original', ' fact', ' acting', ' story', ' premise'])\n",
      "(tensor([0.9181, 0.0212, 0.0108, 0.0061, 0.0051], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' I', ' they'])\n",
      "(tensor([0.1835, 0.1800, 0.1424, 0.0720, 0.0532], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' they', ' this'])\n",
      "(tensor([0.0775, 0.0721, 0.0627, 0.0609, 0.0544], grad_fn=<ToCopyBackward0>), [' movie', ' director', ' film', ' guy', ' people'])\n",
      "(tensor([0.4062, 0.1270, 0.0454, 0.0447, 0.0363], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' did', ' has'])\n",
      "(tensor([0.5338, 0.1722, 0.0204, 0.0187, 0.0151], grad_fn=<ToCopyBackward0>), [' so', ' made', ' able', ' as', ' even'])\n",
      "(tensor([0.3195, 0.0416, 0.0293, 0.0270, 0.0178], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' low', ' awful', ' terrible'])\n",
      "(tensor([0.2936, 0.2913, 0.0914, 0.0828, 0.0414], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' it', ' and'])\n",
      "(tensor([0.7747, 0.0743, 0.0229, 0.0176, 0.0114], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' had', ' could', ' made'])\n",
      "(tensor([0.8211, 0.0294, 0.0233, 0.0118, 0.0083], grad_fn=<ToCopyBackward0>), [' funny', ' hilarious', ' actually', ' a', ' watch'])\n",
      "(tensor([0.8068, 0.0458, 0.0295, 0.0159, 0.0095], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', ' ('])\n",
      "(tensor([0.2310, 0.1048, 0.0938, 0.0367, 0.0312], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' That'])\n",
      "(tensor([0.3205, 0.2379, 0.0508, 0.0439, 0.0290], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' seemed'])\n",
      "(tensor([0.1452, 0.1355, 0.0991, 0.0755, 0.0494], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' just', ' one'])\n",
      "(tensor([0.8030, 0.1301, 0.0126, 0.0106, 0.0034], grad_fn=<ToCopyBackward0>), [' of', ' thing', ' reason', ' big', ' the'])\n",
      "(tensor([0.8375, 0.1266, 0.0255, 0.0031, 0.0009], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' a'])\n",
      "(tensor([0.7762, 0.1796, 0.0034, 0.0033, 0.0020], grad_fn=<ToCopyBackward0>), [' films', ' movies', ' rare', ' film', ' cases'])\n",
      "(tensor([0.5947, 0.1560, 0.0991, 0.0800, 0.0144], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', ' like'])\n",
      "(tensor([0.1989, 0.1565, 0.1127, 0.0645, 0.0507], grad_fn=<ToCopyBackward0>), [' you', ' I', ' is', \"'s\", ' people'])\n",
      "(tensor([0.1442, 0.1332, 0.0670, 0.0403, 0.0351], grad_fn=<ToCopyBackward0>), [' can', ' watch', ' just', ' think', ' sit'])\n",
      "(tensor([0.3812, 0.1982, 0.0743, 0.0530, 0.0323], grad_fn=<ToCopyBackward0>), [' watch', \"'t\", ' tell', ' sit', ' see'])\n",
      "(tensor([0.5119, 0.1576, 0.1012, 0.0725, 0.0509], grad_fn=<ToCopyBackward0>), [' through', ' down', ' there', ' and', ' around'])\n",
      "(tensor([0.3782, 0.0681, 0.0530, 0.0454, 0.0408], grad_fn=<ToCopyBackward0>), [' and', ' in', ',', ' all', ' just'])\n",
      "(tensor([0.2731, 0.0679, 0.0634, 0.0498, 0.0489], grad_fn=<ToCopyBackward0>), [' enjoy', ' think', ' be', ' laugh', ' it'])\n",
      "(tensor([0.1647, 0.1242, 0.1228, 0.0735, 0.0398], grad_fn=<ToCopyBackward0>), ['.', ',', ' the', ' it', ' because'])\n",
      "(tensor([0.1965, 0.1699, 0.0612, 0.0610, 0.0476], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' You', ' But'])\n",
      "(tensor([0.1125, 0.0635, 0.0607, 0.0596, 0.0559], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' just', ' don', ' really'])\n",
      "(tensor([0.3603, 0.1499, 0.0909, 0.0449, 0.0313], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' really'])\n",
      "(tensor([0.1545, 0.1473, 0.1324, 0.1321, 0.0409], grad_fn=<ToCopyBackward0>), [' there', ' that', ' it', ' the', ' I'])\n",
      "(tensor([0.5146, 0.1616, 0.1105, 0.0338, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' will', ' would', \"'ll\"])\n",
      "(tensor([0.3802, 0.0711, 0.0615, 0.0564, 0.0518], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' entertain', ' appeal'])\n",
      "(tensor([0.1165, 0.0931, 0.0544, 0.0438, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' on', ' nominated', ' remembered', ' shown'])\n",
      "(tensor([0.1029, 0.0892, 0.0662, 0.0528, 0.0459], grad_fn=<ToCopyBackward0>), [' hit', ' cult', ' big', ' little', ' classic'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty atrocious that I was subjected to this film in public. I have never been so disappointed to have been so disappointed to have been so disappointed to have been so disappointed. I can see that this is a very sensitive subject for the parents\n",
      "(tensor([0.3838, 0.1719, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7136, 0.1163, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1838, 0.1459, 0.0506, 0.0453, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1138, 0.0956, 0.0750, 0.0608, 0.0494], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([9.9941e-01, 5.8884e-05, 4.6899e-05, 2.4495e-05, 1.5529e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'pic', 'c'])\n",
      "(tensor([0.1344, 0.0910, 0.0805, 0.0621, 0.0619], grad_fn=<ToCopyBackward0>), [' and', ' that', ',', ' how', '.'])\n",
      "(tensor([0.1816, 0.1330, 0.1084, 0.0768, 0.0357], grad_fn=<ToCopyBackward0>), [' I', ' they', ' the', ' this', ' a'])\n",
      "(tensor([0.1319, 0.1161, 0.1071, 0.0590, 0.0409], grad_fn=<ToCopyBackward0>), [' watched', ' had', ' was', \"'m\", ' even'])\n",
      "(tensor([0.1357, 0.0436, 0.0342, 0.0277, 0.0256], grad_fn=<ToCopyBackward0>), [' subjected', ' forced', ' the', ' made', ' actually'])\n",
      "(tensor([9.9461e-01, 8.7422e-04, 5.9257e-04, 3.8287e-04, 3.3444e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' in', ' as', ' the', ' this'])\n",
      "(tensor([0.6984, 0.1237, 0.0333, 0.0188, 0.0166], grad_fn=<ToCopyBackward0>), [' this', ' the', ' such', ' that', ' a'])\n",
      "(tensor([0.3116, 0.1786, 0.0189, 0.0141, 0.0093], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' atroc', ' torture', ' humiliation'])\n",
      "(tensor([0.4230, 0.1005, 0.0910, 0.0714, 0.0288], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' for', ' with'])\n",
      "(tensor([0.3126, 0.1739, 0.1428, 0.0380, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' school', ' public'])\n",
      "(tensor([0.3197, 0.2288, 0.0308, 0.0306, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ' school', ' like', ' and', ' as'])\n",
      "(tensor([0.2636, 0.1293, 0.0762, 0.0456, 0.0206], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1481, 0.0797, 0.0693, 0.0657, 0.0589], grad_fn=<ToCopyBackward0>), [' was', ' mean', \"'m\", ' have', ' don'])\n",
      "(tensor([0.2914, 0.1982, 0.0664, 0.0459, 0.0442], grad_fn=<ToCopyBackward0>), [' to', ' a', ' never', ' no', ' been'])\n",
      "(tensor([0.3339, 0.1859, 0.0474, 0.0400, 0.0399], grad_fn=<ToCopyBackward0>), [' been', ' seen', ' had', ' in', ' walked'])\n",
      "(tensor([0.5312, 0.0708, 0.0691, 0.0475, 0.0469], grad_fn=<ToCopyBackward0>), [' so', ' more', ' in', ' a', ' to'])\n",
      "(tensor([0.3574, 0.1062, 0.0802, 0.0693, 0.0597], grad_fn=<ToCopyBackward0>), [' embarrassed', ' disappointed', ' insulted', ' offended', ' bored'])\n",
      "(tensor([0.6948, 0.0753, 0.0704, 0.0591, 0.0228], grad_fn=<ToCopyBackward0>), [' in', ' with', ' to', ' by', '.'])\n",
      "(tensor([0.4021, 0.4010, 0.0690, 0.0376, 0.0166], grad_fn=<ToCopyBackward0>), [' be', ' see', ' watch', ' have', ' go'])\n",
      "(tensor([0.1835, 0.1498, 0.1213, 0.0679, 0.0556], grad_fn=<ToCopyBackward0>), [' to', ' a', ' been', ' my', ' so'])\n",
      "(tensor([0.3397, 0.1412, 0.1087, 0.0559, 0.0222], grad_fn=<ToCopyBackward0>), [' in', ' so', ' a', ' subjected', ' on'])\n",
      "(tensor([0.1334, 0.1086, 0.0309, 0.0275, 0.0249], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ' publicly', ' completely', ' mis'])\n",
      "(tensor([0.6601, 0.0983, 0.0793, 0.0512, 0.0240], grad_fn=<ToCopyBackward0>), [' in', '.', ' with', ' by', ' to'])\n",
      "(tensor([0.3650, 0.3297, 0.1183, 0.0816, 0.0166], grad_fn=<ToCopyBackward0>), [' have', ' be', ' see', ' watch', ' sit'])\n",
      "(tensor([0.5061, 0.1373, 0.0552, 0.0402, 0.0316], grad_fn=<ToCopyBackward0>), [' been', ' watched', ' to', ' had', ' so'])\n",
      "(tensor([0.4316, 0.1469, 0.1394, 0.0369, 0.0130], grad_fn=<ToCopyBackward0>), [' so', ' subjected', ' in', ' made', ' to'])\n",
      "(tensor([0.9162, 0.0076, 0.0041, 0.0034, 0.0029], grad_fn=<ToCopyBackward0>), [' disappointed', ' misled', ' dis', ' disgusted', ' frustrated'])\n",
      "(tensor([0.3489, 0.2883, 0.1429, 0.0534, 0.0315], grad_fn=<ToCopyBackward0>), [' in', ' to', '.', ' by', ' with'])\n",
      "(tensor([0.5344, 0.2460, 0.0745, 0.0690, 0.0128], grad_fn=<ToCopyBackward0>), [' have', ' be', ' watch', ' see', ' sit'])\n",
      "(tensor([0.6397, 0.1402, 0.0307, 0.0286, 0.0213], grad_fn=<ToCopyBackward0>), [' been', ' watched', ' to', ' seen', ' had'])\n",
      "(tensor([0.6551, 0.0544, 0.0442, 0.0213, 0.0185], grad_fn=<ToCopyBackward0>), [' so', ' in', ' subjected', ' made', ' to'])\n",
      "(tensor([9.8662e-01, 1.5189e-03, 1.1477e-03, 9.3187e-04, 8.2601e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' disappointed', ' disgusted', ' ashamed', ' frustrated', ' angry'])\n",
      "(tensor([0.4793, 0.1786, 0.1615, 0.0228, 0.0216], grad_fn=<ToCopyBackward0>), [' to', '.', ' in', '...', ' as'])\n",
      "(tensor([0.2364, 0.1238, 0.0730, 0.0664, 0.0404], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1419, 0.1044, 0.0654, 0.0582, 0.0538], grad_fn=<ToCopyBackward0>), [' was', ' have', ' am', \"'m\", ' can'])\n",
      "(tensor([0.3086, 0.2562, 0.0952, 0.0427, 0.0255], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' honestly', ' see', ' not'])\n",
      "(tensor([0.1765, 0.1599, 0.1249, 0.1000, 0.0814], grad_fn=<ToCopyBackward0>), [' why', ' how', ' that', ' the', ' now'])\n",
      "(tensor([0.2423, 0.1087, 0.0797, 0.0736, 0.0507], grad_fn=<ToCopyBackward0>), [' the', ' there', ' it', ' in', ' this'])\n",
      "(tensor([0.3639, 0.2706, 0.1529, 0.0886, 0.0257], grad_fn=<ToCopyBackward0>), [' is', ' film', ' movie', ' was', ' has'])\n",
      "(tensor([0.4854, 0.0978, 0.0805, 0.0755, 0.0272], grad_fn=<ToCopyBackward0>), [' a', ' an', ' one', ' the', ' not'])\n",
      "(tensor([0.1426, 0.0690, 0.0366, 0.0277, 0.0193], grad_fn=<ToCopyBackward0>), [' very', ' movie', ' film', ' commercial', ' well'])\n",
      "(tensor([0.0338, 0.0336, 0.0324, 0.0319, 0.0253], grad_fn=<ToCopyBackward0>), [' well', ' low', ' difficult', ' personal', ' sensitive'])\n",
      "(tensor([0.2152, 0.1862, 0.1253, 0.0987, 0.0946], grad_fn=<ToCopyBackward0>), [' subject', ' and', ' film', ' movie', ' time'])\n",
      "(tensor([0.6036, 0.1674, 0.0523, 0.0333, 0.0202], grad_fn=<ToCopyBackward0>), [' for', ' and', ',', ' matter', ' to'])\n",
      "(tensor([0.1439, 0.0977, 0.0750, 0.0708, 0.0624], grad_fn=<ToCopyBackward0>), [' people', ' a', ' the', ' you', ' many'])\n",
      "(tensor([0.0647, 0.0635, 0.0379, 0.0339, 0.0336], grad_fn=<ToCopyBackward0>), [' parents', ' people', ' public', ' rest', ' many'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty bad. This is a good movie for those of you who really enjoyed the original. I really like the original story and I think this one is better. I really like the casting for this one and I think it is a much\n",
      "(tensor([0.3836, 0.1717, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5011, 0.0599, 0.0341, 0.0151, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6235, 0.0400, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2583, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1974, 0.1237, 0.1049, 0.0783, 0.0677], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.5078, 0.1546, 0.0558, 0.0269, 0.0237], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.2509, 0.1303, 0.1200, 0.0228, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.3734, 0.2332, 0.1453, 0.0430, 0.0253], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' one', ' film'])\n",
      "(tensor([0.1742, 0.1618, 0.1522, 0.0571, 0.0485], grad_fn=<ToCopyBackward0>), [' not', ' a', ' the', ' probably', ' one'])\n",
      "(tensor([0.2110, 0.1427, 0.0706, 0.0512, 0.0471], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' good', ' pretty', ' very'])\n",
      "(tensor([0.7469, 0.0635, 0.0221, 0.0107, 0.0087], grad_fn=<ToCopyBackward0>), [' movie', ' cast', ' film', ' family', ' thing'])\n",
      "(tensor([0.1692, 0.1351, 0.1252, 0.0980, 0.0941], grad_fn=<ToCopyBackward0>), ['.', ' if', ' for', ',', ' to'])\n",
      "(tensor([0.6562, 0.0415, 0.0408, 0.0250, 0.0246], grad_fn=<ToCopyBackward0>), [' kids', ' the', ' teenagers', ' adults', ' those'])\n",
      "(tensor([0.3941, 0.3246, 0.0545, 0.0417, 0.0245], grad_fn=<ToCopyBackward0>), [' who', ' that', ' people', ' of', ' with'])\n",
      "(tensor([8.6014e-01, 1.3254e-01, 9.6678e-04, 7.2076e-04, 7.1414e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' us', ' those', ' my', ' the'])\n",
      "(tensor([0.6223, 0.2469, 0.0248, 0.0115, 0.0093], grad_fn=<ToCopyBackward0>), [' who', ' that', ' with', ' looking', ','])\n",
      "(tensor([0.2912, 0.1147, 0.0873, 0.0713, 0.0665], grad_fn=<ToCopyBackward0>), [' like', ' enjoy', ' have', ' really', ' want'])\n",
      "(tensor([0.2080, 0.2049, 0.1772, 0.0618, 0.0585], grad_fn=<ToCopyBackward0>), [' enjoy', ' want', ' like', ' enjoyed', ' hate'])\n",
      "(tensor([0.3158, 0.0810, 0.0446, 0.0324, 0.0195], grad_fn=<ToCopyBackward0>), [' the', ' it', ' \"', ' The', ' this'])\n",
      "(tensor([0.2141, 0.2081, 0.1964, 0.0269, 0.0198], grad_fn=<ToCopyBackward0>), [' original', ' first', ' book', ' movie', ' other'])\n",
      "(tensor([0.7359, 0.0372, 0.0352, 0.0203, 0.0109], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' movie', ' with'])\n",
      "(tensor([0.2762, 0.1324, 0.1206, 0.0398, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.0747, 0.0730, 0.0614, 0.0499, 0.0474], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' really', ' just', ' think'])\n",
      "(tensor([0.0957, 0.0907, 0.0847, 0.0774, 0.0613], grad_fn=<ToCopyBackward0>), [' like', ' don', ' enjoyed', ' didn', ' liked'])\n",
      "(tensor([0.4387, 0.0704, 0.0154, 0.0149, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' \"'])\n",
      "(tensor([0.0989, 0.0923, 0.0749, 0.0566, 0.0438], grad_fn=<ToCopyBackward0>), [' idea', ' first', ' story', ' new', ' original'])\n",
      "(tensor([0.1745, 0.0488, 0.0422, 0.0417, 0.0403], grad_fn=<ToCopyBackward0>), [' story', ' but', ' too', '.', ' movie'])\n",
      "(tensor([0.2864, 0.0850, 0.0707, 0.0607, 0.0416], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', ' as', ' of'])\n",
      "(tensor([0.1106, 0.1068, 0.0823, 0.0389, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' the', ' movie', ' plot', ' director'])\n",
      "(tensor([0.1861, 0.1567, 0.0920, 0.0649, 0.0538], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' think', ' was', ' like'])\n",
      "(tensor([0.3805, 0.1655, 0.1550, 0.1066, 0.0187], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' that', ' they'])\n",
      "(tensor([0.3385, 0.2941, 0.1221, 0.0545, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' one', ' was', ' film'])\n",
      "(tensor([0.3746, 0.1438, 0.0575, 0.0488, 0.0368], grad_fn=<ToCopyBackward0>), [' is', ' was', ' could', ' has', ' does'])\n",
      "(tensor([0.4090, 0.1603, 0.0522, 0.0509, 0.0277], grad_fn=<ToCopyBackward0>), [' better', ' a', ' more', ' pretty', ' much'])\n",
      "(tensor([0.6762, 0.1645, 0.0276, 0.0261, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' but', ' in'])\n",
      "(tensor([0.2489, 0.1184, 0.0995, 0.0517, 0.0285], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' There'])\n",
      "(tensor([0.1085, 0.0620, 0.0600, 0.0569, 0.0557], grad_fn=<ToCopyBackward0>), [' think', ' really', \"'m\", ' just', ' don'])\n",
      "(tensor([0.1888, 0.0978, 0.0680, 0.0640, 0.0596], grad_fn=<ToCopyBackward0>), [' like', ' don', ' wish', ' liked', ' think'])\n",
      "(tensor([0.6321, 0.0254, 0.0159, 0.0106, 0.0097], grad_fn=<ToCopyBackward0>), [' the', ' it', ' how', ' this', ' that'])\n",
      "(tensor([0.0875, 0.0633, 0.0573, 0.0537, 0.0457], grad_fn=<ToCopyBackward0>), [' casting', ' cast', ' story', ' acting', ' new'])\n",
      "(tensor([0.2738, 0.1318, 0.1007, 0.0566, 0.0465], grad_fn=<ToCopyBackward0>), [' of', ' in', ' and', ' too', ' for'])\n",
      "(tensor([0.3788, 0.1435, 0.0187, 0.0079, 0.0077], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' G', ' Jessica'])\n",
      "(tensor([0.7010, 0.1831, 0.0345, 0.0283, 0.0179], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' and', '.', ','])\n",
      "(tensor([0.4327, 0.2300, 0.0798, 0.0694, 0.0642], grad_fn=<ToCopyBackward0>), [' too', '.', ' as', ',', ' and'])\n",
      "(tensor([0.4664, 0.2025, 0.0545, 0.0194, 0.0085], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' they', ' that'])\n",
      "(tensor([0.3213, 0.1939, 0.0997, 0.0439, 0.0276], grad_fn=<ToCopyBackward0>), [' think', ' really', ' thought', ' like', ' hope'])\n",
      "(tensor([0.3384, 0.3355, 0.0716, 0.0391, 0.0303], grad_fn=<ToCopyBackward0>), [' it', ' the', ' they', ' that', ' this'])\n",
      "(tensor([0.2850, 0.1752, 0.1660, 0.0734, 0.0211], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' will'])\n",
      "(tensor([0.2311, 0.2065, 0.1165, 0.0559, 0.0470], grad_fn=<ToCopyBackward0>), [' a', ' better', ' pretty', ' more', ' the'])\n",
      "(tensor([0.2433, 0.1814, 0.1118, 0.1057, 0.0571], grad_fn=<ToCopyBackward0>), [' good', ' better', ' great', ' pretty', ' much'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty bad, but I just didn't think it was as bad as this movie. The acting is terrible, the plot is ridiculous, the script is ridiculous. This movie is just stupid. I was really surprised by it. It's just\n",
      "(tensor([0.3825, 0.1727, 0.0906, 0.0771, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1161, 0.0394, 0.0101, 0.0086], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1831, 0.1470, 0.0505, 0.0452, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1140, 0.0957, 0.0751, 0.0602, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.1290, 0.0995, 0.0796, 0.0609, 0.0476], grad_fn=<ToCopyBackward0>), [' when', ' that', '.', ',', ' news'])\n",
      "(tensor([0.3819, 0.0675, 0.0559, 0.0358, 0.0186], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' so', ' as'])\n",
      "(tensor([0.4010, 0.0784, 0.0411, 0.0390, 0.0324], grad_fn=<ToCopyBackward0>), [' I', ' it', ' after', ' then', ' the'])\n",
      "(tensor([0.1265, 0.0675, 0.0664, 0.0522, 0.0514], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' guess', \"'m\", ' just'])\n",
      "(tensor([0.1612, 0.0840, 0.0643, 0.0513, 0.0460], grad_fn=<ToCopyBackward0>), [' thought', ' didn', ' don', ' couldn', ' laughed'])\n",
      "(tensor([9.9392e-01, 2.5724e-03, 7.3540e-04, 4.6992e-04, 2.1310e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.2744, 0.1587, 0.0983, 0.0870, 0.0526], grad_fn=<ToCopyBackward0>), [' think', ' know', ' want', ' expect', ' have'])\n",
      "(tensor([0.7748, 0.0523, 0.0341, 0.0242, 0.0151], grad_fn=<ToCopyBackward0>), [' it', ' anything', ' that', ' they', ' there'])\n",
      "(tensor([0.6860, 0.1732, 0.1196, 0.0029, 0.0019], grad_fn=<ToCopyBackward0>), [' was', ' could', ' would', ' had', \"'d\"])\n",
      "(tensor([0.3436, 0.2614, 0.0415, 0.0362, 0.0296], grad_fn=<ToCopyBackward0>), [' as', ' going', ' funny', ' that', ' so'])\n",
      "(tensor([9.8072e-01, 3.2749e-03, 1.6218e-03, 9.0261e-04, 7.2556e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' bad', ' good', ' big', ' much', ' g'])\n",
      "(tensor([9.9283e-01, 8.8062e-04, 8.5605e-04, 8.1619e-04, 4.1351e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' as', ' a', ' it', ' or', ' in'])\n",
      "(tensor([0.4878, 0.1408, 0.1043, 0.0372, 0.0346], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' some'])\n",
      "(tensor([0.5142, 0.0740, 0.0671, 0.0653, 0.0080], grad_fn=<ToCopyBackward0>), [' movie', ' one', '.', ' film', '....'])\n",
      "(tensor([0.5973, 0.0809, 0.0678, 0.0430, 0.0346], grad_fn=<ToCopyBackward0>), ['.', ' is', ' was', ' makes', '....'])\n",
      "(tensor([0.3488, 0.1170, 0.0939, 0.0590, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' And'])\n",
      "(tensor([0.2233, 0.2183, 0.0524, 0.0356, 0.0229], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' story', ' only', ' worst'])\n",
      "(tensor([0.5495, 0.1604, 0.1023, 0.0571, 0.0291], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' in', ' and'])\n",
      "(tensor([0.1191, 0.1041, 0.0854, 0.0713, 0.0704], grad_fn=<ToCopyBackward0>), [' terrible', ' pretty', ' so', ' bad', ' horrible'])\n",
      "(tensor([0.5574, 0.1742, 0.1342, 0.0394, 0.0112], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' in', ';'])\n",
      "(tensor([0.6140, 0.1070, 0.0759, 0.0159, 0.0153], grad_fn=<ToCopyBackward0>), [' the', ' but', ' and', ' especially', ' I'])\n",
      "(tensor([0.3787, 0.2168, 0.0895, 0.0222, 0.0187], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' writing', ' directing'])\n",
      "(tensor([0.6185, 0.0510, 0.0426, 0.0205, 0.0193], grad_fn=<ToCopyBackward0>), [' is', ' was', ' makes', ' and', ','])\n",
      "(tensor([0.1003, 0.0894, 0.0618, 0.0482, 0.0423], grad_fn=<ToCopyBackward0>), [' ridiculous', ' terrible', ' even', ' just', ' stupid'])\n",
      "(tensor([0.4276, 0.4093, 0.0929, 0.0168, 0.0071], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.4886, 0.3552, 0.0296, 0.0218, 0.0198], grad_fn=<ToCopyBackward0>), [' and', ' the', ' but', ' it', ' there'])\n",
      "(tensor([0.1751, 0.0600, 0.0452, 0.0431, 0.0409], grad_fn=<ToCopyBackward0>), [' special', ' editing', ' ending', ' script', ' acting'])\n",
      "(tensor([0.7277, 0.0484, 0.0321, 0.0180, 0.0161], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' makes', ' and'])\n",
      "(tensor([0.1567, 0.0601, 0.0488, 0.0443, 0.0413], grad_fn=<ToCopyBackward0>), [' ridiculous', ' terrible', ' awful', ' stupid', ' just'])\n",
      "(tensor([0.5285, 0.2214, 0.1700, 0.0135, 0.0110], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' ('])\n",
      "(tensor([0.2720, 0.1334, 0.1303, 0.0405, 0.0298], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.4207, 0.3291, 0.0640, 0.0239, 0.0217], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' thing', ' has'])\n",
      "(tensor([0.3904, 0.0942, 0.0677, 0.0584, 0.0277], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' should'])\n",
      "(tensor([0.2509, 0.1483, 0.0951, 0.0619, 0.0395], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' not', ' like'])\n",
      "(tensor([0.1756, 0.0674, 0.0605, 0.0497, 0.0496], grad_fn=<ToCopyBackward0>), [' a', ' terrible', ' so', ' ridiculous', ' stupid'])\n",
      "(tensor([0.3162, 0.2921, 0.1500, 0.0413, 0.0329], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' as'])\n",
      "(tensor([0.2461, 0.1262, 0.1175, 0.0259, 0.0245], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' And'])\n",
      "(tensor([0.0981, 0.0864, 0.0830, 0.0558, 0.0479], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' don', ' mean', ' was'])\n",
      "(tensor([0.2211, 0.1167, 0.0416, 0.0348, 0.0323], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' just', ' very', ' so'])\n",
      "(tensor([0.3002, 0.1691, 0.1380, 0.0518, 0.0303], grad_fn=<ToCopyBackward0>), [' surprised', ' disappointed', ' looking', ',', ' hoping'])\n",
      "(tensor([0.2744, 0.1854, 0.1495, 0.0966, 0.0851], grad_fn=<ToCopyBackward0>), [' by', ' at', ' that', ' when', ' to'])\n",
      "(tensor([0.2300, 0.2116, 0.1752, 0.1646, 0.1567], grad_fn=<ToCopyBackward0>), [' the', ' this', ' how', ' it', ' that'])\n",
      "(tensor([0.6165, 0.1483, 0.0642, 0.0182, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' too', ' when'])\n",
      "(tensor([0.4631, 0.1288, 0.0592, 0.0243, 0.0190], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.4772, 0.1149, 0.0818, 0.0320, 0.0226], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' just'])\n",
      "(tensor([0.2541, 0.1551, 0.0927, 0.0700, 0.0508], grad_fn=<ToCopyBackward0>), [' not', ' just', ' like', ' a', ' one'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was a good movie. I thought this movie was pretty funny and pretty interesting and I really liked it and that's what counts I guess. I mean, if you have to give it a rating... I guess it's a 3. It\n",
      "(tensor([0.3840, 0.1721, 0.0902, 0.0769, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4369, 0.2442, 0.1965, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7770, 0.0502, 0.0283, 0.0262, 0.0100], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.0788, 0.0623, 0.0522, 0.0454, 0.0446], grad_fn=<ToCopyBackward0>), [' very', ' waste', ' good', ' great', ' really'])\n",
      "(tensor([0.1428, 0.1274, 0.1010, 0.0736, 0.0526], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' film', ' way'])\n",
      "(tensor([0.3473, 0.0974, 0.0895, 0.0643, 0.0426], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', '!'])\n",
      "(tensor([0.3258, 0.2215, 0.0889, 0.0332, 0.0159], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.1989, 0.1136, 0.0601, 0.0521, 0.0468], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' just'])\n",
      "(tensor([0.5857, 0.1807, 0.0680, 0.0612, 0.0097], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3818, 0.3420, 0.1154, 0.0523, 0.0084], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' is', ' one'])\n",
      "(tensor([0.7894, 0.0359, 0.0158, 0.0149, 0.0143], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' did', ' could'])\n",
      "(tensor([0.1701, 0.0884, 0.0710, 0.0474, 0.0457], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' boring', ' funny', ' bad'])\n",
      "(tensor([0.4902, 0.1040, 0.0665, 0.0388, 0.0377], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' cool', ' bad', ' entertaining'])\n",
      "(tensor([0.5999, 0.1552, 0.0979, 0.0198, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' too', ' but'])\n",
      "(tensor([0.3388, 0.2381, 0.0553, 0.0441, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' pretty', ' interesting', ' the', ' funny'])\n",
      "(tensor([0.1896, 0.0987, 0.0887, 0.0617, 0.0329], grad_fn=<ToCopyBackward0>), [' entertaining', ' interesting', ' funny', ' good', ' well'])\n",
      "(tensor([0.4280, 0.3663, 0.0773, 0.0177, 0.0173], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' but'])\n",
      "(tensor([0.3605, 0.1531, 0.0689, 0.0385, 0.0344], grad_fn=<ToCopyBackward0>), [' pretty', ' I', ' interesting', ' the', ' good'])\n",
      "(tensor([0.3856, 0.1071, 0.0528, 0.0481, 0.0465], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' liked', ' was', ' think'])\n",
      "(tensor([0.1847, 0.1666, 0.1445, 0.0803, 0.0592], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' enjoyed', ' wanted', ' didn'])\n",
      "(tensor([0.3652, 0.2517, 0.0225, 0.0110, 0.0102], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' how', ' seeing'])\n",
      "(tensor([0.7226, 0.0834, 0.0313, 0.0280, 0.0158], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.6799, 0.0553, 0.0423, 0.0391, 0.0195], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' so'])\n",
      "(tensor([0.5167, 0.2680, 0.0637, 0.0113, 0.0104], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' it', ' made'])\n",
      "(tensor([0.2515, 0.2509, 0.0665, 0.0630, 0.0607], grad_fn=<ToCopyBackward0>), [' all', ' why', ' pretty', ' what', ' always'])\n",
      "(tensor([0.3629, 0.0548, 0.0436, 0.0415, 0.0412], grad_fn=<ToCopyBackward0>), [' I', ' really', ' kept', ' it', ' counts'])\n",
      "(tensor([0.6578, 0.0561, 0.0497, 0.0398, 0.0367], grad_fn=<ToCopyBackward0>), ['.', ',', ' for', ' in', ' I'])\n",
      "(tensor([0.8989, 0.0648, 0.0172, 0.0032, 0.0018], grad_fn=<ToCopyBackward0>), [' guess', ' suppose', ' think', ' don', ' really'])\n",
      "(tensor([0.7814, 0.0738, 0.0151, 0.0133, 0.0105], grad_fn=<ToCopyBackward0>), ['.', ',', ' I', '...', ' and'])\n",
      "(tensor([0.2260, 0.0921, 0.0601, 0.0407, 0.0306], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.0775, 0.0773, 0.0720, 0.0696, 0.0655], grad_fn=<ToCopyBackward0>), [' really', ' mean', \"'m\", ' was', ' just'])\n",
      "(tensor([0.1782, 0.1473, 0.1163, 0.0851, 0.0470], grad_fn=<ToCopyBackward0>), [',', ' it', ' I', ' the', ' if'])\n",
      "(tensor([0.2793, 0.1301, 0.0731, 0.0590, 0.0469], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' the', ' this'])\n",
      "(tensor([0.6691, 0.1204, 0.0852, 0.0214, 0.0172], grad_fn=<ToCopyBackward0>), [' you', ' I', ' it', ' people', ' the'])\n",
      "(tensor([0.1827, 0.1540, 0.0905, 0.0694, 0.0415], grad_fn=<ToCopyBackward0>), [' like', \"'re\", ' have', ' make', ' don'])\n",
      "(tensor([0.2765, 0.2426, 0.0478, 0.0393, 0.0326], grad_fn=<ToCopyBackward0>), [' to', ' a', ' an', ' something', ' the'])\n",
      "(tensor([0.3523, 0.0828, 0.0505, 0.0438, 0.0412], grad_fn=<ToCopyBackward0>), [' give', ' be', ' have', ' say', ' tell'])\n",
      "(tensor([0.2630, 0.1608, 0.1185, 0.0919, 0.0408], grad_fn=<ToCopyBackward0>), [' a', ' it', ' your', ' something', ' an'])\n",
      "(tensor([0.4540, 0.2603, 0.0738, 0.0436, 0.0202], grad_fn=<ToCopyBackward0>), [' a', ' away', ' to', ' an', ' some'])\n",
      "(tensor([0.1875, 0.0517, 0.0477, 0.0416, 0.0402], grad_fn=<ToCopyBackward0>), [' rating', ' 1', ' vote', ' 10', ' score'])\n",
      "(tensor([0.3897, 0.0854, 0.0696, 0.0636, 0.0558], grad_fn=<ToCopyBackward0>), [',', '...', ' I', ' you', ' it'])\n",
      "(tensor([0.0625, 0.0504, 0.0379, 0.0338, 0.0302], grad_fn=<ToCopyBackward0>), ['I', ' I', 'it', 'this', 'the'])\n",
      "(tensor([0.1429, 0.0998, 0.0730, 0.0659, 0.0552], grad_fn=<ToCopyBackward0>), [' mean', ' guess', ' don', \"'m\", ' really'])\n",
      "(tensor([0.2146, 0.2054, 0.0674, 0.0617, 0.0570], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' you', ','])\n",
      "(tensor([0.6243, 0.0734, 0.0442, 0.0336, 0.0196], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' would', ' could', ' has'])\n",
      "(tensor([0.6347, 0.0402, 0.0390, 0.0247, 0.0184], grad_fn=<ToCopyBackward0>), [' a', '...', ' like', ' one', ' gonna'])\n",
      "(tensor([0.3324, 0.0858, 0.0810, 0.0758, 0.0617], grad_fn=<ToCopyBackward0>), [' 4', ' 1', ' 2', ' 3', ' 7'])\n",
      "(tensor([0.5006, 0.0659, 0.0605, 0.0492, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' or', ' out'])\n",
      "(tensor([0.1177, 0.0854, 0.0803, 0.0586, 0.0526], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' The'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a bad joke from the start. The idea that the U.S.A. is in the middle of a war is not only insulting but also stupid. The acting was horrible and the story was stupid. I was expecting a lot\n",
      "(tensor([0.3831, 0.1726, 0.0904, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2453, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6533, 0.0595, 0.0361, 0.0355, 0.0261], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1375, 0.0701, 0.0664, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0843, 0.0617, 0.0526, 0.0430, 0.0406], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.2204, 0.1733, 0.1359, 0.0941, 0.0499], grad_fn=<ToCopyBackward0>), [' idea', ' joke', ' copy', ' rip', ' movie'])\n",
      "(tensor([0.2679, 0.1327, 0.1157, 0.0396, 0.0387], grad_fn=<ToCopyBackward0>), [' from', ' when', '.', ' until', ' on'])\n",
      "(tensor([0.7962, 0.0363, 0.0109, 0.0103, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' start', ' day', ' beginning', ' conception'])\n",
      "(tensor([0.2804, 0.2572, 0.2045, 0.0717, 0.0469], grad_fn=<ToCopyBackward0>), [' get', ' start', ' beginning', ' word', ' very'])\n",
      "(tensor([0.7963, 0.0906, 0.0143, 0.0128, 0.0117], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', '!'])\n",
      "(tensor([0.2011, 0.1785, 0.1209, 0.0242, 0.0164], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' What'])\n",
      "(tensor([0.1104, 0.0741, 0.0606, 0.0362, 0.0343], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' premise', ' idea'])\n",
      "(tensor([0.3084, 0.2165, 0.1754, 0.1152, 0.0301], grad_fn=<ToCopyBackward0>), [' was', ' of', ' that', ' is', ' behind'])\n",
      "(tensor([0.1889, 0.0671, 0.0324, 0.0323, 0.0224], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' they', ' this'])\n",
      "(tensor([0.0952, 0.0520, 0.0469, 0.0446, 0.0375], grad_fn=<ToCopyBackward0>), [' US', ' government', ' CIA', ' guy', ' U'])\n",
      "(tensor([9.8095e-01, 2.5514e-03, 1.6330e-03, 9.2014e-04, 7.1674e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['.', ' of', 'gly', '-', 'men'])\n",
      "(tensor([0.9538, 0.0367, 0.0037, 0.0014, 0.0013], grad_fn=<ToCopyBackward0>), ['S', 'N', ' S', 'F', 'K'])\n",
      "(tensor([9.9326e-01, 7.6604e-04, 7.3731e-04, 5.9985e-04, 3.1816e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['.', ' is', '.,', '-', ','])\n",
      "(tensor([0.1570, 0.1366, 0.0576, 0.0363, 0.0268], grad_fn=<ToCopyBackward0>), [' military', 'A', 'S', ' Army', ' is'])\n",
      "(tensor([0.9515, 0.0116, 0.0068, 0.0055, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' is', ' would', ' was', ','])\n",
      "(tensor([0.1619, 0.1246, 0.0722, 0.0702, 0.0670], grad_fn=<ToCopyBackward0>), [' is', ' has', ' (', ' can', ' needs'])\n",
      "(tensor([0.2369, 0.0679, 0.0651, 0.0636, 0.0491], grad_fn=<ToCopyBackward0>), [' so', ' in', ' a', ' going', ' the'])\n",
      "(tensor([0.1604, 0.1191, 0.1110, 0.0818, 0.0516], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' fact', ' trouble'])\n",
      "(tensor([0.6108, 0.0279, 0.0231, 0.0193, 0.0180], grad_fn=<ToCopyBackward0>), [' middle', ' process', ' midst', ' Middle', ' early'])\n",
      "(tensor([0.9339, 0.0378, 0.0060, 0.0051, 0.0024], grad_fn=<ToCopyBackward0>), [' of', ' east', ' East', ' and', '-'])\n",
      "(tensor([0.3163, 0.1219, 0.1072, 0.0821, 0.0691], grad_fn=<ToCopyBackward0>), [' a', ' World', ' WW', ' the', ' some'])\n",
      "(tensor([0.5850, 0.0648, 0.0344, 0.0154, 0.0139], grad_fn=<ToCopyBackward0>), [' war', ' \"', ' civil', ' battle', \" '\"])\n",
      "(tensor([0.2336, 0.0965, 0.0823, 0.0721, 0.0607], grad_fn=<ToCopyBackward0>), [' with', ' and', ' is', ' against', ','])\n",
      "(tensor([0.1678, 0.1042, 0.0703, 0.0659, 0.0489], grad_fn=<ToCopyBackward0>), [' not', ' laughable', ' ridiculous', ' just', ' so'])\n",
      "(tensor([0.3575, 0.1393, 0.1137, 0.0459, 0.0426], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' only', ' a', ' really'])\n",
      "(tensor([0.1462, 0.0933, 0.0807, 0.0745, 0.0622], grad_fn=<ToCopyBackward0>), [' ridiculous', ' absurd', ' stupid', ' laughable', ' insulting'])\n",
      "(tensor([0.6191, 0.1710, 0.0862, 0.0570, 0.0093], grad_fn=<ToCopyBackward0>), [' to', ',', ' but', ' and', '.'])\n",
      "(tensor([0.0813, 0.0760, 0.0640, 0.0548, 0.0546], grad_fn=<ToCopyBackward0>), [' laughable', ' insulting', ' also', ' prep', ' stupid'])\n",
      "(tensor([0.1619, 0.0744, 0.0619, 0.0480, 0.0454], grad_fn=<ToCopyBackward0>), [' laughable', ' stupid', ' insulting', ' ridiculous', ' prep'])\n",
      "(tensor([0.7694, 0.0554, 0.0329, 0.0234, 0.0138], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' to'])\n",
      "(tensor([0.2701, 0.1216, 0.0494, 0.0346, 0.0252], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' Why', ' There'])\n",
      "(tensor([0.2607, 0.0678, 0.0495, 0.0481, 0.0268], grad_fn=<ToCopyBackward0>), [' idea', ' fact', ' acting', ' war', ' only'])\n",
      "(tensor([0.5268, 0.2068, 0.0488, 0.0355, 0.0252], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' in'])\n",
      "(tensor([0.0947, 0.0943, 0.0747, 0.0566, 0.0490], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' so', ' horrible'])\n",
      "(tensor([0.3147, 0.2484, 0.2482, 0.0423, 0.0201], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' too'])\n",
      "(tensor([0.6368, 0.0914, 0.0269, 0.0255, 0.0143], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' they'])\n",
      "(tensor([0.3504, 0.1639, 0.0898, 0.0342, 0.0263], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' dialogue'])\n",
      "(tensor([0.4756, 0.0437, 0.0304, 0.0263, 0.0215], grad_fn=<ToCopyBackward0>), [' was', ' line', ' just', ' is', ' seemed'])\n",
      "(tensor([0.0813, 0.0566, 0.0521, 0.0482, 0.0461], grad_fn=<ToCopyBackward0>), [' stupid', ' so', ' not', ' just', ' even'])\n",
      "(tensor([0.6829, 0.1504, 0.0532, 0.0194, 0.0188], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' too'])\n",
      "(tensor([0.2732, 0.2237, 0.0525, 0.0307, 0.0192], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'I'])\n",
      "(tensor([0.0834, 0.0789, 0.0771, 0.0449, 0.0443], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' don', ' would', ' really'])\n",
      "(tensor([0.1027, 0.0730, 0.0720, 0.0684, 0.0434], grad_fn=<ToCopyBackward0>), [' not', ' so', ' really', ' very', ' expecting'])\n",
      "(tensor([0.5387, 0.1095, 0.0679, 0.0562, 0.0377], grad_fn=<ToCopyBackward0>), [' a', ' more', ' something', ' some', ' to'])\n",
      "(tensor([0.1794, 0.1296, 0.1124, 0.0850, 0.0806], grad_fn=<ToCopyBackward0>), [' better', ' lot', ' good', ' movie', ' real'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was terrible. I was really disappointed. The only reason I watched it was because it's the last movie in the series, and I thought it might make a good movie for a Halloween party. But it's not even funny, it was\n",
      "(tensor([0.3842, 0.1714, 0.0899, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2437, 0.1956, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6520, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1369, 0.0703, 0.0660, 0.0549, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.4709, 0.1414, 0.0500, 0.0453, 0.0377], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', ' when'])\n",
      "(tensor([0.3021, 0.1328, 0.1062, 0.0267, 0.0193], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'I'])\n",
      "(tensor([0.0939, 0.0804, 0.0794, 0.0625, 0.0457], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' have'])\n",
      "(tensor([0.3007, 0.0942, 0.0737, 0.0569, 0.0554], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' in', ' actually'])\n",
      "(tensor([0.2353, 0.1590, 0.0865, 0.0732, 0.0358], grad_fn=<ToCopyBackward0>), [' disappointed', ' looking', ',', ' surprised', ' excited'])\n",
      "(tensor([0.4088, 0.1697, 0.1310, 0.1051, 0.0574], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' when'])\n",
      "(tensor([0.3614, 0.1403, 0.1087, 0.0231, 0.0222], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1558, 0.1000, 0.0836, 0.0695, 0.0479], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' plot'])\n",
      "(tensor([0.2823, 0.1977, 0.1301, 0.0436, 0.0370], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' funny', ' part'])\n",
      "(tensor([0.7831, 0.0624, 0.0368, 0.0344, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' i', ' this'])\n",
      "(tensor([0.3920, 0.0744, 0.0559, 0.0500, 0.0434], grad_fn=<ToCopyBackward0>), [' watched', ' even', \"'m\", ' gave', ' didn'])\n",
      "(tensor([0.8642, 0.0744, 0.0438, 0.0073, 0.0009], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.6311, 0.1526, 0.0379, 0.0237, 0.0132], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' in'])\n",
      "(tensor([0.6426, 0.0866, 0.0565, 0.0386, 0.0379], grad_fn=<ToCopyBackward0>), [' because', ' to', ' I', ' that', ' for'])\n",
      "(tensor([0.4977, 0.0825, 0.0801, 0.0796, 0.0654], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' of', ' it'])\n",
      "(tensor([0.4043, 0.1265, 0.0637, 0.0458, 0.0447], grad_fn=<ToCopyBackward0>), [' was', ' starred', \"'s\", ' had', ' stars'])\n",
      "(tensor([0.1405, 0.0926, 0.0687, 0.0655, 0.0404], grad_fn=<ToCopyBackward0>), [' a', ' the', ' on', ' been', ' so'])\n",
      "(tensor([0.4447, 0.0554, 0.0524, 0.0444, 0.0363], grad_fn=<ToCopyBackward0>), [' only', ' last', ' movie', ' first', ' M'])\n",
      "(tensor([0.6309, 0.0884, 0.0189, 0.0141, 0.0129], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' Batman', ' sequel', ' time'])\n",
      "(tensor([0.4644, 0.1174, 0.0896, 0.0319, 0.0308], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' on', ' of'])\n",
      "(tensor([0.8441, 0.0462, 0.0213, 0.0127, 0.0077], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' this', ' what'])\n",
      "(tensor([0.4637, 0.0231, 0.0219, 0.0188, 0.0179], grad_fn=<ToCopyBackward0>), [' series', ' first', ' last', ' trilogy', ' 3'])\n",
      "(tensor([0.3090, 0.2074, 0.1201, 0.0807, 0.0492], grad_fn=<ToCopyBackward0>), ['.', ',', ' I', ' and', ' ('])\n",
      "(tensor([0.4480, 0.2253, 0.1519, 0.0266, 0.0127], grad_fn=<ToCopyBackward0>), [' and', ' so', ' but', ' I', ' the'])\n",
      "(tensor([0.6586, 0.0832, 0.0499, 0.0261, 0.0188], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' my', ' they'])\n",
      "(tensor([0.1347, 0.1217, 0.0880, 0.0784, 0.0473], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' wanted'])\n",
      "(tensor([0.4693, 0.1514, 0.0896, 0.0701, 0.0655], grad_fn=<ToCopyBackward0>), [' it', ' maybe', ' I', ' the', ' that'])\n",
      "(tensor([0.4017, 0.2784, 0.1401, 0.0458, 0.0277], grad_fn=<ToCopyBackward0>), [' would', ' was', ' might', ' could', ' had'])\n",
      "(tensor([0.6216, 0.1414, 0.0357, 0.0337, 0.0202], grad_fn=<ToCopyBackward0>), [' be', ' get', ' make', ' have', ' give'])\n",
      "(tensor([0.3388, 0.1494, 0.1345, 0.0592, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' some', ' me', ' for', ' up'])\n",
      "(tensor([0.4993, 0.0612, 0.0434, 0.0321, 0.0302], grad_fn=<ToCopyBackward0>), [' good', ' great', ' comeback', ' better', ' movie'])\n",
      "(tensor([0.1337, 0.0556, 0.0543, 0.0478, 0.0458], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' one', ' comedy'])\n",
      "(tensor([0.1986, 0.1478, 0.0770, 0.0601, 0.0513], grad_fn=<ToCopyBackward0>), [' if', ' for', '.', '...', ','])\n",
      "(tensor([0.3566, 0.1752, 0.0574, 0.0383, 0.0190], grad_fn=<ToCopyBackward0>), [' a', ' the', ' Halloween', ' my', ' people'])\n",
      "(tensor([0.0823, 0.0761, 0.0478, 0.0366, 0.0359], grad_fn=<ToCopyBackward0>), [' Halloween', ' kids', ' rainy', ' school', ' college'])\n",
      "(tensor([0.3162, 0.0453, 0.0422, 0.0416, 0.0381], grad_fn=<ToCopyBackward0>), [' party', ' special', ' movie', ' treat', ' night'])\n",
      "(tensor([0.3414, 0.0493, 0.0489, 0.0476, 0.0439], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', ' if', ' where'])\n",
      "(tensor([0.3231, 0.1209, 0.0917, 0.0502, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', 'I'])\n",
      "(tensor([0.2461, 0.1570, 0.1365, 0.0310, 0.0259], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' after', ' that'])\n",
      "(tensor([0.3685, 0.2258, 0.0740, 0.0558, 0.0546], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' didn', ' really'])\n",
      "(tensor([0.4613, 0.1884, 0.0840, 0.0402, 0.0324], grad_fn=<ToCopyBackward0>), [' not', ' just', ' really', ' a', ' so'])\n",
      "(tensor([0.5314, 0.1108, 0.0726, 0.0329, 0.0321], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' a', '.', ' entertaining'])\n",
      "(tensor([0.2223, 0.1713, 0.1333, 0.0934, 0.0599], grad_fn=<ToCopyBackward0>), [' funny', ' worth', ' entertaining', ' good', ' a'])\n",
      "(tensor([0.5088, 0.1850, 0.0412, 0.0268, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', ' in'])\n",
      "(tensor([0.3714, 0.1504, 0.0674, 0.0542, 0.0403], grad_fn=<ToCopyBackward0>), [' it', ' not', ' and', ' the', ' I'])\n",
      "(tensor([0.9517, 0.0102, 0.0078, 0.0049, 0.0036], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' doesn', ' has'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie had a pretty good story to tell. It had a great cast. It had good acting. But when I watched it, I found that the movie had no humor. I found that the story just seemed to be telling the same jokes\n",
      "(tensor([0.3839, 0.1719, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3323, 0.2361, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2612, 0.0541, 0.0274, 0.0218, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5319, 0.0603, 0.0538, 0.0231, 0.0204], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.2545, 0.1236, 0.0659, 0.0474, 0.0430], grad_fn=<ToCopyBackward0>), [' a', ' to', ' some', ' very', ' been'])\n",
      "(tensor([0.3698, 0.0690, 0.0638, 0.0569, 0.0384], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' really', ' lot', ' good'])\n",
      "(tensor([0.7157, 0.0511, 0.0280, 0.0280, 0.0241], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' funny', ' decent', ' cool'])\n",
      "(tensor([0.2056, 0.1281, 0.0764, 0.0559, 0.0463], grad_fn=<ToCopyBackward0>), [' story', ' premise', ' storyline', ' ending', ' idea'])\n",
      "(tensor([0.2064, 0.1308, 0.1168, 0.0903, 0.0861], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' line', ' to'])\n",
      "(tensor([0.7392, 0.0885, 0.0351, 0.0307, 0.0225], grad_fn=<ToCopyBackward0>), [' tell', ' it', ' be', ' go', ' follow'])\n",
      "(tensor([0.4480, 0.1717, 0.1197, 0.0436, 0.0301], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' but'])\n",
      "(tensor([0.1724, 0.1025, 0.1022, 0.0885, 0.0366], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' Unfortunately'])\n",
      "(tensor([0.3715, 0.1366, 0.1270, 0.0527, 0.0299], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' seemed', ' has'])\n",
      "(tensor([0.3361, 0.2358, 0.0487, 0.0311, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' great', ' good'])\n",
      "(tensor([0.2696, 0.1755, 0.0566, 0.0484, 0.0357], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' great', ' really', ' lot'])\n",
      "(tensor([0.1509, 0.1214, 0.1209, 0.0342, 0.0237], grad_fn=<ToCopyBackward0>), [' cast', ' plot', ' premise', ' story', ' set'])\n",
      "(tensor([0.3385, 0.2849, 0.1889, 0.0302, 0.0166], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' in', ' of'])\n",
      "(tensor([0.2495, 0.1446, 0.1275, 0.0881, 0.0784], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' And'])\n",
      "(tensor([0.3396, 0.3337, 0.0507, 0.0424, 0.0237], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' really', ' could'])\n",
      "(tensor([0.4624, 0.0981, 0.0952, 0.0445, 0.0431], grad_fn=<ToCopyBackward0>), [' a', ' great', ' good', ' an', ' some'])\n",
      "(tensor([0.2278, 0.1008, 0.0613, 0.0550, 0.0542], grad_fn=<ToCopyBackward0>), [' acting', ' cinem', ' actors', ' pacing', ' special'])\n",
      "(tensor([0.8270, 0.0471, 0.0207, 0.0151, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' performances', ' and'])\n",
      "(tensor([0.3259, 0.1307, 0.1104, 0.0891, 0.0881], grad_fn=<ToCopyBackward0>), [' It', ' But', ' I', ' The', ' And'])\n",
      "(tensor([0.2283, 0.1059, 0.0930, 0.0481, 0.0476], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' when', ' what'])\n",
      "(tensor([0.5964, 0.1793, 0.0706, 0.0607, 0.0182], grad_fn=<ToCopyBackward0>), [' I', ' you', ' it', ' the', ' this'])\n",
      "(tensor([0.2805, 0.0943, 0.0903, 0.0516, 0.0512], grad_fn=<ToCopyBackward0>), [' saw', ' started', ' watched', ' got', ' went'])\n",
      "(tensor([0.6178, 0.2844, 0.0465, 0.0105, 0.0069], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', \" '\"])\n",
      "(tensor([0.2798, 0.1439, 0.1295, 0.1080, 0.0423], grad_fn=<ToCopyBackward0>), [',', ' I', ' in', ' on', ' for'])\n",
      "(tensor([0.5443, 0.1211, 0.0581, 0.0322, 0.0264], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' what'])\n",
      "(tensor([0.1582, 0.1395, 0.1131, 0.0958, 0.0587], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' just', ' found', ' really'])\n",
      "(tensor([0.4178, 0.2128, 0.1897, 0.0564, 0.0329], grad_fn=<ToCopyBackward0>), [' that', ' myself', ' it', ' out', ' the'])\n",
      "(tensor([0.3411, 0.2894, 0.1076, 0.0941, 0.0235], grad_fn=<ToCopyBackward0>), [' it', ' the', ' there', ' I', ' this'])\n",
      "(tensor([0.2429, 0.2149, 0.0492, 0.0358, 0.0297], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' direction', ' acting', ' script'])\n",
      "(tensor([0.4066, 0.1214, 0.1163, 0.0489, 0.0368], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' didn', ' is'])\n",
      "(tensor([0.2809, 0.2126, 0.0418, 0.0382, 0.0374], grad_fn=<ToCopyBackward0>), [' no', ' a', ' been', ' not', ' the'])\n",
      "(tensor([0.1853, 0.1512, 0.0942, 0.0680, 0.0514], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' point', ' real', ' humor'])\n",
      "(tensor([0.5555, 0.2224, 0.0706, 0.0507, 0.0378], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' to', ' at'])\n",
      "(tensor([0.2668, 0.1184, 0.1035, 0.0842, 0.0521], grad_fn=<ToCopyBackward0>), [' It', ' I', ' There', ' The', ' That'])\n",
      "(tensor([0.4538, 0.0508, 0.0400, 0.0375, 0.0348], grad_fn=<ToCopyBackward0>), [' found', ' mean', ' thought', ' was', ' don'])\n",
      "(tensor([0.6523, 0.0827, 0.0732, 0.0481, 0.0390], grad_fn=<ToCopyBackward0>), [' that', ' no', ' it', ' the', ' nothing'])\n",
      "(tensor([0.5741, 0.2006, 0.0521, 0.0290, 0.0178], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' I', ' when'])\n",
      "(tensor([0.7674, 0.0497, 0.0315, 0.0181, 0.0151], grad_fn=<ToCopyBackward0>), [' movie', ' humor', ' story', ' acting', ' comedy'])\n",
      "(tensor([0.5925, 0.0881, 0.0749, 0.0265, 0.0235], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' and', ' didn'])\n",
      "(tensor([0.2134, 0.1871, 0.1348, 0.0442, 0.0284], grad_fn=<ToCopyBackward0>), [' didn', ' wasn', ' seemed', ' was', ' went'])\n",
      "(tensor([0.4632, 0.0566, 0.0291, 0.0285, 0.0259], grad_fn=<ToCopyBackward0>), [' to', ' like', ' really', ' very', ' boring'])\n",
      "(tensor([0.4133, 0.2131, 0.0696, 0.0333, 0.0266], grad_fn=<ToCopyBackward0>), [' be', ' go', ' have', ' get', ' drag'])\n",
      "(tensor([0.0906, 0.0369, 0.0303, 0.0250, 0.0224], grad_fn=<ToCopyBackward0>), [' boring', ' about', ' trying', ' telling', ' pointless'])\n",
      "(tensor([0.2927, 0.1464, 0.1230, 0.1184, 0.0276], grad_fn=<ToCopyBackward0>), [' the', ' itself', ' you', ' a', ' me'])\n",
      "(tensor([0.4390, 0.1890, 0.0623, 0.0183, 0.0147], grad_fn=<ToCopyBackward0>), [' same', ' audience', ' viewer', ' story', ' most'])\n",
      "(tensor([0.2443, 0.1863, 0.1637, 0.0718, 0.0435], grad_fn=<ToCopyBackward0>), [' jokes', ' old', ' story', ' thing', ' joke'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very boring and predictable, the actors are not good and the whole movie is predictable and boring. It had no point to it. The acting is not good at all. There was no point to the movie, it was boring and there\n",
      "(tensor([0.3854, 0.1706, 0.0892, 0.0769, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4391, 0.2421, 0.1960, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6509, 0.0600, 0.0362, 0.0354, 0.0265], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1367, 0.0702, 0.0658, 0.0547, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5082, 0.0405, 0.0342, 0.0341, 0.0323], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' funny', ' disappointing', ','])\n",
      "(tensor([0.4156, 0.2425, 0.1167, 0.0409, 0.0137], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '!'])\n",
      "(tensor([0.2281, 0.0495, 0.0361, 0.0340, 0.0293], grad_fn=<ToCopyBackward0>), [' predictable', ' was', ' had', ' the', ' un'])\n",
      "(tensor([0.6118, 0.1099, 0.1071, 0.0211, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' from', ' to'])\n",
      "(tensor([0.1843, 0.0962, 0.0838, 0.0519, 0.0335], grad_fn=<ToCopyBackward0>), [' and', ' with', ' but', ' the', ' I'])\n",
      "(tensor([0.2022, 0.0683, 0.0561, 0.0542, 0.0451], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' only', ' same', ' actors'])\n",
      "(tensor([0.3779, 0.0518, 0.0502, 0.0499, 0.0490], grad_fn=<ToCopyBackward0>), [' were', ' seemed', ' didn', ' are', ' did'])\n",
      "(tensor([0.3312, 0.0748, 0.0574, 0.0377, 0.0221], grad_fn=<ToCopyBackward0>), [' not', ' all', ' very', ' so', ' the'])\n",
      "(tensor([0.1774, 0.1287, 0.1097, 0.0933, 0.0589], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' convincing', ' believable', ' that'])\n",
      "(tensor([0.2678, 0.2038, 0.1661, 0.0864, 0.0828], grad_fn=<ToCopyBackward0>), [' and', ' at', ' enough', '.', ','])\n",
      "(tensor([0.5201, 0.0483, 0.0386, 0.0381, 0.0310], grad_fn=<ToCopyBackward0>), [' the', ' there', ' it', ' this', ' I'])\n",
      "(tensor([0.3064, 0.1791, 0.1166, 0.0824, 0.0334], grad_fn=<ToCopyBackward0>), [' story', ' script', ' plot', ' storyline', ' whole'])\n",
      "(tensor([0.5534, 0.1374, 0.0702, 0.0488, 0.0322], grad_fn=<ToCopyBackward0>), [' movie', ' thing', ' story', ' idea', ' film'])\n",
      "(tensor([0.3099, 0.2506, 0.0696, 0.0367, 0.0301], grad_fn=<ToCopyBackward0>), [' is', ' was', ' seemed', ' felt', ' just'])\n",
      "(tensor([0.2951, 0.0770, 0.0594, 0.0369, 0.0269], grad_fn=<ToCopyBackward0>), [' predictable', ' very', ' just', ' not', ' a'])\n",
      "(tensor([0.4503, 0.2114, 0.0909, 0.0501, 0.0204], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' with'])\n",
      "(tensor([0.3193, 0.0999, 0.0494, 0.0437, 0.0385], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' stupid', ' not', ' dumb'])\n",
      "(tensor([0.6023, 0.1148, 0.0590, 0.0332, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', '!'])\n",
      "(tensor([0.2361, 0.1495, 0.0646, 0.0432, 0.0337], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.2512, 0.1789, 0.1381, 0.0528, 0.0294], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' had'])\n",
      "(tensor([0.1264, 0.1244, 0.0949, 0.0827, 0.0491], grad_fn=<ToCopyBackward0>), [' no', ' a', ' some', ' the', ' to'])\n",
      "(tensor([0.2134, 0.0908, 0.0849, 0.0595, 0.0485], grad_fn=<ToCopyBackward0>), [' point', ' plot', ' suspense', ' real', ' story'])\n",
      "(tensor([0.2333, 0.1791, 0.1741, 0.1015, 0.0414], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' at'])\n",
      "(tensor([0.6336, 0.1081, 0.0645, 0.0386, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' the', ' be', ' make', ' tell'])\n",
      "(tensor([0.3429, 0.2532, 0.1149, 0.1041, 0.0168], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' except'])\n",
      "(tensor([0.2211, 0.1518, 0.1094, 0.0267, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', 'The'])\n",
      "(tensor([0.1355, 0.1297, 0.1069, 0.0923, 0.0421], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' whole', ' acting', ' story'])\n",
      "(tensor([0.6333, 0.1729, 0.0298, 0.0285, 0.0160], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ' in'])\n",
      "(tensor([0.2573, 0.0809, 0.0788, 0.0622, 0.0480], grad_fn=<ToCopyBackward0>), [' not', ' bad', ' so', ' very', ' terrible'])\n",
      "(tensor([0.3422, 0.1938, 0.0853, 0.0693, 0.0443], grad_fn=<ToCopyBackward0>), [' convincing', ' good', ' even', ' believable', ' bad'])\n",
      "(tensor([0.2319, 0.1480, 0.1310, 0.1309, 0.1170], grad_fn=<ToCopyBackward0>), [' at', ',', ' and', '.', ' enough'])\n",
      "(tensor([0.9783, 0.0049, 0.0033, 0.0027, 0.0021], grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' least', ' any', ' the'])\n",
      "(tensor([0.3467, 0.2942, 0.1530, 0.0244, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' the'])\n",
      "(tensor([0.3396, 0.0993, 0.0810, 0.0308, 0.0296], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.4172, 0.3251, 0.0864, 0.0574, 0.0334], grad_fn=<ToCopyBackward0>), [' is', ' was', ' are', \"'s\", ' were'])\n",
      "(tensor([0.4594, 0.0982, 0.0768, 0.0527, 0.0412], grad_fn=<ToCopyBackward0>), [' no', ' not', ' nothing', ' a', ' one'])\n",
      "(tensor([0.7492, 0.0430, 0.0243, 0.0115, 0.0109], grad_fn=<ToCopyBackward0>), [' point', ' plot', ' acting', ' chemistry', ' real'])\n",
      "(tensor([0.5194, 0.1785, 0.1018, 0.0380, 0.0371], grad_fn=<ToCopyBackward0>), [' to', ' in', '.', ' at', ' of'])\n",
      "(tensor([0.3641, 0.3506, 0.2354, 0.0097, 0.0033], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' any', ' make'])\n",
      "(tensor([0.9065, 0.0521, 0.0150, 0.0101, 0.0031], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' whole', ' plot'])\n",
      "(tensor([0.4315, 0.1512, 0.1424, 0.0490, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', ' except'])\n",
      "(tensor([0.2118, 0.1206, 0.0698, 0.0564, 0.0413], grad_fn=<ToCopyBackward0>), [' the', ' it', ' and', ' there', ' just'])\n",
      "(tensor([0.4492, 0.1269, 0.1024, 0.0720, 0.0329], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' seemed'])\n",
      "(tensor([0.4278, 0.1661, 0.0843, 0.0433, 0.0246], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' all', ' a'])\n",
      "(tensor([0.7684, 0.0936, 0.0550, 0.0210, 0.0085], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' as'])\n",
      "(tensor([0.3754, 0.0590, 0.0543, 0.0529, 0.0492], grad_fn=<ToCopyBackward0>), [' predictable', ' there', ' the', ' I', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd seen the worst of it by now. I was wrong. This was not a good film. The story was disjointed and confusing. There was no character development. The acting was awful. I could not get past the fact that I\n",
      "(tensor([0.3836, 0.1722, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2251, 0.1946, 0.1572, 0.0695, 0.0536], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2087, 0.1044, 0.0617, 0.0394, 0.0298], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.4487, 0.0924, 0.0917, 0.0738, 0.0531], grad_fn=<ToCopyBackward0>), [' it', ' everything', ' the', ' all', ' a'])\n",
      "(tensor([0.7168, 0.1595, 0.0178, 0.0121, 0.0107], grad_fn=<ToCopyBackward0>), [' worst', ' last', ' end', ' whole', ' worse'])\n",
      "(tensor([0.4058, 0.0581, 0.0570, 0.0556, 0.0545], grad_fn=<ToCopyBackward0>), [' of', ',', ' movie', ' film', '.'])\n",
      "(tensor([0.2855, 0.0880, 0.0864, 0.0245, 0.0200], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' The', ' horror'])\n",
      "(tensor([0.4809, 0.1287, 0.1254, 0.0516, 0.0408], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' by'])\n",
      "(tensor([0.3674, 0.0941, 0.0917, 0.0432, 0.0332], grad_fn=<ToCopyBackward0>), [' now', ' watching', ' this', ' the', ' that'])\n",
      "(tensor([0.5853, 0.2794, 0.0467, 0.0166, 0.0156], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' when', '!'])\n",
      "(tensor([0.2189, 0.1177, 0.0792, 0.0483, 0.0356], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' Not'])\n",
      "(tensor([0.1785, 0.1131, 0.0945, 0.0653, 0.0290], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', \"'m\", ' really'])\n",
      "(tensor([0.9707, 0.0077, 0.0045, 0.0013, 0.0011], grad_fn=<ToCopyBackward0>), [' wrong', ' right', ' so', ' in', ' really'])\n",
      "(tensor([0.9023, 0.0475, 0.0103, 0.0071, 0.0026], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' on', ';'])\n",
      "(tensor([0.2489, 0.1580, 0.0829, 0.0746, 0.0327], grad_fn=<ToCopyBackward0>), [' This', ' I', ' It', ' The', ' There'])\n",
      "(tensor([0.4950, 0.1013, 0.0893, 0.0463, 0.0338], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' thing', ' was'])\n",
      "(tensor([0.1681, 0.0795, 0.0662, 0.0478, 0.0436], grad_fn=<ToCopyBackward0>), [' just', ' not', ' a', ' the', ' only'])\n",
      "(tensor([0.2389, 0.1639, 0.1127, 0.0882, 0.0408], grad_fn=<ToCopyBackward0>), [' the', ' a', ' one', ' even', ' good'])\n",
      "(tensor([0.1769, 0.1699, 0.1591, 0.0535, 0.0526], grad_fn=<ToCopyBackward0>), [' horror', ' good', ' bad', ' one', ' movie'])\n",
      "(tensor([0.5409, 0.4053, 0.0095, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' documentary', ' parody'])\n",
      "(tensor([0.6974, 0.0540, 0.0396, 0.0322, 0.0321], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', '...', ' to'])\n",
      "(tensor([0.1890, 0.1764, 0.1412, 0.0663, 0.0375], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' This', ' Not'])\n",
      "(tensor([0.1514, 0.0845, 0.0675, 0.0597, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' story', ' script'])\n",
      "(tensor([0.4594, 0.1671, 0.0423, 0.0382, 0.0147], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' had', ' has'])\n",
      "(tensor([0.0652, 0.0628, 0.0437, 0.0305, 0.0287], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' dis', ' a', ' not'])\n",
      "(tensor([9.9236e-01, 1.0485e-03, 9.5275e-04, 6.3678e-04, 3.6920e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['j', '-', 'organized', 'ordered', 'em'])\n",
      "(tensor([9.9937e-01, 5.3734e-04, 2.3728e-05, 1.2437e-05, 3.3222e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['ointed', 'oint', 'uked', 'unct', 'acent'])\n",
      "(tensor([0.4976, 0.3813, 0.0446, 0.0175, 0.0131], grad_fn=<ToCopyBackward0>), [' and', ',', ' at', '.', ' from'])\n",
      "(tensor([0.1882, 0.0866, 0.0725, 0.0299, 0.0200], grad_fn=<ToCopyBackward0>), [' the', ' dis', ' confusing', ' had', ' seemed'])\n",
      "(tensor([0.4025, 0.3153, 0.1182, 0.0255, 0.0193], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' at', ';'])\n",
      "(tensor([0.5078, 0.1173, 0.0895, 0.0437, 0.0166], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.5116, 0.3728, 0.0284, 0.0188, 0.0164], grad_fn=<ToCopyBackward0>), [' was', ' were', ' wasn', ' seemed', ' are'])\n",
      "(tensor([0.6217, 0.0518, 0.0423, 0.0348, 0.0199], grad_fn=<ToCopyBackward0>), [' no', ' a', ' not', ' little', ' nothing'])\n",
      "(tensor([0.0972, 0.0521, 0.0415, 0.0402, 0.0385], grad_fn=<ToCopyBackward0>), [' plot', ' real', ' character', ' central', ' cohesion'])\n",
      "(tensor([0.8880, 0.0226, 0.0090, 0.0064, 0.0057], grad_fn=<ToCopyBackward0>), [' development', ' to', ' or', ' that', ' focus'])\n",
      "(tensor([0.3815, 0.2332, 0.1226, 0.0455, 0.0386], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' for', ' in'])\n",
      "(tensor([0.5404, 0.0835, 0.0762, 0.0564, 0.0296], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.4597, 0.0423, 0.0334, 0.0312, 0.0267], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' dialogue', ' ending', ' script'])\n",
      "(tensor([0.8634, 0.0374, 0.0109, 0.0058, 0.0054], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ',', ' and', ' seemed'])\n",
      "(tensor([0.0718, 0.0578, 0.0536, 0.0451, 0.0442], grad_fn=<ToCopyBackward0>), [' awful', ' terrible', ' wooden', ' bad', ' weak'])\n",
      "(tensor([0.8095, 0.0948, 0.0552, 0.0038, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' -', ' to'])\n",
      "(tensor([0.5696, 0.0806, 0.0778, 0.0527, 0.0200], grad_fn=<ToCopyBackward0>), [' The', ' And', ' I', ' It', ' There'])\n",
      "(tensor([0.1211, 0.0641, 0.0441, 0.0391, 0.0363], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' could', ' couldn', ' found'])\n",
      "(tensor([0.5886, 0.0909, 0.0443, 0.0344, 0.0302], grad_fn=<ToCopyBackward0>), [' not', ' barely', ' see', ' only', ' feel'])\n",
      "(tensor([0.1825, 0.1387, 0.1298, 0.0407, 0.0301], grad_fn=<ToCopyBackward0>), [' believe', ' get', ' understand', ' even', ' care'])\n",
      "(tensor([0.2556, 0.1637, 0.1525, 0.0953, 0.0842], grad_fn=<ToCopyBackward0>), [' out', ' over', ' into', ' past', ' through'])\n",
      "(tensor([0.8890, 0.0152, 0.0146, 0.0116, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' how', ' this'])\n",
      "(tensor([0.5221, 0.0140, 0.0125, 0.0114, 0.0099], grad_fn=<ToCopyBackward0>), [' fact', ' script', ' obvious', ' awful', ' terrible'])\n",
      "(tensor([0.9674, 0.0080, 0.0068, 0.0037, 0.0032], grad_fn=<ToCopyBackward0>), [' that', ' I', ' the', ' this', ' there'])\n",
      "(tensor([0.3696, 0.1836, 0.0799, 0.0490, 0.0409], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' this', ' there'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I should watch some horror films, but I found it more interesting than watching a good scary movie like \"Silent Hill\" or \"Silent Hill: A New Beginning\". The reason is that in the movies the characters are acting out some sort of\n",
      "(tensor([0.3837, 0.1723, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2253, 0.1946, 0.1572, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.1264, 0.1032, 0.0823, 0.0576, 0.0531], grad_fn=<ToCopyBackward0>), [' warn', ' give', ' never', ' have', ' watch'])\n",
      "(tensor([0.6118, 0.1875, 0.0457, 0.0328, 0.0197], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.2048, 0.1229, 0.0380, 0.0331, 0.0195], grad_fn=<ToCopyBackward0>), [' horror', ' more', ' other', ' of', ' old'])\n",
      "(tensor([0.6993, 0.1277, 0.0246, 0.0234, 0.0230], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' f', '.', ' movie'])\n",
      "(tensor([0.1738, 0.1349, 0.0883, 0.0858, 0.0622], grad_fn=<ToCopyBackward0>), [' to', ',', ' as', ' just', '.'])\n",
      "(tensor([0.2341, 0.1360, 0.0562, 0.0370, 0.0358], grad_fn=<ToCopyBackward0>), [' but', ' so', ' and', ' since', ' because'])\n",
      "(tensor([0.2431, 0.1636, 0.1315, 0.0512, 0.0382], grad_fn=<ToCopyBackward0>), [' this', ' after', ' I', ' it', ' the'])\n",
      "(tensor([0.1539, 0.0943, 0.0744, 0.0631, 0.0547], grad_fn=<ToCopyBackward0>), [' was', ' found', ' didn', ' just', ' watched'])\n",
      "(tensor([0.2089, 0.1307, 0.1305, 0.1013, 0.0746], grad_fn=<ToCopyBackward0>), [' this', ' them', ' myself', ' it', ' out'])\n",
      "(tensor([0.1404, 0.1352, 0.1124, 0.0776, 0.0763], grad_fn=<ToCopyBackward0>), [' to', ' more', ' was', ' too', ' a'])\n",
      "(tensor([0.1845, 0.1705, 0.1210, 0.0583, 0.0347], grad_fn=<ToCopyBackward0>), [' entertaining', ' enjoyable', ' interesting', ' fun', ' frightening'])\n",
      "(tensor([0.8485, 0.0485, 0.0332, 0.0095, 0.0066], grad_fn=<ToCopyBackward0>), [' to', ' watching', ' than', ' and', ' when'])\n",
      "(tensor([0.2680, 0.1357, 0.1026, 0.0605, 0.0534], grad_fn=<ToCopyBackward0>), [' watching', ' anything', ' I', ' horror', ' scary'])\n",
      "(tensor([0.1930, 0.1419, 0.1126, 0.0980, 0.0356], grad_fn=<ToCopyBackward0>), [' a', ' scary', ' paint', ' them', ' the'])\n",
      "(tensor([0.1289, 0.1071, 0.0616, 0.0489, 0.0443], grad_fn=<ToCopyBackward0>), [' scary', ' good', ' boring', ' movie', ' lot'])\n",
      "(tensor([0.4414, 0.1516, 0.1155, 0.1059, 0.0236], grad_fn=<ToCopyBackward0>), [' horror', ' movie', ' scary', ' one', ' film'])\n",
      "(tensor([0.8666, 0.0346, 0.0216, 0.0120, 0.0091], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' flick', ' story', ' one'])\n",
      "(tensor([0.8096, 0.0330, 0.0299, 0.0184, 0.0155], grad_fn=<ToCopyBackward0>), ['.', ' like', ',', '...', '....'])\n",
      "(tensor([0.1131, 0.0580, 0.0475, 0.0386, 0.0218], grad_fn=<ToCopyBackward0>), [' The', ' \"', ' Friday', ' the', ' House'])\n",
      "(tensor([0.2156, 0.0655, 0.0489, 0.0451, 0.0371], grad_fn=<ToCopyBackward0>), ['Sil', 'Dead', 'Sc', 'The', 'House'])\n",
      "(tensor([8.9905e-01, 9.2085e-02, 1.8513e-03, 1.0458e-03, 2.3593e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ent', 'ence', 'k', 'ents', 'o'])\n",
      "(tensor([0.7072, 0.1143, 0.0514, 0.0164, 0.0111], grad_fn=<ToCopyBackward0>), [' Hill', ' Night', ' House', ' Scream', ' Hills'])\n",
      "(tensor([0.5382, 0.1893, 0.1572, 0.0393, 0.0314], grad_fn=<ToCopyBackward0>), ['\"', '\".', '.\"', '\",', ',\"'])\n",
      "(tensor([0.3533, 0.0988, 0.0699, 0.0634, 0.0302], grad_fn=<ToCopyBackward0>), [' or', ' because', ' and', ' (', '....'])\n",
      "(tensor([0.8033, 0.0349, 0.0207, 0.0182, 0.0126], grad_fn=<ToCopyBackward0>), [' \"', ' even', ' the', ' any', ' a'])\n",
      "(tensor([0.2365, 0.1029, 0.0865, 0.0504, 0.0245], grad_fn=<ToCopyBackward0>), ['Sil', 'Friday', 'The', 'Dead', 'Sc'])\n",
      "(tensor([9.6776e-01, 2.6325e-02, 9.6940e-04, 6.2785e-04, 5.2416e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ent', 'ence', 'ents', 'k', 'ently'])\n",
      "(tensor([0.9418, 0.0114, 0.0093, 0.0033, 0.0010], grad_fn=<ToCopyBackward0>), [' Hill', ' Night', ' Hills', ' House', ' Scope'])\n",
      "(tensor([0.3510, 0.0695, 0.0651, 0.0240, 0.0236], grad_fn=<ToCopyBackward0>), [' 2', ':', ' Down', ' 3', ' II'])\n",
      "(tensor([0.2823, 0.2166, 0.0540, 0.0195, 0.0159], grad_fn=<ToCopyBackward0>), [' The', ' Home', ' Down', ' A', ' the'])\n",
      "(tensor([0.0425, 0.0250, 0.0122, 0.0117, 0.0108], grad_fn=<ToCopyBackward0>), [' New', ' Game', ' Child', ' Daughter', ' Nightmare'])\n",
      "(tensor([0.3002, 0.1229, 0.0965, 0.0850, 0.0197], grad_fn=<ToCopyBackward0>), [' Beginning', ' Hope', ' Reck', ' Nightmare', ' Dawn'])\n",
      "(tensor([0.6077, 0.1832, 0.1117, 0.0601, 0.0155], grad_fn=<ToCopyBackward0>), ['\".', '.\"', '\"', '\",', ',\"'])\n",
      "(tensor([0.3830, 0.0782, 0.0588, 0.0326, 0.0273], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'I'])\n",
      "(tensor([0.1057, 0.0802, 0.0775, 0.0614, 0.0448], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' first', ' reason'])\n",
      "(tensor([0.2287, 0.2053, 0.1351, 0.1321, 0.0590], grad_fn=<ToCopyBackward0>), [' is', ' for', ' I', ' why', ' being'])\n",
      "(tensor([0.6399, 0.0927, 0.0722, 0.0333, 0.0277], grad_fn=<ToCopyBackward0>), [' that', ' because', ',', ' I', ' the'])\n",
      "(tensor([0.3150, 0.0722, 0.0576, 0.0549, 0.0518], grad_fn=<ToCopyBackward0>), [' I', ' the', ' in', ' this', ' it'])\n",
      "(tensor([0.1659, 0.1269, 0.1219, 0.1130, 0.0721], grad_fn=<ToCopyBackward0>), [' the', ' those', ' most', ' these', ' a'])\n",
      "(tensor([0.1605, 0.1432, 0.0867, 0.0774, 0.0380], grad_fn=<ToCopyBackward0>), [' first', ' movies', ' movie', ' beginning', ' majority'])\n",
      "(tensor([0.3313, 0.1050, 0.0863, 0.0806, 0.0673], grad_fn=<ToCopyBackward0>), [',', ' I', ' the', ' you', ' they'])\n",
      "(tensor([0.1901, 0.0871, 0.0587, 0.0552, 0.0515], grad_fn=<ToCopyBackward0>), [' characters', ' acting', ' actors', ' story', ' main'])\n",
      "(tensor([0.4504, 0.0499, 0.0364, 0.0332, 0.0264], grad_fn=<ToCopyBackward0>), [' are', ' don', ' were', ' always', ' do'])\n",
      "(tensor([0.0887, 0.0637, 0.0597, 0.0535, 0.0407], grad_fn=<ToCopyBackward0>), [' acting', ' always', ' not', ' so', ' very'])\n",
      "(tensor([0.2064, 0.1424, 0.0580, 0.0443, 0.0355], grad_fn=<ToCopyBackward0>), [' in', ' like', ' stupid', ' out', ' to'])\n",
      "(tensor([0.3257, 0.1247, 0.1241, 0.0582, 0.0418], grad_fn=<ToCopyBackward0>), [' their', ' the', ' a', ' some', ' what'])\n",
      "(tensor([0.3743, 0.2029, 0.0934, 0.0171, 0.0124], grad_fn=<ToCopyBackward0>), [' kind', ' sort', ' type', ' very', ' story'])\n",
      "(tensor([9.9635e-01, 2.2042e-04, 1.8647e-04, 1.4733e-04, 1.3916e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' or', ' psychological', ' mental', 'a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I had a good enough knowledge about the movie to make a plot out of it. I thought it would be more interesting to watch it from the standpoint of a viewer than from the standpoint of someone involved in the movie. I was wrong. The\n",
      "(tensor([0.3841, 0.1714, 0.0899, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3312, 0.2367, 0.0584, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2143, 0.1251, 0.1123, 0.0828, 0.0744], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.5596, 0.1900, 0.0347, 0.0333, 0.0087], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' read'])\n",
      "(tensor([0.3044, 0.2187, 0.0642, 0.0439, 0.0289], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' chance', ' really', ' brain'])\n",
      "(tensor([0.1436, 0.1232, 0.0807, 0.0278, 0.0258], grad_fn=<ToCopyBackward0>), [' idea', ' enough', ' story', ' life', ' cast'])\n",
      "(tensor([0.1261, 0.0690, 0.0638, 0.0617, 0.0401], grad_fn=<ToCopyBackward0>), [' story', ' mind', ' idea', ' knowledge', ' horror'])\n",
      "(tensor([0.2557, 0.2257, 0.2183, 0.1463, 0.0653], grad_fn=<ToCopyBackward0>), [' of', ' to', ' about', ' base', ' on'])\n",
      "(tensor([0.4462, 0.0507, 0.0500, 0.0193, 0.0095], grad_fn=<ToCopyBackward0>), [' the', ' quantum', ' this', ' what', ' human'])\n",
      "(tensor([0.1363, 0.0582, 0.0519, 0.0267, 0.0257], grad_fn=<ToCopyBackward0>), [' Titanic', ' movie', ' subject', ' technical', ' DVD'])\n",
      "(tensor([0.2681, 0.1682, 0.0612, 0.0602, 0.0577], grad_fn=<ToCopyBackward0>), [' to', ' industry', ',', '.', ' that'])\n",
      "(tensor([0.0992, 0.0801, 0.0785, 0.0691, 0.0415], grad_fn=<ToCopyBackward0>), [' make', ' comment', ' be', ' at', ' not'])\n",
      "(tensor([0.6242, 0.1000, 0.0634, 0.0551, 0.0197], grad_fn=<ToCopyBackward0>), [' a', ' an', ' it', ' some', ' the'])\n",
      "(tensor([0.1830, 0.1601, 0.1095, 0.1013, 0.0535], grad_fn=<ToCopyBackward0>), [' good', ' decent', ' plot', ' movie', ' better'])\n",
      "(tensor([0.1653, 0.1095, 0.0909, 0.0330, 0.0317], grad_fn=<ToCopyBackward0>), [' summary', ' synopsis', ' out', ' about', ' outline'])\n",
      "(tensor([9.8199e-01, 2.8496e-03, 1.4776e-03, 1.3560e-03, 8.1518e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' for', '.', ' myself', ' it'])\n",
      "(tensor([0.9105, 0.0366, 0.0051, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' it', ' the', ' all', ' them', ' that'])\n",
      "(tensor([0.6598, 0.1829, 0.0145, 0.0111, 0.0096], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.2740, 0.0575, 0.0548, 0.0383, 0.0350], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' So'])\n",
      "(tensor([0.1248, 0.0978, 0.0754, 0.0385, 0.0332], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' even', ' didn'])\n",
      "(tensor([0.4017, 0.1758, 0.1613, 0.0631, 0.0185], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ' the', ' this'])\n",
      "(tensor([0.5781, 0.2094, 0.0682, 0.0297, 0.0282], grad_fn=<ToCopyBackward0>), [' would', ' was', ' could', ' might', ' had'])\n",
      "(tensor([0.8602, 0.0313, 0.0196, 0.0106, 0.0099], grad_fn=<ToCopyBackward0>), [' be', ' make', ' have', ' take', ' just'])\n",
      "(tensor([0.2178, 0.1074, 0.0720, 0.0716, 0.0388], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' entertaining', ' more', ' easy'])\n",
      "(tensor([0.2759, 0.2371, 0.0845, 0.0523, 0.0436], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' enjoyable', ' fun', ' difficult'])\n",
      "(tensor([0.5733, 0.0615, 0.0614, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' to', ' if', ' than', ' for', ' and'])\n",
      "(tensor([0.1711, 0.1116, 0.0494, 0.0470, 0.0409], grad_fn=<ToCopyBackward0>), [' make', ' see', ' write', ' watch', ' have'])\n",
      "(tensor([0.2632, 0.2365, 0.0753, 0.0385, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' as', ' a'])\n",
      "(tensor([0.1398, 0.1370, 0.1292, 0.1012, 0.0607], grad_fn=<ToCopyBackward0>), [' in', ' as', ' from', '.', ' with'])\n",
      "(tensor([0.5059, 0.1618, 0.0334, 0.0299, 0.0223], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' beginning', ' an'])\n",
      "(tensor([0.2181, 0.2160, 0.1882, 0.0621, 0.0543], grad_fn=<ToCopyBackward0>), [' perspective', ' point', ' beginning', ' start', ' standpoint'])\n",
      "(tensor([9.7117e-01, 2.3119e-02, 1.1106e-03, 5.9009e-04, 3.8072e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' that', ' as', ',', ' where'])\n",
      "(tensor([0.3494, 0.2811, 0.0627, 0.0514, 0.0258], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' someone', ' what'])\n",
      "(tensor([0.2703, 0.0492, 0.0489, 0.0341, 0.0227], grad_fn=<ToCopyBackward0>), [' viewer', ' spectator', ' person', ' movie', ' child'])\n",
      "(tensor([0.1990, 0.1812, 0.1716, 0.1192, 0.0467], grad_fn=<ToCopyBackward0>), [' who', ' than', '.', ',', ' rather'])\n",
      "(tensor([0.2632, 0.1682, 0.1346, 0.1101, 0.0565], grad_fn=<ToCopyBackward0>), [' to', ' from', ' a', ' as', ' someone'])\n",
      "(tensor([0.8544, 0.0504, 0.0122, 0.0118, 0.0116], grad_fn=<ToCopyBackward0>), [' the', ' a', ' any', ' my', ' that'])\n",
      "(tensor([0.8406, 0.0594, 0.0336, 0.0293, 0.0038], grad_fn=<ToCopyBackward0>), [' standpoint', ' point', ' perspective', ' viewpoint', ' director'])\n",
      "(tensor([9.9648e-01, 1.8590e-03, 3.3466e-04, 9.9308e-05, 9.7374e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' as', ' that', ' the', ' a'])\n",
      "(tensor([0.5019, 0.1406, 0.1096, 0.0998, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' someone', ' an', ' the', ' somebody'])\n",
      "(tensor([0.6364, 0.1916, 0.0315, 0.0204, 0.0118], grad_fn=<ToCopyBackward0>), [' who', ' involved', ' that', ' in', ' actually'])\n",
      "(tensor([0.8120, 0.1324, 0.0355, 0.0034, 0.0029], grad_fn=<ToCopyBackward0>), [' in', ' with', '.', ',', ' the'])\n",
      "(tensor([0.4392, 0.4288, 0.0353, 0.0124, 0.0122], grad_fn=<ToCopyBackward0>), [' the', ' making', ' it', ' this', ' its'])\n",
      "(tensor([0.5794, 0.2146, 0.0471, 0.0458, 0.0178], grad_fn=<ToCopyBackward0>), [' movie', ' making', ' film', ' production', ' plot'])\n",
      "(tensor([0.8567, 0.0288, 0.0125, 0.0103, 0.0082], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' making', ' in'])\n",
      "(tensor([0.2475, 0.0755, 0.0481, 0.0477, 0.0435], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' But'])\n",
      "(tensor([0.0918, 0.0751, 0.0571, 0.0425, 0.0394], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' didn', ' really', ' think'])\n",
      "(tensor([0.0916, 0.0670, 0.0570, 0.0474, 0.0417], grad_fn=<ToCopyBackward0>), [' wrong', ' really', ' interested', ' looking', ' very'])\n",
      "(tensor([0.7019, 0.0823, 0.0532, 0.0519, 0.0248], grad_fn=<ToCopyBackward0>), ['.', ' on', ',', ' in', ' about'])\n",
      "(tensor([0.1811, 0.1560, 0.0890, 0.0621, 0.0462], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad. I really thought it would be funny to watch this movie, but it wasn't. It was just terrible. It was not funny at all, it was not scary at all. The actors are so annoying. It's\n",
      "(tensor([0.3837, 0.1721, 0.0902, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2442, 0.1963, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6531, 0.0595, 0.0361, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1376, 0.0700, 0.0662, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5186, 0.0474, 0.0458, 0.0419, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2378, 0.2247, 0.2059, 0.0697, 0.0468], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.3492, 0.1174, 0.1151, 0.0204, 0.0175], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' Not'])\n",
      "(tensor([0.0852, 0.0764, 0.0707, 0.0677, 0.0598], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', \"'m\", ' can'])\n",
      "(tensor([0.2358, 0.1742, 0.0632, 0.0517, 0.0475], grad_fn=<ToCopyBackward0>), [' thought', ' did', ' didn', ',', ' wanted'])\n",
      "(tensor([0.4202, 0.3375, 0.1273, 0.0291, 0.0192], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' the', ' they'])\n",
      "(tensor([0.7448, 0.1362, 0.0206, 0.0172, 0.0139], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', ' could', ' sucked'])\n",
      "(tensor([0.7375, 0.0607, 0.0573, 0.0352, 0.0326], grad_fn=<ToCopyBackward0>), [' be', ' have', ' get', ' make', ' go'])\n",
      "(tensor([0.3335, 0.1804, 0.0505, 0.0331, 0.0328], grad_fn=<ToCopyBackward0>), [' funny', ' better', ' a', ' hilarious', ' more'])\n",
      "(tensor([0.2566, 0.2162, 0.1950, 0.0507, 0.0412], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' if', ' and'])\n",
      "(tensor([0.4649, 0.1957, 0.0385, 0.0302, 0.0190], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' make', ' have', ' be'])\n",
      "(tensor([0.2460, 0.2446, 0.1502, 0.1057, 0.0139], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ','])\n",
      "(tensor([0.4148, 0.1819, 0.0340, 0.0279, 0.0256], grad_fn=<ToCopyBackward0>), [' movie', '.', ',', ' because', ' in'])\n",
      "(tensor([0.1356, 0.1260, 0.1091, 0.1086, 0.0672], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' with', ' because'])\n",
      "(tensor([0.4314, 0.1096, 0.0960, 0.0261, 0.0193], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' so', ' since'])\n",
      "(tensor([0.2989, 0.2315, 0.0544, 0.0362, 0.0323], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' instead'])\n",
      "(tensor([0.4322, 0.2034, 0.0766, 0.0713, 0.0435], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' really', \"'s\", ' just'])\n",
      "(tensor([9.9714e-01, 9.8731e-04, 3.3121e-04, 2.0618e-04, 1.2204e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', '\"'])\n",
      "(tensor([0.6562, 0.1372, 0.0599, 0.0466, 0.0235], grad_fn=<ToCopyBackward0>), ['.', ' funny', ' even', ' at', ','])\n",
      "(tensor([0.3634, 0.2457, 0.1128, 0.0304, 0.0185], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.5269, 0.1603, 0.1156, 0.0241, 0.0196], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' didn', ' just'])\n",
      "(tensor([0.2999, 0.1335, 0.0675, 0.0421, 0.0332], grad_fn=<ToCopyBackward0>), [' just', ' so', ' not', ' really', ' a'])\n",
      "(tensor([0.1130, 0.0914, 0.0882, 0.0803, 0.0599], grad_fn=<ToCopyBackward0>), [' boring', ' a', ' horrible', ' stupid', ' terrible'])\n",
      "(tensor([0.7775, 0.0711, 0.0552, 0.0375, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.3261, 0.1638, 0.1235, 0.0216, 0.0216], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.4370, 0.1692, 0.0739, 0.0352, 0.0279], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' had', ' made'])\n",
      "(tensor([0.2104, 0.1660, 0.0951, 0.0468, 0.0408], grad_fn=<ToCopyBackward0>), [' just', ' so', ' not', ' like', ' a'])\n",
      "(tensor([0.8065, 0.1117, 0.0099, 0.0094, 0.0062], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' a', ' worth'])\n",
      "(tensor([0.4981, 0.2449, 0.1279, 0.0297, 0.0215], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' to', ' in'])\n",
      "(tensor([9.9769e-01, 7.9790e-04, 6.2531e-04, 2.9205e-04, 1.6533e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' the', ' any', ' least'])\n",
      "(tensor([0.7312, 0.1572, 0.0294, 0.0220, 0.0074], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2845, 0.1861, 0.0939, 0.0559, 0.0544], grad_fn=<ToCopyBackward0>), [' it', ' not', ' and', ' I', ' but'])\n",
      "(tensor([0.7244, 0.1995, 0.0139, 0.0135, 0.0127], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' didn', ' just'])\n",
      "(tensor([0.4460, 0.3174, 0.0449, 0.0178, 0.0147], grad_fn=<ToCopyBackward0>), [' not', ' just', ' stupid', ' really', ' boring'])\n",
      "(tensor([0.2412, 0.2169, 0.1606, 0.0858, 0.0515], grad_fn=<ToCopyBackward0>), [' entertaining', ' funny', ' interesting', ' even', ' scary'])\n",
      "(tensor([0.4528, 0.3830, 0.0458, 0.0340, 0.0221], grad_fn=<ToCopyBackward0>), [' at', ',', '.', ' or', ' and'])\n",
      "(tensor([9.9787e-01, 6.3514e-04, 4.2604e-04, 2.6307e-04, 1.6337e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' any', ' ALL', ' least', ' the'])\n",
      "(tensor([0.5719, 0.3128, 0.0321, 0.0304, 0.0049], grad_fn=<ToCopyBackward0>), [',', '.', '...', ' and', '!'])\n",
      "(tensor([0.3269, 0.3052, 0.0684, 0.0368, 0.0204], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.2436, 0.1197, 0.0492, 0.0385, 0.0329], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' movie', ' actors'])\n",
      "(tensor([0.4169, 0.0560, 0.0490, 0.0369, 0.0319], grad_fn=<ToCopyBackward0>), [' were', ' weren', ' are', ' did', ' in'])\n",
      "(tensor([0.2371, 0.0564, 0.0562, 0.0542, 0.0386], grad_fn=<ToCopyBackward0>), [' not', ' so', ' really', ' all', ' very'])\n",
      "(tensor([0.1971, 0.1443, 0.1041, 0.0276, 0.0240], grad_fn=<ToCopyBackward0>), [' annoying', ' bad', ' wooden', ' fake', ' stupid'])\n",
      "(tensor([0.2410, 0.1867, 0.1755, 0.1220, 0.0868], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' in'])\n",
      "(tensor([0.2816, 0.1517, 0.1244, 0.1147, 0.0356], grad_fn=<ToCopyBackward0>), [' I', ' They', ' It', ' The', ' And'])\n",
      "(tensor([0.4733, 0.3322, 0.0342, 0.0272, 0.0172], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' just'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty funny. It is not a good movie but it is funny. I think I laughed my butt off the whole time. I really enjoyed it. I would like to see the movie again. I really do enjoy it. It is\n",
      "(tensor([0.3837, 0.1722, 0.0902, 0.0770, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2445, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0595, 0.0359, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1757, 0.1514, 0.1088, 0.0942, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.4484, 0.1299, 0.0922, 0.0257, 0.0246], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', ' in'])\n",
      "(tensor([0.2600, 0.1845, 0.0998, 0.0193, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2974, 0.2128, 0.0685, 0.0679, 0.0555], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' has', ' is'])\n",
      "(tensor([0.1760, 0.0936, 0.0745, 0.0351, 0.0297], grad_fn=<ToCopyBackward0>), [' not', ' a', ' supposed', ' pretty', ' kind'])\n",
      "(tensor([0.1602, 0.1096, 0.1017, 0.0951, 0.0512], grad_fn=<ToCopyBackward0>), [' as', ' a', ' the', ' that', ' at'])\n",
      "(tensor([0.1835, 0.1684, 0.0859, 0.0391, 0.0367], grad_fn=<ToCopyBackward0>), [' great', ' good', ' very', ' perfect', ' comedy'])\n",
      "(tensor([0.7206, 0.1627, 0.0710, 0.0070, 0.0051], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' satire', ' plot'])\n",
      "(tensor([0.3334, 0.2433, 0.0654, 0.0578, 0.0525], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' but', ' in'])\n",
      "(tensor([0.4094, 0.0924, 0.0442, 0.0316, 0.0289], grad_fn=<ToCopyBackward0>), [' it', ' I', ' at', ' the', ' a'])\n",
      "(tensor([0.3708, 0.1740, 0.1115, 0.0517, 0.0414], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' does'])\n",
      "(tensor([0.4530, 0.1302, 0.0676, 0.0474, 0.0245], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' not', ' entertaining', ' pretty'])\n",
      "(tensor([0.6032, 0.1323, 0.0441, 0.0247, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ' and', ' as', '!', ','])\n",
      "(tensor([0.2835, 0.1470, 0.1406, 0.0262, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.0958, 0.0872, 0.0641, 0.0525, 0.0502], grad_fn=<ToCopyBackward0>), [' think', ' really', ' like', ' thought', ' am'])\n",
      "(tensor([0.2327, 0.1534, 0.1330, 0.0538, 0.0342], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.6072, 0.0464, 0.0310, 0.0276, 0.0270], grad_fn=<ToCopyBackward0>), [' laughed', ' was', ' am', ' would', ' just'])\n",
      "(tensor([0.3893, 0.1047, 0.0757, 0.0621, 0.0398], grad_fn=<ToCopyBackward0>), [' a', ' the', ' my', ' out', ' most'])\n",
      "(tensor([0.2569, 0.1714, 0.0709, 0.0459, 0.0321], grad_fn=<ToCopyBackward0>), [' butt', ' ass', ' head', ' way', ' nose'])\n",
      "(tensor([0.9187, 0.0193, 0.0077, 0.0047, 0.0043], grad_fn=<ToCopyBackward0>), [' off', ' out', ' right', ' to', ' away'])\n",
      "(tensor([0.1632, 0.1109, 0.0580, 0.0527, 0.0517], grad_fn=<ToCopyBackward0>), [' at', ' the', '.', ' for', ' a'])\n",
      "(tensor([0.5785, 0.2640, 0.0452, 0.0157, 0.0092], grad_fn=<ToCopyBackward0>), [' whole', ' entire', ' first', ' majority', ' 1'])\n",
      "(tensor([0.5840, 0.1903, 0.1416, 0.0352, 0.0211], grad_fn=<ToCopyBackward0>), [' time', ' way', ' movie', ' film', ' thing'])\n",
      "(tensor([0.6093, 0.1819, 0.0425, 0.0276, 0.0198], grad_fn=<ToCopyBackward0>), ['.', ' I', ' and', ' watching', ','])\n",
      "(tensor([0.3186, 0.1473, 0.1348, 0.0230, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.1199, 0.0847, 0.0617, 0.0500, 0.0407], grad_fn=<ToCopyBackward0>), [' really', ' think', ' was', ' thought', ' am'])\n",
      "(tensor([0.1165, 0.0690, 0.0667, 0.0597, 0.0556], grad_fn=<ToCopyBackward0>), [' enjoyed', ' think', ' like', ' wanted', ' do'])\n",
      "(tensor([0.7248, 0.1077, 0.0614, 0.0271, 0.0253], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' myself', ' watching'])\n",
      "(tensor([0.5729, 0.0767, 0.0754, 0.0432, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' I', ' though'])\n",
      "(tensor([0.4021, 0.1395, 0.0969, 0.0289, 0.0224], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', 'I'])\n",
      "(tensor([0.1442, 0.1084, 0.0648, 0.0494, 0.0463], grad_fn=<ToCopyBackward0>), [' think', ' really', ' thought', ' would', ' was'])\n",
      "(tensor([0.5658, 0.1609, 0.0523, 0.0387, 0.0230], grad_fn=<ToCopyBackward0>), [' recommend', ' like', ' not', ' suggest', ' say'])\n",
      "(tensor([0.9190, 0.0361, 0.0077, 0.0067, 0.0062], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' my', ' a'])\n",
      "(tensor([0.7922, 0.0230, 0.0192, 0.0180, 0.0146], grad_fn=<ToCopyBackward0>), [' see', ' think', ' give', ' say', ' watch'])\n",
      "(tensor([0.6116, 0.0896, 0.0826, 0.0485, 0.0474], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' more'])\n",
      "(tensor([0.3886, 0.1834, 0.0428, 0.0310, 0.0191], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' film', ' whole', ' other'])\n",
      "(tensor([0.6106, 0.0503, 0.0358, 0.0336, 0.0188], grad_fn=<ToCopyBackward0>), [' again', ' more', ' in', ' if', ' but'])\n",
      "(tensor([0.2866, 0.1218, 0.1017, 0.0955, 0.0629], grad_fn=<ToCopyBackward0>), ['.', ' if', ' but', ',', ' though'])\n",
      "(tensor([0.4891, 0.1693, 0.0558, 0.0416, 0.0167], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' But'])\n",
      "(tensor([0.1434, 0.1328, 0.1326, 0.0414, 0.0395], grad_fn=<ToCopyBackward0>), [' think', ' really', ' would', ' thought', ' like'])\n",
      "(tensor([0.1581, 0.0868, 0.0803, 0.0769, 0.0713], grad_fn=<ToCopyBackward0>), [' enjoyed', ' would', ' liked', ' do', ' think'])\n",
      "(tensor([0.3820, 0.1938, 0.1222, 0.1101, 0.0385], grad_fn=<ToCopyBackward0>), ['.', ' like', ' think', ' enjoy', ' not'])\n",
      "(tensor([0.2976, 0.0647, 0.0632, 0.0584, 0.0470], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' comed', ' movies'])\n",
      "(tensor([0.5768, 0.1068, 0.0404, 0.0390, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ' though', ' but', ' when', ','])\n",
      "(tensor([0.4837, 0.1348, 0.0419, 0.0358, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' It', ' If', ' The', 'I'])\n",
      "(tensor([0.4610, 0.2129, 0.0655, 0.0588, 0.0248], grad_fn=<ToCopyBackward0>), [' is', ' was', \"'s\", ' has', ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was terrible. It seemed to be a big commercial effort for a TV series. The acting was terrible. The story was awful. The cinematography was terrible. The editing was terrible, too. The music was horrible. I can't even\n",
      "(tensor([0.3847, 0.1714, 0.0898, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2437, 0.1959, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7763, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1274, 0.0770, 0.0659, 0.0557, 0.0419], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4615, 0.1215, 0.0733, 0.0429, 0.0328], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', ' when'])\n",
      "(tensor([0.2959, 0.1417, 0.1014, 0.0334, 0.0190], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'I'])\n",
      "(tensor([0.3068, 0.1546, 0.0596, 0.0532, 0.0441], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' seemed'])\n",
      "(tensor([0.5244, 0.2043, 0.0861, 0.0370, 0.0331], grad_fn=<ToCopyBackward0>), [' to', ' like', ' as', ' more', ' that'])\n",
      "(tensor([0.3259, 0.2998, 0.1299, 0.0177, 0.0147], grad_fn=<ToCopyBackward0>), [' be', ' me', ' have', ' lack', ' go'])\n",
      "(tensor([0.0885, 0.0838, 0.0716, 0.0715, 0.0410], grad_fn=<ToCopyBackward0>), [' trying', ' a', ' shot', ' made', ' written'])\n",
      "(tensor([0.0737, 0.0319, 0.0277, 0.0224, 0.0221], grad_fn=<ToCopyBackward0>), [' big', ' bunch', ' remake', ' very', ' parody'])\n",
      "(tensor([0.5330, 0.0388, 0.0301, 0.0201, 0.0124], grad_fn=<ToCopyBackward0>), [' joke', ',', ' commercial', '-', ' waste'])\n",
      "(tensor([0.1945, 0.0645, 0.0584, 0.0458, 0.0404], grad_fn=<ToCopyBackward0>), [' for', ' effort', ' piece', 'ization', ','])\n",
      "(tensor([0.3980, 0.0653, 0.0613, 0.0506, 0.0499], grad_fn=<ToCopyBackward0>), [' to', ',', ' that', ' and', ' for'])\n",
      "(tensor([0.2869, 0.2427, 0.0467, 0.0353, 0.0309], grad_fn=<ToCopyBackward0>), [' a', ' the', ' no', ' this', ' people'])\n",
      "(tensor([0.1401, 0.0882, 0.0602, 0.0571, 0.0367], grad_fn=<ToCopyBackward0>), [' bunch', ' TV', ' movie', ' film', ' big'])\n",
      "(tensor([0.1722, 0.1612, 0.1575, 0.1533, 0.0589], grad_fn=<ToCopyBackward0>), [' movie', ' station', ' show', ' series', '-'])\n",
      "(tensor([0.3011, 0.1324, 0.1025, 0.0672, 0.0614], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ' that'])\n",
      "(tensor([0.2050, 0.1332, 0.1308, 0.0327, 0.0325], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.2948, 0.0450, 0.0333, 0.0319, 0.0305], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' actors', ' plot', ' story'])\n",
      "(tensor([0.7788, 0.0542, 0.0246, 0.0228, 0.0200], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' seemed', ' is', ' and'])\n",
      "(tensor([0.1290, 0.1231, 0.0767, 0.0431, 0.0394], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' horrible', ' poor'])\n",
      "(tensor([0.4184, 0.2541, 0.2336, 0.0101, 0.0095], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' as', ';'])\n",
      "(tensor([0.5337, 0.1003, 0.0907, 0.0418, 0.0215], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3270, 0.1303, 0.1039, 0.0402, 0.0345], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' writing', ' cinem'])\n",
      "(tensor([0.6600, 0.0739, 0.0271, 0.0209, 0.0209], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' line', ' is', ' just'])\n",
      "(tensor([0.1011, 0.0912, 0.0534, 0.0463, 0.0353], grad_fn=<ToCopyBackward0>), [' terrible', ' ridiculous', ' stupid', ' not', ' awful'])\n",
      "(tensor([0.8126, 0.1128, 0.0331, 0.0057, 0.0050], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', '...'])\n",
      "(tensor([0.4087, 0.1597, 0.0898, 0.0585, 0.0248], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0753, 0.0622, 0.0513, 0.0457, 0.0383], grad_fn=<ToCopyBackward0>), [' cinem', ' script', ' editing', ' ending', ' plot'])\n",
      "(tensor([9.9949e-01, 3.5419e-04, 3.8048e-05, 2.9056e-05, 5.7394e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['atography', 'at', 'as', 'atics', 'atically'])\n",
      "(tensor([0.8191, 0.0298, 0.0191, 0.0133, 0.0128], grad_fn=<ToCopyBackward0>), [' was', ' and', ',', ' wasn', ' looked'])\n",
      "(tensor([0.1363, 0.1196, 0.0440, 0.0343, 0.0322], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' just', ' bad'])\n",
      "(tensor([0.9243, 0.0210, 0.0185, 0.0064, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' ('])\n",
      "(tensor([0.5557, 0.0827, 0.0733, 0.0568, 0.0293], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.2033, 0.0919, 0.0698, 0.0507, 0.0444], grad_fn=<ToCopyBackward0>), [' editing', ' script', ' plot', ' acting', ' directing'])\n",
      "(tensor([0.8936, 0.0162, 0.0130, 0.0110, 0.0096], grad_fn=<ToCopyBackward0>), [' was', ' and', ',', ' is', ' wasn'])\n",
      "(tensor([0.5092, 0.0739, 0.0678, 0.0225, 0.0218], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' horrendous', ' bad'])\n",
      "(tensor([0.9305, 0.0224, 0.0172, 0.0058, 0.0028], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', '!'])\n",
      "(tensor([0.2351, 0.2157, 0.1119, 0.0465, 0.0384], grad_fn=<ToCopyBackward0>), [' and', ' the', ' but', ' too', ' as'])\n",
      "(tensor([0.9414, 0.0232, 0.0064, 0.0041, 0.0034], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' bad', '!'])\n",
      "(tensor([0.2548, 0.1653, 0.1160, 0.0446, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.1129, 0.0703, 0.0694, 0.0519, 0.0479], grad_fn=<ToCopyBackward0>), [' only', ' editing', ' acting', ' script', ' music'])\n",
      "(tensor([0.7867, 0.0245, 0.0195, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' was', ',', ' just', ' and', ' is'])\n",
      "(tensor([0.2721, 0.0729, 0.0553, 0.0502, 0.0325], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' bad', ' not'])\n",
      "(tensor([0.8304, 0.0700, 0.0392, 0.0110, 0.0089], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', '!'])\n",
      "(tensor([0.3994, 0.1228, 0.0934, 0.0657, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0752, 0.0696, 0.0675, 0.0644, 0.0574], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' just', ' don'])\n",
      "(tensor([0.6054, 0.1139, 0.0534, 0.0240, 0.0195], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' see', ' remember', ' only'])\n",
      "(tensor([0.2801, 0.2112, 0.0937, 0.0747, 0.0506], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' really', ' remember', ' imagine'])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83580659-c040-48c7-9dd3-85285d08d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_triple_alt= Model_Import_6.MultiHeadModel_PyTorch_Stacked_Triple_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_triple_alt.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ede3e-a973-4c3d-ac4c-5189683e6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-92ca123d1d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_style_w_dev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_stacked_triple_alt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_token_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mneg_token_ids_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dev implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6280d96fe765>\u001b[0m in \u001b[0;36mtrain_one_style_w_dev\u001b[0;34m(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# print(stacked_context_sample.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mnetwork_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_context_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken_ids_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ONE text id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_triple_alt, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233b97a-94e4-48ee-a50c-a64c0209ce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_triple_alt, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9347c-d59e-40bb-8f6f-142acab9e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_cross_positional= Model_Import_6.MultiHeadModel_PyTorch_Stacked_Positional(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_cross_positional.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857edf07-d221-4069-be41-e0c9411f26a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.3605806827545166\n",
      "DEV LOSS: 3.2910168653964997\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.248650312423706\n",
      "DEV LOSS: 3.280663930606842\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.177448034286499\n",
      "DEV LOSS: 3.2733029612064364\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1034562587738037\n",
      "DEV LOSS: 3.288507435274124\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.025240421295166\n",
      "DEV LOSS: 3.30116226682663\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_cross_positional, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55804d8f-682f-46fd-9435-eb5d9bcf62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "WEIRD TOKEN PREDICTED\n",
      "0: I thought it was good to have some of those guys in there, but I was a big part in getting the movie to where we are now, so it's not really fair. But it's not a good movie, but I don´ve got a feeling it will get a good review when the movie is released in theaters.\"I was in a relationship at that point and we had been dating a year, so it wasn't a good time. But it wasn´ve a lot of good times and a good movie\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_cross_positional, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e7468-7645-4bf3-a8d0-86174cd7f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceebf6-2bb9-4e26-bfc0-886686500c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2936744689941406\n",
      "DEV LOSS: 3.245574431180954\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.223997116088867\n",
      "DEV LOSS: 3.2370047632217407\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1751065254211426\n",
      "DEV LOSS: 3.239879521083832\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.121413230895996\n",
      "DEV LOSS: 3.2442128241062163\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.0633127689361572\n",
      "DEV LOSS: 3.2566996613502504\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0b51a-5a47-4847-96af-90c66df0bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c37b5f-8e26-4c22-aa8c-edb6179fee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_one_not_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked_One_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_one_not_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e8359-2f56-4336-800a-836c557ebe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 1.9614466428756714\n",
      "DEV LOSS: 0.82565540587008\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.582233190536499\n",
      "DEV LOSS: 0.5536075608596206\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.36249151825904846\n",
      "DEV LOSS: 0.45461988470181824\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.27979013323783875\n",
      "DEV LOSS: 0.4196639986075461\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.2359471619129181\n",
      "DEV LOSS: 0.4072770089544356\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_one_not_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb577847-c504-4d6c-aa28-4a80890f770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The movie movie movie movie movie movie movie movie movie movie movie\n",
      "(tensor([9.9798e-01, 1.0970e-03, 1.6808e-04, 1.4440e-04, 8.4563e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', ' movies', 'movie', ' the', ' theater'])\n",
      "(tensor([9.9904e-01, 7.2250e-04, 4.6235e-05, 3.1694e-05, 1.5173e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' movies', ' picture', ' theater'])\n",
      "(tensor([9.9865e-01, 9.7820e-04, 4.1188e-05, 3.6795e-05, 2.6668e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' movies', ' game', ' picture'])\n",
      "(tensor([9.9883e-01, 7.1711e-04, 5.0114e-05, 4.7247e-05, 2.7799e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', ' picture'])\n",
      "(tensor([9.9917e-01, 3.6677e-04, 4.7966e-05, 4.2018e-05, 2.5491e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', ' picture'])\n",
      "(tensor([9.9896e-01, 3.2737e-04, 6.8437e-05, 5.9356e-05, 3.5498e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', '.'])\n",
      "(tensor([9.9859e-01, 2.8834e-04, 9.5757e-05, 8.4732e-05, 6.8489e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', '.'])\n",
      "(tensor([9.9805e-01, 2.6942e-04, 1.2380e-04, 1.1909e-04, 1.1148e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', '.', ' movies'])\n",
      "(tensor([9.9737e-01, 2.5487e-04, 1.7901e-04, 1.5267e-04, 1.3634e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', '.', ' game', ' movies'])\n",
      "(tensor([9.9649e-01, 2.6195e-04, 2.5256e-04, 1.8308e-04, 1.7963e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', '.', 'movie', ' game', ' guy'])\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_one_not_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 10, sen_to_generate = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0df23-e5c1-43c5-bc98-bde3e1bafce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_basic_positional= Model_Import_6.MultiHeadModel_PyTorch_Positional(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_basic_positional.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7770e92-9e63-4cbc-815f-49b19d0e37ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.3547589778900146\n",
      "DEV LOSS: 3.295817288970947\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.257972478866577\n",
      "DEV LOSS: 3.276579099750519\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1971757411956787\n",
      "DEV LOSS: 3.2754206407546995\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1409406661987305\n",
      "DEV LOSS: 3.2812213646888733\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.0841259956359863\n",
      "DEV LOSS: 3.29154849858284\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic_positional, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d163371-ab62-40f8-96fb-8b843e3e3df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_basic_positional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b22fc650e9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_style_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_basic_positional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_basic_positional' is not defined"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic_positional, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f99cb7-a143-46b1-b66e-bf43af2fb9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "pytorch_basic= Model_Import_6.MultiHeadModel_PyTorch(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63fdff-ad55-48dd-85a5-6b73a00c5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866df27-fcc6-4878-8402-a72412c472cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2852327823638916\n",
      "DEV LOSS: 3.245586234807968\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2262442111968994\n",
      "DEV LOSS: 3.2330075203418733\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.182225227355957\n",
      "DEV LOSS: 3.2341347328186036\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 3, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9c3b8-9f82-4758-85aa-cea5ad417938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought this film was a good movie. I liked the cast, I liked the script. It's not the most entertaining movie ever made, but if you're looking for something to get you through a tough time in life, this one's for the job\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "/n/n\n",
      "0: I thought it was funny to watch this movie because it has nothing to do with the original, and it's not even funny because it's just so bad. The original is a classic of the genre. It's one of the greatest films ever. I don\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "/n/n\n",
      "0: I thought I'd like to watch this movie with my friends, because this movie is really funny. It was really boring when it first came out. It's really boring now. I don't know why. It's really boring. It's boring, and\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "/n/n\n",
      "0: I thought this was a pretty bad film. It was just bad in so many ways. It was just bad. It was bad. The acting was bad. The script was bad. And so many of the characters in it were bad, and it was just\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "/n/n\n",
      "0: I thought it was the worst film of all time. I think it's the worst film of all time. It's a bad movie, but the acting is horrible. It's so awful. It's like the best thing you can say about this movie is\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "/n/n\n",
      "0: I thought the worst thing about this movie was that it was so predictable and so bad, but it was so predictable I actually laughed out loud at a few moments in it. It's so predictable, I mean it's so predictable, it's like a soap\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "/n/n\n",
      "0: I thought it would be nice to be a bit of a bad-ass and go into this movie without any preconceptions about how to watch it. This was the first time in a long time that I was not intimidated by the subject matter. I was looking\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "(tensor([0.1569, 0.1451, 0.1326, 0.1002, 0.0757], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4590, 0.2953, 0.0466, 0.0197, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.9160, 0.0392, 0.0055, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' take', ' help'])\n",
      "(tensor([0.2873, 0.1349, 0.0694, 0.0633, 0.0625], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' funny', ' nice', ' fun'])\n",
      "(tensor([0.8317, 0.0872, 0.0547, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' to', ' if', ' for', ',', ' and'])\n",
      "(tensor([0.2233, 0.1080, 0.0635, 0.0575, 0.0444], grad_fn=<ToCopyBackward0>), [' have', ' see', ' show', ' give', ' be'])\n",
      "(tensor([0.4230, 0.1008, 0.0924, 0.0369, 0.0175], grad_fn=<ToCopyBackward0>), [' able', ' a', ' in', ' the', ' with'])\n",
      "(tensor([0.3518, 0.1345, 0.0694, 0.0301, 0.0295], grad_fn=<ToCopyBackward0>), [' little', ' part', ' bit', ' good', ' big'])\n",
      "(tensor([0.4939, 0.2087, 0.0232, 0.0231, 0.0135], grad_fn=<ToCopyBackward0>), [' more', ' of', ' less', ' different', ' better'])\n",
      "(tensor([0.7002, 0.1057, 0.0392, 0.0089, 0.0042], grad_fn=<ToCopyBackward0>), [' a', ' an', ' fun', ' the', ' \"'])\n",
      "(tensor([0.1252, 0.0228, 0.0207, 0.0194, 0.0133], grad_fn=<ToCopyBackward0>), [' bad', ' fan', ' cheer', ' good', ' \"'])\n",
      "(tensor([0.3039, 0.2247, 0.1490, 0.0625, 0.0448], grad_fn=<ToCopyBackward0>), [' guy', '-', ' ass', 'die', ' cop'])\n",
      "(tensor([0.7853, 0.0375, 0.0213, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), ['ass', 'guy', 'boy', 'a', 'cop'])\n",
      "(tensor([0.1451, 0.0947, 0.0941, 0.0629, 0.0334], grad_fn=<ToCopyBackward0>), [' and', ',', ' in', '.', ' for'])\n",
      "(tensor([0.0932, 0.0667, 0.0481, 0.0467, 0.0325], grad_fn=<ToCopyBackward0>), [' show', ' to', ' take', ' go', ' have'])\n",
      "(tensor([0.1624, 0.1536, 0.0694, 0.0592, 0.0535], grad_fn=<ToCopyBackward0>), [' out', ' to', ' into', ' for', ' in'])\n",
      "(tensor([0.5626, 0.1353, 0.1168, 0.0193, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' battle', ' it'])\n",
      "(tensor([0.4028, 0.1172, 0.0959, 0.0198, 0.0197], grad_fn=<ToCopyBackward0>), [' movie', ' with', ' film', ' thing', ' as'])\n",
      "(tensor([0.2565, 0.1413, 0.0597, 0.0397, 0.0372], grad_fn=<ToCopyBackward0>), [' with', ' as', ' without', ' like', ' and'])\n",
      "(tensor([0.3053, 0.1414, 0.0866, 0.0593, 0.0346], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' having', ' being'])\n",
      "(tensor([0.2164, 0.0379, 0.0337, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' precon', ' expectations', ' of', ' knowledge', ' real'])\n",
      "(tensor([6.2642e-01, 3.5667e-01, 1.4625e-02, 2.9604e-04, 2.9408e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ceived', 'ceptions', 'ception', 'c', 'ci'])\n",
      "(tensor([0.2519, 0.1710, 0.0975, 0.0791, 0.0759], grad_fn=<ToCopyBackward0>), ['.', ' about', ',', ' of', ' or'])\n",
      "(tensor([0.2766, 0.2001, 0.1954, 0.1053, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' what', ' the', ' how', ' who'])\n",
      "(tensor([0.3231, 0.1287, 0.0968, 0.0699, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' I', ' to', ' the', ' bad'])\n",
      "(tensor([0.1385, 0.1084, 0.0691, 0.0584, 0.0523], grad_fn=<ToCopyBackward0>), [' act', ' play', ' do', ' approach', ' watch'])\n",
      "(tensor([0.6693, 0.1506, 0.0336, 0.0306, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' a', ' or', ' the', ' this'])\n",
      "(tensor([0.6428, 0.1237, 0.0932, 0.0321, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', '...'])\n",
      "(tensor([0.2183, 0.0685, 0.0617, 0.0481, 0.0375], grad_fn=<ToCopyBackward0>), [' I', ' It', ' So', ' This', ' The'])\n",
      "(tensor([0.3423, 0.2492, 0.0884, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' way'])\n",
      "(tensor([0.2108, 0.1597, 0.0965, 0.0602, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' supposed', ' my'])\n",
      "(tensor([0.2119, 0.2028, 0.0604, 0.0362, 0.0348], grad_fn=<ToCopyBackward0>), [' worst', ' first', ' only', ' most', ' movie'])\n",
      "(tensor([0.5333, 0.1501, 0.1121, 0.0196, 0.0105], grad_fn=<ToCopyBackward0>), [' movie', ' time', ' film', ' horror', ' one'])\n",
      "(tensor([0.7881, 0.0596, 0.0358, 0.0097, 0.0073], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' we', ' since'])\n",
      "(tensor([0.4479, 0.2198, 0.0428, 0.0378, 0.0209], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' years', ' many'])\n",
      "(tensor([0.6355, 0.2477, 0.0271, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' long', ' while', ' very', ' few', ' really'])\n",
      "(tensor([0.7863, 0.1842, 0.0201, 0.0037, 0.0010], grad_fn=<ToCopyBackward0>), [' time', ' while', ',', ' long', '-'])\n",
      "(tensor([0.5275, 0.2670, 0.1061, 0.0269, 0.0149], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' when', ','])\n",
      "(tensor([0.9208, 0.0131, 0.0078, 0.0072, 0.0071], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' we', ' a'])\n",
      "(tensor([0.1189, 0.0911, 0.0725, 0.0655, 0.0561], grad_fn=<ToCopyBackward0>), [' was', ' didn', \"'ve\", ' had', ' watched'])\n",
      "(tensor([0.1623, 0.1010, 0.0830, 0.0684, 0.0371], grad_fn=<ToCopyBackward0>), [' able', ' actually', ' not', ' really', ' looking'])\n",
      "(tensor([0.0670, 0.0607, 0.0386, 0.0268, 0.0243], grad_fn=<ToCopyBackward0>), [' afraid', ' intimidated', ' expecting', ' disappointed', ' looking'])\n",
      "(tensor([0.7763, 0.0367, 0.0298, 0.0209, 0.0190], grad_fn=<ToCopyBackward0>), [' by', ' to', ' or', '.', ' when'])\n",
      "(tensor([0.3455, 0.1528, 0.0787, 0.0572, 0.0229], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' anything', ' something'])\n",
      "(tensor([0.1436, 0.0985, 0.0651, 0.0317, 0.0298], grad_fn=<ToCopyBackward0>), [' idea', ' prospect', ' subject', ' genre', ' thought'])\n",
      "(tensor([0.9292, 0.0202, 0.0166, 0.0120, 0.0057], grad_fn=<ToCopyBackward0>), [' matter', '.', ' of', ' material', ','])\n",
      "(tensor([0.4196, 0.1691, 0.1087, 0.0784, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' or'])\n",
      "(tensor([0.2578, 0.0957, 0.0462, 0.0431, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1626, 0.0484, 0.0475, 0.0404, 0.0304], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' watched', ' didn'])\n",
      "(tensor([0.0837, 0.0786, 0.0496, 0.0426, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' actually', ' looking', ' excited', ' able'])\n",
      "/n/n\n",
      "0: I thought it was funny. This movie is terrible and it is so bad it's funny. This movie was bad, but it was funny. The humor in this movie is just awful. I can only imagine what this movie would be like if it were written\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "(tensor([0.1569, 0.1451, 0.1326, 0.1002, 0.0757], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4590, 0.2953, 0.0466, 0.0197, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.9160, 0.0392, 0.0055, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' take', ' help'])\n",
      "(tensor([0.2873, 0.1349, 0.0694, 0.0633, 0.0625], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' funny', ' nice', ' fun'])\n",
      "(tensor([0.8317, 0.0872, 0.0547, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' to', ' if', ' for', ',', ' and'])\n",
      "(tensor([0.2233, 0.1080, 0.0635, 0.0575, 0.0444], grad_fn=<ToCopyBackward0>), [' have', ' see', ' show', ' give', ' be'])\n",
      "(tensor([0.4230, 0.1008, 0.0924, 0.0369, 0.0175], grad_fn=<ToCopyBackward0>), [' able', ' a', ' in', ' the', ' with'])\n",
      "(tensor([0.3518, 0.1345, 0.0694, 0.0301, 0.0295], grad_fn=<ToCopyBackward0>), [' little', ' part', ' bit', ' good', ' big'])\n",
      "(tensor([0.4939, 0.2087, 0.0232, 0.0231, 0.0135], grad_fn=<ToCopyBackward0>), [' more', ' of', ' less', ' different', ' better'])\n",
      "(tensor([0.7002, 0.1057, 0.0392, 0.0089, 0.0042], grad_fn=<ToCopyBackward0>), [' a', ' an', ' fun', ' the', ' \"'])\n",
      "(tensor([0.1252, 0.0228, 0.0207, 0.0194, 0.0133], grad_fn=<ToCopyBackward0>), [' bad', ' fan', ' cheer', ' good', ' \"'])\n",
      "(tensor([0.3039, 0.2247, 0.1490, 0.0625, 0.0448], grad_fn=<ToCopyBackward0>), [' guy', '-', ' ass', 'die', ' cop'])\n",
      "(tensor([0.7853, 0.0375, 0.0213, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), ['ass', 'guy', 'boy', 'a', 'cop'])\n",
      "(tensor([0.1451, 0.0947, 0.0941, 0.0629, 0.0334], grad_fn=<ToCopyBackward0>), [' and', ',', ' in', '.', ' for'])\n",
      "(tensor([0.0932, 0.0667, 0.0481, 0.0467, 0.0325], grad_fn=<ToCopyBackward0>), [' show', ' to', ' take', ' go', ' have'])\n",
      "(tensor([0.1624, 0.1536, 0.0694, 0.0592, 0.0535], grad_fn=<ToCopyBackward0>), [' out', ' to', ' into', ' for', ' in'])\n",
      "(tensor([0.5626, 0.1353, 0.1168, 0.0193, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' battle', ' it'])\n",
      "(tensor([0.4028, 0.1172, 0.0959, 0.0198, 0.0197], grad_fn=<ToCopyBackward0>), [' movie', ' with', ' film', ' thing', ' as'])\n",
      "(tensor([0.2565, 0.1413, 0.0597, 0.0397, 0.0372], grad_fn=<ToCopyBackward0>), [' with', ' as', ' without', ' like', ' and'])\n",
      "(tensor([0.3053, 0.1414, 0.0866, 0.0593, 0.0346], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' having', ' being'])\n",
      "(tensor([0.2164, 0.0379, 0.0337, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' precon', ' expectations', ' of', ' knowledge', ' real'])\n",
      "(tensor([6.2642e-01, 3.5667e-01, 1.4625e-02, 2.9604e-04, 2.9408e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ceived', 'ceptions', 'ception', 'c', 'ci'])\n",
      "(tensor([0.2519, 0.1710, 0.0975, 0.0791, 0.0759], grad_fn=<ToCopyBackward0>), ['.', ' about', ',', ' of', ' or'])\n",
      "(tensor([0.2766, 0.2001, 0.1954, 0.1053, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' what', ' the', ' how', ' who'])\n",
      "(tensor([0.3231, 0.1287, 0.0968, 0.0699, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' I', ' to', ' the', ' bad'])\n",
      "(tensor([0.1385, 0.1084, 0.0691, 0.0584, 0.0523], grad_fn=<ToCopyBackward0>), [' act', ' play', ' do', ' approach', ' watch'])\n",
      "(tensor([0.6693, 0.1506, 0.0336, 0.0306, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' a', ' or', ' the', ' this'])\n",
      "(tensor([0.6428, 0.1237, 0.0932, 0.0321, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', '...'])\n",
      "(tensor([0.2183, 0.0685, 0.0617, 0.0481, 0.0375], grad_fn=<ToCopyBackward0>), [' I', ' It', ' So', ' This', ' The'])\n",
      "(tensor([0.3423, 0.2492, 0.0884, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' way'])\n",
      "(tensor([0.2108, 0.1597, 0.0965, 0.0602, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' supposed', ' my'])\n",
      "(tensor([0.2119, 0.2028, 0.0604, 0.0362, 0.0348], grad_fn=<ToCopyBackward0>), [' worst', ' first', ' only', ' most', ' movie'])\n",
      "(tensor([0.5333, 0.1501, 0.1121, 0.0196, 0.0105], grad_fn=<ToCopyBackward0>), [' movie', ' time', ' film', ' horror', ' one'])\n",
      "(tensor([0.7881, 0.0596, 0.0358, 0.0097, 0.0073], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' we', ' since'])\n",
      "(tensor([0.4479, 0.2198, 0.0428, 0.0378, 0.0209], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' years', ' many'])\n",
      "(tensor([0.6355, 0.2477, 0.0271, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' long', ' while', ' very', ' few', ' really'])\n",
      "(tensor([0.7863, 0.1842, 0.0201, 0.0037, 0.0010], grad_fn=<ToCopyBackward0>), [' time', ' while', ',', ' long', '-'])\n",
      "(tensor([0.5275, 0.2670, 0.1061, 0.0269, 0.0149], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' when', ','])\n",
      "(tensor([0.9208, 0.0131, 0.0078, 0.0072, 0.0071], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' we', ' a'])\n",
      "(tensor([0.1189, 0.0911, 0.0725, 0.0655, 0.0561], grad_fn=<ToCopyBackward0>), [' was', ' didn', \"'ve\", ' had', ' watched'])\n",
      "(tensor([0.1623, 0.1010, 0.0830, 0.0684, 0.0371], grad_fn=<ToCopyBackward0>), [' able', ' actually', ' not', ' really', ' looking'])\n",
      "(tensor([0.0670, 0.0607, 0.0386, 0.0268, 0.0243], grad_fn=<ToCopyBackward0>), [' afraid', ' intimidated', ' expecting', ' disappointed', ' looking'])\n",
      "(tensor([0.7763, 0.0367, 0.0298, 0.0209, 0.0190], grad_fn=<ToCopyBackward0>), [' by', ' to', ' or', '.', ' when'])\n",
      "(tensor([0.3455, 0.1528, 0.0787, 0.0572, 0.0229], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' anything', ' something'])\n",
      "(tensor([0.1436, 0.0985, 0.0651, 0.0317, 0.0298], grad_fn=<ToCopyBackward0>), [' idea', ' prospect', ' subject', ' genre', ' thought'])\n",
      "(tensor([0.9292, 0.0202, 0.0166, 0.0120, 0.0057], grad_fn=<ToCopyBackward0>), [' matter', '.', ' of', ' material', ','])\n",
      "(tensor([0.4196, 0.1691, 0.1087, 0.0784, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' or'])\n",
      "(tensor([0.2578, 0.0957, 0.0462, 0.0431, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1626, 0.0484, 0.0475, 0.0404, 0.0304], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' watched', ' didn'])\n",
      "(tensor([0.0837, 0.0786, 0.0496, 0.0426, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' actually', ' looking', ' excited', ' able'])\n",
      "(tensor([0.1570, 0.1450, 0.1325, 0.1008, 0.0754], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2951, 0.0466, 0.0197, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1895, 0.1132, 0.0591, 0.0363, 0.0288], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1400, 0.1229, 0.1141, 0.1006, 0.0945], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2292, 0.1684, 0.0536, 0.0443, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' But'])\n",
      "(tensor([0.2715, 0.2610, 0.1030, 0.0703, 0.0180], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.3229, 0.2541, 0.0557, 0.0275, 0.0150], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ','])\n",
      "(tensor([0.1263, 0.0581, 0.0547, 0.0456, 0.0436], grad_fn=<ToCopyBackward0>), [' so', ' supposed', ' a', ' bad', ' terrible'])\n",
      "(tensor([0.5245, 0.1393, 0.1026, 0.0431, 0.0301], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' and', '!'])\n",
      "(tensor([0.1606, 0.0894, 0.0681, 0.0286, 0.0239], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' so'])\n",
      "(tensor([0.3657, 0.0968, 0.0700, 0.0522, 0.0436], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' should', ' has'])\n",
      "(tensor([0.0726, 0.0702, 0.0696, 0.0487, 0.0309], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' the', ' supposed'])\n",
      "(tensor([0.1765, 0.1653, 0.0706, 0.0271, 0.0193], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' boring', ' stupid', ' over'])\n",
      "(tensor([0.5823, 0.1199, 0.0850, 0.0444, 0.0349], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', '.'])\n",
      "(tensor([0.3092, 0.2133, 0.0543, 0.0312, 0.0298], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' makes', ' was', ' has'])\n",
      "(tensor([0.4602, 0.1070, 0.0604, 0.0343, 0.0335], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' hilarious', ' actually', ' not'])\n",
      "(tensor([0.7961, 0.0480, 0.0360, 0.0163, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', '...'])\n",
      "(tensor([0.1728, 0.1234, 0.1148, 0.0521, 0.0193], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.4624, 0.3051, 0.0422, 0.0282, 0.0096], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.5903, 0.0626, 0.0476, 0.0372, 0.0236], grad_fn=<ToCopyBackward0>), [' is', ' has', ' was', ' sucks', ' makes'])\n",
      "(tensor([0.1789, 0.0911, 0.0590, 0.0558, 0.0459], grad_fn=<ToCopyBackward0>), [' so', ' a', ' terrible', ' bad', ' made'])\n",
      "(tensor([0.2456, 0.1712, 0.0852, 0.0600, 0.0543], grad_fn=<ToCopyBackward0>), [' enough', '.', ',', ' but', ' because'])\n",
      "(tensor([0.3649, 0.1292, 0.0728, 0.0634, 0.0327], grad_fn=<ToCopyBackward0>), [' but', ' bad', ' and', ' it', ' I'])\n",
      "(tensor([0.3389, 0.1153, 0.0534, 0.0328, 0.0302], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' at', ' this'])\n",
      "(tensor([0.3593, 0.1995, 0.1575, 0.0586, 0.0286], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' had'])\n",
      "(tensor([0.2421, 0.1695, 0.0517, 0.0316, 0.0271], grad_fn=<ToCopyBackward0>), [' funny', ' so', ' bad', ' fun', ' also'])\n",
      "(tensor([0.6874, 0.0639, 0.0332, 0.0304, 0.0250], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' and', '!'])\n",
      "(tensor([0.1703, 0.1643, 0.1026, 0.0822, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' That'])\n",
      "(tensor([0.1162, 0.1029, 0.0491, 0.0441, 0.0408], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' jokes', ' humor'])\n",
      "(tensor([0.2586, 0.1958, 0.1835, 0.0983, 0.0280], grad_fn=<ToCopyBackward0>), [' is', ' in', ' was', ' of', ' comes'])\n",
      "(tensor([0.7738, 0.0792, 0.0498, 0.0112, 0.0076], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' that'])\n",
      "(tensor([0.7792, 0.1114, 0.0287, 0.0149, 0.0099], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' one', ' was'])\n",
      "(tensor([0.5383, 0.2051, 0.0258, 0.0120, 0.0108], grad_fn=<ToCopyBackward0>), [' is', ' was', ' comes', ' makes', ' just'])\n",
      "(tensor([0.1952, 0.0375, 0.0361, 0.0351, 0.0300], grad_fn=<ToCopyBackward0>), [' so', ' just', ' not', ' terrible', ' pathetic'])\n",
      "(tensor([0.1387, 0.0988, 0.0812, 0.0500, 0.0443], grad_fn=<ToCopyBackward0>), [' so', ' awful', ' terrible', ' bad', ' pathetic'])\n",
      "(tensor([0.7941, 0.0860, 0.0479, 0.0107, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '!'])\n",
      "(tensor([0.1670, 0.1624, 0.1552, 0.0423, 0.0326], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.0772, 0.0706, 0.0586, 0.0473, 0.0470], grad_fn=<ToCopyBackward0>), [' don', ' can', ' mean', \"'m\", ' was'])\n",
      "(tensor([0.7923, 0.0407, 0.0247, 0.0178, 0.0105], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' see', ' not', ' honestly'])\n",
      "(tensor([0.1459, 0.1420, 0.0688, 0.0567, 0.0472], grad_fn=<ToCopyBackward0>), [' assume', ' imagine', ' hope', ' think', ' say'])\n",
      "(tensor([0.4870, 0.1884, 0.1467, 0.0500, 0.0199], grad_fn=<ToCopyBackward0>), [' how', ' what', ' the', ' that', ' this'])\n",
      "(tensor([0.1588, 0.1540, 0.0767, 0.0554, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' kind'])\n",
      "(tensor([0.6165, 0.0531, 0.0302, 0.0222, 0.0209], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' was', ' guy'])\n",
      "(tensor([0.2562, 0.1719, 0.1491, 0.1327, 0.0475], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ' must', \"'s\"])\n",
      "(tensor([0.5026, 0.3210, 0.0721, 0.0305, 0.0124], grad_fn=<ToCopyBackward0>), [' be', ' have', \"'ve\", ' look', ' do'])\n",
      "(tensor([0.9205, 0.0527, 0.0020, 0.0015, 0.0013], grad_fn=<ToCopyBackward0>), [' like', ' if', ' with', ' in', ' called'])\n",
      "(tensor([0.8924, 0.0230, 0.0202, 0.0086, 0.0082], grad_fn=<ToCopyBackward0>), [' if', ' with', ' in', ' to', ' on'])\n",
      "(tensor([0.3246, 0.1283, 0.0680, 0.0325, 0.0258], grad_fn=<ToCopyBackward0>), [' it', ' the', ' they', ' you', ' someone'])\n",
      "(tensor([0.3551, 0.2820, 0.1471, 0.0423, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' were', ' had', ' wasn', ' weren'])\n",
      "(tensor([0.1874, 0.1113, 0.0932, 0.0733, 0.0333], grad_fn=<ToCopyBackward0>), [' made', ' written', ' directed', ' a', ' actually'])\n",
      "/n/n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 31.10 GiB already allocated; 3.56 MiB free; 31.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-69e7f6e4b24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_style_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_tokens_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-9017fe27f7a1>\u001b[0m in \u001b[0;36mone_style_generate\u001b[0;34m(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples, num_tokens_to_generate, sen_to_generate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tokens_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# put tokenized prompt through GPT_transformer to get GPT Logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mGPT_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# put model_context_input and the GPT Logits into SAT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 )\n\u001b[1;32m    888\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    890\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_merge_heads\u001b[0;34m(self, tensor, num_heads, attn_head_size)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mMerges\u001b[0m \u001b[0mattn_head_size\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_attn_heads\u001b[0m \u001b[0mdim\u001b[0m \u001b[0minto\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 31.10 GiB already allocated; 3.56 MiB free; 31.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100,  num_tokens_to_generate = 50, sen_to_generate = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ebbb1-eabd-4f74-ba9d-1fde532fccf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_heads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c6a7cb34810e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_basic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadModel_PyTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_basic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.0001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# could be useful transformers require warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# .0001 best WD so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_heads' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_basic= Model_Import_6.MultiHeadModel_PyTorch(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "optimizer = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827156b7-f005-4486-83c2-becbee5ab331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30357e03341d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_style_w_dev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_embeds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_embeds_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtoken_ids_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dev implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-6280d96fe765>\u001b[0m in \u001b[0;36mtrain_one_style_w_dev\u001b[0;34m(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mrandom_context_samples_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to make sure dev samples are the same... !!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mstacked_context_sample_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_context_samples_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mdev_network_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_context_sample_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mshifted_network_output_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_network_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mshifted_text_ids_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_token_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Model_Import_6.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_x, decoder_x)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMulti_Head_Cross_Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0;31m# query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# print(attention)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1028\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m             \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5254\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5256\u001b[0m     \u001b[0;31m# prep attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection\u001b[0;34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[0m\n\u001b[1;32m   4994\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mb_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4995\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mb_v\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic, optimizer, R_embeds_train, logits_train, token_ids_train, 5, logits_test, R_embeds_test,  token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d277d43-3442-4ee2-97e4-20724501dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088f2d7-afb0-4819-9f32-b0b8c9502452",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_pos_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41faf21a-260a-4f74-a8a5-c0fd01814f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_basic= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1]/num_heads)).to(device) #\n",
    "# MH_basic= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8).to(device) #\n",
    "\n",
    "neg_optimizer = optim.Adam(MH_basic.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aac5c4-b80a-479d-a89d-5d587acc2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style_w_dev(MH_basic, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 4, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b77dc-6bf2-43ac-8ec6-deb93c20a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### R_neg_embeds_train, R_neg_embeds_test, neg_logits_train, neg_logits_test, neg_token_ids_train, neg_token_ids_test\n",
    "#(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4fe95-b4c4-4981-bac8-d05f7238ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.0001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d0b97-4c95-4f96-a919-0bd802bb9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40349e9e-e652-43c2-8579-6d20db619be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_deeper = Model_Import_6.DeeperModel_skip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "neg_optimizer = optim.Adam(neg_alone_model_deeper.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067df1e-f426-45ec-b1e4-7b617c229b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_deeper, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fb89a-310f-4824-8567-4ed48b1d8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_wide = Model_Import_6.WiderModel_skip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) # 2 wide -> 1:46  4 wide ->3:00\n",
    "neg_optimizer = optim.Adam(neg_alone_model_wide.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e12b7-5b99-4d95-8fd5-b58a87f314e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_wide, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db013e03-eb0b-4d7a-84e0-c79590ae0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_wide_deep = Model_Import_6.WiderDeeperModel_2(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(neg_alone_model_wide_deep.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ee9fd-05c7-4a65-ab18-703433499077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_wide_deep, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4781f3-9f3d-47f2-9b9f-4ad1ec35df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d6e2a-c8e9-4055-8f39-5eb3c9fd32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WiderBlock_8_model= Model_Import_6.WiderBlock_8(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(WiderBlock_8_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106085b9-1c37-49e1-881f-9b113f215515",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(WiderBlock_8_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76d106d-0e15-40de-bc5a-2dd488353818",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_head_8_wide_model= Model_Import_6.ProposedModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = neg_logits[0].shape[1]*8).to(device) #\n",
    "neg_optimizer = optim.Adam(one_head_8_wide_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988570cc-b693-420d-adff-5ff1df9bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(one_head_8_wide_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b65c6c-969d-4064-8f03-a67ea8edd64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5615e22-7a27-4a66-9076-8464f9bf8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, WiderBlock_8_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e92a0-b3a6-435d-ad24-870315741c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Overall I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b73618-1488-4544-a1a6-4fd7149da9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, WiderBlock_8_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100306e-4caa-482d-ad4b-ca2636ba7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I sat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f72784-04bc-4dc5-98a0-7c99c54d304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, one_head_8_wide_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e00bb-52a4-4359-ba2a-738c984e2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WiderDeeperModel_Alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d378cb9-f154-4542-99ff-933cdea1fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_deep_alt= Model_Import_6.WiderDeeperModel_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = neg_logits[0].shape[1]).to(device) #\n",
    "neg_optimizer = optim.Adam(Wide_deep_alt.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a0ea8-6597-4327-ade7-04e108a20ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(Wide_deep_alt, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af824f-b8ab-47e5-9f02-0902170df755",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_M_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1]/4)).to(device) #\n",
    "neg_optimizer = optim.Adam(narrow_M_head.parameters(), lr=0.00001,  weight_decay=0.001) # 2:00 per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad6cd9-b7e6-45ce-ba3a-680b1902a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(narrow_M_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d18fbd-9d0e-4923-8edd-ba3baa4e94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    " neg_logits[0].shape[1]/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdbad3-ace3-4264-878e-5145b886a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_neg_embeds[0].shape[0]/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d05b5-3af0-4839-8e98-f538cb976f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_S_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 1, attention_dim = int(neg_logits[0].shape[1]*8)).to(device) #\n",
    "neg_optimizer = optim.Adam(Wide_S_head.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff20260-714b-4fdd-9376-0b1dfcd42e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(Wide_S_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fab96b-2e9f-40be-95b1-d306a8e3c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_M_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(wide_M_head.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6813b71-8ecf-4ef5-bb28-18006176b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(wide_M_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa329ac-34b9-4c00-8053-90aaa6038968",
   "metadata": {},
   "outputs": [],
   "source": [
    "wider_8_fixedM= Model_Import_6.WiderBlock_8_FixedM(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(wider_8_fixedM.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffff76f-ef30-46ef-b606-7ea4264cc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(wider_8_fixedM, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29a8dc-d5bf-4bd2-a64e-13ad70c15fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, wider_8_fixedM, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39bc9e-b7c8-4a57-9f33-80ebfe3ff498",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, narrow_M_head, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2b851-e1db-4026-85f5-4f6e79141d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "middleskip_8= Model_Import_6.MultiHeadModel_WOFinalBlockSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(middleskip_8.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422f630-bea7-449f-aaed-43dac75069c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(middleskip_8, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b24115-e9a9-42b0-a218-57b6a4b620b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoskip_8= Model_Import_6.MultiHeadModel_TwoSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(twoskip_8.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3aa46-7af5-4910-9071-3c2f9b986fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(twoskip_8, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf136a1-3887-4172-bdad-a4726b75b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "noskip= Model_Import_6.MultiHeadModel_WOAnyBlockSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(noskip.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346e947-268c-4a35-a133-525410014b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(noskip, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c180e-7b94-416a-80fa-234ba1189afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraskip= Model_Import_6.MultiHeadModel_ExtraSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(extraskip.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18137d-36a7-4a29-ba55-d5adca88c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(extraskip, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7365f2-e327-46b0-a790-f8ae47fef495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9eaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel\n",
    "import math\n",
    "import Model_Import_6\n",
    "from torch import optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "# from transformers import WarmupLinearSchedule\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "6.43825664\n",
      "6.421954048\n",
      "0.016302592\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c1653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stuff(context_tensors, generator_logits, text_ids_loc):\n",
    "    context = torch.load(context_tensors, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    logits = torch.load(generator_logits, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    text_ids = torch.load(text_ids_loc, map_location=lambda storage, loc: storage.cuda(0))\n",
    "    return context, logits, text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580292fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_stuff(context_tensors, generator_logits, text_ids_loc):\n",
    "#     context = torch.load(context_tensors)\n",
    "#     logits = torch.load(generator_logits)\n",
    "#     text_ids = torch.load(text_ids_loc)\n",
    "#     return context, logits, text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db195a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "29.69567232\n",
      "29.573845504\n",
      "0.121826816\n"
     ]
    }
   ],
   "source": [
    "R_neg_embeds, neg_logits, neg_token_ids = load_stuff('R_neg_embeds.pt', 'neg_logits.pt', 'neg_token_ids.pt')\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_pos_embeds, pos_logits, pos_token_ids = load_stuff('R_pos_embeds.pt', 'pos_logits.pt', 'pos_token_ids.pt')\n",
    "# t = torch.cuda.get_device_properties(0).total_memory\n",
    "# r = torch.cuda.memory_reserved(0)\n",
    "# a = torch.cuda.memory_allocated(0)\n",
    "# f = r-a  # free inside reserved\n",
    "# print(t/1000000000)\n",
    "# print(r/1000000000)\n",
    "# print(a/1000000000)\n",
    "# print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d463bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_embeds_train, R_embeds_test, logits_train, logits_test, token_ids_train, token_ids_test = train_test_split(R_neg_embeds+R_pos_embeds, neg_logits+pos_logits, neg_token_ids+pos_token_ids,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bf125",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_neg_embeds_train, R_neg_embeds_test, neg_logits_train, neg_logits_test, neg_token_ids_train, neg_token_ids_test = train_test_split(R_neg_embeds, neg_logits, neg_token_ids,  test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(R_neg_embeds_train))\n",
    "print(len(neg_logits_train))\n",
    "print(len(neg_token_ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(R_embeds_train))\n",
    "# print(len(logits_train))\n",
    "# print(len(token_ids_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4bdab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_style(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        # print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_style_w_dev(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100):\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    total_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "        random.shuffle(to_shuffle)\n",
    "\n",
    "        logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "        model.train()\n",
    "        ag_loss_epoch = 0\n",
    "        epoch_count = 0\n",
    "        for example in range(len(logits_list)):\n",
    "            random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "            stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "            # print(stacked_context_sample.shape)\n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "            if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "                print(\"ONE text id\")\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss_epoch += loss\n",
    "            epoch_count += 1\n",
    "            total_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if example%1000 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "        model.eval()\n",
    "        CELoss_dev = nn.CrossEntropyLoss()\n",
    "        dev_loss_acum = 0\n",
    "        for dev_example in range(len(dev_logits)):\n",
    "            random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "            stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "            dev_network_output = model(stacked_context_sample_dev, dev_logits[dev_example])\n",
    "            shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous()\n",
    "            dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "            dev_loss_acum += dev_loss.item()\n",
    "        full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "        print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "        print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "        print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563fa81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_style_generate(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples = 100, num_tokens_to_generate = 1, sen_to_generate = 10):\n",
    "  #Put models in eval mode\n",
    "    SAT_model.eval()\n",
    "    GPT_transformer.eval()\n",
    "    \n",
    "    for sent in range(sen_to_generate):\n",
    "        random_context_samples = random.sample(context_to_sample, num_samples)\n",
    "        model_context_input =  torch.stack(random_context_samples, dim = 0)\n",
    "        expierment_list = []\n",
    "        # tokenize prompt\n",
    "        current_tokenization = tokenizer.encode(prompt, return_tensors = 'pt').to(device)\n",
    "\n",
    "        for generation in range(num_tokens_to_generate):\n",
    "            # put tokenized prompt through GPT_transformer to get GPT Logits\n",
    "            GPT_logits = GPT_transformer(current_tokenization).last_hidden_state.squeeze()\n",
    "\n",
    "            # put model_context_input and the GPT Logits into SAT model\n",
    "            adjusted_output = SAT_model(model_context_input, GPT_logits)\n",
    "\n",
    "            # Funtional softmax the SAT model output\n",
    "            SM_adjusted_output = torch.nn.functional.softmax(adjusted_output, dim = 1)\n",
    "\n",
    "            # argmax to get predicted token\n",
    "            # predicted_tokens = torch.argmax(SM_adjusted_output, dim =1)\n",
    "\n",
    "            # get topk tokens\n",
    "            top_predicted_tokens = torch.topk(SM_adjusted_output[-1], 5, dim =0).indices\n",
    "            top_predicted_tokens_prob = torch.topk(SM_adjusted_output[-1], 5, dim =0).values\n",
    "            top_predicted_tokens_numpy = top_predicted_tokens.cpu().detach().numpy()\n",
    "            top_predicted_tokens_prob_numpy = top_predicted_tokens_prob.cpu().detach().numpy()\n",
    "            token_prob_sum = np.sum(top_predicted_tokens_prob_numpy)\n",
    "            token_distribution = top_predicted_tokens_prob_numpy/token_prob_sum\n",
    "            \n",
    "            predicted_token =  torch.from_numpy(np.array(np.random.choice(top_predicted_tokens_numpy, p = token_distribution))).to(device)\n",
    "            \n",
    "            # print(top_predicted_tokens)\n",
    "             # tok_k_predicted_words = tokenizer.decode(top_predicted_tokens, skip_special_tokens=True)\n",
    "            decoded_tokens = []\n",
    "            for token_k in top_predicted_tokens:\n",
    "                decoded_tokens.append(tokenizer.decode(token_k, skip_special_tokens=True))\n",
    "                \n",
    "            top_predicted_tokens_prob_cpu = top_predicted_tokens_prob.cpu()\n",
    "            del top_predicted_tokens_prob\n",
    "            expierment_list.append((top_predicted_tokens_prob_cpu, decoded_tokens))\n",
    "\n",
    "\n",
    "                \n",
    "            #dif way of getting top predicted\n",
    "            # predicted_token = top_predicted_tokens[0]\n",
    "            # print(current_tokenization[-1][0])\n",
    "            # print(predicted_token)\n",
    "            # if predicted_token == 247 or predicted_token == current_tokenization[-1][0]:\n",
    "            if predicted_token == 247:\n",
    "                print(\"WEIRD TOKEN PREDICTED\")\n",
    "                predicted_token = top_predicted_tokens[1]\n",
    "\n",
    "            # print(torch.amax(SM_adjusted_output[-1]))\n",
    "\n",
    "            # print(predicted_tokens[-1].unsqueeze(0).unsqueeze(0))\n",
    "            # print(current_tokenization)\n",
    "\n",
    "            current_tokenization = torch.cat((current_tokenization, predicted_token.unsqueeze(0).unsqueeze(0)), 1)\n",
    "            del predicted_token\n",
    "            gc.collect()\n",
    "            # print(current_tokenization)\n",
    "\n",
    "        # decode\n",
    "        for i, beam in enumerate(current_tokenization):\n",
    "            # print(f\"{i}: {tokenizer.decode(beam)}\")\n",
    "            # print(f\"{i}: {current_tokenization}\")\n",
    "            print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "        del current_tokenization\n",
    "        del random_context_samples\n",
    "        del model_context_input\n",
    "        gc.collect()\n",
    "        for prediction in expierment_list:\n",
    "            print(prediction)\n",
    "        print(\"\\n\\n\")\n",
    "    # for i, beam in enumerate(predicted_tokens):\n",
    "    #     # if i == 0:\n",
    "    #     #   continue\n",
    "    #     print(f\"{i}: {tokenizer.decode(beam, skip_special_tokens=True)} token_id: {beam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0179a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_two_style_w_dev(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100):\n",
    "#     CELoss = nn.CrossEntropyLoss()\n",
    "#     total_count = 0\n",
    "#     for epoch in range(epochs):\n",
    "        \n",
    "#         to_shuffle = list(zip(logits_list, token_ids_list))\n",
    "\n",
    "#         random.shuffle(to_shuffle)\n",
    "\n",
    "#         logits_list, token_ids_list = zip(*to_shuffle)\n",
    "\n",
    "#         model.train()\n",
    "#         ag_loss_epoch = 0\n",
    "#         epoch_count = 0\n",
    "#         for example in range(len(logits_list)):\n",
    "#             random_context_samples = random.sample(context_embeds_list, num_samples) # could use another context to see what happens\n",
    "#             stacked_context_sample = torch.stack(random_context_samples, dim = 0)\n",
    "#             # print(stacked_context_sample.shape)\n",
    "#             optimizer.zero_grad()\n",
    "#             network_output = model(stacked_context_sample.to(device), logits_list[example].to(device))\n",
    "#             if token_ids_list[example]['input_ids'].shape[1] == 1:\n",
    "#                 print(\"ONE text id\")\n",
    "#                 continue\n",
    "#             shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "#             shifted_text_ids = token_ids_list[example]['input_ids'][..., 1:].contiguous().to(device)\n",
    "#             loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "#             ag_loss_epoch += loss\n",
    "#             epoch_count += 1\n",
    "#             total_count += 1\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             if example%1000 == 0:\n",
    "#                 print(\".\", end = \"\")\n",
    "#         model.eval()\n",
    "#         CELoss_dev = nn.CrossEntropyLoss()\n",
    "#         dev_loss_acum = 0\n",
    "#         for dev_example in range(len(dev_logits)):\n",
    "#             random_context_samples_dev = random.sample(dev_context, num_samples) # need to make sure dev samples are the same... !!!!\n",
    "#             stacked_context_sample_dev = torch.stack(random_context_samples_dev, dim = 0)\n",
    "#             dev_network_output = model(stacked_context_sample_dev, dev_logits[dev_example])\n",
    "#             shifted_network_output_dev = dev_network_output[..., :-1, :].contiguous()\n",
    "#             shifted_text_ids_dev = dev_token_ids[dev_example]['input_ids'][..., 1:].contiguous()\n",
    "#             dev_loss = CELoss_dev(shifted_network_output_dev.view(-1, shifted_network_output_dev.size(-1)), shifted_text_ids_dev.view(-1))\n",
    "#             dev_loss_acum += dev_loss.item()\n",
    "#         full_dev_loss = dev_loss_acum / len(dev_logits)\n",
    "        \n",
    "#         print(f\"Epoch: {epoch}, Epoch Examples: {epoch_count}\")\n",
    "#         print(f\"TRAIN LOSS: {ag_loss_epoch / len(logits_list)}\")\n",
    "#         print(f\"DEV LOSS: {full_dev_loss}\")\n",
    "#         print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec308387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "# neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f86bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_one_style(neg_alone_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "# neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_one_style_w_dev(neg_alone_model, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 20, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bdec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.314694656\n",
      "29.69567232\n",
      "29.573845504\n",
      "0.121826816\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_heads = 16\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_two_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_two_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far lr=0.0001,  weight_decay=.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef009f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2931504249572754\n",
      "DEV LOSS: 3.246247928380966\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2244300842285156\n",
      "DEV LOSS: 3.234160268497467\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.175823211669922\n",
      "DEV LOSS: 3.237887247514725\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.122915267944336\n",
      "DEV LOSS: 3.240265000295639\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.06491756439209\n",
      "DEV LOSS: 3.254282879114151\n",
      "----------------------------------------\n",
      "..........Epoch: 5, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.003410577774048\n",
      "DEV LOSS: 3.2699905432224274\n",
      "----------------------------------------\n",
      "..........Epoch: 6, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.937988519668579\n",
      "DEV LOSS: 3.285699222135544\n",
      "----------------------------------------\n",
      "..........Epoch: 7, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.872974395751953\n",
      "DEV LOSS: 3.3122050407409667\n",
      "----------------------------------------\n",
      "..........Epoch: 8, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.8064165115356445\n",
      "DEV LOSS: 3.353215451431274\n",
      "----------------------------------------\n",
      "..........Epoch: 9, Epoch Examples: 10000\n",
      "TRAIN LOSS: 2.741619348526001\n",
      "DEV LOSS: 3.378151350927353\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_two_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 10, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a7ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought that this film was pretty boring. The only reason I watched it was because I was curious about it. But the acting wasn't too bad. The story is not that great either. The acting was good enough for a B-Movie movie. I\n",
      "(tensor([0.3836, 0.1720, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3320, 0.2363, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4145, 0.1970, 0.1690, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6391, 0.0585, 0.0580, 0.0559, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1007, 0.0720, 0.0701, 0.0441, 0.0339], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1270, 0.0969, 0.0869, 0.0650, 0.0502], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' atro'])\n",
      "(tensor([0.3230, 0.2953, 0.1496, 0.0408, 0.0155], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '...'])\n",
      "(tensor([0.1770, 0.1761, 0.1543, 0.0364, 0.0196], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.1856, 0.0623, 0.0580, 0.0477, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' only', ' movie', ' plot'])\n",
      "(tensor([0.2880, 0.1840, 0.0666, 0.0513, 0.0475], grad_fn=<ToCopyBackward0>), [' thing', ' interesting', ' good', ' funny', ' reason'])\n",
      "(tensor([0.6726, 0.0862, 0.0753, 0.0669, 0.0446], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' to', ' i'])\n",
      "(tensor([0.4499, 0.0745, 0.0568, 0.0335, 0.0267], grad_fn=<ToCopyBackward0>), [' watched', ' even', ' gave', ' didn', \"'m\"])\n",
      "(tensor([0.8710, 0.0555, 0.0535, 0.0103, 0.0011], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' all'])\n",
      "(tensor([0.7153, 0.1323, 0.0339, 0.0101, 0.0082], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' from'])\n",
      "(tensor([0.5384, 0.1810, 0.0669, 0.0446, 0.0343], grad_fn=<ToCopyBackward0>), [' because', ' to', ' for', ' the', ' that'])\n",
      "(tensor([0.4096, 0.1437, 0.1180, 0.0730, 0.0315], grad_fn=<ToCopyBackward0>), [' I', ' of', ' the', ' it', ' my'])\n",
      "(tensor([0.1285, 0.1196, 0.0961, 0.0674, 0.0496], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' like', ' really'])\n",
      "(tensor([0.1398, 0.0736, 0.0583, 0.0566, 0.0402], grad_fn=<ToCopyBackward0>), [' a', ' in', ' curious', ' looking', ' so'])\n",
      "(tensor([0.5164, 0.1355, 0.0829, 0.0682, 0.0326], grad_fn=<ToCopyBackward0>), [' to', ' about', ' as', '.', ' what'])\n",
      "(tensor([0.3831, 0.2249, 0.0390, 0.0317, 0.0238], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' what'])\n",
      "(tensor([0.5872, 0.2077, 0.0441, 0.0198, 0.0124], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' being'])\n",
      "(tensor([0.3423, 0.1318, 0.1099, 0.0364, 0.0166], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' If'])\n",
      "(tensor([0.1474, 0.1197, 0.1094, 0.1021, 0.0380], grad_fn=<ToCopyBackward0>), [' I', ' after', ' it', ' the', ','])\n",
      "(tensor([0.1405, 0.1275, 0.0812, 0.0484, 0.0365], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' ending'])\n",
      "(tensor([0.6483, 0.0614, 0.0555, 0.0510, 0.0330], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' wasn', ' in'])\n",
      "(tensor([9.9618e-01, 1.1630e-03, 3.8003e-04, 2.5945e-04, 2.0240e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.1394, 0.1285, 0.1106, 0.0906, 0.0717], grad_fn=<ToCopyBackward0>), [' good', ' very', ' that', ' even', ' too'])\n",
      "(tensor([0.6197, 0.1500, 0.0337, 0.0264, 0.0207], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' great', ' convincing', ' special'])\n",
      "(tensor([0.3879, 0.2684, 0.1192, 0.0318, 0.0267], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' either', ' for'])\n",
      "(tensor([0.1686, 0.1530, 0.1528, 0.0614, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.1792, 0.1721, 0.0983, 0.0564, 0.0360], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' only', ' movie', ' acting'])\n",
      "(tensor([0.5613, 0.1300, 0.0364, 0.0245, 0.0225], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' line', ' seemed'])\n",
      "(tensor([0.2382, 0.0634, 0.0554, 0.0412, 0.0388], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' interesting', ' very', ' OK'])\n",
      "(tensor([0.3121, 0.2087, 0.0821, 0.0710, 0.0535], grad_fn=<ToCopyBackward0>), [' that', ' too', ' very', ' interesting', ' really'])\n",
      "(tensor([0.3979, 0.1820, 0.0986, 0.0452, 0.0252], grad_fn=<ToCopyBackward0>), [' great', ' interesting', ' good', ' bad', ' exciting'])\n",
      "(tensor([0.3459, 0.2182, 0.1813, 0.0828, 0.0380], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' either', ' but'])\n",
      "(tensor([0.7822, 0.1178, 0.0191, 0.0120, 0.0062], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' ('])\n",
      "(tensor([0.1892, 0.1678, 0.1251, 0.0429, 0.0315], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' There', ' But'])\n",
      "(tensor([0.2036, 0.0904, 0.0624, 0.0476, 0.0404], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' main', ' story'])\n",
      "(tensor([0.3962, 0.2501, 0.0573, 0.0514, 0.0229], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' wasn', ' in'])\n",
      "(tensor([0.1504, 0.1471, 0.1429, 0.1149, 0.0400], grad_fn=<ToCopyBackward0>), [' not', ' okay', ' pretty', ' OK', ' good'])\n",
      "(tensor([0.1968, 0.1213, 0.1095, 0.1031, 0.1028], grad_fn=<ToCopyBackward0>), [' though', ',', ' but', ' enough', '.'])\n",
      "(tensor([0.4185, 0.2487, 0.1017, 0.0664, 0.0373], grad_fn=<ToCopyBackward0>), [' to', ' for', ' though', ' that', ','])\n",
      "(tensor([0.5449, 0.1590, 0.0789, 0.0773, 0.0250], grad_fn=<ToCopyBackward0>), [' me', ' it', ' the', ' a', ' what'])\n",
      "(tensor([0.2976, 0.1705, 0.0495, 0.0454, 0.0225], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' low', ' B', ' soap'])\n",
      "(tensor([0.6455, 0.0767, 0.0721, 0.0480, 0.0228], grad_fn=<ToCopyBackward0>), ['-', ' movie', ' film', ' grade', 'ollywood'])\n",
      "(tensor([0.4029, 0.3466, 0.0470, 0.0436, 0.0271], grad_fn=<ToCopyBackward0>), ['Movie', 'movie', 'level', 'film', 'grade'])\n",
      "(tensor([0.3139, 0.0895, 0.0857, 0.0427, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' though', ' movie'])\n",
      "(tensor([0.6159, 0.0994, 0.0572, 0.0547, 0.0248], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' like', ' though'])\n",
      "(tensor([0.2069, 0.1610, 0.0959, 0.0896, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' The', ' But', ' It', ' There'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a really bad movie. And I really don't know if it was the direction the director was taking the movie, or the actors or whatever, but it just was bad. It had no point, it was just bad. It\n",
      "(tensor([0.3832, 0.1726, 0.0905, 0.0771, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3333, 0.2353, 0.0581, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4141, 0.1975, 0.1692, 0.0494, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6398, 0.0582, 0.0582, 0.0558, 0.0175], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1008, 0.0722, 0.0701, 0.0442, 0.0339], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1290, 0.0766, 0.0559, 0.0513, 0.0481], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.4327, 0.0503, 0.0471, 0.0271, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' boring', ' cheesy', ' funny'])\n",
      "(tensor([0.1860, 0.1192, 0.0965, 0.0595, 0.0593], grad_fn=<ToCopyBackward0>), [' idea', ' movie', ' film', ' copy', ' joke'])\n",
      "(tensor([0.5647, 0.0787, 0.0468, 0.0390, 0.0266], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', '....'])\n",
      "(tensor([0.2729, 0.1420, 0.0821, 0.0294, 0.0268], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Not'])\n",
      "(tensor([0.4861, 0.0630, 0.0619, 0.0543, 0.0380], grad_fn=<ToCopyBackward0>), [' I', ' then', ' it', ' that', ' the'])\n",
      "(tensor([0.1439, 0.0961, 0.0810, 0.0466, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' thought', ' have'])\n",
      "(tensor([0.1033, 0.0787, 0.0538, 0.0529, 0.0442], grad_fn=<ToCopyBackward0>), [' wanted', ' thought', ' don', ' didn', ' think'])\n",
      "(tensor([9.9590e-01, 1.4330e-03, 5.8768e-04, 2.5706e-04, 2.0183e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3012, 0.1689, 0.1529, 0.1399, 0.0341], grad_fn=<ToCopyBackward0>), [' like', ' think', ' know', ' understand', ' want'])\n",
      "(tensor([0.5143, 0.1190, 0.0935, 0.0900, 0.0814], grad_fn=<ToCopyBackward0>), [' why', ' how', ' where', ' what', ' if'])\n",
      "(tensor([0.3904, 0.1705, 0.1653, 0.0571, 0.0418], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' this', ' the'])\n",
      "(tensor([0.4792, 0.1526, 0.0666, 0.0514, 0.0317], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' could', ' would'])\n",
      "(tensor([0.3581, 0.1556, 0.0360, 0.0299, 0.0271], grad_fn=<ToCopyBackward0>), [' a', ' the', ' as', ' because', ' an'])\n",
      "(tensor([0.1071, 0.0895, 0.0673, 0.0540, 0.0461], grad_fn=<ToCopyBackward0>), [' story', ' script', ' right', ' acting', ' direction'])\n",
      "(tensor([0.1616, 0.1088, 0.1054, 0.0911, 0.0597], grad_fn=<ToCopyBackward0>), [' the', ' or', ',', ' that', '.'])\n",
      "(tensor([0.2572, 0.1585, 0.1129, 0.0829, 0.0421], grad_fn=<ToCopyBackward0>), [' director', ' movie', ' story', ' film', ' studio'])\n",
      "(tensor([0.6092, 0.1897, 0.0599, 0.0270, 0.0147], grad_fn=<ToCopyBackward0>), [' was', ' wanted', ' took', ' had', ' gave'])\n",
      "(tensor([0.4164, 0.1832, 0.0887, 0.0403, 0.0216], grad_fn=<ToCopyBackward0>), [' going', ' taking', ' trying', ' in', ' shooting'])\n",
      "(tensor([0.5686, 0.2083, 0.0241, 0.0239, 0.0223], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' or', ' me'])\n",
      "(tensor([0.4335, 0.3703, 0.1135, 0.0131, 0.0053], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' film', ' picture', ' audience'])\n",
      "(tensor([0.4558, 0.2453, 0.1025, 0.0573, 0.0200], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' or', ' and'])\n",
      "(tensor([0.4661, 0.2454, 0.1055, 0.0243, 0.0208], grad_fn=<ToCopyBackward0>), [' but', ' or', ' the', ' because', ' I'])\n",
      "(tensor([0.5430, 0.1877, 0.0420, 0.0247, 0.0242], grad_fn=<ToCopyBackward0>), [' the', ' if', ' what', ' it', ' because'])\n",
      "(tensor([0.1066, 0.0634, 0.0594, 0.0586, 0.0575], grad_fn=<ToCopyBackward0>), [' script', ' actors', ' story', ' writing', ' acting'])\n",
      "(tensor([0.3160, 0.1326, 0.0965, 0.0785, 0.0627], grad_fn=<ToCopyBackward0>), [',', ' or', '.', ' that', \"'\"])\n",
      "(tensor([0.8363, 0.0261, 0.0234, 0.0079, 0.0061], grad_fn=<ToCopyBackward0>), [' the', ' what', ' whatever', ' whoever', ' just'])\n",
      "(tensor([0.4610, 0.1739, 0.1389, 0.0806, 0.0380], grad_fn=<ToCopyBackward0>), [',', ' the', '.', ' it', ' was'])\n",
      "(tensor([0.9361, 0.0118, 0.0095, 0.0081, 0.0034], grad_fn=<ToCopyBackward0>), [' but', ' or', ' because', ' I', ' the'])\n",
      "(tensor([0.3058, 0.1765, 0.1157, 0.1054, 0.0293], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' that'])\n",
      "(tensor([0.3855, 0.3011, 0.0679, 0.0473, 0.0444], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' seemed', ' really'])\n",
      "(tensor([0.4018, 0.1491, 0.0750, 0.0696, 0.0260], grad_fn=<ToCopyBackward0>), [' seemed', ' didn', ' wasn', ' was', ' just'])\n",
      "(tensor([0.2136, 0.1854, 0.0621, 0.0545, 0.0540], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' bad', ' just'])\n",
      "(tensor([0.7132, 0.0522, 0.0349, 0.0334, 0.0278], grad_fn=<ToCopyBackward0>), ['.', ' and', ' movie', ',', ' in'])\n",
      "(tensor([0.1754, 0.1541, 0.1339, 0.0531, 0.0324], grad_fn=<ToCopyBackward0>), [' It', ' And', ' I', ' The', 'I'])\n",
      "(tensor([0.5975, 0.0996, 0.0689, 0.0445, 0.0308], grad_fn=<ToCopyBackward0>), [' was', ' just', \"'s\", ' wasn', ' had'])\n",
      "(tensor([0.1994, 0.1938, 0.0836, 0.0769, 0.0653], grad_fn=<ToCopyBackward0>), [' a', ' no', ' the', ' so', ' all'])\n",
      "(tensor([0.2978, 0.1088, 0.0545, 0.0454, 0.0365], grad_fn=<ToCopyBackward0>), [' point', ' story', ' real', ' heart', ' plot'])\n",
      "(tensor([0.7009, 0.1566, 0.0349, 0.0218, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' of', ' and'])\n",
      "(tensor([0.2767, 0.1473, 0.0941, 0.0871, 0.0458], grad_fn=<ToCopyBackward0>), [' it', ' and', ' the', ' no', ' there'])\n",
      "(tensor([0.4814, 0.2240, 0.0999, 0.0723, 0.0276], grad_fn=<ToCopyBackward0>), [' had', ' was', ' didn', ' just', ' really'])\n",
      "(tensor([0.4711, 0.0860, 0.0769, 0.0595, 0.0266], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' stupid', ' pointless', ' not'])\n",
      "(tensor([0.1892, 0.1028, 0.0912, 0.0794, 0.0694], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' pointless', ' stupid', ' boring'])\n",
      "(tensor([0.7364, 0.0592, 0.0289, 0.0270, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ',', ' movie', ' and', ' acting'])\n",
      "(tensor([0.1944, 0.1303, 0.1214, 0.0408, 0.0361], grad_fn=<ToCopyBackward0>), [' And', ' It', ' I', ' The', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the worst movie of all time. I thought it was a really bad movie. I was really disappointed in this movie. It's a really bad movie. The movie is so stupid and predictable. I'm not even going to comment much on\n",
      "(tensor([0.3847, 0.1711, 0.0894, 0.0770, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4384, 0.2424, 0.1962, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4803, 0.1359, 0.1035, 0.0558, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7386, 0.0315, 0.0259, 0.0253, 0.0179], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.6893, 0.0858, 0.0163, 0.0159, 0.0133], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' Christmas', ' horror'])\n",
      "(tensor([0.6267, 0.2445, 0.0753, 0.0088, 0.0068], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.7268, 0.1097, 0.0295, 0.0167, 0.0083], grad_fn=<ToCopyBackward0>), [' all', ' the', ' 2001', ' my', ' 2009'])\n",
      "(tensor([9.9265e-01, 1.2844e-03, 1.2582e-03, 1.2259e-03, 8.6589e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', ' the', ' movies', '-'])\n",
      "(tensor([0.5316, 0.0937, 0.0797, 0.0645, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ',', ' until', ' when', '!'])\n",
      "(tensor([0.3863, 0.0987, 0.0893, 0.0254, 0.0228], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', ' This'])\n",
      "(tensor([0.0960, 0.0748, 0.0691, 0.0584, 0.0575], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' really', ' can'])\n",
      "(tensor([0.4184, 0.2911, 0.0684, 0.0618, 0.0491], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' I'])\n",
      "(tensor([0.7695, 0.0815, 0.0187, 0.0138, 0.0137], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' should'])\n",
      "(tensor([0.2086, 0.1800, 0.0741, 0.0572, 0.0288], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' just'])\n",
      "(tensor([0.0807, 0.0632, 0.0567, 0.0517, 0.0476], grad_fn=<ToCopyBackward0>), [' really', ' big', ' total', ' piece', ' complete'])\n",
      "(tensor([0.5589, 0.2118, 0.0263, 0.0212, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ',', ' dumb', ' boring', ' stupid'])\n",
      "(tensor([0.6231, 0.0668, 0.0595, 0.0270, 0.0201], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' idea', ' joke'])\n",
      "(tensor([0.7959, 0.0518, 0.0289, 0.0153, 0.0103], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' from'])\n",
      "(tensor([0.3265, 0.1143, 0.0578, 0.0449, 0.0239], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.1329, 0.0965, 0.0883, 0.0529, 0.0500], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.3863, 0.1298, 0.0573, 0.0466, 0.0424], grad_fn=<ToCopyBackward0>), [' really', ' very', ' in', ' so', ' actually'])\n",
      "(tensor([0.2730, 0.1458, 0.0982, 0.0399, 0.0311], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ',', ' looking', ' shocked'])\n",
      "(tensor([0.3269, 0.2271, 0.2010, 0.0777, 0.0594], grad_fn=<ToCopyBackward0>), ['.', ' in', ' with', ' when', ' by'])\n",
      "(tensor([0.5884, 0.1157, 0.1017, 0.0825, 0.0299], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' myself', ' that'])\n",
      "(tensor([0.8601, 0.0611, 0.0223, 0.0129, 0.0032], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ','])\n",
      "(tensor([0.6638, 0.0762, 0.0722, 0.0281, 0.0268], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' when', ' and'])\n",
      "(tensor([0.3862, 0.1554, 0.0416, 0.0396, 0.0311], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.4310, 0.2139, 0.0519, 0.0325, 0.0270], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' really'])\n",
      "(tensor([0.2357, 0.1342, 0.1249, 0.1050, 0.0620], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' one', ' really'])\n",
      "(tensor([0.4292, 0.0689, 0.0597, 0.0535, 0.0389], grad_fn=<ToCopyBackward0>), [' really', ' bad', ' movie', ' big', ' very'])\n",
      "(tensor([0.6698, 0.0933, 0.0489, 0.0214, 0.0158], grad_fn=<ToCopyBackward0>), [' bad', ',', ' boring', ' stupid', ' dumb'])\n",
      "(tensor([0.8579, 0.0273, 0.0150, 0.0105, 0.0046], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ',', ' idea'])\n",
      "(tensor([0.7733, 0.0680, 0.0240, 0.0159, 0.0149], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', ' that'])\n",
      "(tensor([0.2644, 0.2403, 0.0406, 0.0306, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.2004, 0.0927, 0.0879, 0.0578, 0.0329], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' story', ' plot'])\n",
      "(tensor([0.3848, 0.1421, 0.0695, 0.0670, 0.0373], grad_fn=<ToCopyBackward0>), [' is', ' was', ' just', ' starts', ' has'])\n",
      "(tensor([0.1268, 0.1212, 0.0969, 0.0820, 0.0674], grad_fn=<ToCopyBackward0>), [' not', ' so', ' a', ' really', ' just'])\n",
      "(tensor([0.4809, 0.0991, 0.0318, 0.0306, 0.0190], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' boring', ' predictable', ' terrible'])\n",
      "(tensor([0.2285, 0.1913, 0.1711, 0.1575, 0.0858], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' it'])\n",
      "(tensor([0.2126, 0.1109, 0.0786, 0.0541, 0.0479], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' dumb', ' the', ' I'])\n",
      "(tensor([0.3816, 0.2529, 0.1331, 0.1263, 0.0115], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' that', ' in'])\n",
      "(tensor([0.2677, 0.1974, 0.1131, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' You'])\n",
      "(tensor([0.1284, 0.0942, 0.0835, 0.0688, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' don', ' can'])\n",
      "(tensor([0.2595, 0.1377, 0.1096, 0.0605, 0.0480], grad_fn=<ToCopyBackward0>), [' not', ' a', ' really', ' just', ' so'])\n",
      "(tensor([0.4940, 0.1247, 0.0866, 0.0393, 0.0347], grad_fn=<ToCopyBackward0>), [' even', ' a', ' going', ' sure', ' one'])\n",
      "(tensor([0.4254, 0.1918, 0.1015, 0.0515, 0.0428], grad_fn=<ToCopyBackward0>), [' going', ' gonna', ' kidding', ' sure', ' joking'])\n",
      "(tensor([9.8654e-01, 4.5132e-03, 1.3710e-03, 1.2296e-03, 8.4083e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' into', ' in', ' there', ' back'])\n",
      "(tensor([0.1700, 0.1066, 0.0790, 0.0681, 0.0642], grad_fn=<ToCopyBackward0>), [' talk', ' try', ' comment', ' mention', ' go'])\n",
      "(tensor([0.5362, 0.1561, 0.0441, 0.0419, 0.0399], grad_fn=<ToCopyBackward0>), [' on', ' about', ' much', ' because', ' any'])\n",
      "(tensor([0.3282, 0.2631, 0.1498, 0.0863, 0.0403], grad_fn=<ToCopyBackward0>), [' about', ' on', ' more', ' because', '.'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty lame, but I was willing to give it another chance. I was really looking forward to seeing it in the theater because I thought it could be funny, but it was just boring. It had some funny parts, but the movie\n",
      "(tensor([0.3825, 0.1726, 0.0905, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4998, 0.0603, 0.0342, 0.0154, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6240, 0.0399, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2590, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1969, 0.1243, 0.1046, 0.0784, 0.0679], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3554, 0.2185, 0.1639, 0.0251, 0.0218], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' at'])\n",
      "(tensor([0.2588, 0.0767, 0.0481, 0.0310, 0.0296], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' so', ' I'])\n",
      "(tensor([0.1904, 0.1677, 0.1130, 0.0569, 0.0418], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' the', ' at'])\n",
      "(tensor([0.1104, 0.0894, 0.0559, 0.0434, 0.0425], grad_fn=<ToCopyBackward0>), [' was', ' really', ' guess', \"'m\", ' didn'])\n",
      "(tensor([0.1107, 0.0450, 0.0422, 0.0381, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' willing', ' looking', ' a', ' in'])\n",
      "(tensor([9.9811e-01, 4.7864e-04, 2.1901e-04, 1.1638e-04, 8.3762e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' at', ' in', ' and'])\n",
      "(tensor([0.5403, 0.0627, 0.0566, 0.0387, 0.0360], grad_fn=<ToCopyBackward0>), [' give', ' overlook', ' admit', ' take', ' watch'])\n",
      "(tensor([0.9382, 0.0202, 0.0072, 0.0039, 0.0018], grad_fn=<ToCopyBackward0>), [' it', ' the', ' them', ' this', ' some'])\n",
      "(tensor([0.7691, 0.0591, 0.0523, 0.0261, 0.0165], grad_fn=<ToCopyBackward0>), [' a', ' another', ' the', ' that', ' some'])\n",
      "(tensor([0.3844, 0.3025, 0.1600, 0.0527, 0.0268], grad_fn=<ToCopyBackward0>), [' chance', ' try', ' shot', ' go', ' watch'])\n",
      "(tensor([0.4700, 0.1104, 0.0859, 0.0531, 0.0421], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' to', ' after'])\n",
      "(tensor([0.2919, 0.0970, 0.0917, 0.0406, 0.0281], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1206, 0.0749, 0.0451, 0.0409, 0.0409], grad_fn=<ToCopyBackward0>), [' was', ' rented', ' really', ' watched', \"'m\"])\n",
      "(tensor([0.1821, 0.1540, 0.0872, 0.0385, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' actually', ' willing', ' so'])\n",
      "(tensor([0.3142, 0.2674, 0.0950, 0.0628, 0.0200], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' surprised', ' hoping', ' impressed'])\n",
      "(tensor([9.9552e-01, 2.9466e-03, 8.3332e-04, 1.9042e-04, 1.8023e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' up'])\n",
      "(tensor([9.9340e-01, 2.4628e-03, 8.7848e-04, 7.4995e-04, 2.6517e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', ' the', ' on'])\n",
      "(tensor([0.3553, 0.1317, 0.1129, 0.0967, 0.0780], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' this', ' watching', ' it'])\n",
      "(tensor([0.3247, 0.1910, 0.1414, 0.0328, 0.0306], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' how', ' what'])\n",
      "(tensor([0.2961, 0.1169, 0.1155, 0.0803, 0.0585], grad_fn=<ToCopyBackward0>), [' again', ',', ' in', '.', ' because'])\n",
      "(tensor([0.6110, 0.1006, 0.0726, 0.0276, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' its', ' a', ' theaters', ' my'])\n",
      "(tensor([0.6397, 0.1759, 0.0395, 0.0243, 0.0074], grad_fn=<ToCopyBackward0>), [' theater', ' theatre', ' theaters', ' cinema', ' theat'])\n",
      "(tensor([0.2008, 0.1851, 0.0871, 0.0827, 0.0561], grad_fn=<ToCopyBackward0>), [' because', '.', ' when', ',', ' after'])\n",
      "(tensor([0.5873, 0.1558, 0.0468, 0.0309, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' of', ' my'])\n",
      "(tensor([0.1252, 0.1090, 0.0979, 0.0913, 0.0597], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' like', \"'m\", ' was'])\n",
      "(tensor([0.6641, 0.1439, 0.0449, 0.0422, 0.0210], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ' this'])\n",
      "(tensor([0.3689, 0.3532, 0.0800, 0.0405, 0.0341], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' had'])\n",
      "(tensor([0.6207, 0.0946, 0.0698, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' be', ' have', ' possibly', ' get', ' really'])\n",
      "(tensor([0.2490, 0.1943, 0.1462, 0.1028, 0.0333], grad_fn=<ToCopyBackward0>), [' funny', ' pretty', ' a', ' really', ' better'])\n",
      "(tensor([0.3114, 0.2280, 0.1341, 0.0450, 0.0332], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' at'])\n",
      "(tensor([0.4024, 0.0751, 0.0300, 0.0265, 0.0260], grad_fn=<ToCopyBackward0>), [' but', ' and', ' too', ' especially', ' like'])\n",
      "(tensor([0.2363, 0.2153, 0.0962, 0.0588, 0.0338], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' then'])\n",
      "(tensor([0.4031, 0.1598, 0.0769, 0.0645, 0.0442], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' just', ' didn', ' really'])\n",
      "(tensor([0.2181, 0.1491, 0.0912, 0.0476, 0.0457], grad_fn=<ToCopyBackward0>), [' just', ' not', ' really', ' more', ' pretty'])\n",
      "(tensor([0.1456, 0.0721, 0.0702, 0.0646, 0.0563], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' boring', ' too'])\n",
      "(tensor([0.4461, 0.2871, 0.0662, 0.0493, 0.0123], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' to'])\n",
      "(tensor([0.3054, 0.1597, 0.1569, 0.0230, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.3566, 0.1563, 0.1029, 0.0689, 0.0540], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' didn', ' had'])\n",
      "(tensor([0.1280, 0.1057, 0.1014, 0.0832, 0.0621], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' no', ' so'])\n",
      "(tensor([0.1999, 0.1532, 0.0783, 0.0701, 0.0435], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' potential', ' of', ' really'])\n",
      "(tensor([0.2619, 0.2443, 0.1198, 0.0854, 0.0781], grad_fn=<ToCopyBackward0>), [' parts', ' scenes', ' lines', ' moments', ' bits'])\n",
      "(tensor([0.6812, 0.1092, 0.0549, 0.0429, 0.0166], grad_fn=<ToCopyBackward0>), [',', ' and', ' but', ' in', ' that'])\n",
      "(tensor([0.8236, 0.0533, 0.0251, 0.0096, 0.0080], grad_fn=<ToCopyBackward0>), [' but', ' like', ' and', ' some', ' I'])\n",
      "(tensor([0.1682, 0.1522, 0.1011, 0.0730, 0.0562], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' they', ' that'])\n",
      "(tensor([0.2585, 0.2173, 0.0588, 0.0512, 0.0404], grad_fn=<ToCopyBackward0>), [' movie', ' rest', ' whole', ' acting', ' story'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this is an awful, awful movie. It's one of my least favorite movies of all time. I can't believe that the studio would make something so horrible. It's one of those movies where you can't believe that this movie was made\n",
      "(tensor([0.3832, 0.1725, 0.0904, 0.0771, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3331, 0.2357, 0.0581, 0.0561, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4139, 0.1970, 0.1697, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.3745, 0.2196, 0.0972, 0.0621, 0.0313], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' not', ' an'])\n",
      "(tensor([0.1188, 0.0606, 0.0509, 0.0469, 0.0436], grad_fn=<ToCopyBackward0>), [' awful', ' opportunity', ' absolutely', ' insult', ' OK'])\n",
      "(tensor([0.5986, 0.2187, 0.0480, 0.0192, 0.0093], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ',', ' script'])\n",
      "(tensor([0.3943, 0.0789, 0.0333, 0.0239, 0.0182], grad_fn=<ToCopyBackward0>), [' awful', ' terrible', ' boring', ' horrible', ' really'])\n",
      "(tensor([0.5731, 0.2600, 0.0421, 0.0317, 0.0140], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' script', ','])\n",
      "(tensor([0.8009, 0.0428, 0.0369, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', '!!'])\n",
      "(tensor([0.2778, 0.1900, 0.0559, 0.0373, 0.0224], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.4251, 0.1644, 0.0911, 0.0471, 0.0263], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' has', ' looks'])\n",
      "(tensor([0.2081, 0.1271, 0.1222, 0.1036, 0.0379], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' one', ' like'])\n",
      "(tensor([0.8908, 0.0445, 0.0097, 0.0045, 0.0038], grad_fn=<ToCopyBackward0>), [' of', ' that', ' thing', ' where', ' the'])\n",
      "(tensor([0.6728, 0.2819, 0.0332, 0.0037, 0.0008], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' a'])\n",
      "(tensor([0.3904, 0.2403, 0.0724, 0.0667, 0.0417], grad_fn=<ToCopyBackward0>), [' all', ' least', ' worst', ' favorites', ' favorite'])\n",
      "(tensor([0.6889, 0.1447, 0.0793, 0.0098, 0.0071], grad_fn=<ToCopyBackward0>), [' favorite', ' favorites', '-', ' liked', ' favourite'])\n",
      "(tensor([0.6710, 0.0703, 0.0228, 0.0161, 0.0104], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' comed', ' movie', ' horror'])\n",
      "(tensor([0.2305, 0.2193, 0.1364, 0.1014, 0.0843], grad_fn=<ToCopyBackward0>), ['.', ' of', ' ever', ',', ' in'])\n",
      "(tensor([9.7358e-01, 1.9609e-02, 1.4275e-03, 6.3293e-04, 5.3254e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' this', ' ALL'])\n",
      "(tensor([9.9041e-01, 3.7183e-03, 3.7019e-03, 1.1400e-03, 1.5117e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' the', ' movies'])\n",
      "(tensor([0.7341, 0.1608, 0.0180, 0.0110, 0.0078], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' ('])\n",
      "(tensor([0.2346, 0.1719, 0.0666, 0.0501, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.0778, 0.0735, 0.0685, 0.0588, 0.0562], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' was', ' really', ' don'])\n",
      "(tensor([0.6206, 0.0499, 0.0487, 0.0395, 0.0290], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' honestly', ' remember', ' watch'])\n",
      "(tensor([0.3556, 0.2052, 0.0555, 0.0492, 0.0466], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' stand', ' imagine', ' remember'])\n",
      "(tensor([0.5679, 0.0691, 0.0617, 0.0418, 0.0285], grad_fn=<ToCopyBackward0>), [' that', ' I', ' this', ' the', ' it'])\n",
      "(tensor([0.1595, 0.1457, 0.1051, 0.0514, 0.0429], grad_fn=<ToCopyBackward0>), [' I', ' the', ' a', ' someone', ' so'])\n",
      "(tensor([0.1836, 0.1082, 0.0628, 0.0495, 0.0409], grad_fn=<ToCopyBackward0>), [' people', ' studio', ' director', ' M', ' creator'])\n",
      "(tensor([0.1930, 0.1373, 0.0605, 0.0591, 0.0541], grad_fn=<ToCopyBackward0>), [' would', ' was', ' put', ' is', ' thought'])\n",
      "(tensor([0.1650, 0.1441, 0.1438, 0.0491, 0.0476], grad_fn=<ToCopyBackward0>), [' actually', ' put', ' have', ' allow', ' make'])\n",
      "(tensor([0.4353, 0.1441, 0.1409, 0.1233, 0.1009], grad_fn=<ToCopyBackward0>), [' this', ' it', ' such', ' something', ' a'])\n",
      "(tensor([0.4193, 0.3275, 0.1189, 0.0620, 0.0212], grad_fn=<ToCopyBackward0>), [' so', ' like', ' this', ' as', ' that'])\n",
      "(tensor([0.1468, 0.0925, 0.0623, 0.0547, 0.0302], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' horrible', ' awful', ' incredibly'])\n",
      "(tensor([0.5546, 0.0928, 0.0861, 0.0459, 0.0390], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' and', ' to'])\n",
      "(tensor([0.2630, 0.1401, 0.0687, 0.0545, 0.0325], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.5302, 0.0956, 0.0637, 0.0392, 0.0245], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' has'])\n",
      "(tensor([0.2327, 0.1482, 0.0919, 0.0732, 0.0669], grad_fn=<ToCopyBackward0>), [' just', ' not', ' one', ' a', ' like'])\n",
      "(tensor([0.9533, 0.0128, 0.0053, 0.0027, 0.0025], grad_fn=<ToCopyBackward0>), [' of', ' thing', ' big', ' that', ' the'])\n",
      "(tensor([0.5659, 0.3874, 0.0275, 0.0057, 0.0025], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' their'])\n",
      "(tensor([0.9269, 0.0335, 0.0033, 0.0018, 0.0013], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' really', ' rare'])\n",
      "(tensor([0.5631, 0.3058, 0.0320, 0.0194, 0.0105], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', '.'])\n",
      "(tensor([0.2891, 0.2429, 0.1362, 0.0588, 0.0563], grad_fn=<ToCopyBackward0>), [' you', ' I', ' the', ' it', ' if'])\n",
      "(tensor([0.1211, 0.1107, 0.0799, 0.0535, 0.0413], grad_fn=<ToCopyBackward0>), [' can', ' just', \"'re\", ' think', ' feel'])\n",
      "(tensor([0.5386, 0.1357, 0.1100, 0.0236, 0.0184], grad_fn=<ToCopyBackward0>), [\"'t\", ' tell', ' see', ' watch', ' say'])\n",
      "(tensor([0.3372, 0.2560, 0.0524, 0.0417, 0.0258], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' really', ' get', ' watch'])\n",
      "(tensor([0.4963, 0.1543, 0.0642, 0.0388, 0.0369], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' what', ' how'])\n",
      "(tensor([0.2458, 0.1160, 0.0860, 0.0776, 0.0598], grad_fn=<ToCopyBackward0>), [' the', ' this', ' they', ' a', ' you'])\n",
      "(tensor([0.4007, 0.1696, 0.0989, 0.0943, 0.0241], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' could', ' was', ' can'])\n",
      "(tensor([0.3327, 0.2781, 0.0731, 0.0520, 0.0488], grad_fn=<ToCopyBackward0>), [' was', ' is', ' could', ' got', ' even'])\n",
      "(tensor([0.6138, 0.1661, 0.0725, 0.0307, 0.0132], grad_fn=<ToCopyBackward0>), [' made', ' even', ' actually', ' ever', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this was a pretty bad movie... I'm not even going to try to explain it. I just don't get it. It was just awful. I can't explain why I didn't get it. I don't understand why I didn't\n",
      "(tensor([0.3851, 0.1710, 0.0893, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3308, 0.2373, 0.0582, 0.0558, 0.0230], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4152, 0.1955, 0.1691, 0.0498, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.5001, 0.1505, 0.0848, 0.0607, 0.0126], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' probably'])\n",
      "(tensor([0.1237, 0.0891, 0.0825, 0.0765, 0.0751], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' really', ' sequel', ' pretty'])\n",
      "(tensor([0.2616, 0.0547, 0.0344, 0.0302, 0.0267], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' dumb', ' interesting', ' original'])\n",
      "(tensor([0.8498, 0.0808, 0.0145, 0.0041, 0.0037], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' horror', ' script'])\n",
      "(tensor([0.6269, 0.0948, 0.0333, 0.0307, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', '...', '!', '....'])\n",
      "(tensor([0.0677, 0.0344, 0.0328, 0.0312, 0.0300], grad_fn=<ToCopyBackward0>), [' I', 'and', 'the', 'but', 'I'])\n",
      "(tensor([0.1083, 0.0861, 0.0829, 0.0774, 0.0594], grad_fn=<ToCopyBackward0>), [' was', ' mean', ' really', ' thought', \"'m\"])\n",
      "(tensor([0.2103, 0.1957, 0.0644, 0.0566, 0.0544], grad_fn=<ToCopyBackward0>), [' not', ' a', ' sure', ' just', ' really'])\n",
      "(tensor([0.2478, 0.1614, 0.1354, 0.1092, 0.0528], grad_fn=<ToCopyBackward0>), [' a', ' sure', ' even', ' one', ' going'])\n",
      "(tensor([0.2944, 0.2009, 0.1408, 0.0706, 0.0373], grad_fn=<ToCopyBackward0>), [' going', ' sure', ' gonna', ' a', ' kidding'])\n",
      "(tensor([0.9867, 0.0034, 0.0019, 0.0014, 0.0012], grad_fn=<ToCopyBackward0>), [' to', ' into', ' that', ' out', ' there'])\n",
      "(tensor([0.1123, 0.0945, 0.0904, 0.0638, 0.0593], grad_fn=<ToCopyBackward0>), [' call', ' comment', ' try', ' bother', ' mention'])\n",
      "(tensor([0.8452, 0.0997, 0.0105, 0.0032, 0.0028], grad_fn=<ToCopyBackward0>), [' to', ' and', ' it', '.', ','])\n",
      "(tensor([0.1295, 0.0761, 0.0589, 0.0443, 0.0436], grad_fn=<ToCopyBackward0>), [' describe', ' explain', ' give', ' understand', ' be'])\n",
      "(tensor([0.5601, 0.1066, 0.0899, 0.0657, 0.0624], grad_fn=<ToCopyBackward0>), [' it', ' how', ' why', ' the', ' that'])\n",
      "(tensor([0.3208, 0.2735, 0.1320, 0.0642, 0.0198], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' because', '...'])\n",
      "(tensor([0.3311, 0.1519, 0.0747, 0.0257, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.1840, 0.0922, 0.0568, 0.0553, 0.0477], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' was', ' don', ' can'])\n",
      "(tensor([0.1218, 0.0788, 0.0701, 0.0519, 0.0490], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' watched', ' couldn', ' found'])\n",
      "(tensor([9.9709e-01, 6.7522e-04, 3.7884e-04, 1.8243e-04, 1.5723e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3713, 0.2120, 0.1159, 0.1008, 0.0611], grad_fn=<ToCopyBackward0>), [' get', ' understand', ' know', ' think', ' like'])\n",
      "(tensor([0.8900, 0.0392, 0.0225, 0.0221, 0.0085], grad_fn=<ToCopyBackward0>), [' it', ' why', ' how', ' the', ' this'])\n",
      "(tensor([0.7996, 0.0582, 0.0448, 0.0228, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', '...', '!'])\n",
      "(tensor([0.2820, 0.1067, 0.1023, 0.0270, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.5256, 0.1089, 0.0491, 0.0323, 0.0284], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' is', ' seems'])\n",
      "(tensor([0.1566, 0.0915, 0.0866, 0.0817, 0.0301], grad_fn=<ToCopyBackward0>), [' just', ' like', ' so', ' a', ' boring'])\n",
      "(tensor([0.1254, 0.1029, 0.0613, 0.0556, 0.0482], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' terrible', ' so', ' awful'])\n",
      "(tensor([0.6913, 0.0871, 0.0697, 0.0597, 0.0357], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2859, 0.1032, 0.0830, 0.0357, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', 'I'])\n",
      "(tensor([0.0942, 0.0783, 0.0761, 0.0702, 0.0618], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' can', ' was', ' really'])\n",
      "(tensor([0.7282, 0.0671, 0.0349, 0.0241, 0.0150], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' only', ' understand', ' honestly'])\n",
      "(tensor([0.3114, 0.2469, 0.0880, 0.0481, 0.0384], grad_fn=<ToCopyBackward0>), [' even', ' believe', ' understand', ' explain', ' really'])\n",
      "(tensor([0.5936, 0.1782, 0.0996, 0.0257, 0.0184], grad_fn=<ToCopyBackward0>), [' it', ' why', ' how', ' to', ' what'])\n",
      "(tensor([0.2554, 0.2199, 0.1552, 0.0715, 0.0476], grad_fn=<ToCopyBackward0>), [' I', ' it', '.', ',', ' this'])\n",
      "(tensor([0.1962, 0.0618, 0.0589, 0.0420, 0.0410], grad_fn=<ToCopyBackward0>), [' didn', ' don', \"'m\", ' found', ' felt'])\n",
      "(tensor([9.9813e-01, 2.6829e-04, 1.8761e-04, 1.3983e-04, 8.8450e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '\"'])\n",
      "(tensor([0.1988, 0.1927, 0.1326, 0.0544, 0.0364], grad_fn=<ToCopyBackward0>), [' get', ' like', ' enjoy', ' think', ' find'])\n",
      "(tensor([0.7436, 0.0648, 0.0340, 0.0181, 0.0167], grad_fn=<ToCopyBackward0>), [' it', ' into', ' the', ' this', ' that'])\n",
      "(tensor([0.6713, 0.1260, 0.0628, 0.0311, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', '...', ' because'])\n",
      "(tensor([0.4603, 0.1132, 0.0466, 0.0239, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Maybe', 'I'])\n",
      "(tensor([0.1295, 0.0900, 0.0761, 0.0751, 0.0743], grad_fn=<ToCopyBackward0>), [' just', \"'m\", ' can', ' don', ' was'])\n",
      "(tensor([9.9809e-01, 2.4039e-04, 2.0675e-04, 1.4947e-04, 9.2702e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", \"'\", '`', ',', '\"'])\n",
      "(tensor([0.2295, 0.2281, 0.1938, 0.1076, 0.0996], grad_fn=<ToCopyBackward0>), [' know', ' even', ' understand', ' get', ' think'])\n",
      "(tensor([0.5026, 0.2037, 0.0941, 0.0500, 0.0340], grad_fn=<ToCopyBackward0>), [' why', ' it', ' how', ' the', ' what'])\n",
      "(tensor([0.3327, 0.1552, 0.0895, 0.0657, 0.0504], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' they', ' people'])\n",
      "(tensor([0.1754, 0.0603, 0.0517, 0.0489, 0.0475], grad_fn=<ToCopyBackward0>), [' didn', ' watched', ' was', \"'m\", ' thought'])\n",
      "(tensor([9.9866e-01, 1.2947e-04, 1.0333e-04, 9.9971e-05, 7.1934e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '\"'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty lame, but at least it wasn't as bad as \"Plan 9 From Outer Space\". This movie had no plot at all, just a bunch of random clips, some of which are funny... but not in the way you would\n",
      "(tensor([0.3832, 0.1724, 0.0903, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4369, 0.2445, 0.1963, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0594, 0.0361, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1375, 0.0700, 0.0663, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1757, 0.1510, 0.1086, 0.0947, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.3829, 0.1678, 0.1165, 0.0297, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' when'])\n",
      "(tensor([0.2211, 0.0975, 0.0507, 0.0363, 0.0308], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' I', ' it'])\n",
      "(tensor([0.2302, 0.1701, 0.1049, 0.0639, 0.0570], grad_fn=<ToCopyBackward0>), [' it', ' I', ' then', ' at', ' the'])\n",
      "(tensor([0.6795, 0.2807, 0.0056, 0.0055, 0.0044], grad_fn=<ToCopyBackward0>), [' least', ' the', ' a', ' some', ' one'])\n",
      "(tensor([0.3378, 0.1584, 0.1511, 0.0339, 0.0265], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' there', ' i'])\n",
      "(tensor([0.2575, 0.1682, 0.1025, 0.0471, 0.0366], grad_fn=<ToCopyBackward0>), [' was', ' had', ' wasn', ' made', ' didn'])\n",
      "(tensor([9.9590e-01, 1.7581e-03, 3.4140e-04, 3.3427e-04, 1.4755e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", ';'])\n",
      "(tensor([0.6776, 0.0294, 0.0280, 0.0178, 0.0176], grad_fn=<ToCopyBackward0>), [' as', ' too', ' a', ' even', ' so'])\n",
      "(tensor([0.3369, 0.1663, 0.0869, 0.0595, 0.0352], grad_fn=<ToCopyBackward0>), [' bad', ' lame', ' boring', ' funny', ' terrible'])\n",
      "(tensor([9.8758e-01, 2.0416e-03, 1.9720e-03, 1.0892e-03, 9.4114e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' as', ' a', '.', ' in', '...'])\n",
      "(tensor([0.0997, 0.0825, 0.0490, 0.0486, 0.0438], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' this', ' Man', ' I'])\n",
      "(tensor([0.0901, 0.0518, 0.0223, 0.0170, 0.0166], grad_fn=<ToCopyBackward0>), ['Plan', 'The', 'Night', 'G', 'Dead'])\n",
      "(tensor([0.8161, 0.0763, 0.0399, 0.0105, 0.0076], grad_fn=<ToCopyBackward0>), [' 9', ' Nine', ' nine', ' 7', ' B'])\n",
      "(tensor([0.5898, 0.2373, 0.0420, 0.0344, 0.0293], grad_fn=<ToCopyBackward0>), [' From', ' from', '\"', '.\"', '\".'])\n",
      "(tensor([0.9757, 0.0062, 0.0032, 0.0015, 0.0010], grad_fn=<ToCopyBackward0>), [' Outer', ' Out', ' outer', ' OUT', ' out'])\n",
      "(tensor([9.8475e-01, 7.1345e-03, 1.7858e-03, 6.8732e-04, 3.9654e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Space', ' space', 'Space', ' Darkness', ' Spac'])\n",
      "(tensor([0.3900, 0.3191, 0.1892, 0.0589, 0.0143], grad_fn=<ToCopyBackward0>), ['\".', '\"', '.\"', '\",', ',\"'])\n",
      "(tensor([0.1749, 0.1393, 0.0849, 0.0750, 0.0274], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'The'])\n",
      "(tensor([0.5374, 0.1055, 0.0992, 0.0865, 0.0355], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' is', ' was', ' film'])\n",
      "(tensor([0.1748, 0.1292, 0.0826, 0.0469, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' sucked', ' has'])\n",
      "(tensor([0.1816, 0.0915, 0.0810, 0.0678, 0.0581], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' so', ' no'])\n",
      "(tensor([0.4356, 0.1171, 0.0778, 0.0278, 0.0268], grad_fn=<ToCopyBackward0>), [' plot', ' redeem', ' real', ' humor', ' story'])\n",
      "(tensor([0.4063, 0.1942, 0.0684, 0.0513, 0.0455], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' at', ' or'])\n",
      "(tensor([0.9841, 0.0047, 0.0038, 0.0036, 0.0011], grad_fn=<ToCopyBackward0>), [' all', ' the', ' least', ' ALL', ' first'])\n",
      "(tensor([0.4591, 0.2128, 0.1542, 0.0283, 0.0183], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '!', '...'])\n",
      "(tensor([0.2293, 0.1624, 0.0687, 0.0676, 0.0584], grad_fn=<ToCopyBackward0>), [' and', ' just', ' no', ' the', ' so'])\n",
      "(tensor([0.1788, 0.1250, 0.0974, 0.0437, 0.0381], grad_fn=<ToCopyBackward0>), [' a', ' random', ' some', ' boring', ' one'])\n",
      "(tensor([0.6152, 0.0427, 0.0231, 0.0212, 0.0173], grad_fn=<ToCopyBackward0>), [' bunch', ' guy', ' lot', ' random', ' group'])\n",
      "(tensor([9.9254e-01, 8.0132e-04, 5.4275e-04, 5.1782e-04, 3.4768e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', 'a', ' o', ' people', ' characters'])\n",
      "(tensor([0.1061, 0.0631, 0.0574, 0.0536, 0.0467], grad_fn=<ToCopyBackward0>), [' people', ' lame', ' random', ' stupid', ' silly'])\n",
      "(tensor([0.3838, 0.0619, 0.0413, 0.0406, 0.0337], grad_fn=<ToCopyBackward0>), [' scenes', ' clips', ' characters', ' people', ' shots'])\n",
      "(tensor([0.4210, 0.3247, 0.0391, 0.0381, 0.0267], grad_fn=<ToCopyBackward0>), [' from', ' of', ' that', ' and', ','])\n",
      "(tensor([0.1944, 0.1744, 0.0791, 0.0329, 0.0312], grad_fn=<ToCopyBackward0>), [' and', ' some', ' which', ' clips', ' a'])\n",
      "(tensor([0.4061, 0.0347, 0.0310, 0.0256, 0.0159], grad_fn=<ToCopyBackward0>), [' of', ' even', ' that', ' funny', ' people'])\n",
      "(tensor([0.8291, 0.1272, 0.0130, 0.0031, 0.0020], grad_fn=<ToCopyBackward0>), [' which', ' them', ' the', ' it', ' crappy'])\n",
      "(tensor([0.4196, 0.1221, 0.0439, 0.0355, 0.0353], grad_fn=<ToCopyBackward0>), [' were', ' are', ' I', ' looked', ' weren'])\n",
      "(tensor([0.1071, 0.0995, 0.0666, 0.0587, 0.0491], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' quite', ' pretty', ' just'])\n",
      "(tensor([0.3890, 0.3625, 0.0848, 0.0546, 0.0204], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' (', '...'])\n",
      "(tensor([0.2156, 0.1000, 0.0781, 0.0689, 0.0312], grad_fn=<ToCopyBackward0>), [' and', ' but', 'but', 'and', ' I'])\n",
      "(tensor([0.0947, 0.0898, 0.0890, 0.0790, 0.0680], grad_fn=<ToCopyBackward0>), [' the', ' some', ' that', ' not', ' most'])\n",
      "(tensor([0.2073, 0.1587, 0.1065, 0.0989, 0.0717], grad_fn=<ToCopyBackward0>), [' really', ' much', ' in', ' very', ' funny'])\n",
      "(tensor([0.4084, 0.3913, 0.1287, 0.0139, 0.0121], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' this', ' an'])\n",
      "(tensor([0.6228, 0.0529, 0.0333, 0.0235, 0.0191], grad_fn=<ToCopyBackward0>), [' way', ' least', ' right', ' \"', ' funny'])\n",
      "(tensor([0.2372, 0.1744, 0.1228, 0.0812, 0.0802], grad_fn=<ToCopyBackward0>), [' you', ' that', ' the', ' \"', ' this'])\n",
      "(tensor([0.3840, 0.1920, 0.1122, 0.0595, 0.0547], grad_fn=<ToCopyBackward0>), [' would', \"'d\", ' think', ' might', ' expect'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that a sequel would not be a good idea because this was a good movie, a good movie for kids... but I thought this would be a great story for kids and a great story for adults. I was very disappointed. It's not as good\n",
      "(tensor([0.3834, 0.1719, 0.0902, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3318, 0.2363, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.3453, 0.2848, 0.2643, 0.0092, 0.0066], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' sequel', ' documentary', ' remake'])\n",
      "(tensor([0.5781, 0.1848, 0.0614, 0.0322, 0.0221], grad_fn=<ToCopyBackward0>), [' to', ' was', ' would', ' of', ' is'])\n",
      "(tensor([0.7213, 0.0815, 0.0367, 0.0322, 0.0149], grad_fn=<ToCopyBackward0>), [' be', ' have', ' not', ' never', ' make'])\n",
      "(tensor([0.5291, 0.2712, 0.0382, 0.0271, 0.0183], grad_fn=<ToCopyBackward0>), [' be', ' only', ' have', ' work', ' make'])\n",
      "(tensor([0.3624, 0.0797, 0.0778, 0.0713, 0.0478], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' as', ' too'])\n",
      "(tensor([0.7911, 0.0719, 0.0158, 0.0104, 0.0093], grad_fn=<ToCopyBackward0>), [' good', ' great', ' bad', ' sequel', ' very'])\n",
      "(tensor([0.7524, 0.0693, 0.0464, 0.0395, 0.0140], grad_fn=<ToCopyBackward0>), [' idea', ' movie', ' move', ' sequel', ' thing'])\n",
      "(tensor([0.3026, 0.2533, 0.1049, 0.0633, 0.0598], grad_fn=<ToCopyBackward0>), [' for', '.', ',', ' because', ' as'])\n",
      "(tensor([0.1443, 0.1411, 0.1242, 0.1147, 0.0965], grad_fn=<ToCopyBackward0>), [' the', ' this', ' I', ' of', ' it'])\n",
      "(tensor([0.3961, 0.2048, 0.1461, 0.0437, 0.0307], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' one', ' film'])\n",
      "(tensor([0.3812, 0.1960, 0.0762, 0.0577, 0.0427], grad_fn=<ToCopyBackward0>), [' a', ' the', ' such', ' not', ' an'])\n",
      "(tensor([0.2715, 0.1269, 0.0932, 0.0572, 0.0516], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' great', ' very', ' real'])\n",
      "(tensor([9.3029e-01, 3.4639e-02, 1.6948e-02, 4.6010e-03, 8.7913e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' sequel', ' book'])\n",
      "(tensor([0.7240, 0.0648, 0.0624, 0.0219, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '!'])\n",
      "(tensor([0.3412, 0.1105, 0.0850, 0.0526, 0.0432], grad_fn=<ToCopyBackward0>), [' but', ' and', ' a', ' so', ' the'])\n",
      "(tensor([0.3739, 0.1806, 0.0850, 0.0832, 0.0243], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' great', ' real'])\n",
      "(tensor([0.4472, 0.2351, 0.0342, 0.0296, 0.0266], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' sequel', ' idea', ' film'])\n",
      "(tensor([0.2653, 0.1899, 0.1017, 0.0677, 0.0654], grad_fn=<ToCopyBackward0>), [' that', ' with', '.', ' in', ' for'])\n",
      "(tensor([0.6295, 0.0858, 0.0255, 0.0234, 0.0221], grad_fn=<ToCopyBackward0>), [' kids', ' the', ' everybody', ' young', ' adults'])\n",
      "(tensor([0.4056, 0.1103, 0.0761, 0.0482, 0.0304], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.1176, 0.0837, 0.0823, 0.0639, 0.0452], grad_fn=<ToCopyBackward0>), [' I', 'I', ' but', 'but', ' and'])\n",
      "(tensor([0.1164, 0.0840, 0.0639, 0.0590, 0.0375], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' a'])\n",
      "(tensor([0.1834, 0.0700, 0.0540, 0.0513, 0.0446], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' guess', \"'m\"])\n",
      "(tensor([0.3482, 0.1932, 0.0558, 0.0539, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' the', ' kids'])\n",
      "(tensor([0.3809, 0.2115, 0.0958, 0.0775, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' movie', ' would', ' is', ' could'])\n",
      "(tensor([0.9118, 0.0159, 0.0108, 0.0101, 0.0070], grad_fn=<ToCopyBackward0>), [' be', ' make', ' have', ' get', ' not'])\n",
      "(tensor([0.7291, 0.0528, 0.0216, 0.0205, 0.0178], grad_fn=<ToCopyBackward0>), [' a', ' an', ' better', ' something', ' the'])\n",
      "(tensor([0.6490, 0.2036, 0.0334, 0.0152, 0.0134], grad_fn=<ToCopyBackward0>), [' good', ' great', ' really', ' very', ' sequel'])\n",
      "(tensor([0.7444, 0.0925, 0.0440, 0.0227, 0.0204], grad_fn=<ToCopyBackward0>), [' movie', ' sequel', ' story', ' idea', ' film'])\n",
      "(tensor([0.3299, 0.2904, 0.1135, 0.0616, 0.0596], grad_fn=<ToCopyBackward0>), [' for', ' to', ' and', '.', ','])\n",
      "(tensor([0.3651, 0.2710, 0.1543, 0.0451, 0.0339], grad_fn=<ToCopyBackward0>), [' kids', ' a', ' the', ' children', ' adults'])\n",
      "(tensor([0.2379, 0.1880, 0.1558, 0.0965, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ' to', '...', ',', ' and'])\n",
      "(tensor([0.1454, 0.1375, 0.1047, 0.0597, 0.0560], grad_fn=<ToCopyBackward0>), [' adults', ' I', ' a', ' that', ' for'])\n",
      "(tensor([0.6911, 0.2176, 0.0192, 0.0081, 0.0073], grad_fn=<ToCopyBackward0>), [' great', ' good', ' really', ' very', ' story'])\n",
      "(tensor([0.4930, 0.3253, 0.0351, 0.0308, 0.0137], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' family', ' sequel', ' way'])\n",
      "(tensor([0.8893, 0.0631, 0.0090, 0.0064, 0.0050], grad_fn=<ToCopyBackward0>), [' for', ' to', '.', '...', ' about'])\n",
      "(tensor([0.8712, 0.0218, 0.0198, 0.0155, 0.0144], grad_fn=<ToCopyBackward0>), [' adults', ' kids', ' parents', ' the', ' families'])\n",
      "(tensor([0.5625, 0.1273, 0.0743, 0.0538, 0.0291], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', '...', ' as'])\n",
      "(tensor([0.2620, 0.1951, 0.0856, 0.0697, 0.0450], grad_fn=<ToCopyBackward0>), [' I', ' So', ' And', ' It', ' But'])\n",
      "(tensor([0.1513, 0.1160, 0.0843, 0.0509, 0.0462], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' think', \"'m\"])\n",
      "(tensor([0.3112, 0.1054, 0.0867, 0.0636, 0.0370], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' really', ' so', ' not'])\n",
      "(tensor([0.3629, 0.1411, 0.1072, 0.0968, 0.0285], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' wrong', ' excited', ' much'])\n",
      "(tensor([0.2970, 0.2106, 0.1405, 0.0826, 0.0624], grad_fn=<ToCopyBackward0>), [' when', ' with', '.', ' by', ' in'])\n",
      "(tensor([0.2778, 0.1517, 0.0708, 0.0570, 0.0489], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.3667, 0.2663, 0.0626, 0.0339, 0.0310], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' just', ' seemed'])\n",
      "(tensor([0.2154, 0.1824, 0.1550, 0.0359, 0.0322], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' like', ' really'])\n",
      "(tensor([0.3916, 0.1263, 0.0980, 0.0796, 0.0385], grad_fn=<ToCopyBackward0>), [' a', ' as', ' the', ' even', ' that'])\n",
      "(tensor([0.3401, 0.1603, 0.0307, 0.0302, 0.0290], grad_fn=<ToCopyBackward0>), [' good', ' if', ' though', ' bad', ' interesting'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was one of the worst movies I have ever seen, and I have seen a lot. I really don't even know where I saw this movie. I don't really know where I even got it, but I know I rented it from a\n",
      "(tensor([0.3831, 0.1722, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2442, 0.1957, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4799, 0.1354, 0.1037, 0.0554, 0.0254], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.9005, 0.0167, 0.0077, 0.0068, 0.0066], grad_fn=<ToCopyBackward0>), [' of', ' really', ' more', ' the', ' that'])\n",
      "(tensor([0.8493, 0.1164, 0.0142, 0.0015, 0.0009], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' a'])\n",
      "(tensor([0.5760, 0.0963, 0.0949, 0.0428, 0.0284], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' best', ' weakest', ' funn'])\n",
      "(tensor([0.7962, 0.0917, 0.0114, 0.0044, 0.0036], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' western', ' British'])\n",
      "(tensor([0.7148, 0.1294, 0.0736, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' to'])\n",
      "(tensor([0.5015, 0.3088, 0.0859, 0.0514, 0.0264], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([0.9157, 0.0755, 0.0020, 0.0014, 0.0012], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9086, 0.0213, 0.0170, 0.0083, 0.0052], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' wasted', ' been'])\n",
      "(tensor([0.6193, 0.1188, 0.0816, 0.0498, 0.0249], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' in', '...'])\n",
      "(tensor([0.1898, 0.1576, 0.0598, 0.0320, 0.0318], grad_fn=<ToCopyBackward0>), [' and', ' but', ' the', ' with', ' not'])\n",
      "(tensor([0.5730, 0.1095, 0.0497, 0.0254, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' was'])\n",
      "(tensor([0.1622, 0.1057, 0.0850, 0.0744, 0.0632], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', \"'m\", ' really'])\n",
      "(tensor([0.8145, 0.0272, 0.0180, 0.0143, 0.0101], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' watched', ' to', ' only'])\n",
      "(tensor([0.4974, 0.2505, 0.1108, 0.0294, 0.0144], grad_fn=<ToCopyBackward0>), [' some', ' quite', ' a', ' many', ' worse'])\n",
      "(tensor([0.7360, 0.1139, 0.0563, 0.0200, 0.0191], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' LOT', ' couple', ' ton'])\n",
      "(tensor([0.7103, 0.2246, 0.0235, 0.0113, 0.0086], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' in', '!'])\n",
      "(tensor([0.2072, 0.1386, 0.0846, 0.0813, 0.0399], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.1121, 0.0832, 0.0807, 0.0482, 0.0413], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', ' really', ' can'])\n",
      "(tensor([0.1056, 0.0572, 0.0539, 0.0483, 0.0477], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' have', ' think', ' do'])\n",
      "(tensor([9.9361e-01, 2.6193e-03, 8.0819e-04, 5.7466e-04, 2.4280e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.3411, 0.1665, 0.1603, 0.0963, 0.0422], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' like', ' even'])\n",
      "(tensor([0.4296, 0.2163, 0.0577, 0.0557, 0.0549], grad_fn=<ToCopyBackward0>), [' know', ' remember', ' think', ' like', ' want'])\n",
      "(tensor([0.4559, 0.2166, 0.1719, 0.0485, 0.0464], grad_fn=<ToCopyBackward0>), [' where', ' how', ' if', ' why', ' what'])\n",
      "(tensor([0.9641, 0.0132, 0.0088, 0.0050, 0.0017], grad_fn=<ToCopyBackward0>), [' to', ' I', ' it', ' the', ' this'])\n",
      "(tensor([0.2115, 0.0989, 0.0635, 0.0630, 0.0467], grad_fn=<ToCopyBackward0>), [' am', ' was', ' saw', \"'m\", ' put'])\n",
      "(tensor([0.7345, 0.1653, 0.0475, 0.0154, 0.0035], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.5154, 0.1296, 0.1164, 0.0597, 0.0255], grad_fn=<ToCopyBackward0>), [' movie', '.', ' one', ',', ' film'])\n",
      "(tensor([0.4837, 0.3355, 0.0201, 0.0132, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' at', ' before'])\n",
      "(tensor([0.4455, 0.1358, 0.0539, 0.0181, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0869, 0.0806, 0.0741, 0.0728, 0.0541], grad_fn=<ToCopyBackward0>), [' am', ' think', ' was', ' have', ' don'])\n",
      "(tensor([9.9615e-01, 1.1408e-03, 5.7369e-04, 2.5213e-04, 1.7665e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.4826, 0.1905, 0.1161, 0.0837, 0.0349], grad_fn=<ToCopyBackward0>), [' even', ' remember', ' know', ' think', ' really'])\n",
      "(tensor([0.3591, 0.2065, 0.0963, 0.0555, 0.0440], grad_fn=<ToCopyBackward0>), [' remember', ' know', ' even', ' think', ' have'])\n",
      "(tensor([0.2157, 0.1896, 0.1768, 0.1111, 0.0896], grad_fn=<ToCopyBackward0>), [' where', ' how', ' if', ' what', ' why'])\n",
      "(tensor([0.3823, 0.2511, 0.1576, 0.0758, 0.0251], grad_fn=<ToCopyBackward0>), [' I', ' it', ' to', ' this', ' the'])\n",
      "(tensor([0.3827, 0.0829, 0.0643, 0.0517, 0.0402], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' found', ' even', ' have'])\n",
      "(tensor([0.4717, 0.1145, 0.0581, 0.0462, 0.0308], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' got', ' watched', ' see'])\n",
      "(tensor([0.5885, 0.2107, 0.0718, 0.0254, 0.0195], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' my'])\n",
      "(tensor([0.5670, 0.1952, 0.1342, 0.0219, 0.0115], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', ' in', ' because'])\n",
      "(tensor([0.5403, 0.0670, 0.0349, 0.0344, 0.0341], grad_fn=<ToCopyBackward0>), [' but', ' I', ' so', ' except', ' although'])\n",
      "(tensor([0.6438, 0.1409, 0.0126, 0.0115, 0.0110], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' my', ' this'])\n",
      "(tensor([0.1076, 0.0689, 0.0588, 0.0559, 0.0481], grad_fn=<ToCopyBackward0>), [' have', ' really', \"'m\", ' am', ' know'])\n",
      "(tensor([0.4494, 0.1987, 0.1405, 0.0490, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' where', ' someone'])\n",
      "(tensor([0.1511, 0.0965, 0.0846, 0.0487, 0.0370], grad_fn=<ToCopyBackward0>), [' have', ' saw', ' watched', \"'m\", ' rented'])\n",
      "(tensor([0.9703, 0.0077, 0.0073, 0.0022, 0.0020], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' something', ' a'])\n",
      "(tensor([0.2779, 0.1983, 0.1001, 0.0601, 0.0564], grad_fn=<ToCopyBackward0>), [' from', '.', ' at', ' because', ' and'])\n",
      "(tensor([0.2528, 0.1392, 0.0892, 0.0666, 0.0546], grad_fn=<ToCopyBackward0>), [' Block', ' a', ' some', ' Netflix', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie would be more like \"Rocky Horror Picture Show\" than \"The Incredible Melting Man\", but it was pretty funny and interesting, though it's hard to tell if it was the acting or the plot. The acting was pretty good,\n",
      "(tensor([0.3827, 0.1725, 0.0905, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2452, 0.1956, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6534, 0.0594, 0.0360, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.8300, 0.0817, 0.0134, 0.0116, 0.0080], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' get'])\n",
      "(tensor([0.1440, 0.1144, 0.0935, 0.0926, 0.0752], grad_fn=<ToCopyBackward0>), [' more', ' better', ' great', ' a', ' really'])\n",
      "(tensor([0.3965, 0.1125, 0.0565, 0.0445, 0.0286], grad_fn=<ToCopyBackward0>), [' like', ' entertaining', ' of', ' along', ' boring'])\n",
      "(tensor([0.2051, 0.1870, 0.1372, 0.0575, 0.0365], grad_fn=<ToCopyBackward0>), [' a', ' \"', ' the', ' an', \" '\"])\n",
      "(tensor([0.0563, 0.0354, 0.0222, 0.0181, 0.0154], grad_fn=<ToCopyBackward0>), ['The', 'Plan', 'Ext', 'G', 'Rock'])\n",
      "(tensor([0.7577, 0.0591, 0.0124, 0.0117, 0.0074], grad_fn=<ToCopyBackward0>), ['y', 'n', ' Star', ' Hudson', ' n'])\n",
      "(tensor([0.7972, 0.0282, 0.0254, 0.0180, 0.0119], grad_fn=<ToCopyBackward0>), [' Horror', '\"', ' Bal', '\",', '\".'])\n",
      "(tensor([0.9555, 0.0124, 0.0041, 0.0025, 0.0024], grad_fn=<ToCopyBackward0>), [' Picture', ' Show', '\",', '\".', '\"'])\n",
      "(tensor([9.8957e-01, 2.5770e-03, 2.5452e-03, 1.2363e-03, 4.2469e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Show', ' Shows', ' SHOW', 'Show', ' Theater'])\n",
      "(tensor([0.6760, 0.0959, 0.0807, 0.0634, 0.0594], grad_fn=<ToCopyBackward0>), ['\"', '\",', '\".', ',\"', '.\"'])\n",
      "(tensor([0.2468, 0.1582, 0.0870, 0.0818, 0.0632], grad_fn=<ToCopyBackward0>), [' than', ' or', ' with', ' but', ' and'])\n",
      "(tensor([0.4485, 0.1770, 0.1459, 0.0284, 0.0212], grad_fn=<ToCopyBackward0>), [' \"', ' a', ' it', ' the', ' an'])\n",
      "(tensor([0.0897, 0.0732, 0.0382, 0.0227, 0.0157], grad_fn=<ToCopyBackward0>), ['Rock', 'Plan', 'The', 'C', 'My'])\n",
      "(tensor([0.0973, 0.0866, 0.0699, 0.0499, 0.0407], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Incredible', ' Shining', ' Wizard'])\n",
      "(tensor([0.3097, 0.3007, 0.0660, 0.0630, 0.0471], grad_fn=<ToCopyBackward0>), [' Mel', ' Mr', ' Dr', ' B', ' Sh'])\n",
      "(tensor([0.9329, 0.0155, 0.0061, 0.0059, 0.0043], grad_fn=<ToCopyBackward0>), ['ting', 'ted', 'ons', 'anch', 'ts'])\n",
      "(tensor([9.7694e-01, 6.3904e-03, 1.2792e-03, 7.8092e-04, 7.8045e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Man', ' Woman', 'Man', ' Thing', ' Pot'])\n",
      "(tensor([0.3752, 0.2939, 0.1151, 0.1055, 0.0717], grad_fn=<ToCopyBackward0>), ['\".', '.\"', '\",', '\"', ',\"'])\n",
      "(tensor([0.5629, 0.0682, 0.0475, 0.0230, 0.0204], grad_fn=<ToCopyBackward0>), [' but', ' and', ' which', ' with', ' I'])\n",
      "(tensor([0.3554, 0.1112, 0.0595, 0.0452, 0.0326], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' maybe'])\n",
      "(tensor([0.2055, 0.0730, 0.0681, 0.0625, 0.0484], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' is', ' turned', \"'s\"])\n",
      "(tensor([0.1709, 0.0808, 0.0715, 0.0473, 0.0462], grad_fn=<ToCopyBackward0>), [' pretty', ' actually', ' still', ' worse', ' more'])\n",
      "(tensor([0.2639, 0.1164, 0.0776, 0.0595, 0.0557], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' boring', ' awful', ' cool'])\n",
      "(tensor([0.4762, 0.1797, 0.0857, 0.0275, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '!'])\n",
      "(tensor([0.1246, 0.1015, 0.0931, 0.0560, 0.0428], grad_fn=<ToCopyBackward0>), [' I', ' interesting', ' funny', ' entertaining', ' the'])\n",
      "(tensor([0.2979, 0.1502, 0.0769, 0.0614, 0.0566], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', ' at'])\n",
      "(tensor([0.1451, 0.1297, 0.1280, 0.0640, 0.0609], grad_fn=<ToCopyBackward0>), [' though', ' too', ' especially', ' and', ' considering'])\n",
      "(tensor([0.1791, 0.1266, 0.1155, 0.0761, 0.0546], grad_fn=<ToCopyBackward0>), [' not', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.1433, 0.1254, 0.0779, 0.0675, 0.0580], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' did', ' had', ' didn'])\n",
      "(tensor([0.2060, 0.1020, 0.0891, 0.0682, 0.0427], grad_fn=<ToCopyBackward0>), [' not', ' a', ' too', ' more', ' hard'])\n",
      "(tensor([9.6828e-01, 2.0040e-02, 8.0740e-03, 2.8977e-04, 2.0630e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' not', ' at', ' on'])\n",
      "(tensor([0.1870, 0.0934, 0.0756, 0.0677, 0.0632], grad_fn=<ToCopyBackward0>), [' imagine', ' tell', ' compare', ' believe', ' follow'])\n",
      "(tensor([0.1946, 0.0981, 0.0913, 0.0873, 0.0601], grad_fn=<ToCopyBackward0>), [' how', ' what', ' from', ' if', ' whether'])\n",
      "(tensor([0.3101, 0.2461, 0.0762, 0.0609, 0.0453], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3933, 0.2431, 0.0589, 0.0438, 0.0251], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' would', ' will', ' is'])\n",
      "(tensor([0.2198, 0.1473, 0.0688, 0.0422, 0.0318], grad_fn=<ToCopyBackward0>), [' the', ' a', ' meant', ' supposed', ' more'])\n",
      "(tensor([0.1755, 0.0964, 0.0534, 0.0274, 0.0230], grad_fn=<ToCopyBackward0>), [' actors', ' same', ' acting', ' funny', ' fact'])\n",
      "(tensor([0.7487, 0.1591, 0.0239, 0.0142, 0.0132], grad_fn=<ToCopyBackward0>), [' or', ',', ' of', ' that', ' ('])\n",
      "(tensor([0.9014, 0.0111, 0.0062, 0.0057, 0.0045], grad_fn=<ToCopyBackward0>), [' the', ' story', ' writing', ' just', ' not'])\n",
      "(tensor([0.1687, 0.1361, 0.0969, 0.0217, 0.0210], grad_fn=<ToCopyBackward0>), [' story', ' script', ' plot', ' humor', ' special'])\n",
      "(tensor([0.5912, 0.1577, 0.0725, 0.0414, 0.0311], grad_fn=<ToCopyBackward0>), [' that', '.', ',', ' which', ' or'])\n",
      "(tensor([0.2100, 0.1495, 0.0957, 0.0377, 0.0341], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'The', ' There'])\n",
      "(tensor([0.0831, 0.0791, 0.0383, 0.0293, 0.0282], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ' acting', ' main'])\n",
      "(tensor([0.6697, 0.0738, 0.0519, 0.0192, 0.0190], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.3118, 0.0687, 0.0396, 0.0286, 0.0282], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' very', ' weak', ' bad'])\n",
      "(tensor([0.5024, 0.1548, 0.0227, 0.0214, 0.0209], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' poor', ' weak', ' funny'])\n",
      "(tensor([0.5565, 0.1192, 0.1037, 0.0361, 0.0207], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' for', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a good movie. I think it's a good movie. But the only reason I watched it is because I thought that this movie had to be in theaters. It was not a movie that I wanted to sit through, but if it\n",
      "(tensor([0.3830, 0.1722, 0.0902, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2443, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6525, 0.0595, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0662, 0.0548, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0850, 0.0616, 0.0528, 0.0427, 0.0407], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.3097, 0.1906, 0.0864, 0.0349, 0.0293], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' story', ' premise'])\n",
      "(tensor([0.3240, 0.1274, 0.0885, 0.0761, 0.0577], grad_fn=<ToCopyBackward0>), ['.', '...', '....', '!', ','])\n",
      "(tensor([0.3265, 0.2301, 0.0793, 0.0280, 0.0153], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2052, 0.1358, 0.0547, 0.0522, 0.0415], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' just'])\n",
      "(tensor([0.4451, 0.1474, 0.0753, 0.0494, 0.0190], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' they'])\n",
      "(tensor([0.3173, 0.2336, 0.0711, 0.0524, 0.0451], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' could', ' has', ' is'])\n",
      "(tensor([0.4072, 0.0739, 0.0402, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' one', ' funny', ' an', ' very'])\n",
      "(tensor([0.4142, 0.0721, 0.0717, 0.0575, 0.0487], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' movie', ' great'])\n",
      "(tensor([0.8772, 0.0348, 0.0247, 0.0159, 0.0039], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' story', ' plot'])\n",
      "(tensor([0.3963, 0.1688, 0.0456, 0.0434, 0.0433], grad_fn=<ToCopyBackward0>), ['.', ' for', ' in', ' if', ','])\n",
      "(tensor([0.3826, 0.1046, 0.0869, 0.0343, 0.0311], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.1600, 0.1375, 0.1081, 0.0824, 0.0267], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' when'])\n",
      "(tensor([0.1343, 0.1302, 0.0672, 0.0539, 0.0431], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' story', ' only', ' one'])\n",
      "(tensor([0.3407, 0.1675, 0.1243, 0.0522, 0.0512], grad_fn=<ToCopyBackward0>), [' reason', ' thing', ' way', ' person', ' good'])\n",
      "(tensor([0.6353, 0.1090, 0.0704, 0.0459, 0.0409], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' this', ' it'])\n",
      "(tensor([0.1371, 0.1265, 0.1217, 0.0479, 0.0427], grad_fn=<ToCopyBackward0>), [' gave', ' watched', \"'m\", ' didn', ' was'])\n",
      "(tensor([0.8107, 0.1107, 0.0373, 0.0094, 0.0019], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.4388, 0.1705, 0.0757, 0.0243, 0.0237], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' I'])\n",
      "(tensor([0.7169, 0.0714, 0.0507, 0.0217, 0.0194], grad_fn=<ToCopyBackward0>), [' because', ' I', ' that', ' the', ' for'])\n",
      "(tensor([0.6089, 0.0675, 0.0651, 0.0612, 0.0439], grad_fn=<ToCopyBackward0>), [' I', ' my', ' the', ' it', ' of'])\n",
      "(tensor([0.1623, 0.1414, 0.1215, 0.0865, 0.0332], grad_fn=<ToCopyBackward0>), [\"'m\", ' thought', ' like', ' was', ' really'])\n",
      "(tensor([0.5034, 0.1115, 0.0893, 0.0598, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ','])\n",
      "(tensor([0.2037, 0.1228, 0.0886, 0.0690, 0.0375], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' maybe'])\n",
      "(tensor([0.3984, 0.2302, 0.1926, 0.0297, 0.0113], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' guy', ' girl'])\n",
      "(tensor([0.4900, 0.1079, 0.0618, 0.0234, 0.0199], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' would', ' could'])\n",
      "(tensor([0.3035, 0.2667, 0.0771, 0.0484, 0.0293], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' some', ' an'])\n",
      "(tensor([0.7447, 0.0837, 0.0684, 0.0160, 0.0113], grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', ' come', ' go'])\n",
      "(tensor([0.1515, 0.0900, 0.0724, 0.0569, 0.0369], grad_fn=<ToCopyBackward0>), [' made', ' a', ' funny', ' in', ' good'])\n",
      "(tensor([0.5818, 0.1380, 0.0694, 0.0336, 0.0273], grad_fn=<ToCopyBackward0>), [' the', ' there', ' my', ' theaters', ' it'])\n",
      "(tensor([0.4067, 0.1904, 0.0932, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' to', ' in'])\n",
      "(tensor([0.3148, 0.0956, 0.0901, 0.0378, 0.0377], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' So'])\n",
      "(tensor([0.2977, 0.1900, 0.1142, 0.0796, 0.0358], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' has', ' is'])\n",
      "(tensor([0.1693, 0.1093, 0.0980, 0.0863, 0.0632], grad_fn=<ToCopyBackward0>), [' like', ' so', ' not', ' a', ' just'])\n",
      "(tensor([0.1777, 0.0941, 0.0606, 0.0565, 0.0435], grad_fn=<ToCopyBackward0>), [' a', ' like', ' just', ' funny', ' something'])\n",
      "(tensor([0.1113, 0.0539, 0.0356, 0.0322, 0.0320], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' very', ' waste', ' bad'])\n",
      "(tensor([0.5698, 0.1869, 0.0751, 0.0396, 0.0237], grad_fn=<ToCopyBackward0>), [' that', ' I', '.', ' for', ' to'])\n",
      "(tensor([0.5226, 0.1559, 0.0494, 0.0218, 0.0196], grad_fn=<ToCopyBackward0>), [' I', ' was', ' a', ' you', ' the'])\n",
      "(tensor([0.1299, 0.0905, 0.0597, 0.0588, 0.0522], grad_fn=<ToCopyBackward0>), [' would', ' was', ' wanted', ' really', ' could'])\n",
      "(tensor([0.8085, 0.0510, 0.0374, 0.0107, 0.0088], grad_fn=<ToCopyBackward0>), [' to', '.', ' on', ' out', ' in'])\n",
      "(tensor([0.3927, 0.2069, 0.0944, 0.0615, 0.0470], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' make', ' sit', ' be'])\n",
      "(tensor([0.7283, 0.1774, 0.0242, 0.0170, 0.0155], grad_fn=<ToCopyBackward0>), [' through', ' down', ' and', ' around', ' in'])\n",
      "(tensor([0.1833, 0.1346, 0.0950, 0.0780, 0.0485], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' just'])\n",
      "(tensor([0.3962, 0.0546, 0.0457, 0.0433, 0.0360], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' so', ' like'])\n",
      "(tensor([0.4695, 0.2242, 0.0276, 0.0215, 0.0140], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' if', ' at'])\n",
      "(tensor([0.3871, 0.2326, 0.2279, 0.0200, 0.0175], grad_fn=<ToCopyBackward0>), [' I', ' it', ' you', ' there', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this movie was pretty awful. It was slow moving with a lot of boring scenes and acting. It was not scary at all. There was no tension. The acting was so bad, I was not entertained at all. I was not even scared\n",
      "(tensor([0.3838, 0.1717, 0.0900, 0.0772, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3317, 0.2365, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4149, 0.1967, 0.1688, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.5735, 0.0626, 0.0570, 0.0477, 0.0273], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.0899, 0.0876, 0.0655, 0.0531, 0.0410], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' very', ' so', ' really'])\n",
      "(tensor([0.1798, 0.1534, 0.0867, 0.0836, 0.0777], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' awful', ' boring', ' lame'])\n",
      "(tensor([0.6090, 0.0810, 0.0774, 0.0375, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2235, 0.1430, 0.1414, 0.0220, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.2898, 0.1659, 0.0754, 0.0562, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' has'])\n",
      "(tensor([0.0658, 0.0607, 0.0584, 0.0572, 0.0562], grad_fn=<ToCopyBackward0>), [' just', ' slow', ' boring', ' a', ' so'])\n",
      "(tensor([0.3545, 0.2933, 0.1951, 0.0560, 0.0145], grad_fn=<ToCopyBackward0>), [' and', ' moving', ',', ' paced', '-'])\n",
      "(tensor([0.5905, 0.2870, 0.0376, 0.0182, 0.0072], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' with', ' ('])\n",
      "(tensor([0.1784, 0.1596, 0.0501, 0.0486, 0.0468], grad_fn=<ToCopyBackward0>), [' no', ' a', ' bad', ' lots', ' stupid'])\n",
      "(tensor([0.2300, 0.0926, 0.0529, 0.0440, 0.0316], grad_fn=<ToCopyBackward0>), [' lot', ' weak', ' very', ' few', ' bunch'])\n",
      "(tensor([9.9097e-01, 1.5135e-03, 1.4034e-03, 4.3509e-04, 4.1624e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' less', ' more', ' to', ' going'])\n",
      "(tensor([0.0985, 0.0497, 0.0494, 0.0248, 0.0184], grad_fn=<ToCopyBackward0>), [' scenes', ' pointless', ' boring', ' wasted', ' plot'])\n",
      "(tensor([0.3121, 0.1409, 0.0610, 0.0167, 0.0154], grad_fn=<ToCopyBackward0>), [' scenes', ' dialogue', ' dialog', ' visuals', ' talking'])\n",
      "(tensor([0.5604, 0.1385, 0.0927, 0.0413, 0.0251], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' that', ' of'])\n",
      "(tensor([0.0765, 0.0746, 0.0514, 0.0461, 0.0428], grad_fn=<ToCopyBackward0>), [' a', ' stupid', ' the', ' acting', ' it'])\n",
      "(tensor([0.4141, 0.3251, 0.0744, 0.0406, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' and', ' with'])\n",
      "(tensor([0.2278, 0.1549, 0.1443, 0.0373, 0.0284], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', 'The'])\n",
      "(tensor([0.3246, 0.0825, 0.0817, 0.0780, 0.0713], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', ' wasn', \"'s\"])\n",
      "(tensor([0.1199, 0.0988, 0.0764, 0.0720, 0.0580], grad_fn=<ToCopyBackward0>), [' not', ' also', ' boring', ' a', ' just'])\n",
      "(tensor([0.4413, 0.2222, 0.0631, 0.0232, 0.0226], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' entertaining', ' a', ' very'])\n",
      "(tensor([0.4341, 0.1762, 0.0893, 0.0462, 0.0417], grad_fn=<ToCopyBackward0>), [' at', ' or', ',', ' and', '.'])\n",
      "(tensor([9.9631e-01, 1.0215e-03, 7.5475e-04, 6.0882e-04, 3.6801e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' least', ' the', ' ALL', ' any'])\n",
      "(tensor([0.6242, 0.1352, 0.0760, 0.0233, 0.0193], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.2041, 0.1955, 0.1550, 0.0340, 0.0263], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', 'The'])\n",
      "(tensor([0.7147, 0.1110, 0.0839, 0.0378, 0.0102], grad_fn=<ToCopyBackward0>), [' was', ' were', ' wasn', ' is', ' are'])\n",
      "(tensor([0.2880, 0.1270, 0.0911, 0.0830, 0.0819], grad_fn=<ToCopyBackward0>), [' no', ' not', ' nothing', ' a', ' one'])\n",
      "(tensor([0.4803, 0.1683, 0.0392, 0.0229, 0.0226], grad_fn=<ToCopyBackward0>), [' suspense', ' gore', ' horror', ' plot', ' tension'])\n",
      "(tensor([0.2111, 0.1504, 0.1270, 0.0860, 0.0825], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' whatsoever', ' at'])\n",
      "(tensor([0.2313, 0.2138, 0.1477, 0.0795, 0.0384], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.3777, 0.0425, 0.0358, 0.0280, 0.0269], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' actors', ' movie'])\n",
      "(tensor([0.8674, 0.0400, 0.0103, 0.0086, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' and', ' in'])\n",
      "(tensor([0.1496, 0.0836, 0.0728, 0.0701, 0.0660], grad_fn=<ToCopyBackward0>), [' not', ' terrible', ' bad', ' horrible', ' so'])\n",
      "(tensor([0.4598, 0.0674, 0.0439, 0.0401, 0.0335], grad_fn=<ToCopyBackward0>), [' bad', ' fake', '-', ' wooden', ' cor'])\n",
      "(tensor([0.5630, 0.0902, 0.0662, 0.0622, 0.0579], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.4731, 0.1680, 0.0697, 0.0495, 0.0292], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' and'])\n",
      "(tensor([0.1015, 0.1007, 0.0923, 0.0869, 0.0737], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' actually', ' could', ' almost'])\n",
      "(tensor([0.1024, 0.0841, 0.0792, 0.0564, 0.0537], grad_fn=<ToCopyBackward0>), [' not', ' embarrassed', ' thinking', ' wondering', ' surprised'])\n",
      "(tensor([0.3024, 0.0691, 0.0666, 0.0497, 0.0446], grad_fn=<ToCopyBackward0>), [' even', ' entertained', ' surprised', ' expecting', ' able'])\n",
      "(tensor([0.2701, 0.2468, 0.1048, 0.0781, 0.0700], grad_fn=<ToCopyBackward0>), [' by', ' at', '.', ' for', ' even'])\n",
      "(tensor([9.8867e-01, 6.8837e-03, 1.4650e-03, 1.0026e-03, 5.6426e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' ALL', ' least'])\n",
      "(tensor([0.7134, 0.1041, 0.0315, 0.0196, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' by', ',', ' and', ' at'])\n",
      "(tensor([0.2708, 0.2020, 0.1141, 0.0338, 0.0295], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1036, 0.0847, 0.0597, 0.0595, 0.0504], grad_fn=<ToCopyBackward0>), [' was', ' think', ' really', ' would', ' just'])\n",
      "(tensor([0.1063, 0.0706, 0.0630, 0.0492, 0.0454], grad_fn=<ToCopyBackward0>), [' not', ' just', ' very', ' really', ' so'])\n",
      "(tensor([0.2038, 0.1032, 0.0720, 0.0705, 0.0595], grad_fn=<ToCopyBackward0>), [' even', ' impressed', ' scared', ' entertained', ' expecting'])\n",
      "(tensor([0.1199, 0.1091, 0.0990, 0.0597, 0.0351], grad_fn=<ToCopyBackward0>), [' creep', ' scared', ' bored', ' interested', ' entertained'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie was pretty funny. I think that it was a pretty good movie. I just thought it wasn't very believable and it just didn't feel to me as a viewer, as a storyteller, as an actor, that this was\n",
      "(tensor([0.3843, 0.1714, 0.0896, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3307, 0.2372, 0.0583, 0.0559, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2635, 0.0545, 0.0269, 0.0218, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5308, 0.0605, 0.0537, 0.0233, 0.0206], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.1790, 0.0810, 0.0522, 0.0426, 0.0358], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' a', ' so', ' terrible'])\n",
      "(tensor([0.1903, 0.1174, 0.0817, 0.0803, 0.0608], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3077, 0.1816, 0.1439, 0.0466, 0.0444], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' but'])\n",
      "(tensor([0.2807, 0.1434, 0.0694, 0.0339, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1148, 0.0826, 0.0812, 0.0546, 0.0542], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' mean', ' think'])\n",
      "(tensor([0.3384, 0.2077, 0.1389, 0.0349, 0.0289], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' I', ' this'])\n",
      "(tensor([0.3011, 0.2324, 0.0693, 0.0178, 0.0167], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' there', ' I'])\n",
      "(tensor([0.2519, 0.1951, 0.1196, 0.0507, 0.0261], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' has', ' had'])\n",
      "(tensor([0.2367, 0.1244, 0.1098, 0.0472, 0.0204], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' pretty', ' very', ' more'])\n",
      "(tensor([0.1900, 0.1898, 0.1028, 0.0385, 0.0331], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' very', ' lot', ' great'])\n",
      "(tensor([0.1612, 0.1169, 0.0755, 0.0434, 0.0423], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' accurate', ' average', ' boring'])\n",
      "(tensor([0.6318, 0.1898, 0.0279, 0.0123, 0.0120], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' satire', ' spoof', ' film'])\n",
      "(tensor([0.6565, 0.0802, 0.0354, 0.0352, 0.0216], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' for', '....'])\n",
      "(tensor([0.2994, 0.1077, 0.0687, 0.0450, 0.0331], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.1100, 0.1095, 0.0713, 0.0662, 0.0660], grad_fn=<ToCopyBackward0>), [' think', ' thought', ' really', ' just', ' mean'])\n",
      "(tensor([0.1460, 0.1419, 0.0971, 0.0705, 0.0665], grad_fn=<ToCopyBackward0>), [' don', ' thought', ' think', ' wish', ' didn'])\n",
      "(tensor([0.3687, 0.3377, 0.1357, 0.0129, 0.0116], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.6623, 0.0475, 0.0319, 0.0301, 0.0244], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' wasn', ' could'])\n",
      "(tensor([9.9743e-01, 7.4067e-04, 3.0100e-04, 1.4555e-04, 1.4195e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2459, 0.1938, 0.1840, 0.0409, 0.0346], grad_fn=<ToCopyBackward0>), [' as', ' very', ' funny', ' the', ' that'])\n",
      "(tensor([0.3396, 0.2159, 0.0722, 0.0550, 0.0424], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' well', ' believable', ' scary'])\n",
      "(tensor([0.2637, 0.2219, 0.1004, 0.0917, 0.0607], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' to'])\n",
      "(tensor([0.2046, 0.1602, 0.1181, 0.0929, 0.0647], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' believable'])\n",
      "(tensor([0.2894, 0.2074, 0.1662, 0.0737, 0.0454], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' didn', ' just', ' seemed'])\n",
      "(tensor([0.4210, 0.2256, 0.0819, 0.0133, 0.0120], grad_fn=<ToCopyBackward0>), [' seemed', ' didn', ' wasn', ' was', ' seems'])\n",
      "(tensor([9.9759e-01, 6.9868e-04, 2.8767e-04, 1.4823e-04, 1.2023e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.1608, 0.0808, 0.0680, 0.0625, 0.0606], grad_fn=<ToCopyBackward0>), [' have', ' ring', ' feel', ' come', ' move'])\n",
      "(tensor([0.3745, 0.1611, 0.0726, 0.0676, 0.0451], grad_fn=<ToCopyBackward0>), [' real', ' like', ' very', ' to', ' believable'])\n",
      "(tensor([0.8822, 0.0331, 0.0240, 0.0086, 0.0065], grad_fn=<ToCopyBackward0>), [' me', ' be', ' bad', ' good', ' much'])\n",
      "(tensor([0.3159, 0.1511, 0.0979, 0.0876, 0.0630], grad_fn=<ToCopyBackward0>), [' that', ' as', ' to', ' like', '.'])\n",
      "(tensor([0.3383, 0.1163, 0.1109, 0.0809, 0.0565], grad_fn=<ToCopyBackward0>), [' believable', ' if', ' a', ' realistic', ' though'])\n",
      "(tensor([0.2134, 0.1248, 0.1236, 0.0972, 0.0846], grad_fn=<ToCopyBackward0>), [' viewer', ' movie', ' story', ' real', ' character'])\n",
      "(tensor([0.3695, 0.1772, 0.1297, 0.0982, 0.0357], grad_fn=<ToCopyBackward0>), [' that', '.', ',', ' as', ' to'])\n",
      "(tensor([0.3855, 0.0829, 0.0558, 0.0472, 0.0389], grad_fn=<ToCopyBackward0>), [' as', ' the', ' that', ' like', ' I'])\n",
      "(tensor([0.5435, 0.1597, 0.0667, 0.0398, 0.0348], grad_fn=<ToCopyBackward0>), [' a', ' an', ' to', ' someone', ' I'])\n",
      "(tensor([0.2496, 0.0870, 0.0798, 0.0702, 0.0662], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' person', ' viewer', ' film'])\n",
      "(tensor([0.7454, 0.0983, 0.0454, 0.0352, 0.0133], grad_fn=<ToCopyBackward0>), ['te', '-', ' tell', ' telling', ','])\n",
      "(tensor([9.9187e-01, 3.7861e-03, 8.3504e-04, 7.9672e-04, 2.9606e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ller', 'acher', 'llers', 'aser', 'll'])\n",
      "(tensor([0.7686, 0.0699, 0.0412, 0.0353, 0.0129], grad_fn=<ToCopyBackward0>), [',', '.', ' that', ' and', ' or'])\n",
      "(tensor([0.5255, 0.2834, 0.0491, 0.0313, 0.0171], grad_fn=<ToCopyBackward0>), [' as', ' that', ' the', ' what', ' and'])\n",
      "(tensor([0.6675, 0.2133, 0.0260, 0.0145, 0.0134], grad_fn=<ToCopyBackward0>), [' a', ' an', ' someone', ' the', ' somebody'])\n",
      "(tensor([0.7461, 0.1135, 0.0247, 0.0159, 0.0083], grad_fn=<ToCopyBackward0>), [' actor', ' artist', ' actress', ' acting', ' artistic'])\n",
      "(tensor([0.5775, 0.1247, 0.0719, 0.0262, 0.0243], grad_fn=<ToCopyBackward0>), [',', '.', ' that', ' and', ' to'])\n",
      "(tensor([0.6053, 0.2421, 0.0285, 0.0270, 0.0240], grad_fn=<ToCopyBackward0>), [' as', ' that', ' the', ' and', ' what'])\n",
      "(tensor([0.3200, 0.2577, 0.1320, 0.0622, 0.0466], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.5674, 0.0974, 0.0803, 0.0652, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' is', ' could', ' movie', ' really'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a sequel to a really good movie. I'm sure it was not a sequel to the good movie. I'm sure it was a totally different movie. I'm not sure if I liked it. I don't really remember. But it\n",
      "(tensor([0.3840, 0.1717, 0.0898, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1166, 0.0397, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1840, 0.1457, 0.0508, 0.0454, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1645, 0.1160, 0.0576, 0.0492, 0.0449], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8522, 0.0310, 0.0140, 0.0123, 0.0085], grad_fn=<ToCopyBackward0>), [' to', ' of', ',', '...', ' that'])\n",
      "(tensor([0.1701, 0.0653, 0.0591, 0.0367, 0.0355], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.1922, 0.1524, 0.0636, 0.0577, 0.0540], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' real', ' classic', ' good'])\n",
      "(tensor([0.3910, 0.3174, 0.0473, 0.0202, 0.0175], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' great', ' old', ','])\n",
      "(tensor([0.5521, 0.1037, 0.0440, 0.0320, 0.0266], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' action', ' story', ' horror'])\n",
      "(tensor([0.5223, 0.0777, 0.0658, 0.0475, 0.0274], grad_fn=<ToCopyBackward0>), ['.', ',', ' called', ' that', ' I'])\n",
      "(tensor([0.2653, 0.1004, 0.0615, 0.0613, 0.0446], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.1307, 0.1150, 0.1049, 0.0476, 0.0300], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' just'])\n",
      "(tensor([0.2018, 0.1497, 0.1005, 0.0673, 0.0646], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' sure', ' just'])\n",
      "(tensor([0.1716, 0.1663, 0.1206, 0.1073, 0.0879], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' there', ' this'])\n",
      "(tensor([0.3367, 0.2322, 0.1280, 0.0682, 0.0538], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' will', ' wasn'])\n",
      "(tensor([0.3329, 0.0787, 0.0719, 0.0528, 0.0473], grad_fn=<ToCopyBackward0>), [' a', '.', ' not', ' something', ','])\n",
      "(tensor([0.2813, 0.0990, 0.0625, 0.0549, 0.0454], grad_fn=<ToCopyBackward0>), [' a', ' as', ' even', '.', ' the'])\n",
      "(tensor([0.3804, 0.1265, 0.1196, 0.1147, 0.0234], grad_fn=<ToCopyBackward0>), [' sequel', ' great', ' remake', ' good', ' big'])\n",
      "(tensor([0.7713, 0.0778, 0.0398, 0.0206, 0.0144], grad_fn=<ToCopyBackward0>), [' to', ',', '.', ' at', ' but'])\n",
      "(tensor([0.4237, 0.1084, 0.0468, 0.0454, 0.0341], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' any', ' this'])\n",
      "(tensor([0.2534, 0.1697, 0.1235, 0.0773, 0.0590], grad_fn=<ToCopyBackward0>), [' original', ' great', ' best', ' good', ' excellent'])\n",
      "(tensor([0.9107, 0.0109, 0.0075, 0.0039, 0.0020], grad_fn=<ToCopyBackward0>), [' movie', ' original', ' book', ' one', ' movies'])\n",
      "(tensor([0.5037, 0.2475, 0.0516, 0.0248, 0.0241], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' because', ' I'])\n",
      "(tensor([0.3656, 0.0981, 0.0458, 0.0383, 0.0364], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' But'])\n",
      "(tensor([0.1170, 0.0752, 0.0723, 0.0634, 0.0589], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' don', ' just', ' really'])\n",
      "(tensor([0.4072, 0.1928, 0.1129, 0.0430, 0.0174], grad_fn=<ToCopyBackward0>), [' sure', ' not', ' just', ' a', ' really'])\n",
      "(tensor([0.3173, 0.1278, 0.1177, 0.0867, 0.0791], grad_fn=<ToCopyBackward0>), [' it', ' that', ' the', ' there', ' they'])\n",
      "(tensor([0.6869, 0.1159, 0.0779, 0.0215, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' had', ' is'])\n",
      "(tensor([0.3864, 0.2382, 0.0674, 0.0379, 0.0336], grad_fn=<ToCopyBackward0>), [' a', ' not', ' something', ' just', ' more'])\n",
      "(tensor([0.1368, 0.1291, 0.0662, 0.0622, 0.0528], grad_fn=<ToCopyBackward0>), [' good', ' really', ' sequel', ' very', ' totally'])\n",
      "(tensor([0.6877, 0.0743, 0.0264, 0.0127, 0.0107], grad_fn=<ToCopyBackward0>), [' original', ' different', ' new', ' unrelated', ' weird'])\n",
      "(tensor([0.8866, 0.0444, 0.0159, 0.0087, 0.0065], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' thing', ' kind'])\n",
      "(tensor([0.5256, 0.1404, 0.0500, 0.0386, 0.0330], grad_fn=<ToCopyBackward0>), ['.', ',', ' than', ' from', ' with'])\n",
      "(tensor([0.2444, 0.0987, 0.0602, 0.0411, 0.0383], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.1784, 0.1132, 0.0737, 0.0566, 0.0558], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' can', ' was', ' really'])\n",
      "(tensor([0.3018, 0.2921, 0.0973, 0.0289, 0.0265], grad_fn=<ToCopyBackward0>), [' sure', ' not', ' just', ' a', ' really'])\n",
      "(tensor([0.3223, 0.2364, 0.0641, 0.0463, 0.0448], grad_fn=<ToCopyBackward0>), [' sure', ' even', ' a', ' quite', ' going'])\n",
      "(tensor([0.2192, 0.1321, 0.1013, 0.0721, 0.0605], grad_fn=<ToCopyBackward0>), [' if', '.', ' it', ' what', ' that'])\n",
      "(tensor([0.4723, 0.0991, 0.0773, 0.0660, 0.0533], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' that', ' this'])\n",
      "(tensor([0.0897, 0.0806, 0.0789, 0.0576, 0.0554], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' even', ' liked', ' was'])\n",
      "(tensor([0.6769, 0.1533, 0.0502, 0.0437, 0.0211], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' any'])\n",
      "(tensor([0.3082, 0.1971, 0.1895, 0.0757, 0.0408], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' at', ' as'])\n",
      "(tensor([0.4728, 0.0923, 0.0519, 0.0367, 0.0255], grad_fn=<ToCopyBackward0>), [' I', ' It', ' Maybe', 'I', ' The'])\n",
      "(tensor([0.1261, 0.0713, 0.0704, 0.0700, 0.0640], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' really', ' don', ' was'])\n",
      "(tensor([9.9676e-01, 5.4846e-04, 4.5642e-04, 2.5229e-04, 1.5378e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3377, 0.1604, 0.1388, 0.1205, 0.0643], grad_fn=<ToCopyBackward0>), [' know', ' think', ' really', ' remember', ' even'])\n",
      "(tensor([0.3205, 0.2528, 0.1107, 0.0633, 0.0391], grad_fn=<ToCopyBackward0>), [' remember', ' know', ' like', ' think', ' have'])\n",
      "(tensor([0.3728, 0.1918, 0.1312, 0.0403, 0.0394], grad_fn=<ToCopyBackward0>), [' it', ' the', '.', ' much', ' that'])\n",
      "(tensor([0.2996, 0.1060, 0.0629, 0.0550, 0.0334], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' Maybe', ' The'])\n",
      "(tensor([0.3053, 0.1565, 0.0682, 0.0353, 0.0273], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' that'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. It's not even funny. There's a guy who plays a genie. He comes into this town. He's got magic powers. And he's supposed to make a film. He makes a documentary about it.\n",
      "(tensor([0.3834, 0.1724, 0.0902, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2449, 0.1958, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7770, 0.0502, 0.0283, 0.0262, 0.0100], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1267, 0.1050, 0.0742, 0.0603], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5982, 0.0806, 0.0744, 0.0363, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2062, 0.1829, 0.1379, 0.0233, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.2557, 0.1761, 0.0682, 0.0656, 0.0397], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' is'])\n",
      "(tensor([0.2347, 0.0945, 0.0671, 0.0630, 0.0562], grad_fn=<ToCopyBackward0>), [' not', ' a', ' like', ' one', ' just'])\n",
      "(tensor([0.5789, 0.0793, 0.0614, 0.0409, 0.0316], grad_fn=<ToCopyBackward0>), [' even', ' as', ' funny', ' that', ' a'])\n",
      "(tensor([0.5284, 0.0570, 0.0338, 0.0313, 0.0287], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' a', ' entertaining', ' worth'])\n",
      "(tensor([0.5128, 0.1308, 0.0572, 0.0404, 0.0364], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' in', ' because'])\n",
      "(tensor([0.2579, 0.1572, 0.1451, 0.0269, 0.0155], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' This'])\n",
      "(tensor([0.2931, 0.2021, 0.1883, 0.1763, 0.0594], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' are', ' was', ' were'])\n",
      "(tensor([0.3000, 0.1271, 0.1135, 0.1060, 0.0351], grad_fn=<ToCopyBackward0>), [' no', ' not', ' a', ' nothing', ' some'])\n",
      "(tensor([0.2466, 0.1929, 0.0621, 0.0415, 0.0333], grad_fn=<ToCopyBackward0>), [' guy', ' lot', ' couple', ' really', ' scene'])\n",
      "(tensor([0.4204, 0.1010, 0.1003, 0.0381, 0.0365], grad_fn=<ToCopyBackward0>), [' who', ' that', ' named', ' with', ' called'])\n",
      "(tensor([0.0931, 0.0677, 0.0622, 0.0552, 0.0549], grad_fn=<ToCopyBackward0>), [' plays', ' falls', \"'s\", ' is', ' thinks'])\n",
      "(tensor([0.6531, 0.0638, 0.0363, 0.0247, 0.0173], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' himself', ' his'])\n",
      "(tensor([0.0996, 0.0606, 0.0509, 0.0208, 0.0206], grad_fn=<ToCopyBackward0>), [' gen', ' guy', ' really', ' doctor', ' character'])\n",
      "(tensor([0.9448, 0.0179, 0.0130, 0.0073, 0.0013], grad_fn=<ToCopyBackward0>), ['ie', 'oc', 'ial', 'ious', 'ki'])\n",
      "(tensor([0.5685, 0.0572, 0.0536, 0.0520, 0.0508], grad_fn=<ToCopyBackward0>), [' who', ' named', ' that', '.', ','])\n",
      "(tensor([0.3751, 0.0950, 0.0575, 0.0550, 0.0492], grad_fn=<ToCopyBackward0>), [' He', ' The', ' And', ' There', ' I'])\n",
      "(tensor([0.1508, 0.1082, 0.0569, 0.0482, 0.0381], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' comes', ' can', ' wants'])\n",
      "(tensor([0.2529, 0.2126, 0.1197, 0.0885, 0.0835], grad_fn=<ToCopyBackward0>), [' in', ' out', ' into', ' back', ' to'])\n",
      "(tensor([0.4553, 0.1806, 0.1205, 0.1146, 0.0332], grad_fn=<ToCopyBackward0>), [' a', ' the', ' your', ' this', ' my'])\n",
      "(tensor([0.1282, 0.1164, 0.0917, 0.0748, 0.0479], grad_fn=<ToCopyBackward0>), [' hotel', ' movie', ' town', ' house', ' apartment'])\n",
      "(tensor([0.3622, 0.2579, 0.0562, 0.0520, 0.0475], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' with', ' where'])\n",
      "(tensor([0.5505, 0.0913, 0.0825, 0.0291, 0.0282], grad_fn=<ToCopyBackward0>), [' He', ' The', ' And', ' I', ' There'])\n",
      "(tensor([0.1392, 0.1259, 0.1240, 0.0569, 0.0468], grad_fn=<ToCopyBackward0>), [' wants', ' has', \"'s\", ' says', ' makes'])\n",
      "(tensor([0.1592, 0.1238, 0.0743, 0.0494, 0.0311], grad_fn=<ToCopyBackward0>), [' supposed', ' a', ' got', ' not', ' been'])\n",
      "(tensor([0.3541, 0.1400, 0.0754, 0.0567, 0.0552], grad_fn=<ToCopyBackward0>), [' this', ' a', ' all', ' the', ' magic'])\n",
      "(tensor([0.7786, 0.0246, 0.0136, 0.0120, 0.0074], grad_fn=<ToCopyBackward0>), [' powers', ' eyes', ' tricks', '.', ' that'])\n",
      "(tensor([0.6660, 0.1473, 0.0512, 0.0311, 0.0107], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' to'])\n",
      "(tensor([0.5183, 0.1468, 0.0546, 0.0315, 0.0232], grad_fn=<ToCopyBackward0>), [' He', ' And', ' The', ' I', ' So'])\n",
      "(tensor([0.6080, 0.0568, 0.0449, 0.0219, 0.0218], grad_fn=<ToCopyBackward0>), [' he', ' the', ' then', ' they', ' his'])\n",
      "(tensor([0.2059, 0.1612, 0.0812, 0.0357, 0.0285], grad_fn=<ToCopyBackward0>), [\"'s\", ' wants', ' has', ' tries', ' makes'])\n",
      "(tensor([0.1463, 0.1173, 0.0830, 0.0672, 0.0428], grad_fn=<ToCopyBackward0>), [' got', ' supposed', ' been', ' trying', ' like'])\n",
      "(tensor([9.9941e-01, 1.5674e-04, 1.1741e-04, 1.6684e-05, 1.6093e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ',', ' be', ' in', ' -'])\n",
      "(tensor([0.2176, 0.1951, 0.1712, 0.0948, 0.0219], grad_fn=<ToCopyBackward0>), [' help', ' be', ' make', ' bring', ' protect'])\n",
      "(tensor([0.2630, 0.2182, 0.1029, 0.0361, 0.0348], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' all', ' everybody'])\n",
      "(tensor([0.4862, 0.2810, 0.0159, 0.0063, 0.0059], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' documentary', ' really', ' wish'])\n",
      "(tensor([0.1734, 0.1681, 0.0867, 0.0802, 0.0764], grad_fn=<ToCopyBackward0>), ['.', ' for', ' that', ' with', ' about'])\n",
      "(tensor([0.2360, 0.1614, 0.0875, 0.0664, 0.0591], grad_fn=<ToCopyBackward0>), [' And', ' He', ' But', ' The', ' So'])\n",
      "(tensor([0.2026, 0.0877, 0.0736, 0.0580, 0.0501], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' goes', ' makes', ' can'])\n",
      "(tensor([0.4134, 0.3294, 0.0535, 0.0522, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' this', ' it', ' the', ' some'])\n",
      "(tensor([0.4600, 0.2019, 0.0395, 0.0379, 0.0324], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' stupid', ' documentary', ' really'])\n",
      "(tensor([0.6255, 0.0990, 0.0928, 0.0225, 0.0127], grad_fn=<ToCopyBackward0>), [' about', ' on', '.', ' called', ' with'])\n",
      "(tensor([0.2346, 0.1949, 0.1179, 0.0962, 0.0699], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' a', ' his'])\n",
      "(tensor([0.8386, 0.0720, 0.0277, 0.0066, 0.0059], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' in'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was so bad that I thought it would be better to make a sequel. I think this is the worst movie I have ever made. It was so bad. I have never made a movie so bad, and I have made some really good\n",
      "(tensor([0.3831, 0.1724, 0.0902, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4998, 0.0602, 0.0342, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6241, 0.0399, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.5385, 0.0405, 0.0323, 0.0297, 0.0250], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' atro', ' boring', ' terrible'])\n",
      "(tensor([0.2687, 0.2584, 0.1541, 0.0618, 0.0435], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.5330, 0.1157, 0.0823, 0.0517, 0.0392], grad_fn=<ToCopyBackward0>), [' I', ' it', ' i', ' the', ' even'])\n",
      "(tensor([0.1631, 0.0699, 0.0613, 0.0440, 0.0337], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' couldn', ' just'])\n",
      "(tensor([0.3442, 0.1753, 0.1327, 0.0610, 0.0608], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' maybe', ' this'])\n",
      "(tensor([0.3268, 0.2630, 0.1548, 0.0521, 0.0416], grad_fn=<ToCopyBackward0>), [' was', ' would', ' must', ' might', ' had'])\n",
      "(tensor([0.7026, 0.0633, 0.0423, 0.0374, 0.0339], grad_fn=<ToCopyBackward0>), [' be', ' have', ' never', ' get', ' make'])\n",
      "(tensor([0.3057, 0.0727, 0.0631, 0.0531, 0.0433], grad_fn=<ToCopyBackward0>), [' funny', ' better', ' good', ' funn', ' a'])\n",
      "(tensor([0.7165, 0.0757, 0.0226, 0.0200, 0.0151], grad_fn=<ToCopyBackward0>), [' to', ' if', ' than', ' with', ' as'])\n",
      "(tensor([0.1704, 0.0888, 0.0835, 0.0466, 0.0463], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' make', ' have', ' be'])\n",
      "(tensor([0.6380, 0.0873, 0.0435, 0.0350, 0.0329], grad_fn=<ToCopyBackward0>), [' a', ' another', ' it', ' an', ' the'])\n",
      "(tensor([0.1693, 0.1559, 0.1003, 0.0331, 0.0255], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' second', ' really', ' short'])\n",
      "(tensor([0.4299, 0.1122, 0.0878, 0.0502, 0.0328], grad_fn=<ToCopyBackward0>), ['.', ' than', ' to', ',', ' that'])\n",
      "(tensor([0.2906, 0.0580, 0.0563, 0.0501, 0.0480], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' But'])\n",
      "(tensor([0.1149, 0.0904, 0.0758, 0.0402, 0.0377], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' really', ' think'])\n",
      "(tensor([0.1756, 0.1628, 0.1092, 0.0999, 0.0532], grad_fn=<ToCopyBackward0>), [' that', ' the', ' this', ' it', ' I'])\n",
      "(tensor([0.3128, 0.2480, 0.1214, 0.0686, 0.0362], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' was', ' one', ' sequel'])\n",
      "(tensor([0.4268, 0.3418, 0.0403, 0.0230, 0.0128], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' one', ' more'])\n",
      "(tensor([0.1510, 0.1449, 0.1447, 0.1412, 0.0899], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' second', ' only', ' first'])\n",
      "(tensor([0.5086, 0.3745, 0.0362, 0.0108, 0.0052], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' film', ' thing', ' adaptation'])\n",
      "(tensor([0.6323, 0.1274, 0.0781, 0.0540, 0.0236], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' i', ' of'])\n",
      "(tensor([0.5947, 0.3287, 0.0433, 0.0056, 0.0042], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' ever', ' had', ' can'])\n",
      "(tensor([0.8640, 0.1020, 0.0134, 0.0038, 0.0019], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' made', ' had', ' done'])\n",
      "(tensor([0.6099, 0.2209, 0.0316, 0.0296, 0.0130], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' had', ' been', ' directed'])\n",
      "(tensor([0.6934, 0.1047, 0.0928, 0.0188, 0.0139], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', '.\"'])\n",
      "(tensor([0.3130, 0.1364, 0.0857, 0.0466, 0.0278], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.3395, 0.2420, 0.1632, 0.0366, 0.0355], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' was', ' has', ' looks'])\n",
      "(tensor([0.2529, 0.1174, 0.0667, 0.0572, 0.0407], grad_fn=<ToCopyBackward0>), [' so', ' a', ' like', ' just', ' not'])\n",
      "(tensor([0.6668, 0.0561, 0.0367, 0.0283, 0.0191], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ' awful'])\n",
      "(tensor([0.4396, 0.2744, 0.0702, 0.0679, 0.0370], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.3616, 0.1443, 0.0820, 0.0314, 0.0199], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.0904, 0.0818, 0.0710, 0.0645, 0.0529], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' have', ' think', ' don'])\n",
      "(tensor([0.2221, 0.1886, 0.0917, 0.0825, 0.0534], grad_fn=<ToCopyBackward0>), [' never', ' to', ' made', ' a', ' been'])\n",
      "(tensor([0.2206, 0.2115, 0.1865, 0.1017, 0.0238], grad_fn=<ToCopyBackward0>), [' made', ' seen', ' been', ' had', ' done'])\n",
      "(tensor([0.7697, 0.1061, 0.0415, 0.0249, 0.0144], grad_fn=<ToCopyBackward0>), [' a', ' such', ' anything', ' any', ' something'])\n",
      "(tensor([0.5229, 0.3233, 0.0519, 0.0273, 0.0187], grad_fn=<ToCopyBackward0>), [' worse', ' movie', ' bad', ' film', ' sequel'])\n",
      "(tensor([0.2929, 0.1259, 0.1068, 0.0923, 0.0921], grad_fn=<ToCopyBackward0>), [' so', ' that', ' as', ' this', ' worse'])\n",
      "(tensor([0.8210, 0.0628, 0.0261, 0.0167, 0.0074], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ' awful'])\n",
      "(tensor([0.2887, 0.2695, 0.2077, 0.0535, 0.0425], grad_fn=<ToCopyBackward0>), [' in', ' that', '.', ',', ' before'])\n",
      "(tensor([0.2705, 0.1269, 0.1176, 0.1175, 0.0690], grad_fn=<ToCopyBackward0>), [' so', ' I', ' but', ' and', ' it'])\n",
      "(tensor([0.7058, 0.0641, 0.0370, 0.0220, 0.0156], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' so'])\n",
      "(tensor([0.3839, 0.0635, 0.0455, 0.0397, 0.0382], grad_fn=<ToCopyBackward0>), [' have', ' am', ' don', \"'m\", ' was'])\n",
      "(tensor([0.4928, 0.0837, 0.0686, 0.0607, 0.0305], grad_fn=<ToCopyBackward0>), [' made', ' been', ' done', ' never', ' to'])\n",
      "(tensor([0.3810, 0.1814, 0.0492, 0.0489, 0.0289], grad_fn=<ToCopyBackward0>), [' some', ' a', ' many', ' quite', ' very'])\n",
      "(tensor([0.2134, 0.1998, 0.1306, 0.1221, 0.0835], grad_fn=<ToCopyBackward0>), [' really', ' movies', ' very', ' pretty', ' bad'])\n",
      "(tensor([0.5052, 0.1987, 0.1412, 0.0586, 0.0247], grad_fn=<ToCopyBackward0>), [' bad', ' good', ',', ' great', ' funny'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a waste of time. I'm a huge Van Damme fan and even he couldn't save this one. The movie starts off strong with Van Damme in the lead, then he is pushed aside to the side and then completely humiliated\n",
      "(tensor([0.3849, 0.1711, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2427, 0.1962, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6513, 0.0599, 0.0362, 0.0355, 0.0264], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1367, 0.0702, 0.0658, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0856, 0.0612, 0.0531, 0.0423, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.9688, 0.0080, 0.0021, 0.0019, 0.0014], grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' because'])\n",
      "(tensor([0.5771, 0.0885, 0.0884, 0.0430, 0.0164], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' 90', ' 2'])\n",
      "(tensor([0.4436, 0.1747, 0.1162, 0.0264, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' because', '...'])\n",
      "(tensor([0.2371, 0.1609, 0.1432, 0.0242, 0.0222], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1043, 0.0796, 0.0624, 0.0461, 0.0442], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' thought', ' have'])\n",
      "(tensor([0.2223, 0.1792, 0.0641, 0.0403, 0.0353], grad_fn=<ToCopyBackward0>), [' not', ' a', ' sure', ' glad', ' so'])\n",
      "(tensor([0.4407, 0.1469, 0.1261, 0.0300, 0.0159], grad_fn=<ToCopyBackward0>), [' big', ' fan', ' huge', ' movie', ' Christian'])\n",
      "(tensor([0.1570, 0.1296, 0.0907, 0.0292, 0.0267], grad_fn=<ToCopyBackward0>), [' fan', ' Van', ' Adam', ' Steven', ' Michael'])\n",
      "(tensor([0.8791, 0.0182, 0.0163, 0.0076, 0.0059], grad_fn=<ToCopyBackward0>), [' Dam', 'esa', 'ishing', ' Hels', ' Wild'])\n",
      "(tensor([9.9312e-01, 1.4758e-03, 1.1263e-03, 5.9191e-04, 2.9165e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['me', 'm', 'en', 'mer', 'ne'])\n",
      "(tensor([0.9635, 0.0051, 0.0049, 0.0037, 0.0029], grad_fn=<ToCopyBackward0>), [' fan', ' and', ' comple', '/', ','])\n",
      "(tensor([0.5611, 0.1840, 0.0973, 0.0522, 0.0290], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' but', ' ('])\n",
      "(tensor([0.1291, 0.1183, 0.0906, 0.0446, 0.0422], grad_fn=<ToCopyBackward0>), [' this', ' I', ' even', ' thought', ' was'])\n",
      "(tensor([0.3000, 0.1435, 0.0825, 0.0494, 0.0304], grad_fn=<ToCopyBackward0>), [' though', ' he', ' thought', ' I', ' his'])\n",
      "(tensor([0.1126, 0.0859, 0.0829, 0.0780, 0.0532], grad_fn=<ToCopyBackward0>), [' doesn', ' couldn', ' can', ' has', \"'s\"])\n",
      "(tensor([9.9741e-01, 6.3031e-04, 2.8224e-04, 1.6152e-04, 1.1787e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.7359, 0.0562, 0.0193, 0.0130, 0.0126], grad_fn=<ToCopyBackward0>), [' save', ' make', ' do', ' have', ' keep'])\n",
      "(tensor([0.9038, 0.0454, 0.0237, 0.0083, 0.0028], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' Van', ' a'])\n",
      "(tensor([0.4861, 0.1648, 0.0789, 0.0250, 0.0197], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' film', '.', ' fl'])\n",
      "(tensor([0.8752, 0.0399, 0.0212, 0.0139, 0.0044], grad_fn=<ToCopyBackward0>), ['.', ' from', '!', ',', '...'])\n",
      "(tensor([0.2111, 0.1676, 0.0701, 0.0278, 0.0267], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' Van', ' This'])\n",
      "(tensor([0.1540, 0.1469, 0.0959, 0.0866, 0.0742], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' acting', ' story', ' plot'])\n",
      "(tensor([0.1671, 0.1291, 0.0529, 0.0469, 0.0441], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' starts', ' just'])\n",
      "(tensor([0.4420, 0.3949, 0.0328, 0.0217, 0.0150], grad_fn=<ToCopyBackward0>), [' out', ' off', ' with', ' of', ' well'])\n",
      "(tensor([0.1030, 0.0988, 0.0812, 0.0733, 0.0632], grad_fn=<ToCopyBackward0>), [' with', ' great', ' looking', ' strong', ' well'])\n",
      "(tensor([0.2469, 0.2279, 0.2169, 0.1193, 0.0316], grad_fn=<ToCopyBackward0>), [' and', ' but', ' with', ',', ' enough'])\n",
      "(tensor([0.5522, 0.1182, 0.0880, 0.0454, 0.0158], grad_fn=<ToCopyBackward0>), [' Van', ' a', ' the', ' some', ' an'])\n",
      "(tensor([9.9838e-01, 5.4855e-04, 2.7383e-04, 1.6647e-04, 4.9396e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' Dam', 'Dam', ' dam', ' D', ' being'])\n",
      "(tensor([9.9676e-01, 1.8935e-03, 3.2634e-04, 1.9066e-04, 1.7789e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['me', 'mes', 'm', 'men', 'mel'])\n",
      "(tensor([0.2943, 0.0805, 0.0796, 0.0771, 0.0376], grad_fn=<ToCopyBackward0>), [' as', ' and', \"'s\", ' in', ' getting'])\n",
      "(tensor([0.5814, 0.1493, 0.0771, 0.0317, 0.0313], grad_fn=<ToCopyBackward0>), [' a', ' the', ' his', ' some', ' an'])\n",
      "(tensor([0.6442, 0.0202, 0.0168, 0.0146, 0.0128], grad_fn=<ToCopyBackward0>), [' lead', ' role', ' beginning', ' opening', ' main'])\n",
      "(tensor([0.2294, 0.1778, 0.1495, 0.1372, 0.1083], grad_fn=<ToCopyBackward0>), [' and', ' role', ',', ' as', ' but'])\n",
      "(tensor([0.6715, 0.0512, 0.0197, 0.0121, 0.0084], grad_fn=<ToCopyBackward0>), [' but', ' and', ' then', ' fighting', ' he'])\n",
      "(tensor([0.2413, 0.2224, 0.0622, 0.0350, 0.0324], grad_fn=<ToCopyBackward0>), [' it', ' the', ' he', ' goes', ' falls'])\n",
      "(tensor([0.1958, 0.1125, 0.1086, 0.0694, 0.0543], grad_fn=<ToCopyBackward0>), [\"'s\", ' falls', ' gets', ' goes', ' is'])\n",
      "(tensor([0.1841, 0.0314, 0.0291, 0.0279, 0.0214], grad_fn=<ToCopyBackward0>), [' completely', ' paired', ' surrounded', ' reduced', ' pushed'])\n",
      "(tensor([0.3497, 0.1507, 0.0770, 0.0717, 0.0703], grad_fn=<ToCopyBackward0>), [' to', ' down', ' aside', ' out', ' around'])\n",
      "(tensor([0.4782, 0.1303, 0.0896, 0.0676, 0.0670], grad_fn=<ToCopyBackward0>), [' by', ' and', ' to', ' for', ' in'])\n",
      "(tensor([0.4630, 0.1102, 0.0423, 0.0415, 0.0202], grad_fn=<ToCopyBackward0>), [' be', ' the', ' a', ' make', ' \"'])\n",
      "(tensor([0.2385, 0.0809, 0.0357, 0.0300, 0.0284], grad_fn=<ToCopyBackward0>), [' side', ' point', ' second', ' bad', ' background'])\n",
      "(tensor([0.2069, 0.2046, 0.1492, 0.0813, 0.0599], grad_fn=<ToCopyBackward0>), [' and', ' by', ' for', ' to', ' of'])\n",
      "(tensor([0.1939, 0.0293, 0.0264, 0.0251, 0.0224], grad_fn=<ToCopyBackward0>), [' the', ' forgotten', ' played', ' his', ' then'])\n",
      "(tensor([0.0629, 0.0472, 0.0413, 0.0406, 0.0324], grad_fn=<ToCopyBackward0>), [' the', ' replaced', ' he', ' completely', ' pushed'])\n",
      "(tensor([0.2869, 0.0512, 0.0368, 0.0348, 0.0241], grad_fn=<ToCopyBackward0>), [' forgotten', ' dis', ' wasted', ' ignored', ' humiliated'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty funny watching this movie as a kid. I mean, this movie is just dumb! It's just stupid! I mean the acting is bad, and they have a lot of dumb plot devices, and I mean, I'm not even\n",
      "(tensor([0.3842, 0.1715, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1165, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1455, 0.0509, 0.0455, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1141, 0.0954, 0.0749, 0.0610, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.2840, 0.0858, 0.0657, 0.0654, 0.0539], grad_fn=<ToCopyBackward0>), [' when', ' how', ' to', ' watching', ' and'])\n",
      "(tensor([0.6909, 0.0933, 0.0134, 0.0065, 0.0063], grad_fn=<ToCopyBackward0>), [' this', ' the', ' all', ' Will', ' it'])\n",
      "(tensor([0.3628, 0.0838, 0.0396, 0.0350, 0.0298], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ' show', ' on'])\n",
      "(tensor([0.1946, 0.1577, 0.1327, 0.0677, 0.0429], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' at', ' as'])\n",
      "(tensor([0.9074, 0.0293, 0.0223, 0.0135, 0.0107], grad_fn=<ToCopyBackward0>), [' a', ' an', ' I', ' the', ' it'])\n",
      "(tensor([0.1993, 0.0644, 0.0515, 0.0370, 0.0364], grad_fn=<ToCopyBackward0>), [' kid', ' teenager', ' child', ' young', ' 10'])\n",
      "(tensor([0.5015, 0.2007, 0.0640, 0.0339, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' because'])\n",
      "(tensor([0.4078, 0.1069, 0.0566, 0.0169, 0.0122], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1274, 0.0668, 0.0574, 0.0565, 0.0464], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' remember', ' mean'])\n",
      "(tensor([0.4846, 0.0887, 0.0563, 0.0390, 0.0256], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' this'])\n",
      "(tensor([0.1836, 0.1293, 0.1124, 0.0647, 0.0294], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' \"'])\n",
      "(tensor([0.3570, 0.2606, 0.2367, 0.0140, 0.0120], grad_fn=<ToCopyBackward0>), [' is', ' was', ' movie', ' thing', ' kid'])\n",
      "(tensor([0.4084, 0.1416, 0.0566, 0.0409, 0.0300], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' tries'])\n",
      "(tensor([0.1817, 0.1392, 0.1225, 0.0767, 0.0582], grad_fn=<ToCopyBackward0>), [' so', ' about', ' not', ' a', ' just'])\n",
      "(tensor([0.1707, 0.0727, 0.0659, 0.0496, 0.0370], grad_fn=<ToCopyBackward0>), [' stupid', ' a', ' so', ' dumb', ' bad'])\n",
      "(tensor([0.2506, 0.1307, 0.1062, 0.0866, 0.0612], grad_fn=<ToCopyBackward0>), ['.', ' as', '!', ' fun', ' and'])\n",
      "(tensor([0.1494, 0.0937, 0.0804, 0.0346, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.6488, 0.0582, 0.0462, 0.0326, 0.0244], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' makes'])\n",
      "(tensor([0.1507, 0.1406, 0.1271, 0.1271, 0.0661], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' just', ' not', ' like'])\n",
      "(tensor([0.4697, 0.2388, 0.0503, 0.0247, 0.0191], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' a', ' silly', ' so'])\n",
      "(tensor([0.4760, 0.1400, 0.1320, 0.0900, 0.0217], grad_fn=<ToCopyBackward0>), ['!', ' and', '.', ',', ' fun'])\n",
      "(tensor([0.1958, 0.1251, 0.0732, 0.0574, 0.0248], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.2100, 0.0776, 0.0672, 0.0625, 0.0528], grad_fn=<ToCopyBackward0>), [' mean', \"'m\", ' was', ' don', ' can'])\n",
      "(tensor([0.7660, 0.0382, 0.0249, 0.0180, 0.0136], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' this', '...'])\n",
      "(tensor([0.1769, 0.0651, 0.0628, 0.0615, 0.0482], grad_fn=<ToCopyBackward0>), [' acting', ' whole', ' plot', ' guy', ' idea'])\n",
      "(tensor([0.4941, 0.1866, 0.1267, 0.0215, 0.0179], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', \"'s\", ' sucks'])\n",
      "(tensor([0.2234, 0.1223, 0.1201, 0.0643, 0.0511], grad_fn=<ToCopyBackward0>), [' dumb', ' stupid', ' bad', ' terrible', ' so'])\n",
      "(tensor([0.3987, 0.1527, 0.1515, 0.1350, 0.0279], grad_fn=<ToCopyBackward0>), [',', '!', '.', ' and', '...'])\n",
      "(tensor([0.6217, 0.0987, 0.0639, 0.0603, 0.0316], grad_fn=<ToCopyBackward0>), [' the', ' but', ' it', ' and', ' there'])\n",
      "(tensor([0.5914, 0.1090, 0.0673, 0.0384, 0.0283], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' there', ' they'])\n",
      "(tensor([0.1079, 0.0733, 0.0723, 0.0697, 0.0635], grad_fn=<ToCopyBackward0>), [\"'re\", ' try', ' should', ' have', ' show'])\n",
      "(tensor([0.1358, 0.1085, 0.0947, 0.0852, 0.0699], grad_fn=<ToCopyBackward0>), [' to', ' a', ' stupid', ' no', ' the'])\n",
      "(tensor([0.3643, 0.1136, 0.0655, 0.0535, 0.0309], grad_fn=<ToCopyBackward0>), [' stupid', ' really', ' dumb', ' lot', ' guy'])\n",
      "(tensor([9.8482e-01, 7.0771e-03, 3.0467e-03, 4.4704e-04, 3.9404e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' to', ' more', ' wrong', ' less'])\n",
      "(tensor([0.0668, 0.0526, 0.0299, 0.0288, 0.0268], grad_fn=<ToCopyBackward0>), [' dumb', ' people', ' nudity', ' stupid', ' lame'])\n",
      "(tensor([0.2878, 0.0640, 0.0545, 0.0412, 0.0327], grad_fn=<ToCopyBackward0>), [' plot', ' music', ' lines', ' dialogue', ' special'])\n",
      "(tensor([0.2368, 0.1089, 0.1059, 0.0708, 0.0660], grad_fn=<ToCopyBackward0>), [' points', ' devices', ' twists', ' lines', ' holes'])\n",
      "(tensor([0.2810, 0.2501, 0.1293, 0.0542, 0.0516], grad_fn=<ToCopyBackward0>), [',', '.', '!', ' and', ' in'])\n",
      "(tensor([0.4580, 0.3614, 0.0487, 0.0099, 0.0090], grad_fn=<ToCopyBackward0>), [' but', ' and', ' like', ' the', ' so'])\n",
      "(tensor([0.2374, 0.2096, 0.1277, 0.0482, 0.0416], grad_fn=<ToCopyBackward0>), [' the', ' it', ' they', ' I', ' there'])\n",
      "(tensor([0.2114, 0.1007, 0.0908, 0.0825, 0.0712], grad_fn=<ToCopyBackward0>), [' just', ' don', ' think', \"'m\", ' mean'])\n",
      "(tensor([0.2854, 0.1054, 0.0775, 0.0465, 0.0368], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' really', ' this'])\n",
      "(tensor([0.1764, 0.1219, 0.1072, 0.0858, 0.0539], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' what'])\n",
      "(tensor([0.1313, 0.0989, 0.0654, 0.0520, 0.0440], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' think', ' can', ' know'])\n",
      "(tensor([0.3431, 0.2119, 0.0949, 0.0514, 0.0266], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' still'])\n",
      "(tensor([0.3880, 0.1091, 0.1037, 0.0560, 0.0342], grad_fn=<ToCopyBackward0>), [' even', ' a', ' going', ' one', ' gonna'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I was going to be in for another long night watching this. This one was a bit better than my first. I really thought I could do better. I was really looking forward to watching this. I really enjoyed it. I think I could\n",
      "(tensor([0.3839, 0.1717, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3312, 0.2363, 0.0584, 0.0561, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2146, 0.1253, 0.1123, 0.0826, 0.0740], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.4719, 0.1360, 0.0312, 0.0289, 0.0225], grad_fn=<ToCopyBackward0>), [' going', ' in', ' so', ' watching', ' a'])\n",
      "(tensor([9.8503e-01, 2.6783e-03, 1.9229e-03, 1.1664e-03, 9.1092e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' through', ' in', ' into'])\n",
      "(tensor([0.2914, 0.2073, 0.1065, 0.0924, 0.0713], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' get', ' see', ' have'])\n",
      "(tensor([0.0819, 0.0704, 0.0492, 0.0424, 0.0414], grad_fn=<ToCopyBackward0>), [' a', ' in', ' very', ' the', ' able'])\n",
      "(tensor([0.2841, 0.2381, 0.0847, 0.0720, 0.0594], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' this', ' trouble'])\n",
      "(tensor([0.7160, 0.0795, 0.0248, 0.0208, 0.0189], grad_fn=<ToCopyBackward0>), [' a', ' an', ' another', ' some', ' the'])\n",
      "(tensor([0.1017, 0.0496, 0.0488, 0.0414, 0.0312], grad_fn=<ToCopyBackward0>), [' movie', ' long', ' torture', ' one', ' disappointment'])\n",
      "(tensor([0.1668, 0.0759, 0.0285, 0.0276, 0.0201], grad_fn=<ToCopyBackward0>), [' night', ' day', ' 90', ' summer', ' movie'])\n",
      "(tensor([0.2476, 0.2436, 0.1166, 0.0399, 0.0343], grad_fn=<ToCopyBackward0>), [' watching', ' of', '.', ' when', ','])\n",
      "(tensor([0.8653, 0.0365, 0.0196, 0.0171, 0.0051], grad_fn=<ToCopyBackward0>), [' this', ' the', ' paint', ' it', ' a'])\n",
      "(tensor([0.3632, 0.1246, 0.1207, 0.0573, 0.0257], grad_fn=<ToCopyBackward0>), [' movie', ' film', '.', ' one', ' thing'])\n",
      "(tensor([0.2964, 0.1203, 0.0892, 0.0251, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' But'])\n",
      "(tensor([0.3603, 0.2100, 0.1762, 0.0587, 0.0350], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4552, 0.0906, 0.0387, 0.0340, 0.0233], grad_fn=<ToCopyBackward0>), [' was', ' is', ' really', ' had', ' however'])\n",
      "(tensor([0.0941, 0.0818, 0.0584, 0.0493, 0.0482], grad_fn=<ToCopyBackward0>), [' really', ' a', ' just', ' supposed', ' so'])\n",
      "(tensor([0.3201, 0.2044, 0.0695, 0.0321, 0.0225], grad_fn=<ToCopyBackward0>), [' little', ' real', ' bit', ' complete', ' lot'])\n",
      "(tensor([0.3693, 0.1108, 0.1107, 0.0863, 0.0532], grad_fn=<ToCopyBackward0>), [' more', ' longer', ' better', ' of', ' easier'])\n",
      "(tensor([0.4116, 0.3957, 0.0363, 0.0330, 0.0329], grad_fn=<ToCopyBackward0>), [' than', '.', ',', ' but', ' as'])\n",
      "(tensor([0.7408, 0.0500, 0.0407, 0.0361, 0.0203], grad_fn=<ToCopyBackward0>), [' the', ' I', ' most', ' my', ' any'])\n",
      "(tensor([0.3249, 0.1309, 0.0869, 0.0640, 0.0596], grad_fn=<ToCopyBackward0>), [' other', ' first', ' others', ' previous', ' last'])\n",
      "(tensor([0.2931, 0.0987, 0.0769, 0.0565, 0.0475], grad_fn=<ToCopyBackward0>), [' one', ' attempt', '.', ' viewing', ' review'])\n",
      "(tensor([0.3204, 0.2109, 0.0859, 0.0342, 0.0242], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'I'])\n",
      "(tensor([0.1566, 0.0809, 0.0585, 0.0490, 0.0424], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' actually'])\n",
      "(tensor([0.1238, 0.0898, 0.0679, 0.0664, 0.0445], grad_fn=<ToCopyBackward0>), [' enjoyed', ' thought', ' like', ' wanted', ' wish'])\n",
      "(tensor([0.3727, 0.2255, 0.1603, 0.0690, 0.0549], grad_fn=<ToCopyBackward0>), [' I', ' that', ' this', ' it', ' the'])\n",
      "(tensor([0.6601, 0.1122, 0.0768, 0.0467, 0.0349], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', \"'d\", ' could'])\n",
      "(tensor([0.1813, 0.1228, 0.1100, 0.0772, 0.0555], grad_fn=<ToCopyBackward0>), [' see', ' get', ' make', ' have', ' do'])\n",
      "(tensor([0.5793, 0.1230, 0.0629, 0.0314, 0.0281], grad_fn=<ToCopyBackward0>), [' better', ' a', ' it', ' something', ' some'])\n",
      "(tensor([0.5308, 0.2356, 0.0633, 0.0282, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' but', ' with'])\n",
      "(tensor([0.4411, 0.0838, 0.0567, 0.0501, 0.0288], grad_fn=<ToCopyBackward0>), [' I', ' The', 'I', ' It', ' But'])\n",
      "(tensor([0.1570, 0.1108, 0.0648, 0.0491, 0.0421], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' actually'])\n",
      "(tensor([0.2396, 0.2027, 0.0440, 0.0336, 0.0303], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' so', ' very', ' actually'])\n",
      "(tensor([0.4100, 0.0900, 0.0620, 0.0298, 0.0265], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' hoping', ' not', ' in'])\n",
      "(tensor([9.9693e-01, 1.1204e-03, 6.0906e-04, 5.6166e-04, 1.9581e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' up'])\n",
      "(tensor([9.9050e-01, 2.0298e-03, 1.0432e-03, 1.0158e-03, 8.7772e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', '.', ' too'])\n",
      "(tensor([0.2583, 0.2130, 0.1626, 0.0915, 0.0683], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' watching', ' seeing'])\n",
      "(tensor([0.4106, 0.3194, 0.1426, 0.0179, 0.0099], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3424, 0.1793, 0.1338, 0.0354, 0.0190], grad_fn=<ToCopyBackward0>), [' one', ' because', '.', ' movie', ','])\n",
      "(tensor([0.3733, 0.1032, 0.0711, 0.0437, 0.0400], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', 'I'])\n",
      "(tensor([0.2051, 0.1702, 0.0982, 0.0401, 0.0298], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' had', ' watched'])\n",
      "(tensor([0.1634, 0.1624, 0.1446, 0.0594, 0.0467], grad_fn=<ToCopyBackward0>), [' thought', ' wanted', ' was', ' enjoyed', ' did'])\n",
      "(tensor([0.3752, 0.2667, 0.1254, 0.1152, 0.0129], grad_fn=<ToCopyBackward0>), [' it', ' watching', ' the', ' this', ' seeing'])\n",
      "(tensor([0.4683, 0.1229, 0.1090, 0.0461, 0.0346], grad_fn=<ToCopyBackward0>), ['.', ',', ' though', ' and', ' a'])\n",
      "(tensor([0.4252, 0.1753, 0.0715, 0.0341, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2129, 0.1933, 0.0928, 0.0475, 0.0276], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' think', ' just'])\n",
      "(tensor([0.3794, 0.1359, 0.1093, 0.0998, 0.0962], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' this'])\n",
      "(tensor([0.1485, 0.0705, 0.0698, 0.0477, 0.0471], grad_fn=<ToCopyBackward0>), [' was', ' enjoyed', ' could', ' did', ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a good movie, and I thought that it was a very interesting story, but I don't think I can really give it a 10. Maybe I'm being too generous. I don't really have anything to say about it.\n",
      "(tensor([0.3840, 0.1718, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3317, 0.2366, 0.0581, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4144, 0.1967, 0.1693, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6391, 0.0586, 0.0580, 0.0558, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1006, 0.0720, 0.0702, 0.0442, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1287, 0.0778, 0.0562, 0.0513, 0.0483], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2105, 0.1407, 0.0736, 0.0603, 0.0574], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' example', ' way'])\n",
      "(tensor([0.3457, 0.0968, 0.0940, 0.0760, 0.0391], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', ' in'])\n",
      "(tensor([0.5755, 0.0391, 0.0249, 0.0234, 0.0174], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' so', ' a'])\n",
      "(tensor([0.4274, 0.1462, 0.1058, 0.0434, 0.0252], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' the', ' then'])\n",
      "(tensor([0.1724, 0.1127, 0.1071, 0.0390, 0.0338], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' think'])\n",
      "(tensor([0.4767, 0.2742, 0.1140, 0.0340, 0.0116], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.3824, 0.2129, 0.1400, 0.0268, 0.0161], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.4560, 0.0723, 0.0722, 0.0527, 0.0352], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' had', ' represented'])\n",
      "(tensor([0.2337, 0.0953, 0.0600, 0.0443, 0.0380], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' interesting', ' well', ' very'])\n",
      "(tensor([0.3720, 0.0929, 0.0516, 0.0356, 0.0343], grad_fn=<ToCopyBackward0>), [' good', ' very', ' really', ' great', ' bad'])\n",
      "(tensor([0.1512, 0.1131, 0.0833, 0.0788, 0.0692], grad_fn=<ToCopyBackward0>), [' good', ' well', ' interesting', ' funny', ' boring'])\n",
      "(tensor([0.4249, 0.3415, 0.0711, 0.0192, 0.0143], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' story', ' concept', ' and'])\n",
      "(tensor([0.3245, 0.1503, 0.1026, 0.0965, 0.0606], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' with'])\n",
      "(tensor([0.5579, 0.2443, 0.0213, 0.0117, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' very', ' which'])\n",
      "(tensor([0.1797, 0.1109, 0.1043, 0.0536, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' when'])\n",
      "(tensor([0.1349, 0.1165, 0.0710, 0.0661, 0.0594], grad_fn=<ToCopyBackward0>), [' just', ' was', ' really', ' didn', ' don'])\n",
      "(tensor([9.9280e-01, 3.1983e-03, 1.0797e-03, 6.7266e-04, 2.4050e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.4325, 0.1628, 0.1289, 0.0505, 0.0265], grad_fn=<ToCopyBackward0>), [' think', ' really', ' know', ' like', ' necessarily'])\n",
      "(tensor([0.5464, 0.2242, 0.0618, 0.0497, 0.0252], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ' this', ' the'])\n",
      "(tensor([0.0945, 0.0926, 0.0706, 0.0695, 0.0645], grad_fn=<ToCopyBackward0>), [' would', ' can', \"'m\", ' was', ' could'])\n",
      "(tensor([0.2893, 0.2565, 0.0547, 0.0458, 0.0239], grad_fn=<ToCopyBackward0>), [' recommend', ' really', ' give', ' say', ' ever'])\n",
      "(tensor([0.2059, 0.1315, 0.0614, 0.0427, 0.0422], grad_fn=<ToCopyBackward0>), [' recommend', ' say', ' make', ' give', ' comment'])\n",
      "(tensor([0.5849, 0.1269, 0.0796, 0.0340, 0.0265], grad_fn=<ToCopyBackward0>), [' it', ' a', ' away', ' any', ' an'])\n",
      "(tensor([0.4601, 0.1569, 0.0742, 0.0482, 0.0398], grad_fn=<ToCopyBackward0>), [' a', ' more', ' away', ' any', ' an'])\n",
      "(tensor([0.1635, 0.0740, 0.0583, 0.0328, 0.0312], grad_fn=<ToCopyBackward0>), [' rating', ' vote', ' review', ' good', ' 10'])\n",
      "(tensor([0.5933, 0.0927, 0.0685, 0.0545, 0.0297], grad_fn=<ToCopyBackward0>), ['.', ' because', '/', ' out', ','])\n",
      "(tensor([0.2966, 0.1321, 0.0449, 0.0316, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Maybe', ' This'])\n",
      "(tensor([0.3134, 0.1787, 0.1322, 0.0537, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' I', ' it', ' if', ' that'])\n",
      "(tensor([0.2046, 0.1946, 0.0905, 0.0780, 0.0584], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' just', ' was', \"'ll\"])\n",
      "(tensor([0.1901, 0.1221, 0.1199, 0.0790, 0.0629], grad_fn=<ToCopyBackward0>), [' being', ' not', ' wrong', ' too', ' a'])\n",
      "(tensor([0.7129, 0.0887, 0.0222, 0.0210, 0.0115], grad_fn=<ToCopyBackward0>), [' too', ' a', ' hard', ' very', ' overly'])\n",
      "(tensor([0.2683, 0.2041, 0.0916, 0.0498, 0.0414], grad_fn=<ToCopyBackward0>), [' hard', ' generous', ' kind', ' nice', ' harsh'])\n",
      "(tensor([0.4992, 0.1458, 0.0837, 0.0545, 0.0372], grad_fn=<ToCopyBackward0>), ['.', ',', ' here', ' in', ' to'])\n",
      "(tensor([0.3109, 0.0964, 0.0846, 0.0667, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' It', ' Maybe', ' But', ' The'])\n",
      "(tensor([0.1263, 0.0772, 0.0733, 0.0683, 0.0618], grad_fn=<ToCopyBackward0>), [' can', \"'m\", ' just', ' really', ' don'])\n",
      "(tensor([9.9769e-01, 3.8526e-04, 3.1883e-04, 2.0237e-04, 1.1644e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3586, 0.2709, 0.2335, 0.0204, 0.0190], grad_fn=<ToCopyBackward0>), [' think', ' really', ' know', ' even', ' like'])\n",
      "(tensor([0.3338, 0.1853, 0.0826, 0.0709, 0.0667], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' have', ' understand'])\n",
      "(tensor([0.4197, 0.2119, 0.0746, 0.0690, 0.0529], grad_fn=<ToCopyBackward0>), [' a', ' an', ' anything', ' much', ' any'])\n",
      "(tensor([0.1974, 0.1671, 0.0613, 0.0580, 0.0540], grad_fn=<ToCopyBackward0>), [' good', ' to', ' more', ' positive', ' else'])\n",
      "(tensor([0.5766, 0.1614, 0.0383, 0.0365, 0.0286], grad_fn=<ToCopyBackward0>), [' say', ' compare', ' add', ' really', ' comment'])\n",
      "(tensor([0.7117, 0.0515, 0.0410, 0.0307, 0.0202], grad_fn=<ToCopyBackward0>), [' about', ' that', ' other', ' beyond', '.'])\n",
      "(tensor([0.7464, 0.1291, 0.0740, 0.0182, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' how'])\n",
      "(tensor([0.6110, 0.1920, 0.0547, 0.0274, 0.0150], grad_fn=<ToCopyBackward0>), ['.', ',', ' other', ' that', ' at'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was funny when this movie came out. I was in grade 11 when this was released. I'm still trying to get over how stupid it is. I can't even believe that people actually liked this. It has a really bad premise, and\n",
      "(tensor([0.3836, 0.1720, 0.0902, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7134, 0.1164, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1837, 0.1462, 0.0507, 0.0452, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.2108, 0.1262, 0.0831, 0.0781, 0.0706], grad_fn=<ToCopyBackward0>), [' when', ' to', ' at', ' how', ' that'])\n",
      "(tensor([0.4422, 0.1595, 0.0853, 0.0691, 0.0330], grad_fn=<ToCopyBackward0>), [' I', ' this', ' i', ' the', ' a'])\n",
      "(tensor([0.3490, 0.0386, 0.0298, 0.0285, 0.0282], grad_fn=<ToCopyBackward0>), [' movie', ' show', ' film', ' guy', ' was'])\n",
      "(tensor([0.2992, 0.2352, 0.1652, 0.0195, 0.0160], grad_fn=<ToCopyBackward0>), [' started', ' was', ' came', ' starts', ' finally'])\n",
      "(tensor([9.9408e-01, 2.5609e-03, 1.1112e-03, 6.1794e-04, 2.8323e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' out', ' on', ' up', ' along', ' about'])\n",
      "(tensor([0.1981, 0.1881, 0.1839, 0.0741, 0.0640], grad_fn=<ToCopyBackward0>), [',', ' in', '.', ' because', ' and'])\n",
      "(tensor([0.5284, 0.0942, 0.0392, 0.0201, 0.0133], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1619, 0.1018, 0.0552, 0.0483, 0.0454], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' mean', ' think'])\n",
      "(tensor([0.1497, 0.0898, 0.0719, 0.0660, 0.0456], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' in', ' so', ' like'])\n",
      "(tensor([0.2988, 0.1873, 0.1219, 0.0338, 0.0219], grad_fn=<ToCopyBackward0>), [' the', ' a', ' college', ' grade', ' it'])\n",
      "(tensor([0.3308, 0.1460, 0.0771, 0.0715, 0.0595], grad_fn=<ToCopyBackward0>), [' school', ' 11', ' 10', ' 6', ' 12'])\n",
      "(tensor([0.2883, 0.2812, 0.0811, 0.0510, 0.0440], grad_fn=<ToCopyBackward0>), [' when', ' at', ',', ' and', '.'])\n",
      "(tensor([0.4337, 0.2991, 0.1615, 0.0271, 0.0189], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' that'])\n",
      "(tensor([0.3290, 0.2646, 0.2396, 0.0306, 0.0125], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' came', ' one', ' thing'])\n",
      "(tensor([0.7543, 0.1049, 0.0122, 0.0111, 0.0107], grad_fn=<ToCopyBackward0>), [' released', ' made', ' in', ' filmed', ' originally'])\n",
      "(tensor([0.4043, 0.2350, 0.1539, 0.0681, 0.0451], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ' so'])\n",
      "(tensor([0.6860, 0.0646, 0.0264, 0.0242, 0.0167], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' We'])\n",
      "(tensor([0.1582, 0.1151, 0.0971, 0.0740, 0.0334], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' remember', \"'m\", ' still'])\n",
      "(tensor([0.2947, 0.1115, 0.0465, 0.0445, 0.0316], grad_fn=<ToCopyBackward0>), [' still', ' not', ' a', ' in', ' now'])\n",
      "(tensor([0.2141, 0.1142, 0.0549, 0.0487, 0.0329], grad_fn=<ToCopyBackward0>), [' in', ' trying', ' friends', ' a', ' not'])\n",
      "(tensor([9.9855e-01, 3.4107e-04, 1.2451e-04, 9.1226e-05, 6.4224e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' not', ' really', ' hard', ','])\n",
      "(tensor([0.2182, 0.1401, 0.0941, 0.0864, 0.0337], grad_fn=<ToCopyBackward0>), [' get', ' figure', ' understand', ' find', ' believe'])\n",
      "(tensor([0.2289, 0.1877, 0.1429, 0.0648, 0.0400], grad_fn=<ToCopyBackward0>), [' the', ' my', ' over', ' all', ' back'])\n",
      "(tensor([0.4120, 0.2923, 0.1346, 0.0566, 0.0103], grad_fn=<ToCopyBackward0>), [' it', ' how', ' the', ' that', ' this'])\n",
      "(tensor([0.3708, 0.1495, 0.0927, 0.0521, 0.0286], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' much', ' terrible', ' awful'])\n",
      "(tensor([0.4413, 0.3102, 0.0580, 0.0399, 0.0283], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' and'])\n",
      "(tensor([0.3376, 0.3214, 0.1581, 0.0425, 0.0396], grad_fn=<ToCopyBackward0>), [' is', ' was', ' really', ' all', ' actually'])\n",
      "(tensor([0.5064, 0.1237, 0.1026, 0.0332, 0.0289], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' in', ' to'])\n",
      "(tensor([0.3182, 0.1141, 0.0972, 0.0224, 0.0170], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1323, 0.0817, 0.0575, 0.0529, 0.0477], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' don', ' think'])\n",
      "(tensor([0.6813, 0.0376, 0.0344, 0.0252, 0.0236], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' tell', ' honestly', ' only'])\n",
      "(tensor([0.6150, 0.1181, 0.0468, 0.0221, 0.0189], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' understand', ' imagine', ' really'])\n",
      "(tensor([0.1524, 0.1203, 0.1133, 0.1107, 0.0935], grad_fn=<ToCopyBackward0>), [' describe', ' believe', ' explain', ' tell', ' begin'])\n",
      "(tensor([0.4916, 0.0952, 0.0678, 0.0458, 0.0452], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' how', ' I'])\n",
      "(tensor([0.1894, 0.0929, 0.0511, 0.0508, 0.0408], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' they', ' people'])\n",
      "(tensor([0.3400, 0.1163, 0.0627, 0.0425, 0.0322], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' think', ' like', ' liked'])\n",
      "(tensor([0.2870, 0.2334, 0.1036, 0.0613, 0.0445], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' made', ' like', ' think'])\n",
      "(tensor([0.6567, 0.3010, 0.0288, 0.0060, 0.0007], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' the', ' to'])\n",
      "(tensor([0.6415, 0.1174, 0.0723, 0.0189, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ',', ' stupid'])\n",
      "(tensor([0.3737, 0.1209, 0.0798, 0.0213, 0.0179], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' They'])\n",
      "(tensor([0.6381, 0.0976, 0.0513, 0.0361, 0.0248], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' has'])\n",
      "(tensor([0.1393, 0.1093, 0.1045, 0.0796, 0.0599], grad_fn=<ToCopyBackward0>), [' no', ' the', ' a', ' to', ' absolutely'])\n",
      "(tensor([0.1284, 0.0601, 0.0395, 0.0307, 0.0286], grad_fn=<ToCopyBackward0>), [' really', ' very', ' lot', ' totally', ' couple'])\n",
      "(tensor([0.2656, 0.0997, 0.0929, 0.0688, 0.0549], grad_fn=<ToCopyBackward0>), [' cheesy', ' dumb', ' bad', ' good', ' stupid'])\n",
      "(tensor([0.1281, 0.0910, 0.0435, 0.0352, 0.0318], grad_fn=<ToCopyBackward0>), [' premise', ' cast', ' storyline', ',', ' plot'])\n",
      "(tensor([0.3737, 0.2980, 0.1291, 0.0253, 0.0174], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' that', ' to'])\n",
      "(tensor([0.3495, 0.0832, 0.0692, 0.0507, 0.0422], grad_fn=<ToCopyBackward0>), [' and', ' but', ' bad', ' the', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was a very boring film. I really thought it was boring because I was watching it and I really didn't care at all what was going on in the film. The acting was terrible in this film and that was really the only reason\n",
      "(tensor([0.3844, 0.1716, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3314, 0.2368, 0.0581, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4150, 0.1963, 0.1691, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6390, 0.0587, 0.0579, 0.0559, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1006, 0.0719, 0.0701, 0.0441, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.1286, 0.0781, 0.0563, 0.0512, 0.0483], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' really', ' pretty'])\n",
      "(tensor([0.1168, 0.0684, 0.0508, 0.0446, 0.0400], grad_fn=<ToCopyBackward0>), [' boring', ' interesting', ' good', ' well', ' bad'])\n",
      "(tensor([0.4320, 0.2730, 0.0544, 0.0509, 0.0299], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' and', ','])\n",
      "(tensor([0.3182, 0.1166, 0.1132, 0.0573, 0.0446], grad_fn=<ToCopyBackward0>), ['.', '...', ',', ' to', '....'])\n",
      "(tensor([0.1968, 0.1640, 0.1569, 0.0369, 0.0226], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' And'])\n",
      "(tensor([0.1052, 0.0841, 0.0811, 0.0524, 0.0498], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' mean', ' don'])\n",
      "(tensor([0.2234, 0.1296, 0.0987, 0.0634, 0.0539], grad_fn=<ToCopyBackward0>), [' didn', ' thought', ' don', ' wanted', ' did'])\n",
      "(tensor([0.5073, 0.2715, 0.1184, 0.0360, 0.0072], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' the', ' I'])\n",
      "(tensor([0.7196, 0.1132, 0.0279, 0.0160, 0.0135], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', ' could', ' sucked'])\n",
      "(tensor([0.2767, 0.2188, 0.0594, 0.0506, 0.0378], grad_fn=<ToCopyBackward0>), [' boring', ' a', '.', ' very', ' one'])\n",
      "(tensor([0.3144, 0.3067, 0.0923, 0.0574, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' because', ' to'])\n",
      "(tensor([0.2295, 0.1974, 0.1699, 0.0860, 0.0721], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' there', ' of'])\n",
      "(tensor([0.0907, 0.0774, 0.0595, 0.0554, 0.0516], grad_fn=<ToCopyBackward0>), [' didn', ' was', ' really', \"'m\", ' thought'])\n",
      "(tensor([0.3151, 0.1650, 0.0855, 0.0466, 0.0309], grad_fn=<ToCopyBackward0>), [' expecting', ' not', ' watching', ' so', ' looking'])\n",
      "(tensor([0.6161, 0.1708, 0.0486, 0.0403, 0.0147], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' all'])\n",
      "(tensor([0.1587, 0.1560, 0.0740, 0.0712, 0.0597], grad_fn=<ToCopyBackward0>), [' in', ' and', ' with', ' at', ' on'])\n",
      "(tensor([0.3306, 0.1161, 0.0912, 0.0554, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' it', ' there', ' thinking', ' the'])\n",
      "(tensor([0.2469, 0.1022, 0.0737, 0.0719, 0.0659], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' just', ' didn', ' really'])\n",
      "(tensor([0.2170, 0.2145, 0.0928, 0.0543, 0.0444], grad_fn=<ToCopyBackward0>), [' thought', ' didn', ' wanted', ' felt', ' was'])\n",
      "(tensor([9.9567e-01, 1.6981e-03, 4.9344e-04, 3.4433e-04, 1.5712e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2640, 0.1526, 0.1125, 0.0850, 0.0792], grad_fn=<ToCopyBackward0>), [' care', ' get', ' enjoy', ' understand', ' like'])\n",
      "(tensor([0.2816, 0.1392, 0.1274, 0.0822, 0.0728], grad_fn=<ToCopyBackward0>), [' about', ' for', ' at', ' what', '.'])\n",
      "(tensor([0.9239, 0.0371, 0.0249, 0.0060, 0.0016], grad_fn=<ToCopyBackward0>), [' all', ' the', ' that', ' any', ' first'])\n",
      "(tensor([0.4562, 0.2482, 0.0546, 0.0405, 0.0336], grad_fn=<ToCopyBackward0>), [' about', '.', ' for', ',', ' what'])\n",
      "(tensor([0.4810, 0.3468, 0.0480, 0.0267, 0.0245], grad_fn=<ToCopyBackward0>), [' happened', ' was', ' the', ' they', ' it'])\n",
      "(tensor([0.5933, 0.1775, 0.1212, 0.0308, 0.0155], grad_fn=<ToCopyBackward0>), [' going', ' happening', ' in', ' on', ' said'])\n",
      "(tensor([9.7073e-01, 2.2805e-02, 2.0222e-03, 1.7954e-03, 4.8726e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' on', ' to', ' through', ' in', ' down'])\n",
      "(tensor([0.3917, 0.3668, 0.0572, 0.0353, 0.0270], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' with', ' and'])\n",
      "(tensor([0.4280, 0.3041, 0.1166, 0.0693, 0.0509], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' there', ' that'])\n",
      "(tensor([0.7509, 0.2103, 0.0099, 0.0028, 0.0023], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' story', ' background', ' script'])\n",
      "(tensor([0.6359, 0.1053, 0.0424, 0.0330, 0.0236], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' at'])\n",
      "(tensor([0.2709, 0.1089, 0.0859, 0.0732, 0.0382], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', 'I'])\n",
      "(tensor([0.2483, 0.1326, 0.0448, 0.0404, 0.0389], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' movie', ' story', ' film'])\n",
      "(tensor([0.7792, 0.0429, 0.0360, 0.0251, 0.0251], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' in', ',', ' and'])\n",
      "(tensor([0.1089, 0.0844, 0.0746, 0.0645, 0.0498], grad_fn=<ToCopyBackward0>), [' terrible', ' not', ' so', ' very', ' bad'])\n",
      "(tensor([0.2965, 0.2625, 0.2549, 0.0338, 0.0169], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', '...'])\n",
      "(tensor([0.6542, 0.1378, 0.1240, 0.0254, 0.0124], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' my'])\n",
      "(tensor([0.8596, 0.1036, 0.0141, 0.0050, 0.0025], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', '.', ','])\n",
      "(tensor([0.5848, 0.1330, 0.1004, 0.0301, 0.0197], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' as'])\n",
      "(tensor([0.3570, 0.3300, 0.0704, 0.0579, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' there'])\n",
      "(tensor([0.3257, 0.2823, 0.2114, 0.0469, 0.0153], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' really', ' just'])\n",
      "(tensor([0.4204, 0.0894, 0.0597, 0.0456, 0.0394], grad_fn=<ToCopyBackward0>), [' the', ' one', ' really', ' a', ' probably'])\n",
      "(tensor([0.2863, 0.0768, 0.0729, 0.0683, 0.0465], grad_fn=<ToCopyBackward0>), [' the', ' sad', ' boring', ' disappointing', ','])\n",
      "(tensor([0.9198, 0.0103, 0.0078, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' only', ' reason', ' main', ' most', ' one'])\n",
      "(tensor([0.4095, 0.2925, 0.0508, 0.0404, 0.0360], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' redeem', ' good', ' part'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it would be interesting to see how the characters acted in real-life. I expected a good film with a good plot and decent acting. What a disappointment. The plot was so bad that even Christopher Lee couldn't save the movie. The acting,\n",
      "(tensor([0.3848, 0.1711, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7130, 0.1166, 0.0399, 0.0102, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.9410, 0.0268, 0.0064, 0.0053, 0.0018], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' not'])\n",
      "(tensor([0.6378, 0.0575, 0.0476, 0.0393, 0.0169], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' an', ' more', ' better'])\n",
      "(tensor([0.7088, 0.1274, 0.0340, 0.0231, 0.0210], grad_fn=<ToCopyBackward0>), [' to', ' for', ' if', ' and', ' as'])\n",
      "(tensor([0.7823, 0.0215, 0.0178, 0.0144, 0.0110], grad_fn=<ToCopyBackward0>), [' see', ' review', ' compare', ' have', ' watch'])\n",
      "(tensor([0.4932, 0.1178, 0.0927, 0.0834, 0.0331], grad_fn=<ToCopyBackward0>), [' how', ' the', ' if', ' what', ' a'])\n",
      "(tensor([0.1363, 0.1168, 0.0928, 0.0736, 0.0683], grad_fn=<ToCopyBackward0>), [' many', ' much', ' other', ' the', ' far'])\n",
      "(tensor([0.1026, 0.0666, 0.0638, 0.0543, 0.0468], grad_fn=<ToCopyBackward0>), [' original', ' actors', ' other', ' characters', ' real'])\n",
      "(tensor([0.2069, 0.1148, 0.0562, 0.0487, 0.0478], grad_fn=<ToCopyBackward0>), [' acted', ' would', ' did', ' performed', ' behaved'])\n",
      "(tensor([0.4564, 0.1126, 0.1025, 0.0504, 0.0348], grad_fn=<ToCopyBackward0>), [' in', ' when', ' around', ' on', ' after'])\n",
      "(tensor([0.2752, 0.2722, 0.1165, 0.0505, 0.0402], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' real', ' their'])\n",
      "(tensor([9.1351e-01, 6.3722e-02, 8.7865e-03, 1.7124e-03, 6.7258e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' life', '-', ' time', ' space', ' LIFE'])\n",
      "(tensor([0.9211, 0.0562, 0.0113, 0.0044, 0.0017], grad_fn=<ToCopyBackward0>), ['life', 'time', 'world', 'space', 'live'])\n",
      "(tensor([0.1918, 0.1889, 0.1528, 0.0424, 0.0354], grad_fn=<ToCopyBackward0>), ['.', ',', ' situations', ' when', ' and'])\n",
      "(tensor([0.4531, 0.0999, 0.0720, 0.0238, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' This'])\n",
      "(tensor([0.0933, 0.0808, 0.0387, 0.0385, 0.0374], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' expected', ' think', ' have'])\n",
      "(tensor([0.3045, 0.1364, 0.1134, 0.0681, 0.0644], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' them', ' it'])\n",
      "(tensor([0.1430, 0.1405, 0.0811, 0.0423, 0.0321], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' lot', ' film', ' \"'])\n",
      "(tensor([0.3148, 0.3126, 0.0751, 0.0403, 0.0239], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ',', ' comedy'])\n",
      "(tensor([0.2563, 0.1593, 0.1385, 0.0989, 0.0487], grad_fn=<ToCopyBackward0>), [',', '.', ' from', ' with', ' but'])\n",
      "(tensor([0.1955, 0.1629, 0.0729, 0.0526, 0.0351], grad_fn=<ToCopyBackward0>), [' good', ' a', ' interesting', ' some', ' funny'])\n",
      "(tensor([0.4952, 0.0374, 0.0349, 0.0322, 0.0265], grad_fn=<ToCopyBackward0>), [' good', ' lot', ' decent', ' strong', ' great'])\n",
      "(tensor([0.4456, 0.1555, 0.1534, 0.0575, 0.0483], grad_fn=<ToCopyBackward0>), [' plot', ' storyline', ' story', ' cast', ' script'])\n",
      "(tensor([0.2527, 0.2362, 0.1762, 0.1348, 0.0316], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', '.', ' with'])\n",
      "(tensor([0.3310, 0.1364, 0.0767, 0.0706, 0.0338], grad_fn=<ToCopyBackward0>), [' good', ' decent', ' interesting', ' a', ' acting'])\n",
      "(tensor([0.9617, 0.0057, 0.0054, 0.0041, 0.0025], grad_fn=<ToCopyBackward0>), [' acting', ' performances', ' actors', ' special', ' casting'])\n",
      "(tensor([0.7550, 0.1232, 0.0304, 0.0161, 0.0071], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' from', ' by'])\n",
      "(tensor([0.2257, 0.1702, 0.0728, 0.0636, 0.0483], grad_fn=<ToCopyBackward0>), [' What', ' I', ' But', ' Instead', ' The'])\n",
      "(tensor([0.9579, 0.0116, 0.0063, 0.0045, 0.0042], grad_fn=<ToCopyBackward0>), [' I', ' i', ' was', ' we', ' a'])\n",
      "(tensor([0.3505, 0.1548, 0.0723, 0.0392, 0.0288], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' let', ' mistake', ' load'])\n",
      "(tensor([0.4447, 0.4046, 0.0594, 0.0264, 0.0189], grad_fn=<ToCopyBackward0>), ['.', '!', '!!', '!!!', '...'])\n",
      "(tensor([0.3229, 0.1714, 0.0539, 0.0534, 0.0433], grad_fn=<ToCopyBackward0>), [' The', ' I', ' This', ' It', 'The'])\n",
      "(tensor([0.2060, 0.1267, 0.1136, 0.0710, 0.0526], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' acting', ' characters', ' actors'])\n",
      "(tensor([0.4452, 0.2771, 0.0585, 0.0137, 0.0122], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' had'])\n",
      "(tensor([0.1075, 0.0534, 0.0439, 0.0438, 0.0367], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' so', ' terrible', ' awful'])\n",
      "(tensor([0.1705, 0.0559, 0.0420, 0.0350, 0.0246], grad_fn=<ToCopyBackward0>), [' weak', ' bad', ' predictable', ' thin', ' stupid'])\n",
      "(tensor([0.4035, 0.1728, 0.1155, 0.1106, 0.0764], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' I', ' it'])\n",
      "(tensor([0.4155, 0.1890, 0.1041, 0.0847, 0.0300], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' even', ' you'])\n",
      "(tensor([0.3676, 0.0452, 0.0440, 0.0378, 0.0346], grad_fn=<ToCopyBackward0>), [' the', ' Christopher', ' if', ' I', ' after'])\n",
      "(tensor([0.5443, 0.1886, 0.1372, 0.0224, 0.0190], grad_fn=<ToCopyBackward0>), [' Lee', ' Walk', ' Pl', ' Guest', ' Lloyd'])\n",
      "(tensor([0.2682, 0.0610, 0.0500, 0.0444, 0.0353], grad_fn=<ToCopyBackward0>), [' couldn', \"'s\", ' was', ',', ' ('])\n",
      "(tensor([9.9525e-01, 1.3625e-03, 4.6694e-04, 2.7642e-04, 1.9858e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.7473, 0.0401, 0.0383, 0.0325, 0.0213], grad_fn=<ToCopyBackward0>), [' save', ' rescue', ' make', ' salvage', ' help'])\n",
      "(tensor([0.7087, 0.1416, 0.1084, 0.0093, 0.0074], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' me', ' his'])\n",
      "(tensor([0.5207, 0.3869, 0.0221, 0.0090, 0.0056], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' day', ' picture', ' poor'])\n",
      "(tensor([0.8102, 0.0390, 0.0378, 0.0320, 0.0133], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', '!', ' for'])\n",
      "(tensor([0.4859, 0.0860, 0.0382, 0.0327, 0.0265], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3470, 0.1147, 0.0725, 0.0435, 0.0389], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' story', ' actors'])\n",
      "(tensor([0.7575, 0.0515, 0.0296, 0.0171, 0.0136], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' is', ',', '?'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a really bad movie. I was in the audience when it was released and I thought it was a really bad movie. It was a big joke in my movie when I was in college, and I was really disappointed. I'm still not\n",
      "(tensor([0.3841, 0.1716, 0.0898, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1167, 0.0397, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1841, 0.1454, 0.0509, 0.0451, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1638, 0.1161, 0.0577, 0.0491, 0.0450], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.6034, 0.0516, 0.0459, 0.0383, 0.0233], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8282, 0.0665, 0.0241, 0.0073, 0.0053], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' horror', ' sequel'])\n",
      "(tensor([0.6668, 0.0630, 0.0363, 0.0345, 0.0291], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '...', '!'])\n",
      "(tensor([0.2885, 0.1358, 0.0640, 0.0330, 0.0296], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.1346, 0.0843, 0.0826, 0.0615, 0.0498], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', ' mean', \"'m\"])\n",
      "(tensor([0.4459, 0.0942, 0.0450, 0.0394, 0.0374], grad_fn=<ToCopyBackward0>), [' really', ' very', ' in', ' so', ' not'])\n",
      "(tensor([0.3283, 0.1709, 0.1366, 0.0280, 0.0240], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' awe', ' college'])\n",
      "(tensor([0.2608, 0.1290, 0.0693, 0.0284, 0.0239], grad_fn=<ToCopyBackward0>), [' theater', ' mood', ' audience', ' middle', ' army'])\n",
      "(tensor([0.3856, 0.1077, 0.0750, 0.0401, 0.0377], grad_fn=<ToCopyBackward0>), [' when', ' for', ' at', ' the', ' with'])\n",
      "(tensor([0.2920, 0.2046, 0.1960, 0.1407, 0.0829], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' they'])\n",
      "(tensor([0.5864, 0.1485, 0.0844, 0.0529, 0.0195], grad_fn=<ToCopyBackward0>), [' was', ' came', ' first', ' opened', ' started'])\n",
      "(tensor([0.3352, 0.0790, 0.0685, 0.0680, 0.0493], grad_fn=<ToCopyBackward0>), [' released', ' first', ' being', ' in', ' originally'])\n",
      "(tensor([0.3157, 0.2484, 0.1656, 0.1316, 0.0148], grad_fn=<ToCopyBackward0>), [' in', ',', ' and', '.', ' at'])\n",
      "(tensor([0.5113, 0.0915, 0.0543, 0.0209, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' that'])\n",
      "(tensor([0.1239, 0.1119, 0.0768, 0.0767, 0.0515], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', ' just', ' actually'])\n",
      "(tensor([0.8235, 0.0543, 0.0377, 0.0276, 0.0111], grad_fn=<ToCopyBackward0>), [' it', ' that', ' the', ' this', ','])\n",
      "(tensor([0.9218, 0.0089, 0.0083, 0.0063, 0.0061], grad_fn=<ToCopyBackward0>), [' was', ' really', ' had', ' wasn', \"'s\"])\n",
      "(tensor([0.4147, 0.0924, 0.0484, 0.0457, 0.0404], grad_fn=<ToCopyBackward0>), [' a', ' terrible', ' one', ' the', ' really'])\n",
      "(tensor([0.6916, 0.0638, 0.0230, 0.0229, 0.0200], grad_fn=<ToCopyBackward0>), [' really', ' bad', ' very', ' pretty', ' terrible'])\n",
      "(tensor([0.7566, 0.0976, 0.0264, 0.0240, 0.0103], grad_fn=<ToCopyBackward0>), [' bad', ',', ' dumb', ' stupid', ' terrible'])\n",
      "(tensor([0.9632, 0.0182, 0.0051, 0.0016, 0.0015], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' idea'])\n",
      "(tensor([0.9032, 0.0278, 0.0203, 0.0056, 0.0050], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', '!'])\n",
      "(tensor([0.3100, 0.1090, 0.0770, 0.0431, 0.0359], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' But'])\n",
      "(tensor([0.3532, 0.2533, 0.0521, 0.0316, 0.0301], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' has', ' had'])\n",
      "(tensor([0.0962, 0.0783, 0.0602, 0.0483, 0.0459], grad_fn=<ToCopyBackward0>), [' a', ' so', ' just', ' not', ' like'])\n",
      "(tensor([0.1778, 0.1114, 0.0599, 0.0409, 0.0269], grad_fn=<ToCopyBackward0>), [' big', ' really', ' bad', ' very', ' total'])\n",
      "(tensor([0.1944, 0.1620, 0.0652, 0.0521, 0.0458], grad_fn=<ToCopyBackward0>), [' bomb', ' disappointment', ',', ' waste', ' joke'])\n",
      "(tensor([0.1680, 0.1636, 0.1239, 0.0712, 0.0626], grad_fn=<ToCopyBackward0>), [' in', ' to', ' for', ' on', '.'])\n",
      "(tensor([0.7267, 0.0395, 0.0362, 0.0252, 0.0167], grad_fn=<ToCopyBackward0>), [' the', ' Hollywood', ' my', ' some', ' a'])\n",
      "(tensor([0.1144, 0.0970, 0.0677, 0.0526, 0.0505], grad_fn=<ToCopyBackward0>), [' family', ' class', ' movie', ' book', ' household'])\n",
      "(tensor([0.4552, 0.0689, 0.0574, 0.0322, 0.0308], grad_fn=<ToCopyBackward0>), [' class', ',', ' when', ' classes', ' theater'])\n",
      "(tensor([0.5687, 0.0842, 0.0813, 0.0430, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' they', ' the', ' it', ' my'])\n",
      "(tensor([0.1583, 0.1456, 0.0682, 0.0426, 0.0313], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' saw', \"'m\", ' played'])\n",
      "(tensor([0.2605, 0.0569, 0.0445, 0.0386, 0.0375], grad_fn=<ToCopyBackward0>), [' in', ' a', ' doing', ' at', ' actually'])\n",
      "(tensor([0.3174, 0.2601, 0.0989, 0.0588, 0.0326], grad_fn=<ToCopyBackward0>), [' the', ' college', ' it', ' high', ' grade'])\n",
      "(tensor([0.4099, 0.2133, 0.1077, 0.0338, 0.0316], grad_fn=<ToCopyBackward0>), [',', '.', ' when', ' that', ' and'])\n",
      "(tensor([0.1685, 0.1088, 0.0972, 0.0742, 0.0626], grad_fn=<ToCopyBackward0>), [' and', ' I', ' so', ' when', ' but'])\n",
      "(tensor([0.6263, 0.0654, 0.0414, 0.0333, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' now', ' that'])\n",
      "(tensor([0.1505, 0.1180, 0.0885, 0.0594, 0.0520], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' really', ' just'])\n",
      "(tensor([0.4155, 0.0651, 0.0585, 0.0541, 0.0478], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' in', ' very', ' like'])\n",
      "(tensor([0.1291, 0.0901, 0.0775, 0.0710, 0.0647], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' into', ' excited', ' embarrassed'])\n",
      "(tensor([0.2220, 0.2016, 0.1973, 0.1845, 0.0677], grad_fn=<ToCopyBackward0>), [' with', ' in', ' when', '.', ' that'])\n",
      "(tensor([0.3925, 0.0950, 0.0699, 0.0447, 0.0409], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' So'])\n",
      "(tensor([0.1227, 0.1137, 0.0759, 0.0662, 0.0464], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', \"'m\", ' think'])\n",
      "(tensor([0.2058, 0.1565, 0.1274, 0.0893, 0.0417], grad_fn=<ToCopyBackward0>), [' not', ' a', ' still', ' really', ' sure'])\n",
      "(tensor([0.1823, 0.1561, 0.1459, 0.0915, 0.0784], grad_fn=<ToCopyBackward0>), [' a', ' really', ' disappointed', ' very', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a joke from the start. The acting wasn't even good enough to make it worth watching. The only thing that kept me watching was the fact that I could hear the voice of the narrator. He's just a guy who says the\n",
      "(tensor([0.3840, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2436, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6521, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0614, 0.0529, 0.0427, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.2055, 0.1292, 0.1210, 0.1075, 0.0586], grad_fn=<ToCopyBackward0>), [' when', ' until', ' from', '.', ' at'])\n",
      "(tensor([0.8153, 0.0273, 0.0101, 0.0083, 0.0076], grad_fn=<ToCopyBackward0>), [' the', ' start', ' begin', ' when', ' beginning'])\n",
      "(tensor([0.2862, 0.2696, 0.1872, 0.0440, 0.0285], grad_fn=<ToCopyBackward0>), [' start', ' get', ' beginning', ' very', ' word'])\n",
      "(tensor([0.7726, 0.0825, 0.0215, 0.0196, 0.0128], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' when', ' and'])\n",
      "(tensor([0.2079, 0.1735, 0.1073, 0.0235, 0.0176], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' What'])\n",
      "(tensor([0.0907, 0.0704, 0.0647, 0.0385, 0.0334], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' movie', ' idea'])\n",
      "(tensor([0.7478, 0.0807, 0.0319, 0.0234, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' wasn', ' and'])\n",
      "(tensor([9.9609e-01, 1.4471e-03, 3.6415e-04, 3.0534e-04, 2.1965e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2387, 0.2063, 0.1014, 0.0950, 0.0476], grad_fn=<ToCopyBackward0>), [' even', ' bad', ' funny', ' good', ' very'])\n",
      "(tensor([0.3532, 0.1252, 0.0770, 0.0525, 0.0350], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' that', ' convincing', ' close'])\n",
      "(tensor([0.4103, 0.1836, 0.1733, 0.0566, 0.0228], grad_fn=<ToCopyBackward0>), ['.', ' enough', ',', ' and', ' in'])\n",
      "(tensor([0.7921, 0.0991, 0.0441, 0.0265, 0.0084], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ',', ' and'])\n",
      "(tensor([0.4125, 0.3332, 0.0262, 0.0180, 0.0173], grad_fn=<ToCopyBackward0>), [' make', ' be', ' keep', ' save', ' get'])\n",
      "(tensor([0.4030, 0.1805, 0.0940, 0.0838, 0.0758], grad_fn=<ToCopyBackward0>), [' it', ' you', ' this', ' a', ' me'])\n",
      "(tensor([0.3653, 0.0736, 0.0653, 0.0449, 0.0275], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' bear', ' worth', ' pass'])\n",
      "(tensor([0.7853, 0.0408, 0.0316, 0.0207, 0.0163], grad_fn=<ToCopyBackward0>), [' watching', ' the', ' sitting', ' seeing', ' it'])\n",
      "(tensor([0.6015, 0.0812, 0.0461, 0.0232, 0.0172], grad_fn=<ToCopyBackward0>), ['.', ' the', ',', ' for', '...'])\n",
      "(tensor([0.3152, 0.1134, 0.0911, 0.0298, 0.0243], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' This'])\n",
      "(tensor([0.2792, 0.0933, 0.0777, 0.0509, 0.0407], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' only', ' movie', ' script'])\n",
      "(tensor([0.2575, 0.1703, 0.1044, 0.0526, 0.0371], grad_fn=<ToCopyBackward0>), [' thing', ' good', ' reason', ' funny', ' actor'])\n",
      "(tensor([0.6621, 0.0825, 0.0427, 0.0235, 0.0212], grad_fn=<ToCopyBackward0>), [' that', ' I', ' this', ' interesting', ' worse'])\n",
      "(tensor([0.3759, 0.1442, 0.1298, 0.0537, 0.0301], grad_fn=<ToCopyBackward0>), [' was', ' made', ' kept', ' could', ' would'])\n",
      "(tensor([0.8913, 0.0430, 0.0315, 0.0070, 0.0066], grad_fn=<ToCopyBackward0>), [' me', ' watching', ' my', ' this', ' it'])\n",
      "(tensor([0.9291, 0.0235, 0.0099, 0.0037, 0.0033], grad_fn=<ToCopyBackward0>), [' watching', ' from', ' interested', ' going', ' watch'])\n",
      "(tensor([0.6361, 0.1604, 0.0770, 0.0546, 0.0116], grad_fn=<ToCopyBackward0>), [' was', ' the', ' it', ' this', ' were'])\n",
      "(tensor([0.6868, 0.0491, 0.0219, 0.0202, 0.0175], grad_fn=<ToCopyBackward0>), [' the', ' that', ' to', ' watching', ' because'])\n",
      "(tensor([0.0569, 0.0564, 0.0436, 0.0225, 0.0213], grad_fn=<ToCopyBackward0>), [' plot', ' beautiful', ' fact', ' nudity', ' \"'])\n",
      "(tensor([0.9138, 0.0244, 0.0167, 0.0081, 0.0055], grad_fn=<ToCopyBackward0>), [' that', ' the', ' I', ' it', ' of'])\n",
      "(tensor([0.4444, 0.1912, 0.1220, 0.0280, 0.0276], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' there', ' they'])\n",
      "(tensor([0.2609, 0.0758, 0.0618, 0.0443, 0.0422], grad_fn=<ToCopyBackward0>), [' was', ' could', ' had', ' wanted', ' really'])\n",
      "(tensor([0.4446, 0.1044, 0.0762, 0.0446, 0.0217], grad_fn=<ToCopyBackward0>), [' see', ' not', ' tell', ' hear', ' say'])\n",
      "(tensor([0.5130, 0.0241, 0.0223, 0.0219, 0.0209], grad_fn=<ToCopyBackward0>), [' the', ' that', ' what', ' how', ' Christopher'])\n",
      "(tensor([0.0567, 0.0323, 0.0204, 0.0194, 0.0189], grad_fn=<ToCopyBackward0>), [' \"', ' two', ' other', ' little', ' voice'])\n",
      "(tensor([0.8817, 0.0298, 0.0235, 0.0066, 0.0033], grad_fn=<ToCopyBackward0>), [' of', '-', ' over', ',', ' from'])\n",
      "(tensor([0.2060, 0.0237, 0.0223, 0.0194, 0.0187], grad_fn=<ToCopyBackward0>), [' the', ' William', ' John', ' Richard', ' Michael'])\n",
      "(tensor([0.1702, 0.0509, 0.0472, 0.0406, 0.0271], grad_fn=<ToCopyBackward0>), [' narrator', ' director', ' movie', ' actor', ' actress'])\n",
      "(tensor([0.1003, 0.0973, 0.0623, 0.0600, 0.0477], grad_fn=<ToCopyBackward0>), [' saying', '.', ' talking', ',', ' ('])\n",
      "(tensor([0.2489, 0.0830, 0.0813, 0.0372, 0.0339], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' He', ' This'])\n",
      "(tensor([0.2463, 0.0859, 0.0782, 0.0631, 0.0346], grad_fn=<ToCopyBackward0>), [' was', ' sounded', \"'s\", ' kept', ' said'])\n",
      "(tensor([0.1348, 0.0723, 0.0653, 0.0480, 0.0471], grad_fn=<ToCopyBackward0>), [' the', ' supposed', ' so', ' saying', ' just'])\n",
      "(tensor([0.1034, 0.0797, 0.0632, 0.0454, 0.0343], grad_fn=<ToCopyBackward0>), [' a', ' as', ' like', ' that', ' so'])\n",
      "(tensor([0.1051, 0.1003, 0.0956, 0.0646, 0.0432], grad_fn=<ToCopyBackward0>), [' really', ' voice', ' guy', ' kid', ' little'])\n",
      "(tensor([0.2195, 0.2002, 0.1068, 0.0940, 0.0558], grad_fn=<ToCopyBackward0>), [' who', ' that', ' with', ' in', ' talking'])\n",
      "(tensor([0.1999, 0.0792, 0.0514, 0.0450, 0.0296], grad_fn=<ToCopyBackward0>), [' talks', ' says', ' narr', ' keeps', \"'s\"])\n",
      "(tensor([0.3392, 0.2126, 0.0395, 0.0354, 0.0324], grad_fn=<ToCopyBackward0>), [' \"', ' the', ' that', ' a', ' all'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was pretty atrocious, so I went to see the film for free. The first thing I found was that the director had been paid to produce this film. This film had been written by someone who also appeared in \"The Naked Gun\" and\n",
      "(tensor([0.3840, 0.1718, 0.0899, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2439, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4801, 0.1356, 0.1035, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1266, 0.0736, 0.0625, 0.0621, 0.0415], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' atro', ' awful', ' obvious'])\n",
      "(tensor([9.9925e-01, 6.2431e-05, 4.1673e-05, 3.2910e-05, 2.6897e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'pic', 'cc'])\n",
      "(tensor([0.2320, 0.1035, 0.0960, 0.0383, 0.0325], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', '!'])\n",
      "(tensor([0.1886, 0.1063, 0.0856, 0.0403, 0.0343], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' as', ' so'])\n",
      "(tensor([0.8395, 0.0233, 0.0146, 0.0138, 0.0088], grad_fn=<ToCopyBackward0>), [' I', ' i', ' it', ' the', ' when'])\n",
      "(tensor([0.0812, 0.0791, 0.0738, 0.0486, 0.0410], grad_fn=<ToCopyBackward0>), [' was', ' decided', ' went', ' thought', \"'m\"])\n",
      "(tensor([0.5620, 0.1784, 0.0614, 0.0378, 0.0198], grad_fn=<ToCopyBackward0>), [' to', ' into', ' on', ' and', ' back'])\n",
      "(tensor([0.7052, 0.0554, 0.0550, 0.0434, 0.0184], grad_fn=<ToCopyBackward0>), [' see', ' the', ' watch', ' IM', ' check'])\n",
      "(tensor([0.3806, 0.1437, 0.0545, 0.0506, 0.0500], grad_fn=<ToCopyBackward0>), [' the', ' what', ' a', ' this', ' if'])\n",
      "(tensor([0.1870, 0.1230, 0.1100, 0.0442, 0.0218], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' video', ' director', ' people'])\n",
      "(tensor([0.1199, 0.0806, 0.0796, 0.0736, 0.0614], grad_fn=<ToCopyBackward0>), [' for', ' myself', ' with', ' in', ' to'])\n",
      "(tensor([0.4258, 0.1421, 0.0815, 0.0683, 0.0434], grad_fn=<ToCopyBackward0>), [' myself', ' free', ' my', ' the', ' a'])\n",
      "(tensor([0.2281, 0.1560, 0.1440, 0.0871, 0.0561], grad_fn=<ToCopyBackward0>), [' at', '.', ' in', ' with', ' on'])\n",
      "(tensor([0.2973, 0.1259, 0.1084, 0.0380, 0.0369], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' What', ' And'])\n",
      "(tensor([0.0878, 0.0724, 0.0701, 0.0505, 0.0300], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' film', ' first', ' people'])\n",
      "(tensor([0.5302, 0.0722, 0.0354, 0.0345, 0.0206], grad_fn=<ToCopyBackward0>), [' thing', ' question', ' comment', ' scene', ' impression'])\n",
      "(tensor([0.5334, 0.4180, 0.0104, 0.0057, 0.0056], grad_fn=<ToCopyBackward0>), [' I', ' that', ' you', ' i', ' they'])\n",
      "(tensor([0.4137, 0.1211, 0.0697, 0.0500, 0.0337], grad_fn=<ToCopyBackward0>), [' noticed', ' thought', ' did', ' saw', ' found'])\n",
      "(tensor([0.6602, 0.0703, 0.0272, 0.0251, 0.0225], grad_fn=<ToCopyBackward0>), [' was', ' out', ' when', ' in', ' that'])\n",
      "(tensor([0.5627, 0.1628, 0.1024, 0.0543, 0.0170], grad_fn=<ToCopyBackward0>), [' that', ' the', ' a', ' this', ' an'])\n",
      "(tensor([0.3319, 0.2236, 0.1184, 0.0697, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' there', ' I'])\n",
      "(tensor([0.1253, 0.0779, 0.0540, 0.0299, 0.0246], grad_fn=<ToCopyBackward0>), [' director', ' film', ' movie', ' people', ' \"'])\n",
      "(tensor([0.2032, 0.1738, 0.0821, 0.0251, 0.0236], grad_fn=<ToCopyBackward0>), [' had', ' was', ',', ' of', ' and'])\n",
      "(tensor([0.1224, 0.1211, 0.0612, 0.0394, 0.0308], grad_fn=<ToCopyBackward0>), [' completely', ' made', ' been', ' cut', ' not'])\n",
      "(tensor([0.0742, 0.0720, 0.0499, 0.0380, 0.0321], grad_fn=<ToCopyBackward0>), [' fired', ' in', ' working', ' paid', ' let'])\n",
      "(tensor([0.3465, 0.1778, 0.1642, 0.0680, 0.0434], grad_fn=<ToCopyBackward0>), [' to', ' by', ' for', ' a', ' off'])\n",
      "(tensor([0.1890, 0.1554, 0.0744, 0.0679, 0.0526], grad_fn=<ToCopyBackward0>), [' make', ' be', ' promote', ' produce', ' appear'])\n",
      "(tensor([0.6045, 0.2196, 0.0385, 0.0322, 0.0306], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' a', ' and'])\n",
      "(tensor([0.4790, 0.1230, 0.0554, 0.0408, 0.0238], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ' piece', ','])\n",
      "(tensor([0.5289, 0.1336, 0.0767, 0.0460, 0.0389], grad_fn=<ToCopyBackward0>), ['.', ',', ' by', '!', ' and'])\n",
      "(tensor([0.1447, 0.1305, 0.0810, 0.0449, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' The', ' He', ' That', ' This'])\n",
      "(tensor([0.2870, 0.2279, 0.0918, 0.0327, 0.0214], grad_fn=<ToCopyBackward0>), [' is', ' was', ' film', ' movie', ' made'])\n",
      "(tensor([0.2971, 0.2590, 0.0925, 0.0506, 0.0150], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' has', \"'s\"])\n",
      "(tensor([0.5269, 0.0929, 0.0884, 0.0395, 0.0285], grad_fn=<ToCopyBackward0>), [' been', ' nothing', ' no', ' not', ' already'])\n",
      "(tensor([0.3010, 0.1684, 0.0765, 0.0402, 0.0364], grad_fn=<ToCopyBackward0>), [' made', ' written', ' done', ' produced', ' directed'])\n",
      "(tensor([0.6701, 0.1602, 0.0580, 0.0127, 0.0092], grad_fn=<ToCopyBackward0>), [' by', ' and', ',', ' for', ' in'])\n",
      "(tensor([0.3231, 0.2050, 0.0676, 0.0660, 0.0612], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' someone'])\n",
      "(tensor([0.5399, 0.1131, 0.1028, 0.0234, 0.0232], grad_fn=<ToCopyBackward0>), [' else', ' with', ' who', ' that', ' in'])\n",
      "(tensor([0.2123, 0.2122, 0.0465, 0.0457, 0.0371], grad_fn=<ToCopyBackward0>), [' was', ' had', ' has', ' is', ' also'])\n",
      "(tensor([0.0893, 0.0785, 0.0696, 0.0662, 0.0641], grad_fn=<ToCopyBackward0>), [' wrote', ' had', ' did', ' appears', ' appeared'])\n",
      "(tensor([0.6572, 0.1665, 0.1070, 0.0350, 0.0114], grad_fn=<ToCopyBackward0>), [' in', ' on', ' as', ' to', ' at'])\n",
      "(tensor([0.3392, 0.2531, 0.0529, 0.0355, 0.0319], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' \"', ' another'])\n",
      "(tensor([0.0968, 0.0177, 0.0140, 0.0133, 0.0124], grad_fn=<ToCopyBackward0>), ['The', 'Re', 'C', 'B', 'G'])\n",
      "(tensor([0.0927, 0.0802, 0.0246, 0.0162, 0.0153], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Pro', ' House', ' Sop'])\n",
      "(tensor([0.7317, 0.0383, 0.0240, 0.0232, 0.0145], grad_fn=<ToCopyBackward0>), [' Gun', ' Man', ' Pre', ' City', ' War'])\n",
      "(tensor([0.7089, 0.0855, 0.0585, 0.0406, 0.0363], grad_fn=<ToCopyBackward0>), ['\"', '\".', '\",', '.\"', ',\"'])\n",
      "(tensor([0.1349, 0.0948, 0.0868, 0.0831, 0.0650], grad_fn=<ToCopyBackward0>), [' and', ' movies', ' movie', ' TV', ' films'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very well done. It had some good effects. But it seemed to be a bit too long and drawn out and predictable. The movie had some potential, but it just didn't deliver. I really didn't care for this movie at\n",
      "(tensor([0.3831, 0.1723, 0.0904, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2450, 0.1959, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0595, 0.0360, 0.0356, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0663, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5069, 0.0403, 0.0346, 0.0338, 0.0324], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' funny', ','])\n",
      "(tensor([0.5459, 0.2534, 0.1076, 0.0180, 0.0147], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.4158, 0.1612, 0.1353, 0.0447, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.1966, 0.1902, 0.1733, 0.0249, 0.0242], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' Unfortunately'])\n",
      "(tensor([0.4034, 0.1107, 0.0827, 0.0611, 0.0546], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' has', ' is'])\n",
      "(tensor([0.3173, 0.1128, 0.0878, 0.0665, 0.0563], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' good', ' great'])\n",
      "(tensor([0.1103, 0.0945, 0.0895, 0.0663, 0.0472], grad_fn=<ToCopyBackward0>), [' funny', ' great', ' of', ' good', ' really'])\n",
      "(tensor([0.2882, 0.1067, 0.0698, 0.0506, 0.0358], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' effects', ' special', ' action'])\n",
      "(tensor([0.3802, 0.1916, 0.1756, 0.0638, 0.0268], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' as'])\n",
      "(tensor([0.1665, 0.1485, 0.1478, 0.1257, 0.0478], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' But', ' Unfortunately'])\n",
      "(tensor([0.3539, 0.1493, 0.0591, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' when', ','])\n",
      "(tensor([0.2903, 0.1088, 0.0685, 0.0541, 0.0508], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' seemed'])\n",
      "(tensor([0.3666, 0.3339, 0.0575, 0.0468, 0.0364], grad_fn=<ToCopyBackward0>), [' to', ' like', ' that', ' as', ' more'])\n",
      "(tensor([0.4445, 0.1983, 0.0615, 0.0475, 0.0299], grad_fn=<ToCopyBackward0>), [' me', ' be', ' lack', ' have', ' fall'])\n",
      "(tensor([0.1218, 0.0947, 0.0870, 0.0307, 0.0300], grad_fn=<ToCopyBackward0>), [' trying', ' a', ' more', ' shot', ' very'])\n",
      "(tensor([0.1083, 0.0773, 0.0549, 0.0534, 0.0473], grad_fn=<ToCopyBackward0>), [' very', ' little', ' lot', ' bit', ' big'])\n",
      "(tensor([0.4347, 0.1665, 0.0630, 0.0411, 0.0244], grad_fn=<ToCopyBackward0>), [' too', ' of', ' over', ' more', ' slow'])\n",
      "(tensor([0.0988, 0.0920, 0.0423, 0.0409, 0.0401], grad_fn=<ToCopyBackward0>), [' long', ' much', ' talk', ' \"', ' preach'])\n",
      "(tensor([0.3625, 0.2229, 0.1603, 0.0665, 0.0281], grad_fn=<ToCopyBackward0>), ['.', ' and', ' for', ',', ' at'])\n",
      "(tensor([0.1298, 0.0670, 0.0665, 0.0522, 0.0333], grad_fn=<ToCopyBackward0>), [' too', ' boring', ' drawn', ' dragged', ' way'])\n",
      "(tensor([9.8603e-01, 8.0845e-03, 9.3043e-04, 8.0534e-04, 6.7848e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' out', '-', ' to', ' together', ' from'])\n",
      "(tensor([0.4555, 0.1460, 0.1047, 0.0693, 0.0607], grad_fn=<ToCopyBackward0>), ['.', ' for', ' to', ' and', ','])\n",
      "(tensor([0.1691, 0.0463, 0.0399, 0.0386, 0.0330], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' the', ' too', ' had'])\n",
      "(tensor([0.5928, 0.0686, 0.0678, 0.0614, 0.0482], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' for'])\n",
      "(tensor([0.2117, 0.1506, 0.1469, 0.0325, 0.0260], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.1435, 0.0862, 0.0843, 0.0573, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' ending', ' movie', ' story', ' whole'])\n",
      "(tensor([0.1476, 0.1453, 0.0840, 0.0828, 0.0327], grad_fn=<ToCopyBackward0>), [' just', ' was', ' seemed', ' had', ' dragged'])\n",
      "(tensor([0.1820, 0.1519, 0.0619, 0.0325, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' no', ' so'])\n",
      "(tensor([0.1202, 0.1146, 0.0508, 0.0490, 0.0427], grad_fn=<ToCopyBackward0>), [' potential', ' good', ' great', ' of', ' interesting'])\n",
      "(tensor([0.3784, 0.2825, 0.1037, 0.0654, 0.0368], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' to', ' and'])\n",
      "(tensor([0.9265, 0.0130, 0.0070, 0.0044, 0.0041], grad_fn=<ToCopyBackward0>), [' but', ' and', ' though', ' however', ' I'])\n",
      "(tensor([0.2214, 0.1627, 0.0970, 0.0509, 0.0253], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' was', ' didn'])\n",
      "(tensor([0.2355, 0.1026, 0.0990, 0.0846, 0.0643], grad_fn=<ToCopyBackward0>), [' just', ' was', ' seemed', ' needed', ' didn'])\n",
      "(tensor([0.2626, 0.2447, 0.0474, 0.0431, 0.0383], grad_fn=<ToCopyBackward0>), [' didn', ' seemed', ' ended', ' never', ' wasn'])\n",
      "(tensor([9.9395e-01, 2.8449e-03, 6.4710e-04, 5.9127e-04, 2.0090e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', '\"'])\n",
      "(tensor([0.1259, 0.0947, 0.0595, 0.0500, 0.0499], grad_fn=<ToCopyBackward0>), [' deliver', ' work', ' have', ' seem', ' come'])\n",
      "(tensor([0.6346, 0.1680, 0.0290, 0.0204, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ' on', ' for', ' as', ' the'])\n",
      "(tensor([0.2896, 0.1439, 0.1262, 0.0228, 0.0220], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.1013, 0.0895, 0.0631, 0.0619, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' really', ' would', ' think', \"'m\"])\n",
      "(tensor([0.1037, 0.0933, 0.0798, 0.0737, 0.0682], grad_fn=<ToCopyBackward0>), [' wanted', ' didn', ' would', ' don', ' wish'])\n",
      "(tensor([9.9745e-01, 8.0415e-04, 3.4515e-04, 1.4039e-04, 1.3895e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2327, 0.2278, 0.1130, 0.1118, 0.0884], grad_fn=<ToCopyBackward0>), [' care', ' like', ' get', ' enjoy', ' understand'])\n",
      "(tensor([0.6824, 0.1193, 0.0361, 0.0238, 0.0209], grad_fn=<ToCopyBackward0>), [' for', ' about', ' what', ' much', ' too'])\n",
      "(tensor([0.4366, 0.3323, 0.1484, 0.0428, 0.0037], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' any', ' how'])\n",
      "(tensor([0.7396, 0.1741, 0.0301, 0.0095, 0.0038], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', '.', ' flick'])\n",
      "(tensor([0.4283, 0.2242, 0.0975, 0.0430, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', ' one'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that it was a sequel to The Man From Snowy River. I had very little to compare it to, except maybe the Twilight Zone episode, \"The Man Who Wasn't There.\" The acting was atrocious, the script was atrocious and\n",
      "(tensor([0.3844, 0.1713, 0.0895, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3307, 0.2370, 0.0582, 0.0559, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.5333, 0.1179, 0.0422, 0.0373, 0.0248], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' had'])\n",
      "(tensor([0.3211, 0.0614, 0.0520, 0.0493, 0.0346], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' the', ' very'])\n",
      "(tensor([0.2478, 0.1079, 0.0600, 0.0546, 0.0376], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' great', ' pretty', ' really'])\n",
      "(tensor([0.7697, 0.0747, 0.0244, 0.0181, 0.0103], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ',', '...'])\n",
      "(tensor([0.1701, 0.0628, 0.0565, 0.0384, 0.0336], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.0466, 0.0421, 0.0274, 0.0191, 0.0173], grad_fn=<ToCopyBackward0>), [' Ring', ' Naked', ' Boo', ' Legend', ' Man'])\n",
      "(tensor([0.2979, 0.2944, 0.2069, 0.0643, 0.0249], grad_fn=<ToCopyBackward0>), [' Who', ' From', 'ch', ' With', ' Called'])\n",
      "(tensor([9.5484e-01, 5.6520e-03, 5.4174e-03, 1.0819e-03, 8.6503e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Snow', ' UN', ' Earth', ' Planet', ' U'])\n",
      "(tensor([9.9409e-01, 1.0875e-03, 3.9713e-04, 2.3034e-04, 2.2254e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['y', 'ys', 'ies', ' City', 'don'])\n",
      "(tensor([0.9237, 0.0198, 0.0074, 0.0024, 0.0021], grad_fn=<ToCopyBackward0>), [' River', ' Mountain', ' Lake', ' City', ' Island'])\n",
      "(tensor([0.2944, 0.1563, 0.0628, 0.0598, 0.0535], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' which', ' so'])\n",
      "(tensor([0.2048, 0.0971, 0.0854, 0.0405, 0.0290], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' That'])\n",
      "(tensor([0.1706, 0.1123, 0.0445, 0.0402, 0.0356], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' think', ' had'])\n",
      "(tensor([0.1230, 0.1199, 0.1032, 0.0816, 0.0740], grad_fn=<ToCopyBackward0>), [' a', ' never', ' to', ' no', ' very'])\n",
      "(tensor([0.3834, 0.3388, 0.1126, 0.0420, 0.0305], grad_fn=<ToCopyBackward0>), [' high', ' low', ' little', ' good', ' much'])\n",
      "(tensor([0.1721, 0.1706, 0.1154, 0.0994, 0.0891], grad_fn=<ToCopyBackward0>), [' expectation', ' expectations', ' idea', ' knowledge', ' to'])\n",
      "(tensor([0.5723, 0.0708, 0.0680, 0.0590, 0.0303], grad_fn=<ToCopyBackward0>), [' do', ' go', ' work', ' compare', ' no'])\n",
      "(tensor([0.9448, 0.0149, 0.0062, 0.0061, 0.0037], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' The', ' to'])\n",
      "(tensor([0.8738, 0.1060, 0.0099, 0.0016, 0.0012], grad_fn=<ToCopyBackward0>), [' to', ' with', ' too', ',', '.'])\n",
      "(tensor([0.5733, 0.2895, 0.0169, 0.0165, 0.0132], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' and', ' other'])\n",
      "(tensor([0.3474, 0.1251, 0.0902, 0.0670, 0.0592], grad_fn=<ToCopyBackward0>), [' but', ' other', ' except', ' so', ' and'])\n",
      "(tensor([0.2538, 0.2001, 0.1028, 0.0992, 0.0372], grad_fn=<ToCopyBackward0>), [' for', ' maybe', ' that', ' the', ' The'])\n",
      "(tensor([0.2239, 0.1472, 0.0285, 0.0245, 0.0216], grad_fn=<ToCopyBackward0>), [' the', ' The', ' that', ' some', ' Snow'])\n",
      "(tensor([0.0628, 0.0604, 0.0275, 0.0172, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' first', ' TV', ' book', ' Twilight'])\n",
      "(tensor([9.9052e-01, 2.2819e-03, 2.1433e-03, 5.3978e-04, 3.5936e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' Zone', ' zone', ' series', ' movies', ' movie'])\n",
      "(tensor([0.4089, 0.1609, 0.1022, 0.0638, 0.0314], grad_fn=<ToCopyBackward0>), [' episode', '.', ' episodes', ' movie', ','])\n",
      "(tensor([0.2555, 0.1292, 0.0993, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' \"', ',', ' of', ' where', ' that'])\n",
      "(tensor([0.2004, 0.1990, 0.1249, 0.1027, 0.0939], grad_fn=<ToCopyBackward0>), [' and', ' \"', ' but', \" '\", ' which'])\n",
      "(tensor([0.2101, 0.0604, 0.0159, 0.0155, 0.0124], grad_fn=<ToCopyBackward0>), ['The', 'Night', 'Time', 'A', 'Christmas'])\n",
      "(tensor([0.0678, 0.0620, 0.0208, 0.0183, 0.0146], grad_fn=<ToCopyBackward0>), [' Brain', ' Man', ' Blue', ' Straight', ' Grey'])\n",
      "(tensor([0.6382, 0.1290, 0.0834, 0.0313, 0.0164], grad_fn=<ToCopyBackward0>), [' From', ' Who', ' from', 'ch', ' In'])\n",
      "(tensor([0.2725, 0.1959, 0.0367, 0.0362, 0.0305], grad_fn=<ToCopyBackward0>), [' Was', ' Fell', ' Would', ' Came', ' K'])\n",
      "(tensor([0.3020, 0.0278, 0.0151, 0.0118, 0.0097], grad_fn=<ToCopyBackward0>), ['n', ' Naked', ' Blind', ' Dead', ' Drunk'])\n",
      "(tensor([0.9775, 0.0081, 0.0030, 0.0023, 0.0011], grad_fn=<ToCopyBackward0>), [\"'t\", '`', '\"', \"'\", ','])\n",
      "(tensor([0.8600, 0.0372, 0.0069, 0.0052, 0.0025], grad_fn=<ToCopyBackward0>), [' There', ' there', ' Me', ' Here', ' Even'])\n",
      "(tensor([0.3910, 0.2553, 0.1343, 0.1244, 0.0816], grad_fn=<ToCopyBackward0>), ['.\"', '\".', '\"', '\",', ',\"'])\n",
      "(tensor([0.1211, 0.0853, 0.0776, 0.0733, 0.0590], grad_fn=<ToCopyBackward0>), [' I', ' The', ' And', ' But', ' That'])\n",
      "(tensor([0.1104, 0.0749, 0.0377, 0.0297, 0.0281], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' Man', ' plot', ' special'])\n",
      "(tensor([0.4917, 0.1365, 0.1082, 0.0577, 0.0489], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' in'])\n",
      "(tensor([0.0705, 0.0625, 0.0619, 0.0525, 0.0479], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' awful', ' very', ' atro'])\n",
      "(tensor([9.9990e-01, 8.4930e-06, 5.2630e-06, 4.9977e-06, 3.6752e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'vol', 'par'])\n",
      "(tensor([0.3994, 0.2152, 0.1845, 0.0705, 0.0259], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' in', ';'])\n",
      "(tensor([0.5352, 0.1219, 0.1215, 0.0234, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' but', ' and', ' as', ' though'])\n",
      "(tensor([0.1349, 0.1074, 0.1035, 0.0774, 0.0522], grad_fn=<ToCopyBackward0>), [' script', ' plot', ' story', ' writing', ' production'])\n",
      "(tensor([0.7417, 0.0304, 0.0100, 0.0099, 0.0091], grad_fn=<ToCopyBackward0>), [' was', ',', ' sounded', ' wasn', ' had'])\n",
      "(tensor([0.1352, 0.1324, 0.0510, 0.0360, 0.0290], grad_fn=<ToCopyBackward0>), [' atro', ' awful', ' terrible', ' bad', ' a'])\n",
      "(tensor([9.9935e-01, 4.5853e-05, 2.4795e-05, 2.1589e-05, 2.1558e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'cular', 'cul', 'cept', 'cient'])\n",
      "(tensor([0.4883, 0.2448, 0.2037, 0.0130, 0.0057], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ';', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so terrible I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see it in the theater. I actually paid to see\n",
      "(tensor([0.3833, 0.1719, 0.0901, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4375, 0.2442, 0.1961, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6524, 0.0596, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1374, 0.0700, 0.0663, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5192, 0.0474, 0.0457, 0.0420, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2037, 0.1736, 0.1379, 0.1202, 0.0641], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', '.', ','])\n",
      "(tensor([0.1177, 0.0852, 0.0671, 0.0589, 0.0580], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' could', ' was', ' had'])\n",
      "(tensor([0.1145, 0.0891, 0.0672, 0.0487, 0.0457], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' closed', ' started'])\n",
      "(tensor([0.1914, 0.1229, 0.0887, 0.0744, 0.0558], grad_fn=<ToCopyBackward0>), [' to', ' $', ' money', ' for', ' 1'])\n",
      "(tensor([0.3781, 0.1587, 0.1250, 0.0665, 0.0301], grad_fn=<ToCopyBackward0>), [' see', ' get', ' rent', ' watch', ' have'])\n",
      "(tensor([0.8478, 0.0498, 0.0374, 0.0074, 0.0023], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' something'])\n",
      "(tensor([0.3048, 0.1846, 0.0507, 0.0449, 0.0434], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' on', ' at'])\n",
      "(tensor([0.2057, 0.1313, 0.1215, 0.0691, 0.0318], grad_fn=<ToCopyBackward0>), [' the', ' theat', ' theaters', ' a', ' theater'])\n",
      "(tensor([0.4096, 0.3619, 0.1031, 0.0119, 0.0110], grad_fn=<ToCopyBackward0>), [' theatre', ' theater', ' bargain', ' Theatre', ' cinema'])\n",
      "(tensor([0.8047, 0.0229, 0.0191, 0.0148, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' just', ' because'])\n",
      "(tensor([0.3303, 0.1255, 0.0802, 0.0199, 0.0155], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' What'])\n",
      "(tensor([0.0961, 0.0820, 0.0482, 0.0468, 0.0370], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' can', ' actually', ' really'])\n",
      "(tensor([0.6126, 0.0412, 0.0339, 0.0303, 0.0168], grad_fn=<ToCopyBackward0>), [' paid', ' rented', ' thought', ' pay', ' made'])\n",
      "(tensor([0.5270, 0.0911, 0.0888, 0.0509, 0.0446], grad_fn=<ToCopyBackward0>), [' to', ' $', ' for', ' a', ' money'])\n",
      "(tensor([0.6948, 0.1178, 0.0599, 0.0240, 0.0161], grad_fn=<ToCopyBackward0>), [' see', ' rent', ' watch', ' go', ' get'])\n",
      "(tensor([0.4967, 0.3880, 0.0298, 0.0164, 0.0122], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' that'])\n",
      "(tensor([0.8089, 0.0713, 0.0288, 0.0247, 0.0152], grad_fn=<ToCopyBackward0>), [' in', ' at', ' on', '.', ' because'])\n",
      "(tensor([0.8078, 0.1082, 0.0313, 0.0114, 0.0060], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' theaters', ' an'])\n",
      "(tensor([0.9582, 0.0298, 0.0024, 0.0013, 0.0012], grad_fn=<ToCopyBackward0>), [' theater', ' theatre', ' Theater', ' theaters', ' cinema'])\n",
      "(tensor([0.6478, 0.0743, 0.0406, 0.0237, 0.0181], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' when'])\n",
      "(tensor([0.3483, 0.0627, 0.0438, 0.0418, 0.0271], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' That'])\n",
      "(tensor([0.1429, 0.0594, 0.0529, 0.0484, 0.0441], grad_fn=<ToCopyBackward0>), [' actually', ' really', ' think', \"'m\", ' thought'])\n",
      "(tensor([0.8195, 0.0406, 0.0175, 0.0104, 0.0082], grad_fn=<ToCopyBackward0>), [' paid', ' pay', ' thought', ' rented', ' watched'])\n",
      "(tensor([0.8955, 0.0381, 0.0118, 0.0112, 0.0061], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ' a', ' $'])\n",
      "(tensor([0.9722, 0.0069, 0.0031, 0.0023, 0.0022], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' go', ' get', ' buy'])\n",
      "(tensor([0.8977, 0.0634, 0.0068, 0.0064, 0.0027], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' that'])\n",
      "(tensor([0.9662, 0.0159, 0.0047, 0.0039, 0.0025], grad_fn=<ToCopyBackward0>), [' in', ' at', '.', ' on', ' because'])\n",
      "(tensor([9.7715e-01, 1.3647e-02, 4.3248e-03, 1.3725e-03, 6.9287e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' theaters', ' theater'])\n",
      "(tensor([9.9275e-01, 1.8321e-03, 1.7190e-03, 9.8754e-04, 7.8996e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' theater', ' movie', ' theatre', ' Theater', ' theaters'])\n",
      "(tensor([0.8531, 0.0280, 0.0206, 0.0123, 0.0089], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' when', '!'])\n",
      "(tensor([0.5545, 0.0421, 0.0414, 0.0345, 0.0193], grad_fn=<ToCopyBackward0>), [' I', 'I', ' It', ' This', ' And'])\n",
      "(tensor([0.7372, 0.0651, 0.0338, 0.0128, 0.0080], grad_fn=<ToCopyBackward0>), [' actually', ' even', ' really', ' think', ' thought'])\n",
      "(tensor([0.9566, 0.0107, 0.0028, 0.0022, 0.0015], grad_fn=<ToCopyBackward0>), [' paid', ' pay', ' thought', '...', ' watched'])\n",
      "(tensor([0.9773, 0.0072, 0.0066, 0.0013, 0.0011], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', '...', ','])\n",
      "(tensor([9.9173e-01, 2.6661e-03, 8.6059e-04, 5.8124e-04, 4.1451e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' see', ' watch', ' hear', ' get', ' go'])\n",
      "(tensor([0.9754, 0.0117, 0.0015, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([9.9042e-01, 3.1325e-03, 2.5710e-03, 6.2622e-04, 4.9987e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' in', '.', ' at', ' on', ' because'])\n",
      "(tensor([9.8578e-01, 3.7631e-03, 3.6560e-03, 5.4283e-04, 4.6540e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' theaters', ' theater'])\n",
      "(tensor([9.9405e-01, 1.9511e-03, 9.6096e-04, 7.0463e-04, 4.4009e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' theater', ' movie', ' theaters', ' Theater', ' theatre'])\n",
      "(tensor([0.9513, 0.0058, 0.0045, 0.0040, 0.0034], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' I'])\n",
      "(tensor([0.7901, 0.0466, 0.0163, 0.0151, 0.0065], grad_fn=<ToCopyBackward0>), [' I', 'I', ' This', ' It', ' You'])\n",
      "(tensor([0.8915, 0.0445, 0.0162, 0.0047, 0.0020], grad_fn=<ToCopyBackward0>), [' actually', ' even', ' really', ' think', ' was'])\n",
      "(tensor([9.7739e-01, 3.6733e-03, 1.7105e-03, 1.1079e-03, 8.5328e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' paid', ' pay', '...', ' thought', ' watched'])\n",
      "(tensor([9.8834e-01, 3.1176e-03, 2.3023e-03, 6.1779e-04, 4.5673e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', '.', ' for', ',', '...'])\n",
      "(tensor([9.8779e-01, 2.2967e-03, 1.3456e-03, 8.8954e-04, 6.7809e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' see', ' watch', ' hear', ' get', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of those films that is made for TV movies. I was so wrong. It was a great film with great actors. I was very disappointed with this film. I thought I would enjoy it more as it went on, but it was\n",
      "(tensor([0.3839, 0.1718, 0.0897, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1459, 0.0508, 0.0453, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9170, 0.0124, 0.0066, 0.0065, 0.0039], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7602, 0.1941, 0.0167, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.4908, 0.2099, 0.0214, 0.0133, 0.0075], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' \"', ' B', \" '\"])\n",
      "(tensor([0.5348, 0.2359, 0.0748, 0.0261, 0.0166], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', ' like'])\n",
      "(tensor([0.1621, 0.0644, 0.0446, 0.0442, 0.0388], grad_fn=<ToCopyBackward0>), [' was', ' would', ' the', ' you', ' is'])\n",
      "(tensor([0.2067, 0.1526, 0.0288, 0.0275, 0.0259], grad_fn=<ToCopyBackward0>), [' so', ' funny', ' made', ' fun', ' going'])\n",
      "(tensor([0.2812, 0.2084, 0.1376, 0.0825, 0.0570], grad_fn=<ToCopyBackward0>), [' for', ' in', ' by', ' to', ' on'])\n",
      "(tensor([0.3581, 0.1241, 0.1180, 0.0935, 0.0330], grad_fn=<ToCopyBackward0>), [' kids', ' TV', ' the', ' one', ' children'])\n",
      "(tensor([0.1391, 0.1179, 0.0974, 0.0289, 0.0230], grad_fn=<ToCopyBackward0>), [' movie', ' movies', '.', ' screens', ' audiences'])\n",
      "(tensor([0.2618, 0.1675, 0.0672, 0.0642, 0.0500], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' that', ' and'])\n",
      "(tensor([0.1732, 0.1010, 0.0978, 0.0444, 0.0437], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' And'])\n",
      "(tensor([0.1040, 0.0927, 0.0515, 0.0514, 0.0500], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' don', ' really', \"'m\"])\n",
      "(tensor([0.1639, 0.1596, 0.0532, 0.0516, 0.0409], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' surprised', ' looking'])\n",
      "(tensor([0.4436, 0.2531, 0.0588, 0.0255, 0.0238], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ' bored', ' surprised', ' excited'])\n",
      "(tensor([0.7618, 0.1319, 0.0164, 0.0124, 0.0109], grad_fn=<ToCopyBackward0>), ['.', '!', ' about', ' on', ','])\n",
      "(tensor([0.2373, 0.1930, 0.1528, 0.1046, 0.0419], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', 'This'])\n",
      "(tensor([0.3405, 0.2746, 0.1935, 0.0200, 0.0198], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' wasn', ' has'])\n",
      "(tensor([0.1785, 0.0965, 0.0453, 0.0417, 0.0350], grad_fn=<ToCopyBackward0>), [' so', ' a', ' really', ' one', ' awful'])\n",
      "(tensor([0.3487, 0.1123, 0.0709, 0.0475, 0.0216], grad_fn=<ToCopyBackward0>), [' really', ' very', ' real', ' great', ' major'])\n",
      "(tensor([0.1628, 0.1509, 0.0877, 0.0729, 0.0565], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' disappointment', ',', ' way'])\n",
      "(tensor([0.5334, 0.0771, 0.0581, 0.0498, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', '!', ' to'])\n",
      "(tensor([0.4482, 0.1521, 0.0448, 0.0324, 0.0263], grad_fn=<ToCopyBackward0>), [' great', ' a', ' an', ' very', ' wonderful'])\n",
      "(tensor([0.4138, 0.1776, 0.0974, 0.0481, 0.0229], grad_fn=<ToCopyBackward0>), [' actors', ' acting', ' characters', ' people', ' performances'])\n",
      "(tensor([0.2975, 0.2959, 0.2577, 0.0368, 0.0317], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' but', ' in'])\n",
      "(tensor([0.2660, 0.1559, 0.1101, 0.0425, 0.0335], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Unfortunately', ' But'])\n",
      "(tensor([0.0851, 0.0761, 0.0747, 0.0593, 0.0556], grad_fn=<ToCopyBackward0>), [' really', ' was', \"'m\", ' am', ' can'])\n",
      "(tensor([0.3158, 0.2554, 0.0630, 0.0206, 0.0176], grad_fn=<ToCopyBackward0>), [' very', ' really', ' so', ' disappointed', ' a'])\n",
      "(tensor([0.5503, 0.0802, 0.0506, 0.0310, 0.0277], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' much', ' impressed', ' excited'])\n",
      "(tensor([0.4182, 0.1625, 0.0941, 0.0687, 0.0673], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', ' by', ' that'])\n",
      "(tensor([0.3763, 0.2154, 0.2142, 0.0338, 0.0281], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' how', ' my'])\n",
      "(tensor([0.5781, 0.2574, 0.0475, 0.0084, 0.0061], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' sequel', ' turkey'])\n",
      "(tensor([0.5576, 0.1362, 0.1107, 0.0478, 0.0166], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' and', ' in'])\n",
      "(tensor([0.2473, 0.2282, 0.1435, 0.0333, 0.0186], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1332, 0.0877, 0.0606, 0.0528, 0.0483], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' think'])\n",
      "(tensor([0.5916, 0.1472, 0.0644, 0.0511, 0.0495], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3282, 0.2877, 0.1197, 0.0431, 0.0317], grad_fn=<ToCopyBackward0>), [' would', ' was', ' had', \"'d\", ' could'])\n",
      "(tensor([0.2242, 0.1567, 0.1103, 0.0850, 0.0729], grad_fn=<ToCopyBackward0>), [' enjoy', ' be', ' like', ' love', ' have'])\n",
      "(tensor([0.5875, 0.1004, 0.0975, 0.0613, 0.0314], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' watching', ' seeing'])\n",
      "(tensor([0.4548, 0.1751, 0.0771, 0.0632, 0.0493], grad_fn=<ToCopyBackward0>), [' more', ',', ' but', '.', ' a'])\n",
      "(tensor([0.1945, 0.1780, 0.1287, 0.1114, 0.0745], grad_fn=<ToCopyBackward0>), ['.', ' than', ' after', ' because', ' as'])\n",
      "(tensor([0.5042, 0.1587, 0.1498, 0.1178, 0.0156], grad_fn=<ToCopyBackward0>), [' a', ' an', ' I', ' it', ' the'])\n",
      "(tensor([0.3887, 0.0596, 0.0489, 0.0410, 0.0296], grad_fn=<ToCopyBackward0>), [' was', ' went', ' is', ' progressed', ' goes'])\n",
      "(tensor([0.7113, 0.2522, 0.0053, 0.0047, 0.0038], grad_fn=<ToCopyBackward0>), [' along', ' on', ' through', ' down', ' into'])\n",
      "(tensor([0.4234, 0.2994, 0.0602, 0.0534, 0.0442], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' the', ' because'])\n",
      "(tensor([0.8198, 0.0340, 0.0212, 0.0163, 0.0087], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' then', ' so'])\n",
      "(tensor([0.3439, 0.2303, 0.0622, 0.0517, 0.0317], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' as'])\n",
      "(tensor([0.3138, 0.2931, 0.0426, 0.0361, 0.0277], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', ' never', \"'s\"])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film had some potential to be something interesting, but I just couldn't get in the mood to watch it. It seemed too much like a movie that someone had filmed at a college fraternity party and was proud of. The plot was very weak\n",
      "(tensor([0.3828, 0.1726, 0.0903, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3319, 0.2362, 0.0581, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4140, 0.1971, 0.1695, 0.0495, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6394, 0.0584, 0.0581, 0.0558, 0.0175], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.3331, 0.1618, 0.0983, 0.0669, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' some', ' all'])\n",
      "(tensor([0.4147, 0.1236, 0.1102, 0.0453, 0.0425], grad_fn=<ToCopyBackward0>), [' potential', ' real', ' great', ' promise', ' good'])\n",
      "(tensor([0.4142, 0.2487, 0.1042, 0.0338, 0.0282], grad_fn=<ToCopyBackward0>), [',', '.', ' to', ' for', ' as'])\n",
      "(tensor([0.7645, 0.0312, 0.0235, 0.0211, 0.0200], grad_fn=<ToCopyBackward0>), [' be', ' do', ' really', ' make', ' become'])\n",
      "(tensor([0.1990, 0.1592, 0.1186, 0.1093, 0.0516], grad_fn=<ToCopyBackward0>), [' a', ' something', ' funny', ' really', ' very'])\n",
      "(tensor([0.1783, 0.1156, 0.1005, 0.0711, 0.0547], grad_fn=<ToCopyBackward0>), [' special', ' really', ' good', ' interesting', ' very'])\n",
      "(tensor([0.3189, 0.2313, 0.1493, 0.0892, 0.0345], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', '...'])\n",
      "(tensor([0.6068, 0.0407, 0.0317, 0.0228, 0.0214], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' something', ' to'])\n",
      "(tensor([0.2029, 0.1200, 0.0616, 0.0514, 0.0506], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' after', ' ultimately'])\n",
      "(tensor([0.3116, 0.1292, 0.0522, 0.0422, 0.0365], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', ' don', ' found'])\n",
      "(tensor([0.2733, 0.1448, 0.1145, 0.0799, 0.0268], grad_fn=<ToCopyBackward0>), [' didn', ' wasn', ' couldn', ' found', ' don'])\n",
      "(tensor([9.9435e-01, 2.6024e-03, 5.7849e-04, 5.5657e-04, 1.6789e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2401, 0.1589, 0.0772, 0.0558, 0.0428], grad_fn=<ToCopyBackward0>), [' believe', ' get', ' see', ' bring', ' figure'])\n",
      "(tensor([0.3071, 0.1066, 0.1019, 0.0815, 0.0625], grad_fn=<ToCopyBackward0>), [' into', ' over', ' on', ' in', ' behind'])\n",
      "(tensor([0.3220, 0.3070, 0.0465, 0.0456, 0.0333], grad_fn=<ToCopyBackward0>), [' the', ' to', ' there', ' on', '.'])\n",
      "(tensor([0.6441, 0.0332, 0.0155, 0.0131, 0.0125], grad_fn=<ToCopyBackward0>), [' mood', ' right', ' heads', ' frame', ' mindset'])\n",
      "(tensor([0.7627, 0.1403, 0.0507, 0.0045, 0.0036], grad_fn=<ToCopyBackward0>), [' to', ' for', '.', ' and', ' when'])\n",
      "(tensor([0.7499, 0.0701, 0.0199, 0.0182, 0.0156], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' sit', ' be', ' enjoy'])\n",
      "(tensor([0.8544, 0.0447, 0.0175, 0.0169, 0.0138], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' any'])\n",
      "(tensor([0.7427, 0.0499, 0.0174, 0.0166, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', ' again', ' at', ' after'])\n",
      "(tensor([0.2278, 0.1918, 0.1292, 0.0254, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'The', ' This'])\n",
      "(tensor([0.2784, 0.1554, 0.0830, 0.0692, 0.0534], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' just', ' wasn'])\n",
      "(tensor([0.2883, 0.2619, 0.0948, 0.0709, 0.0303], grad_fn=<ToCopyBackward0>), [' to', ' like', ' too', ' more', ' as'])\n",
      "(tensor([0.0961, 0.0763, 0.0555, 0.0430, 0.0291], grad_fn=<ToCopyBackward0>), [' pret', ' much', ' slow', ' long', ' artificial'])\n",
      "(tensor([0.7202, 0.0973, 0.0229, 0.0088, 0.0081], grad_fn=<ToCopyBackward0>), [' like', ' of', ' for', ' to', ' too'])\n",
      "(tensor([0.6116, 0.1447, 0.0236, 0.0223, 0.0216], grad_fn=<ToCopyBackward0>), [' a', ' an', ' one', ' it', ' another'])\n",
      "(tensor([0.0581, 0.0445, 0.0329, 0.0284, 0.0267], grad_fn=<ToCopyBackward0>), [' TV', ' drama', ' \"', ' movie', ' filmed'])\n",
      "(tensor([0.1605, 0.1579, 0.1214, 0.0633, 0.0553], grad_fn=<ToCopyBackward0>), [' about', ' that', ' for', ' made', '.'])\n",
      "(tensor([0.1715, 0.1455, 0.1125, 0.0802, 0.0519], grad_fn=<ToCopyBackward0>), [' someone', ' was', ' a', ' the', ' had'])\n",
      "(tensor([0.3754, 0.1315, 0.0973, 0.0377, 0.0290], grad_fn=<ToCopyBackward0>), [' made', ' else', ' had', ' at', ' was'])\n",
      "(tensor([0.7734, 0.0531, 0.0234, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [' made', ' written', ' filmed', ' put', ' done'])\n",
      "(tensor([0.1458, 0.0898, 0.0898, 0.0870, 0.0860], grad_fn=<ToCopyBackward0>), [' in', ' at', ' to', ' on', ' for'])\n",
      "(tensor([0.4728, 0.1323, 0.0991, 0.0502, 0.0194], grad_fn=<ToCopyBackward0>), [' a', ' the', ' home', ' an', ' someone'])\n",
      "(tensor([0.1109, 0.0677, 0.0563, 0.0486, 0.0394], grad_fn=<ToCopyBackward0>), [' friend', ' party', ' college', ' rave', ' friends'])\n",
      "(tensor([0.2074, 0.0708, 0.0538, 0.0439, 0.0425], grad_fn=<ToCopyBackward0>), [' party', ' reunion', ' campus', ' football', ' fraternity'])\n",
      "(tensor([0.3002, 0.2205, 0.0385, 0.0149, 0.0139], grad_fn=<ToCopyBackward0>), [' party', ' house', ' ha', ' function', ' dinner'])\n",
      "(tensor([0.3918, 0.1580, 0.1350, 0.0376, 0.0312], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' -'])\n",
      "(tensor([0.0917, 0.0916, 0.0796, 0.0626, 0.0453], grad_fn=<ToCopyBackward0>), [' was', ' edited', ' made', ' had', ' then'])\n",
      "(tensor([0.0807, 0.0549, 0.0337, 0.0201, 0.0200], grad_fn=<ToCopyBackward0>), [' trying', ' then', ' proud', ' hoping', ' making'])\n",
      "(tensor([9.3235e-01, 5.0483e-02, 6.4520e-03, 4.6498e-03, 8.0550e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' to', ' enough', ' that', ' as'])\n",
      "(tensor([0.5248, 0.0743, 0.0629, 0.0595, 0.0331], grad_fn=<ToCopyBackward0>), ['.', ',', ' for', ' it', ' the'])\n",
      "(tensor([0.1919, 0.1620, 0.1131, 0.0453, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.1789, 0.0675, 0.0435, 0.0431, 0.0386], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' characters', ' only', ' story'])\n",
      "(tensor([0.6074, 0.0808, 0.0281, 0.0251, 0.0134], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' seemed', ' and'])\n",
      "(tensor([0.0761, 0.0494, 0.0429, 0.0374, 0.0367], grad_fn=<ToCopyBackward0>), [' predictable', ' weak', ' too', ' very', ' so'])\n",
      "(tensor([0.5280, 0.0349, 0.0337, 0.0315, 0.0243], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' confusing', ' weak', ' cliché'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie would be great. It is not.I was really looking forward to seeing it, but it was not. It was slow, boring and had a few plot holes. The movie had a lot of potential, but was not. I would\n",
      "(tensor([0.3839, 0.1720, 0.0897, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2437, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6522, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.8301, 0.0816, 0.0135, 0.0115, 0.0080], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' never', ' get'])\n",
      "(tensor([0.1441, 0.1157, 0.0932, 0.0924, 0.0748], grad_fn=<ToCopyBackward0>), [' more', ' better', ' great', ' a', ' really'])\n",
      "(tensor([0.3179, 0.2666, 0.0777, 0.0772, 0.0764], grad_fn=<ToCopyBackward0>), ['.', ' for', ' because', ',', ' to'])\n",
      "(tensor([0.3114, 0.1360, 0.0718, 0.0513, 0.0201], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' So'])\n",
      "(tensor([0.2426, 0.2209, 0.0916, 0.0536, 0.0529], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' has'])\n",
      "(tensor([0.1498, 0.1258, 0.0934, 0.0655, 0.0611], grad_fn=<ToCopyBackward0>), [' not', '.', ',', ' a', ' so'])\n",
      "(tensor([0.5901, 0.0642, 0.0596, 0.0464, 0.0308], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' at', ' that'])\n",
      "(tensor([0.2150, 0.1896, 0.1227, 0.0446, 0.0233], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', 'The', 'I'])\n",
      "(tensor([0.0938, 0.0660, 0.0650, 0.0609, 0.0467], grad_fn=<ToCopyBackward0>), [' am', ' was', ' have', \"'m\", ' think'])\n",
      "(tensor([0.3300, 0.1004, 0.0501, 0.0479, 0.0454], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' expecting', ' not'])\n",
      "(tensor([0.6456, 0.0577, 0.0494, 0.0425, 0.0301], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' hoping', ' excited', ' surprised'])\n",
      "(tensor([9.9555e-01, 2.6168e-03, 7.9290e-04, 2.6924e-04, 2.4733e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' at'])\n",
      "(tensor([9.9074e-01, 4.9087e-03, 6.9240e-04, 5.1710e-04, 4.2665e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' on', ' and'])\n",
      "(tensor([0.2890, 0.2559, 0.2200, 0.0542, 0.0463], grad_fn=<ToCopyBackward0>), [' seeing', ' this', ' watching', ' it', ' the'])\n",
      "(tensor([0.4323, 0.2582, 0.0582, 0.0116, 0.0089], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.2606, 0.2300, 0.1059, 0.0795, 0.0628], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.2193, 0.1284, 0.0852, 0.0580, 0.0487], grad_fn=<ToCopyBackward0>), [' but', ' and', ' since', ' I', ' as'])\n",
      "(tensor([0.2551, 0.1919, 0.1129, 0.0950, 0.0325], grad_fn=<ToCopyBackward0>), [' it', ' I', ' after', ' the', ' was'])\n",
      "(tensor([0.3401, 0.1697, 0.0545, 0.0525, 0.0429], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' really', ' turned'])\n",
      "(tensor([0.2235, 0.1976, 0.1176, 0.0669, 0.0344], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' really'])\n",
      "(tensor([0.2140, 0.1719, 0.0847, 0.0720, 0.0511], grad_fn=<ToCopyBackward0>), ['.', ' at', ' as', ' that', ' funny'])\n",
      "(tensor([0.1395, 0.1203, 0.1099, 0.1010, 0.0983], grad_fn=<ToCopyBackward0>), [' It', ' I', 'I', ' The', 'The'])\n",
      "(tensor([0.4183, 0.1746, 0.1006, 0.0273, 0.0199], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' had', ' has'])\n",
      "(tensor([0.1926, 0.0822, 0.0769, 0.0642, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' slow', ' so'])\n",
      "(tensor([0.3362, 0.2755, 0.1706, 0.0936, 0.0194], grad_fn=<ToCopyBackward0>), [' moving', ' and', ',', ' paced', ' going'])\n",
      "(tensor([0.2961, 0.0692, 0.0550, 0.0367, 0.0240], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' and', ' dull', ' the'])\n",
      "(tensor([6.6049e-01, 3.2726e-01, 1.8256e-03, 1.7683e-03, 6.1964e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' &', ' with'])\n",
      "(tensor([0.1393, 0.0637, 0.0596, 0.0502, 0.0454], grad_fn=<ToCopyBackward0>), [' predictable', ' had', ' not', ' the', ' was'])\n",
      "(tensor([0.3333, 0.1264, 0.0791, 0.0233, 0.0224], grad_fn=<ToCopyBackward0>), [' no', ' a', ' some', ' the', ' nothing'])\n",
      "(tensor([0.1629, 0.0890, 0.0600, 0.0546, 0.0512], grad_fn=<ToCopyBackward0>), [' lot', ' very', ' stupid', ' weak', ' few'])\n",
      "(tensor([0.1362, 0.0716, 0.0534, 0.0455, 0.0307], grad_fn=<ToCopyBackward0>), [' plot', ' stupid', ' flaws', ' funny', ' parts'])\n",
      "(tensor([0.9577, 0.0073, 0.0062, 0.0058, 0.0029], grad_fn=<ToCopyBackward0>), [' holes', ' flaws', ' points', ' twists', '-'])\n",
      "(tensor([0.6420, 0.0621, 0.0554, 0.0521, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ' in', ' that', ',', ' and'])\n",
      "(tensor([0.1518, 0.1187, 0.0801, 0.0772, 0.0619], grad_fn=<ToCopyBackward0>), [' The', ' I', 'The', ' It', 'I'])\n",
      "(tensor([0.2367, 0.0777, 0.0761, 0.0364, 0.0351], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' plot', ' story'])\n",
      "(tensor([0.1940, 0.1140, 0.0950, 0.0472, 0.0329], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' just', ' did'])\n",
      "(tensor([0.2103, 0.0854, 0.0783, 0.0711, 0.0381], grad_fn=<ToCopyBackward0>), [' a', ' no', ' some', ' the', ' so'])\n",
      "(tensor([0.1181, 0.1116, 0.0717, 0.0521, 0.0341], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' great', ' very', ' really'])\n",
      "(tensor([0.9027, 0.0458, 0.0294, 0.0076, 0.0048], grad_fn=<ToCopyBackward0>), [' of', ' more', ' to', ' going', ' in'])\n",
      "(tensor([0.5316, 0.0587, 0.0305, 0.0196, 0.0151], grad_fn=<ToCopyBackward0>), [' potential', ' promise', ' people', ' nudity', ' the'])\n",
      "(tensor([0.4945, 0.1590, 0.1022, 0.0699, 0.0555], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' to'])\n",
      "(tensor([0.8790, 0.0268, 0.0091, 0.0069, 0.0068], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' it', ' if'])\n",
      "(tensor([0.1974, 0.1599, 0.0786, 0.0457, 0.0296], grad_fn=<ToCopyBackward0>), [' it', ' the', ' was', ' I', ' didn'])\n",
      "(tensor([0.1833, 0.1177, 0.0544, 0.0416, 0.0397], grad_fn=<ToCopyBackward0>), [' not', ' just', ' ruined', ' never', ' sadly'])\n",
      "(tensor([0.1386, 0.0525, 0.0488, 0.0403, 0.0383], grad_fn=<ToCopyBackward0>), ['.', ' worthy', ' as', ' able', ' worth'])\n",
      "(tensor([0.1657, 0.1131, 0.1039, 0.0718, 0.0463], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.1001, 0.0740, 0.0698, 0.0695, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' really', ' would', ' think', ' am'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was so bad that I actually made a better film. It's not even close to the worst movie I have ever made, I'm just saying it was so bad that it made the other films in the series look great! This is the\n",
      "(tensor([0.3841, 0.1713, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4381, 0.2434, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7761, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1275, 0.0769, 0.0660, 0.0556, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4452, 0.0487, 0.0405, 0.0300, 0.0288], grad_fn=<ToCopyBackward0>), [' bad', ' atro', ' awful', ' boring', ' terrible'])\n",
      "(tensor([0.2855, 0.2250, 0.2116, 0.0481, 0.0310], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', ' when'])\n",
      "(tensor([0.5353, 0.1432, 0.1194, 0.0342, 0.0270], grad_fn=<ToCopyBackward0>), [' I', ' it', ' i', ' the', ' even'])\n",
      "(tensor([0.1934, 0.0708, 0.0537, 0.0412, 0.0354], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' thought', ' had', ' couldn'])\n",
      "(tensor([0.1438, 0.0973, 0.0583, 0.0455, 0.0413], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' felt', ' started'])\n",
      "(tensor([0.5752, 0.1661, 0.0433, 0.0321, 0.0278], grad_fn=<ToCopyBackward0>), [' a', ' it', ' the', ' myself', ' an'])\n",
      "(tensor([0.5714, 0.0438, 0.0296, 0.0277, 0.0259], grad_fn=<ToCopyBackward0>), [' comment', ' note', ' review', ' list', ' better'])\n",
      "(tensor([0.6046, 0.1199, 0.0707, 0.0268, 0.0200], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' one', ' documentary', ' version'])\n",
      "(tensor([0.2096, 0.1354, 0.0998, 0.0661, 0.0402], grad_fn=<ToCopyBackward0>), [' myself', '.', ' with', ' than', ' just'])\n",
      "(tensor([0.3919, 0.0796, 0.0634, 0.0548, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', 'I'])\n",
      "(tensor([0.4170, 0.2611, 0.0552, 0.0299, 0.0181], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' has'])\n",
      "(tensor([0.1262, 0.1033, 0.1031, 0.0974, 0.0802], grad_fn=<ToCopyBackward0>), [' not', ' so', ' just', ' called', ' like'])\n",
      "(tensor([0.5883, 0.1118, 0.0707, 0.0339, 0.0238], grad_fn=<ToCopyBackward0>), [' even', ' as', ' that', ' a', ' like'])\n",
      "(tensor([0.2832, 0.1484, 0.1218, 0.0617, 0.0550], grad_fn=<ToCopyBackward0>), [' close', ' funny', ' a', ' good', ' that'])\n",
      "(tensor([0.5432, 0.3716, 0.0419, 0.0051, 0.0032], grad_fn=<ToCopyBackward0>), [' to', '.', ',', '!', ' in'])\n",
      "(tensor([0.2143, 0.1469, 0.1320, 0.1227, 0.0289], grad_fn=<ToCopyBackward0>), [' as', ' the', ' being', ' good', ' a'])\n",
      "(tensor([0.2235, 0.1322, 0.0916, 0.0443, 0.0442], grad_fn=<ToCopyBackward0>), [' worst', ' same', ' original', ' first', ' level'])\n",
      "(tensor([0.7113, 0.2139, 0.0252, 0.0066, 0.0024], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' I', ' thing', ' one'])\n",
      "(tensor([0.8142, 0.0787, 0.0350, 0.0282, 0.0132], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' i', ' of', ' that'])\n",
      "(tensor([0.6097, 0.3521, 0.0149, 0.0052, 0.0021], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' saw', ' can'])\n",
      "(tensor([0.8160, 0.1647, 0.0048, 0.0041, 0.0015], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' made', ' watched', ' had'])\n",
      "(tensor([0.8260, 0.0692, 0.0364, 0.0199, 0.0049], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' watched', ' had', ' been'])\n",
      "(tensor([0.3960, 0.3899, 0.0738, 0.0321, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' in', ' and'])\n",
      "(tensor([0.7460, 0.0408, 0.0295, 0.0161, 0.0158], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' though'])\n",
      "(tensor([0.1547, 0.0891, 0.0742, 0.0702, 0.0648], grad_fn=<ToCopyBackward0>), [\"'m\", ' think', ' mean', ' just', ' am'])\n",
      "(tensor([0.1622, 0.1193, 0.1011, 0.0563, 0.0531], grad_fn=<ToCopyBackward0>), [' not', ' just', ' sure', ' talking', ' still'])\n",
      "(tensor([0.1250, 0.0937, 0.0924, 0.0733, 0.0349], grad_fn=<ToCopyBackward0>), [' saying', ' really', ' not', ' a', ' trying'])\n",
      "(tensor([0.3426, 0.1283, 0.0919, 0.0494, 0.0447], grad_fn=<ToCopyBackward0>), [' it', ' that', '.', ' the', ' this'])\n",
      "(tensor([0.3049, 0.2537, 0.0894, 0.0830, 0.0333], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' wasn', ' is', '.'])\n",
      "(tensor([0.2659, 0.1550, 0.0582, 0.0378, 0.0304], grad_fn=<ToCopyBackward0>), [' so', ' better', ' pretty', ' more', ' worse'])\n",
      "(tensor([0.8208, 0.0315, 0.0276, 0.0189, 0.0084], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' horrible', ' much'])\n",
      "(tensor([0.4335, 0.3414, 0.0902, 0.0532, 0.0103], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ',', '.'])\n",
      "(tensor([0.6535, 0.1824, 0.0148, 0.0139, 0.0121], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' the', ' my'])\n",
      "(tensor([0.3092, 0.1029, 0.0690, 0.0497, 0.0382], grad_fn=<ToCopyBackward0>), [' was', ' made', \"'s\", ' became', ' is'])\n",
      "(tensor([0.3269, 0.2807, 0.0402, 0.0386, 0.0312], grad_fn=<ToCopyBackward0>), [' me', ' a', ' something', ' the', ' better'])\n",
      "(tensor([0.2789, 0.0891, 0.0659, 0.0401, 0.0349], grad_fn=<ToCopyBackward0>), [' original', ' other', ' best', ' first', ' script'])\n",
      "(tensor([0.1654, 0.1277, 0.1233, 0.0780, 0.0580], grad_fn=<ToCopyBackward0>), [' movie', ' movies', ' film', ' one', ' films'])\n",
      "(tensor([0.2713, 0.2632, 0.1398, 0.0476, 0.0352], grad_fn=<ToCopyBackward0>), [' in', ' I', ' that', ' better', ' on'])\n",
      "(tensor([0.5387, 0.3295, 0.0423, 0.0211, 0.0189], grad_fn=<ToCopyBackward0>), [' my', ' the', ' this', ' which', ' that'])\n",
      "(tensor([0.8284, 0.0286, 0.0244, 0.0043, 0.0034], grad_fn=<ToCopyBackward0>), [' series', ' genre', ' same', ' process', ' category'])\n",
      "(tensor([0.3173, 0.1837, 0.0547, 0.0454, 0.0290], grad_fn=<ToCopyBackward0>), [' look', ' better', ' so', ' that', ','])\n",
      "(tensor([0.4917, 0.2759, 0.0397, 0.0255, 0.0205], grad_fn=<ToCopyBackward0>), [' like', ' good', ' pretty', ' great', ' better'])\n",
      "(tensor([0.7295, 0.0643, 0.0565, 0.0446, 0.0253], grad_fn=<ToCopyBackward0>), ['.', ' in', '!', '...', ','])\n",
      "(tensor([0.2114, 0.1286, 0.0862, 0.0529, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'The'])\n",
      "(tensor([0.2248, 0.2215, 0.2063, 0.1020, 0.0501], grad_fn=<ToCopyBackward0>), [' is', ' film', ' movie', ' was', ' one'])\n",
      "(tensor([0.1706, 0.1266, 0.0540, 0.0524, 0.0391], grad_fn=<ToCopyBackward0>), [' the', ' a', ' one', ' definitely', ' by'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a really bad movie. I thought the acting was awful. I thought the plot was ridiculous. I was really surprised that I enjoyed the movie so much. But I really enjoyed this movie too. I really did enjoy it. I really do\n",
      "(tensor([0.3835, 0.1721, 0.0904, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1160, 0.0395, 0.0101, 0.0086], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1832, 0.1468, 0.0505, 0.0452, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1642, 0.1158, 0.0582, 0.0491, 0.0444], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.6009, 0.0521, 0.0461, 0.0388, 0.0233], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8276, 0.0668, 0.0244, 0.0073, 0.0053], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' horror', ' sequel'])\n",
      "(tensor([0.6649, 0.0633, 0.0364, 0.0350, 0.0293], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '...', '!'])\n",
      "(tensor([0.2890, 0.1360, 0.0639, 0.0329, 0.0295], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' Really'])\n",
      "(tensor([0.1344, 0.0846, 0.0824, 0.0618, 0.0498], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', ' mean', \"'m\"])\n",
      "(tensor([0.7384, 0.0816, 0.0486, 0.0364, 0.0197], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.1689, 0.1574, 0.0835, 0.0517, 0.0315], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' story', ' plot', ' guy'])\n",
      "(tensor([0.8583, 0.0497, 0.0163, 0.0129, 0.0124], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' in', ' and'])\n",
      "(tensor([0.2888, 0.1132, 0.0922, 0.0655, 0.0592], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' horrible', ' awful', ' really'])\n",
      "(tensor([0.5081, 0.2160, 0.2018, 0.0189, 0.0076], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ';'])\n",
      "(tensor([0.3205, 0.1497, 0.1167, 0.0923, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' The', ' And', ' It', 'I'])\n",
      "(tensor([0.2675, 0.0811, 0.0685, 0.0516, 0.0501], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' really', ' mean', ' don'])\n",
      "(tensor([0.6268, 0.1988, 0.0282, 0.0274, 0.0230], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' that', ' they'])\n",
      "(tensor([0.2603, 0.2018, 0.1471, 0.0424, 0.0348], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' writing', ' movie'])\n",
      "(tensor([0.9054, 0.0130, 0.0077, 0.0070, 0.0054], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' wasn', ',', ' had'])\n",
      "(tensor([0.1677, 0.1260, 0.0745, 0.0597, 0.0457], grad_fn=<ToCopyBackward0>), [' stupid', ' ridiculous', ' terrible', ' really', ' awful'])\n",
      "(tensor([0.6171, 0.2443, 0.0722, 0.0044, 0.0043], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ';'])\n",
      "(tensor([0.3768, 0.1360, 0.0628, 0.0549, 0.0513], grad_fn=<ToCopyBackward0>), [' I', ' And', ' The', ' It', ' But'])\n",
      "(tensor([0.3402, 0.0941, 0.0645, 0.0547, 0.0527], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' just', ' really'])\n",
      "(tensor([0.2924, 0.1324, 0.0458, 0.0436, 0.0422], grad_fn=<ToCopyBackward0>), [' really', ' very', ' a', ' not', ' so'])\n",
      "(tensor([0.3719, 0.1281, 0.1226, 0.0340, 0.0187], grad_fn=<ToCopyBackward0>), [' disappointed', ',', ' surprised', ' annoyed', ' offended'])\n",
      "(tensor([0.3078, 0.1796, 0.1687, 0.1182, 0.0728], grad_fn=<ToCopyBackward0>), [' when', ' that', ' by', ' at', ' to'])\n",
      "(tensor([0.1379, 0.1111, 0.0846, 0.0669, 0.0637], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' a'])\n",
      "(tensor([0.1033, 0.0936, 0.0766, 0.0541, 0.0519], grad_fn=<ToCopyBackward0>), [' liked', ' was', ' enjoyed', ' actually', ' watched'])\n",
      "(tensor([0.5631, 0.2027, 0.1347, 0.0211, 0.0109], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' watching', ' that'])\n",
      "(tensor([0.8524, 0.0262, 0.0178, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' whole', ' book', ' original'])\n",
      "(tensor([0.2443, 0.2323, 0.0929, 0.0820, 0.0647], grad_fn=<ToCopyBackward0>), ['.', ' at', ' so', ',', ' as'])\n",
      "(tensor([9.9760e-01, 3.2446e-04, 2.5675e-04, 2.5562e-04, 1.9618e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' much', ' little', ' well', ' very', ' that'])\n",
      "(tensor([0.7481, 0.0835, 0.0306, 0.0202, 0.0166], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' after', ' when'])\n",
      "(tensor([0.3878, 0.1187, 0.0416, 0.0358, 0.0317], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' But'])\n",
      "(tensor([0.2661, 0.1057, 0.0701, 0.0665, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' after'])\n",
      "(tensor([0.1342, 0.1139, 0.0555, 0.0537, 0.0383], grad_fn=<ToCopyBackward0>), [' was', ' really', ' did', ' just', ' have'])\n",
      "(tensor([0.0855, 0.0801, 0.0693, 0.0643, 0.0601], grad_fn=<ToCopyBackward0>), [' didn', ' don', ' enjoyed', ' was', ' did'])\n",
      "(tensor([0.4575, 0.2037, 0.0911, 0.0376, 0.0228], grad_fn=<ToCopyBackward0>), [' it', ' the', ' watching', ' this', ' \"'])\n",
      "(tensor([0.5811, 0.1214, 0.0512, 0.0463, 0.0166], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' episode', '.'])\n",
      "(tensor([0.2600, 0.1527, 0.0840, 0.0818, 0.0564], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' too', ' a'])\n",
      "(tensor([0.8556, 0.0789, 0.0094, 0.0093, 0.0065], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' because', ' so'])\n",
      "(tensor([0.4425, 0.1755, 0.0387, 0.0258, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.2199, 0.0819, 0.0683, 0.0653, 0.0494], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' think', ' was', ' just'])\n",
      "(tensor([0.2009, 0.0942, 0.0825, 0.0712, 0.0671], grad_fn=<ToCopyBackward0>), [' enjoyed', ' like', ' liked', ' thought', ' did'])\n",
      "(tensor([0.4104, 0.3171, 0.1434, 0.0343, 0.0212], grad_fn=<ToCopyBackward0>), [' enjoy', '.', ' like', ' not', ','])\n",
      "(tensor([0.5757, 0.2140, 0.0918, 0.0310, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' watching', ' \"'])\n",
      "(tensor([0.4811, 0.1149, 0.0814, 0.0328, 0.0274], grad_fn=<ToCopyBackward0>), ['.', ',', ' a', ' in', ' more'])\n",
      "(tensor([0.4021, 0.1355, 0.0445, 0.0388, 0.0324], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', 'I', ' So'])\n",
      "(tensor([0.2526, 0.0868, 0.0622, 0.0521, 0.0514], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' just', ' think'])\n",
      "(tensor([0.4794, 0.0713, 0.0650, 0.0593, 0.0342], grad_fn=<ToCopyBackward0>), [' did', ' enjoyed', ' do', ' really', ' thought'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was watching a bad soap opera until I saw this horrid thing in the middle of the road in the middle of the night. I was truly shocked to see this in the middle of a busy road in my hometown of Regina, Canada. I\n",
      "(tensor([0.3834, 0.1723, 0.0903, 0.0770, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2250, 0.1946, 0.1573, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4599, 0.2360, 0.0590, 0.0170, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.5143, 0.1712, 0.0812, 0.0384, 0.0320], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' this', ' some'])\n",
      "(tensor([0.0846, 0.0680, 0.0573, 0.0535, 0.0469], grad_fn=<ToCopyBackward0>), [' good', ' really', ' horror', ' bad', ' movie'])\n",
      "(tensor([0.1895, 0.1225, 0.0917, 0.0593, 0.0405], grad_fn=<ToCopyBackward0>), [' comedy', ' movie', ' soap', ' acid', ' porn'])\n",
      "(tensor([0.9160, 0.0114, 0.0106, 0.0100, 0.0073], grad_fn=<ToCopyBackward0>), [' opera', '-', ' oper', ' movie', ','])\n",
      "(tensor([0.3453, 0.1797, 0.1248, 0.0433, 0.0353], grad_fn=<ToCopyBackward0>), [' when', '.', ',', ' until', ' and'])\n",
      "(tensor([0.8802, 0.0275, 0.0236, 0.0145, 0.0142], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' my'])\n",
      "(tensor([0.2042, 0.1849, 0.1176, 0.0637, 0.0488], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' realized', ' started', ' watched'])\n",
      "(tensor([0.3291, 0.2484, 0.0757, 0.0460, 0.0250], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' that', ' my'])\n",
      "(tensor([0.1683, 0.1150, 0.0761, 0.0408, 0.0385], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' one', ' on', ' hor'])\n",
      "(tensor([9.9195e-01, 1.4938e-03, 5.4124e-04, 5.2939e-04, 3.8777e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['rid', 'sey', 'c', 'r', 'nd'])\n",
      "(tensor([0.2588, 0.1607, 0.0795, 0.0700, 0.0324], grad_fn=<ToCopyBackward0>), [' thing', ' little', ' scene', ',', ' CGI'])\n",
      "(tensor([0.4743, 0.1139, 0.0447, 0.0391, 0.0296], grad_fn=<ToCopyBackward0>), ['.', ' in', ' on', ':', '!'])\n",
      "(tensor([0.5761, 0.3138, 0.0316, 0.0092, 0.0056], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' its', ' this'])\n",
      "(tensor([0.1667, 0.1287, 0.0520, 0.0487, 0.0340], grad_fn=<ToCopyBackward0>), [' middle', ' sky', ' background', ' corner', ' summer'])\n",
      "(tensor([9.8557e-01, 1.3800e-03, 1.2653e-03, 7.0500e-04, 3.3893e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' east', '.', ' ground', ' and'])\n",
      "(tensor([0.7948, 0.0357, 0.0290, 0.0186, 0.0169], grad_fn=<ToCopyBackward0>), [' the', ' nowhere', ' my', ' a', ' it'])\n",
      "(tensor([0.3299, 0.0328, 0.0326, 0.0287, 0.0233], grad_fn=<ToCopyBackward0>), [' road', ' woods', ' screen', ' street', ' movie'])\n",
      "(tensor([0.4743, 0.0505, 0.0468, 0.0334, 0.0307], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' with'])\n",
      "(tensor([0.7984, 0.0377, 0.0321, 0.0225, 0.0068], grad_fn=<ToCopyBackward0>), [' the', ' my', ' front', ' a', ' New'])\n",
      "(tensor([0.8389, 0.0244, 0.0160, 0.0056, 0.0050], grad_fn=<ToCopyBackward0>), [' middle', ' pouring', ' early', ' south', ' South'])\n",
      "(tensor([9.9779e-01, 1.1346e-04, 8.9741e-05, 7.7389e-05, 4.0067e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '-', ' o', ' east', ' and'])\n",
      "(tensor([0.9465, 0.0084, 0.0045, 0.0045, 0.0032], grad_fn=<ToCopyBackward0>), [' the', ' a', ' nowhere', ' my', ' South'])\n",
      "(tensor([0.6718, 0.0776, 0.0162, 0.0156, 0.0154], grad_fn=<ToCopyBackward0>), [' day', ' night', ' middle', ' city', ' country'])\n",
      "(tensor([0.5593, 0.0559, 0.0344, 0.0246, 0.0206], grad_fn=<ToCopyBackward0>), ['.', '...', ',', ' when', ' while'])\n",
      "(tensor([0.4100, 0.1423, 0.0586, 0.0253, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' No', ' A'])\n",
      "(tensor([0.1295, 0.1182, 0.0802, 0.0495, 0.0457], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' have', ' mean'])\n",
      "(tensor([0.1457, 0.0919, 0.0757, 0.0486, 0.0481], grad_fn=<ToCopyBackward0>), [' really', ' so', ' truly', ' actually', ' very'])\n",
      "(tensor([0.2149, 0.1638, 0.1246, 0.0781, 0.0282], grad_fn=<ToCopyBackward0>), [' fre', ' shocked', ' scared', ' surprised', ' in'])\n",
      "(tensor([0.3203, 0.1493, 0.1277, 0.1007, 0.0745], grad_fn=<ToCopyBackward0>), [' to', '.', ' and', ' by', ' when'])\n",
      "(tensor([0.7656, 0.0927, 0.0229, 0.0171, 0.0148], grad_fn=<ToCopyBackward0>), [' see', ' find', ' watch', ' say', ' be'])\n",
      "(tensor([0.3802, 0.1411, 0.0920, 0.0870, 0.0705], grad_fn=<ToCopyBackward0>), [' what', ' it', ' the', ' this', ' that'])\n",
      "(tensor([0.3816, 0.1066, 0.0468, 0.0276, 0.0230], grad_fn=<ToCopyBackward0>), ['.', ' in', ' thing', '!', ' on'])\n",
      "(tensor([0.5271, 0.3560, 0.0222, 0.0193, 0.0053], grad_fn=<ToCopyBackward0>), [' my', ' the', ' a', ' our', ' this'])\n",
      "(tensor([0.6998, 0.0793, 0.0289, 0.0096, 0.0078], grad_fn=<ToCopyBackward0>), [' middle', ' UK', ' daylight', ' road', ' dark'])\n",
      "(tensor([9.9876e-01, 3.9991e-05, 3.0721e-05, 2.7236e-05, 2.4720e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' on', ' in', '.', ' lane'])\n",
      "(tensor([0.8145, 0.0632, 0.0488, 0.0050, 0.0035], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' an', ' our'])\n",
      "(tensor([0.2995, 0.0942, 0.0417, 0.0329, 0.0305], grad_fn=<ToCopyBackward0>), [' busy', ' residential', ' dark', ' public', ' road'])\n",
      "(tensor([0.3580, 0.1142, 0.1121, 0.0948, 0.0415], grad_fn=<ToCopyBackward0>), [' road', ' highway', ' street', ' intersection', ' city'])\n",
      "(tensor([0.3141, 0.2453, 0.0958, 0.0504, 0.0407], grad_fn=<ToCopyBackward0>), [' in', '.', ' and', ',', ' with'])\n",
      "(tensor([0.6804, 0.1111, 0.0540, 0.0322, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' my', ' broad', ' a', ' New'])\n",
      "(tensor([0.3752, 0.1579, 0.0676, 0.0414, 0.0314], grad_fn=<ToCopyBackward0>), [' hometown', ' home', ' own', ' town', ' country'])\n",
      "(tensor([0.3873, 0.1554, 0.0361, 0.0321, 0.0255], grad_fn=<ToCopyBackward0>), ['.', ' of', '.\"', ' town', ','])\n",
      "(tensor([0.2154, 0.0576, 0.0360, 0.0324, 0.0267], grad_fn=<ToCopyBackward0>), [' Regina', ' Chicago', ' Ann', ' Toronto', ' New'])\n",
      "(tensor([0.6612, 0.1592, 0.0223, 0.0196, 0.0195], grad_fn=<ToCopyBackward0>), [',', '.', ' (', ' Saskatchewan', '.\"'])\n",
      "(tensor([0.8879, 0.0547, 0.0098, 0.0092, 0.0061], grad_fn=<ToCopyBackward0>), [' Saskatchewan', ' Canada', ' Manitoba', ' Sask', ' Regina'])\n",
      "(tensor([0.8730, 0.0328, 0.0108, 0.0104, 0.0102], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '.\"', ' ('])\n",
      "(tensor([0.4617, 0.0879, 0.0536, 0.0192, 0.0171], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would never watch it again. I thought it would be the one movie I wouldn't watch. I'm a movie-lover. I'm a movie-lover. But I watched this movie just to see what was in store for the\n",
      "(tensor([0.3840, 0.1715, 0.0898, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2241, 0.1944, 0.1575, 0.0695, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2254, 0.1218, 0.0840, 0.0368, 0.0235], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.3918, 0.0916, 0.0783, 0.0620, 0.0475], grad_fn=<ToCopyBackward0>), [' see', ' get', ' be', ' watch', ' find'])\n",
      "(tensor([0.4014, 0.2581, 0.0892, 0.0498, 0.0225], grad_fn=<ToCopyBackward0>), [' this', ' it', ' anything', ' a', ' any'])\n",
      "(tensor([0.4597, 0.2126, 0.0781, 0.0285, 0.0236], grad_fn=<ToCopyBackward0>), [' again', '.', ',', ' because', ' until'])\n",
      "(tensor([0.6431, 0.1704, 0.0538, 0.0357, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ' after', ',', ' because', ' but'])\n",
      "(tensor([0.4027, 0.1076, 0.0757, 0.0301, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' How'])\n",
      "(tensor([0.1532, 0.0574, 0.0525, 0.0430, 0.0430], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' watched', \"'m\", ' am'])\n",
      "(tensor([0.4324, 0.1771, 0.1231, 0.0649, 0.0421], grad_fn=<ToCopyBackward0>), [' it', ' I', ' this', ' the', ' that'])\n",
      "(tensor([0.5239, 0.3597, 0.0271, 0.0156, 0.0141], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' might', ' must'])\n",
      "(tensor([0.6981, 0.0855, 0.0537, 0.0377, 0.0191], grad_fn=<ToCopyBackward0>), [' be', ' never', ' have', ' go', ' make'])\n",
      "(tensor([0.2877, 0.1020, 0.0819, 0.0725, 0.0511], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' too', ' like'])\n",
      "(tensor([0.1918, 0.1615, 0.0855, 0.0782, 0.0734], grad_fn=<ToCopyBackward0>), [' last', ' worst', ' same', ' end', ' one'])\n",
      "(tensor([0.1914, 0.0885, 0.0698, 0.0460, 0.0396], grad_fn=<ToCopyBackward0>), [' movie', ' and', ' time', '.', ' that'])\n",
      "(tensor([0.4864, 0.3870, 0.0277, 0.0275, 0.0157], grad_fn=<ToCopyBackward0>), [' that', ' I', ' i', ' where', ' my'])\n",
      "(tensor([0.5313, 0.1151, 0.0814, 0.0549, 0.0334], grad_fn=<ToCopyBackward0>), [' would', \"'d\", ' could', ' couldn', ' wouldn'])\n",
      "(tensor([9.9541e-01, 1.5765e-03, 3.6455e-04, 2.8853e-04, 2.6109e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.3331, 0.1489, 0.0604, 0.0414, 0.0407], grad_fn=<ToCopyBackward0>), [' watch', ' ever', ' want', ' laugh', ' even'])\n",
      "(tensor([0.5520, 0.1346, 0.0689, 0.0423, 0.0343], grad_fn=<ToCopyBackward0>), [' again', '.', ' at', ' in', ','])\n",
      "(tensor([0.3031, 0.1097, 0.0726, 0.0458, 0.0319], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' Then', ' And'])\n",
      "(tensor([0.1832, 0.0948, 0.0610, 0.0410, 0.0372], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' watched', ' actually'])\n",
      "(tensor([0.1518, 0.1445, 0.1124, 0.0629, 0.0573], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' really', ' actually'])\n",
      "(tensor([0.3262, 0.1398, 0.0594, 0.0315, 0.0184], grad_fn=<ToCopyBackward0>), [' big', ' fan', ' huge', ' movie', ' film'])\n",
      "(tensor([0.2382, 0.0956, 0.0810, 0.0792, 0.0692], grad_fn=<ToCopyBackward0>), ['-', ' guy', ' person', ' buff', ' sn'])\n",
      "(tensor([0.3636, 0.1437, 0.0602, 0.0308, 0.0278], grad_fn=<ToCopyBackward0>), ['l', 'go', 'j', 'buff', 'loving'])\n",
      "(tensor([0.9427, 0.0140, 0.0052, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), ['over', 'oser', 'ifer', 'one', 'ion'])\n",
      "(tensor([0.4345, 0.2269, 0.1119, 0.0494, 0.0158], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' at', ';'])\n",
      "(tensor([0.6660, 0.0728, 0.0527, 0.0195, 0.0146], grad_fn=<ToCopyBackward0>), [' I', ' But', ' And', ' It', ' So'])\n",
      "(tensor([0.1388, 0.0769, 0.0552, 0.0522, 0.0491], grad_fn=<ToCopyBackward0>), [' love', \"'m\", ' like', ' can', ' just'])\n",
      "(tensor([0.4226, 0.1704, 0.0512, 0.0311, 0.0256], grad_fn=<ToCopyBackward0>), [' a', ' not', ' an', ' always', ' the'])\n",
      "(tensor([0.2385, 0.1811, 0.1277, 0.0745, 0.0506], grad_fn=<ToCopyBackward0>), [' movie', ' big', ' fan', ' film', ' huge'])\n",
      "(tensor([0.7980, 0.0276, 0.0171, 0.0168, 0.0160], grad_fn=<ToCopyBackward0>), ['-', ' buff', ' freak', ' fan', ' lover'])\n",
      "(tensor([0.7833, 0.0318, 0.0198, 0.0126, 0.0124], grad_fn=<ToCopyBackward0>), ['l', 'h', 's', 'go', 'w'])\n",
      "(tensor([0.9129, 0.0066, 0.0057, 0.0050, 0.0043], grad_fn=<ToCopyBackward0>), ['over', 'ifer', 'ob', 'oved', 'iver'])\n",
      "(tensor([0.2080, 0.1364, 0.1325, 0.0900, 0.0376], grad_fn=<ToCopyBackward0>), [' to', ' at', '.', ',', ' for'])\n",
      "(tensor([0.4528, 0.1248, 0.0916, 0.0403, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' But', ' And', ' So', ' It'])\n",
      "(tensor([0.3049, 0.2483, 0.0430, 0.0336, 0.0328], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' after', ' the'])\n",
      "(tensor([0.1128, 0.1010, 0.0985, 0.0452, 0.0410], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' just', \"'m\", ' really'])\n",
      "(tensor([0.7210, 0.1652, 0.0482, 0.0102, 0.0045], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' \"', ' that'])\n",
      "(tensor([0.7574, 0.0747, 0.0335, 0.0135, 0.0088], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', ' just', ' because'])\n",
      "(tensor([0.3568, 0.0589, 0.0474, 0.0353, 0.0344], grad_fn=<ToCopyBackward0>), [' just', ' and', ',', '.', ' the'])\n",
      "(tensor([0.3278, 0.2092, 0.2086, 0.0457, 0.0318], grad_fn=<ToCopyBackward0>), [' a', ' to', ' the', ' one', ' because'])\n",
      "(tensor([0.5724, 0.0608, 0.0553, 0.0329, 0.0257], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' watch', ' laugh'])\n",
      "(tensor([0.1543, 0.1378, 0.1198, 0.0914, 0.0703], grad_fn=<ToCopyBackward0>), [' how', ' if', ' what', ' it', ' the'])\n",
      "(tensor([0.2904, 0.1115, 0.0602, 0.0495, 0.0362], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' was', ' they'])\n",
      "(tensor([0.2520, 0.2388, 0.2326, 0.0297, 0.0203], grad_fn=<ToCopyBackward0>), [' in', ' on', ' going', ' at', ' so'])\n",
      "(tensor([0.5408, 0.0973, 0.0892, 0.0660, 0.0617], grad_fn=<ToCopyBackward0>), [' it', ' store', ' this', ' the', ' there'])\n",
      "(tensor([0.8336, 0.1135, 0.0145, 0.0058, 0.0049], grad_fn=<ToCopyBackward0>), [' for', '.', ',', ' and', ' after'])\n",
      "(tensor([0.2021, 0.0702, 0.0638, 0.0218, 0.0216], grad_fn=<ToCopyBackward0>), [' the', ' us', ' me', ' my', ' this'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it could be the worst movie ever made, and it was. The acting was horrible. The plot was ridiculous. It was a complete joke. And that's why I'm a fan of it: because it was so terrible. The plot was ridiculous\n",
      "(tensor([0.3837, 0.1720, 0.0900, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.5409, 0.2984, 0.0181, 0.0169, 0.0167], grad_fn=<ToCopyBackward0>), [' be', ' have', ' only', ' not', ' make'])\n",
      "(tensor([0.4211, 0.1057, 0.1011, 0.0476, 0.0408], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' the', ' an', ' better'])\n",
      "(tensor([0.1704, 0.1415, 0.0383, 0.0364, 0.0338], grad_fn=<ToCopyBackward0>), [' worst', ' movie', ' best', ' film', ' same'])\n",
      "(tensor([0.8054, 0.0750, 0.0168, 0.0110, 0.0103], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' horror', ' comedy', ' Christmas'])\n",
      "(tensor([0.6224, 0.2171, 0.0759, 0.0281, 0.0112], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' i', ' of', ' in'])\n",
      "(tensor([0.6445, 0.1518, 0.0615, 0.0148, 0.0090], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' if', '...'])\n",
      "(tensor([0.5466, 0.2410, 0.0296, 0.0194, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ',', ' if', '!', '...'])\n",
      "(tensor([0.3214, 0.1518, 0.0710, 0.0228, 0.0201], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' the', ' then'])\n",
      "(tensor([0.2952, 0.1935, 0.0710, 0.0634, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' that', ' the'])\n",
      "(tensor([0.2902, 0.2753, 0.0874, 0.0398, 0.0225], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' is', \"'s\", ' turned'])\n",
      "(tensor([0.6324, 0.0683, 0.0331, 0.0228, 0.0146], grad_fn=<ToCopyBackward0>), ['.', ' not', ',', ' so', ' funny'])\n",
      "(tensor([0.2806, 0.1129, 0.0975, 0.0370, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0978, 0.0884, 0.0671, 0.0671, 0.0567], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' movie', ' plot'])\n",
      "(tensor([0.5918, 0.1714, 0.0760, 0.0219, 0.0204], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' and', '?'])\n",
      "(tensor([0.1816, 0.1047, 0.0816, 0.0733, 0.0658], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' so', ' horrible'])\n",
      "(tensor([0.5288, 0.2408, 0.1589, 0.0108, 0.0073], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ';', ' ('])\n",
      "(tensor([0.5557, 0.0908, 0.0666, 0.0313, 0.0131], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.3227, 0.1727, 0.0836, 0.0346, 0.0274], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' only'])\n",
      "(tensor([0.8442, 0.0157, 0.0110, 0.0108, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' had', ' sounded'])\n",
      "(tensor([0.1542, 0.0783, 0.0600, 0.0441, 0.0406], grad_fn=<ToCopyBackward0>), [' ridiculous', ' stupid', ' terrible', ' predictable', ' even'])\n",
      "(tensor([0.6953, 0.1732, 0.0867, 0.0068, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ';'])\n",
      "(tensor([0.5475, 0.0991, 0.0801, 0.0614, 0.0120], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.4398, 0.1241, 0.0619, 0.0360, 0.0356], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' wasn', ' seemed'])\n",
      "(tensor([0.1262, 0.0958, 0.0884, 0.0586, 0.0272], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' like', ' the'])\n",
      "(tensor([0.1251, 0.0831, 0.0613, 0.0431, 0.0343], grad_fn=<ToCopyBackward0>), [' big', ' bad', ' complete', ' total', ' mess'])\n",
      "(tensor([0.3197, 0.1240, 0.0817, 0.0444, 0.0403], grad_fn=<ToCopyBackward0>), [' waste', ' joke', ' disaster', ' mess', ' rip'])\n",
      "(tensor([0.8385, 0.0322, 0.0270, 0.0177, 0.0153], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' from', ' with'])\n",
      "(tensor([0.2037, 0.1379, 0.1208, 0.0369, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', 'I'])\n",
      "(tensor([0.2796, 0.1850, 0.0801, 0.0603, 0.0305], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' that', ','])\n",
      "(tensor([0.3581, 0.2894, 0.1388, 0.0479, 0.0082], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' movie', ' makes'])\n",
      "(tensor([0.2792, 0.1491, 0.0659, 0.0570, 0.0521], grad_fn=<ToCopyBackward0>), [' not', ' why', ' the', ' just', ' saying'])\n",
      "(tensor([0.4660, 0.3218, 0.0409, 0.0250, 0.0160], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' people'])\n",
      "(tensor([0.1147, 0.1056, 0.0644, 0.0372, 0.0343], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' watched', ' stopped', ' didn'])\n",
      "(tensor([0.2395, 0.1056, 0.0712, 0.0654, 0.0591], grad_fn=<ToCopyBackward0>), [' not', ' a', ' giving', ' still', ' really'])\n",
      "(tensor([0.1913, 0.1412, 0.1184, 0.0392, 0.0312], grad_fn=<ToCopyBackward0>), [' movie', ' fan', ' film', ' huge', ' big'])\n",
      "(tensor([0.9394, 0.0116, 0.0092, 0.0078, 0.0065], grad_fn=<ToCopyBackward0>), [' of', ' now', '.', ',', ' for'])\n",
      "(tensor([0.1053, 0.0930, 0.0396, 0.0315, 0.0220], grad_fn=<ToCopyBackward0>), [' the', ' it', ' bad', ' movies', ' low'])\n",
      "(tensor([0.4430, 0.1341, 0.0549, 0.0536, 0.0327], grad_fn=<ToCopyBackward0>), ['.', ',', ':', ' now', ' -'])\n",
      "(tensor([0.2052, 0.1864, 0.1674, 0.1117, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' because', ' It', ' Because', ' it'])\n",
      "(tensor([0.3420, 0.2915, 0.0845, 0.0289, 0.0221], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' of', ' there'])\n",
      "(tensor([0.2870, 0.2812, 0.0810, 0.0356, 0.0199], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' had'])\n",
      "(tensor([0.5403, 0.0507, 0.0450, 0.0345, 0.0259], grad_fn=<ToCopyBackward0>), [' so', ' funny', ' a', ' bad', ' such'])\n",
      "(tensor([0.4597, 0.0747, 0.0563, 0.0482, 0.0289], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' stupid', ' ridiculous', ' funny'])\n",
      "(tensor([0.7431, 0.0734, 0.0526, 0.0264, 0.0193], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' and', ' that'])\n",
      "(tensor([0.1866, 0.1173, 0.0470, 0.0402, 0.0390], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' The'])\n",
      "(tensor([0.1181, 0.0687, 0.0313, 0.0284, 0.0280], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' plot', ' worst', ' people'])\n",
      "(tensor([0.6220, 0.0777, 0.0230, 0.0212, 0.0194], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' had', ' makes'])\n",
      "(tensor([0.1909, 0.1318, 0.1291, 0.0455, 0.0415], grad_fn=<ToCopyBackward0>), [' so', ' ridiculous', ' stupid', ' just', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty funny how much the film was dated in the movie world, and I really enjoyed watching it because it was so dated, I just really enjoyed watching it. But I really don't think I would recommend it. It's not funny,\n",
      "(tensor([0.3844, 0.1713, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7133, 0.1165, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1843, 0.1452, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1139, 0.0954, 0.0748, 0.0613, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.2842, 0.0860, 0.0656, 0.0651, 0.0540], grad_fn=<ToCopyBackward0>), [' when', ' how', ' to', ' watching', ' and'])\n",
      "(tensor([0.1828, 0.1761, 0.0659, 0.0386, 0.0307], grad_fn=<ToCopyBackward0>), [' the', ' many', ' much', ' some', ' a'])\n",
      "(tensor([0.0807, 0.0645, 0.0576, 0.0523, 0.0398], grad_fn=<ToCopyBackward0>), [' of', ' they', ' money', ' the', ' it'])\n",
      "(tensor([0.1684, 0.0936, 0.0605, 0.0602, 0.0376], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' writers', ' DVD', ' script'])\n",
      "(tensor([0.2903, 0.0424, 0.0403, 0.0316, 0.0281], grad_fn=<ToCopyBackward0>), [' was', ' resembled', ' tried', ' is', ' had'])\n",
      "(tensor([0.1584, 0.0391, 0.0369, 0.0315, 0.0282], grad_fn=<ToCopyBackward0>), [' based', ' about', ' dated', ' compared', ' focused'])\n",
      "(tensor([0.1934, 0.1904, 0.1308, 0.1031, 0.0562], grad_fn=<ToCopyBackward0>), [' in', ' and', '.', ',', ' at'])\n",
      "(tensor([0.1522, 0.1106, 0.0657, 0.0355, 0.0349], grad_fn=<ToCopyBackward0>), [' the', ' 2001', ' its', ' terms', ' spots'])\n",
      "(tensor([0.1593, 0.0864, 0.0558, 0.0477, 0.0386], grad_fn=<ToCopyBackward0>), [' first', ' beginning', ' way', ' movie', ' 80'])\n",
      "(tensor([0.1186, 0.0978, 0.0556, 0.0419, 0.0258], grad_fn=<ToCopyBackward0>), ['.', ' industry', ',', ' theater', ' world'])\n",
      "(tensor([0.1695, 0.1241, 0.1051, 0.0889, 0.0794], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' at', ' back'])\n",
      "(tensor([0.1591, 0.1536, 0.0400, 0.0392, 0.0371], grad_fn=<ToCopyBackward0>), [' but', ' and', ' especially', ' how', ' the'])\n",
      "(tensor([0.1608, 0.1408, 0.1161, 0.0751, 0.0678], grad_fn=<ToCopyBackward0>), [' that', ' I', ' how', ' the', ' then'])\n",
      "(tensor([0.1175, 0.1097, 0.1021, 0.0641, 0.0564], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' really', ' just'])\n",
      "(tensor([0.1496, 0.1089, 0.0976, 0.0747, 0.0452], grad_fn=<ToCopyBackward0>), [' enjoyed', ' wanted', ' liked', ' didn', ' don'])\n",
      "(tensor([0.2886, 0.2006, 0.1759, 0.0497, 0.0294], grad_fn=<ToCopyBackward0>), [' the', ' it', ' watching', ' seeing', ' how'])\n",
      "(tensor([0.4384, 0.1369, 0.0174, 0.0171, 0.0128], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' all', ' this'])\n",
      "(tensor([0.3785, 0.1140, 0.0789, 0.0731, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ' in', ' as', ',', ' because'])\n",
      "(tensor([0.2872, 0.2193, 0.1141, 0.1066, 0.0334], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.4886, 0.0805, 0.0473, 0.0384, 0.0289], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' wasn', ' had'])\n",
      "(tensor([0.4200, 0.0699, 0.0460, 0.0430, 0.0332], grad_fn=<ToCopyBackward0>), [' so', ' like', ' a', ' such', ' very'])\n",
      "(tensor([0.1517, 0.1046, 0.0900, 0.0484, 0.0388], grad_fn=<ToCopyBackward0>), [' bad', ' low', ' dated', ' much', ' over'])\n",
      "(tensor([0.4255, 0.1938, 0.1196, 0.0802, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' in', ' and', ',', ' at'])\n",
      "(tensor([0.2809, 0.2764, 0.0428, 0.0380, 0.0245], grad_fn=<ToCopyBackward0>), [' and', ' but', ' I', ' it', ' so'])\n",
      "(tensor([0.1205, 0.1110, 0.1099, 0.1007, 0.0703], grad_fn=<ToCopyBackward0>), [' thought', ' just', ' really', ' mean', ' was'])\n",
      "(tensor([0.3266, 0.0790, 0.0562, 0.0452, 0.0422], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' think', ' couldn', ' didn'])\n",
      "(tensor([0.4154, 0.0972, 0.0784, 0.0492, 0.0427], grad_fn=<ToCopyBackward0>), [' enjoyed', ' liked', ' thought', ' wanted', ' enjoy'])\n",
      "(tensor([0.4461, 0.2322, 0.1235, 0.0466, 0.0227], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' seeing', ' how'])\n",
      "(tensor([0.5959, 0.1255, 0.0191, 0.0112, 0.0106], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' all', ' this'])\n",
      "(tensor([0.5143, 0.1056, 0.0899, 0.0636, 0.0561], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' and', ' in'])\n",
      "(tensor([0.2695, 0.1020, 0.0821, 0.0628, 0.0339], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' The'])\n",
      "(tensor([0.1206, 0.1013, 0.0882, 0.0748, 0.0509], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' when', ' then'])\n",
      "(tensor([0.1213, 0.0944, 0.0739, 0.0432, 0.0394], grad_fn=<ToCopyBackward0>), [' really', ' was', ' just', ' don', ' also'])\n",
      "(tensor([0.1282, 0.0743, 0.0705, 0.0697, 0.0658], grad_fn=<ToCopyBackward0>), [' enjoyed', ' wanted', ' didn', ' don', ' enjoy'])\n",
      "(tensor([9.9632e-01, 1.1528e-03, 5.4706e-04, 2.1709e-04, 1.5447e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3201, 0.1647, 0.1125, 0.1019, 0.0412], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' remember', ' want'])\n",
      "(tensor([0.4339, 0.2745, 0.0458, 0.0422, 0.0354], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' I', ' the'])\n",
      "(tensor([0.1490, 0.1404, 0.0782, 0.0695, 0.0602], grad_fn=<ToCopyBackward0>), [\"'m\", ' would', ' was', ' could', ' can'])\n",
      "(tensor([0.2354, 0.1310, 0.0871, 0.0688, 0.0510], grad_fn=<ToCopyBackward0>), [' have', ' recommend', ' watch', ' ever', ' like'])\n",
      "(tensor([0.5529, 0.1622, 0.0874, 0.0261, 0.0231], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' people', ' watching'])\n",
      "(tensor([0.3178, 0.2413, 0.0813, 0.0762, 0.0575], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' as', ' for'])\n",
      "(tensor([0.3345, 0.1518, 0.0506, 0.0288, 0.0244], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' If'])\n",
      "(tensor([0.4979, 0.1798, 0.0646, 0.0494, 0.0225], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' just', ' has'])\n",
      "(tensor([0.1672, 0.1102, 0.1027, 0.0927, 0.0815], grad_fn=<ToCopyBackward0>), [' just', ' dated', ' not', ' a', ' kind'])\n",
      "(tensor([0.3740, 0.0919, 0.0750, 0.0739, 0.0738], grad_fn=<ToCopyBackward0>), [' really', ' that', ' funny', ' as', ' even'])\n",
      "(tensor([0.3195, 0.2370, 0.0799, 0.0402, 0.0399], grad_fn=<ToCopyBackward0>), [',', '.', ' any', ' in', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I was going to be in for a long time. I thought I was in for a couple of years. It's not the same as it used to be. I'm just happy that I'm still here, that other players have given me\n",
      "(tensor([0.3839, 0.1715, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3315, 0.2364, 0.0584, 0.0561, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2144, 0.1253, 0.1123, 0.0826, 0.0741], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.4713, 0.1363, 0.0312, 0.0291, 0.0225], grad_fn=<ToCopyBackward0>), [' going', ' in', ' so', ' watching', ' a'])\n",
      "(tensor([9.8498e-01, 2.6942e-03, 1.9285e-03, 1.1708e-03, 9.1285e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' through', ' in', ' into'])\n",
      "(tensor([0.2921, 0.2070, 0.1062, 0.0924, 0.0712], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' get', ' see', ' have'])\n",
      "(tensor([0.0818, 0.0703, 0.0493, 0.0423, 0.0415], grad_fn=<ToCopyBackward0>), [' a', ' in', ' very', ' the', ' able'])\n",
      "(tensor([0.2838, 0.2381, 0.0847, 0.0720, 0.0592], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' this', ' trouble'])\n",
      "(tensor([0.7159, 0.0795, 0.0248, 0.0208, 0.0189], grad_fn=<ToCopyBackward0>), [' a', ' an', ' another', ' some', ' the'])\n",
      "(tensor([0.2050, 0.0913, 0.0610, 0.0526, 0.0324], grad_fn=<ToCopyBackward0>), [' long', ' big', ' real', ' movie', ' good'])\n",
      "(tensor([0.2201, 0.1140, 0.0510, 0.0292, 0.0286], grad_fn=<ToCopyBackward0>), [' night', ' time', ' day', ',', ' haul'])\n",
      "(tensor([0.2816, 0.1089, 0.1052, 0.0917, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ',', ' watching', ' with', ' and'])\n",
      "(tensor([0.3780, 0.1134, 0.0846, 0.0415, 0.0241], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2662, 0.1202, 0.0439, 0.0439, 0.0378], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' just', \"'m\", ' had'])\n",
      "(tensor([0.4194, 0.1618, 0.0767, 0.0598, 0.0463], grad_fn=<ToCopyBackward0>), [' I', ' that', ' it', ' this', ' my'])\n",
      "(tensor([0.6219, 0.1395, 0.0545, 0.0424, 0.0360], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', \"'d\", ' could'])\n",
      "(tensor([0.8654, 0.0812, 0.0055, 0.0033, 0.0028], grad_fn=<ToCopyBackward0>), [' going', ' in', ' not', ' gonna', ' getting'])\n",
      "(tensor([0.8422, 0.0362, 0.0279, 0.0273, 0.0147], grad_fn=<ToCopyBackward0>), [' for', ' a', ' the', ' my', ' to'])\n",
      "(tensor([0.4467, 0.0910, 0.0383, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' two', ' the', ' 10', ' one'])\n",
      "(tensor([0.3089, 0.2173, 0.1255, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' long', ' lifetime', ' couple', ' year', ' sequel'])\n",
      "(tensor([9.8679e-01, 6.8076e-03, 1.6777e-03, 6.4761e-04, 3.8786e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' years', ' more', ' months', ' hundred'])\n",
      "(tensor([0.5493, 0.1169, 0.0705, 0.0439, 0.0406], grad_fn=<ToCopyBackward0>), [' years', ' hours', ' days', ' weeks', ' months'])\n",
      "(tensor([0.3397, 0.1284, 0.0932, 0.0481, 0.0362], grad_fn=<ToCopyBackward0>), ['.', ',', ' before', ' at', ' but'])\n",
      "(tensor([0.3758, 0.0786, 0.0647, 0.0516, 0.0387], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' And'])\n",
      "(tensor([0.2859, 0.2770, 0.0566, 0.0524, 0.0326], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' seemed', ' is'])\n",
      "(tensor([0.1990, 0.1935, 0.1517, 0.0313, 0.0305], grad_fn=<ToCopyBackward0>), [' been', ' not', ' a', ' one', ' just'])\n",
      "(tensor([0.1374, 0.1040, 0.1038, 0.0973, 0.0576], grad_fn=<ToCopyBackward0>), [' that', ' even', ' the', ' like', ' a'])\n",
      "(tensor([0.1620, 0.1386, 0.1191, 0.0735, 0.0603], grad_fn=<ToCopyBackward0>), [' case', ' end', ' best', ' worst', ' same'])\n",
      "(tensor([0.4744, 0.0891, 0.0629, 0.0376, 0.0317], grad_fn=<ToCopyBackward0>), [' movie', ' as', ' film', ' story', ' scene'])\n",
      "(tensor([0.1371, 0.0963, 0.0881, 0.0460, 0.0226], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' a', ' my'])\n",
      "(tensor([0.4630, 0.3338, 0.0603, 0.0245, 0.0236], grad_fn=<ToCopyBackward0>), [' used', ' was', ' once', ' is', \"'s\"])\n",
      "(tensor([9.9789e-01, 1.4036e-03, 1.0226e-04, 3.4782e-05, 2.4609e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' be', '-', ' been', ' too'])\n",
      "(tensor([9.9388e-01, 3.3567e-03, 5.8066e-04, 5.3769e-04, 1.3721e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' feel', '.', ' have', ' get'])\n",
      "(tensor([0.5569, 0.1259, 0.0794, 0.0562, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' but'])\n",
      "(tensor([0.3054, 0.2131, 0.1046, 0.0252, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' You'])\n",
      "(tensor([0.1342, 0.0867, 0.0802, 0.0627, 0.0595], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' am', ' can', ' just'])\n",
      "(tensor([0.1949, 0.1159, 0.0728, 0.0480, 0.0432], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' in', ' going'])\n",
      "(tensor([0.0934, 0.0682, 0.0672, 0.0622, 0.0606], grad_fn=<ToCopyBackward0>), [' trying', ' not', ' happy', ' out', ' really'])\n",
      "(tensor([0.4551, 0.2211, 0.2046, 0.0254, 0.0176], grad_fn=<ToCopyBackward0>), [' to', ' that', ' I', ' with', ' it'])\n",
      "(tensor([0.8733, 0.0377, 0.0117, 0.0068, 0.0067], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' we', ' they'])\n",
      "(tensor([0.4538, 0.0556, 0.0553, 0.0497, 0.0409], grad_fn=<ToCopyBackward0>), [\"'m\", ' am', \"'ve\", ' was', ' can'])\n",
      "(tensor([0.2964, 0.1015, 0.0860, 0.0555, 0.0492], grad_fn=<ToCopyBackward0>), [' here', ' still', ' not', ' able', ' in'])\n",
      "(tensor([0.5657, 0.1336, 0.0316, 0.0270, 0.0217], grad_fn=<ToCopyBackward0>), [' here', ' able', ' playing', ' alive', ' around'])\n",
      "(tensor([0.5145, 0.1406, 0.0616, 0.0399, 0.0380], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' with', ' in'])\n",
      "(tensor([0.2797, 0.1259, 0.0944, 0.0906, 0.0549], grad_fn=<ToCopyBackward0>), [' and', ' but', ' that', ' still', ' even'])\n",
      "(tensor([0.8142, 0.0269, 0.0169, 0.0126, 0.0101], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' people', ' other'])\n",
      "(tensor([0.6644, 0.1096, 0.0950, 0.0118, 0.0108], grad_fn=<ToCopyBackward0>), [' people', ' guys', ' kids', ' players', ' young'])\n",
      "(tensor([0.2530, 0.0514, 0.0410, 0.0374, 0.0357], grad_fn=<ToCopyBackward0>), [' are', ' have', ' on', ' like', ' still'])\n",
      "(tensor([0.1182, 0.0985, 0.0587, 0.0416, 0.0347], grad_fn=<ToCopyBackward0>), [' given', ' been', ' made', ' gone', ' come'])\n",
      "(tensor([0.9059, 0.0367, 0.0157, 0.0059, 0.0043], grad_fn=<ToCopyBackward0>), [' me', ' it', ' up', ' the', ' back'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was the worst movie I have ever seen in my life. The plot was ridiculous and the acting was terrible! I really wanted to shoot my agent in the head when I saw this movie. I was actually really disappointed. I thought that after I\n",
      "(tensor([0.3833, 0.1721, 0.0902, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1161, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1835, 0.1464, 0.0506, 0.0452, 0.0435], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.3200, 0.1424, 0.0620, 0.0385, 0.0218], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' best', ' biggest', ' movie'])\n",
      "(tensor([0.7418, 0.0792, 0.0225, 0.0204, 0.0085], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' horror', ' Christmas'])\n",
      "(tensor([0.7029, 0.1339, 0.1136, 0.0098, 0.0074], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.3362, 0.2562, 0.1934, 0.1118, 0.0703], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([9.2145e-01, 7.1587e-02, 1.4959e-03, 8.8932e-04, 6.4594e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9133, 0.0204, 0.0128, 0.0099, 0.0096], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' made'])\n",
      "(tensor([0.5241, 0.1623, 0.1292, 0.0602, 0.0136], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', '!', ' and'])\n",
      "(tensor([9.8826e-01, 4.2716e-03, 3.4804e-03, 1.2302e-03, 5.5441e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' my', ' the', ' all', ' a', ' this'])\n",
      "(tensor([7.5756e-01, 2.1177e-01, 2.2331e-02, 3.0274e-03, 4.0710e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' life', ' entire', ' whole', ' lifetime', ' movie'])\n",
      "(tensor([0.7093, 0.0662, 0.0423, 0.0223, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' until', ' when'])\n",
      "(tensor([0.3455, 0.1358, 0.0892, 0.0258, 0.0234], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.1514, 0.1148, 0.0744, 0.0706, 0.0590], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' story', ' movie'])\n",
      "(tensor([0.7674, 0.0904, 0.0140, 0.0102, 0.0090], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' sounded', ' had'])\n",
      "(tensor([0.1588, 0.1039, 0.0955, 0.0504, 0.0358], grad_fn=<ToCopyBackward0>), [' so', ' stupid', ' ridiculous', ' terrible', ' just'])\n",
      "(tensor([0.3873, 0.2812, 0.2264, 0.0117, 0.0109], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ';', '!'])\n",
      "(tensor([0.4778, 0.0825, 0.0333, 0.0178, 0.0176], grad_fn=<ToCopyBackward0>), [' the', ' predictable', ' I', ' it', ' stupid'])\n",
      "(tensor([0.7376, 0.0421, 0.0263, 0.0184, 0.0153], grad_fn=<ToCopyBackward0>), [' acting', ' dialogue', ' actors', ' movie', ' characters'])\n",
      "(tensor([0.7393, 0.0199, 0.0167, 0.0116, 0.0111], grad_fn=<ToCopyBackward0>), [' was', ',', '...', ' (', ' so'])\n",
      "(tensor([0.0955, 0.0846, 0.0723, 0.0399, 0.0397], grad_fn=<ToCopyBackward0>), [' terrible', ' even', ' so', ' horrible', ' worse'])\n",
      "(tensor([0.8413, 0.0374, 0.0357, 0.0165, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.3114, 0.1867, 0.0839, 0.0182, 0.0178], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' No'])\n",
      "(tensor([0.0734, 0.0554, 0.0505, 0.0448, 0.0419], grad_fn=<ToCopyBackward0>), [' was', ' have', ' don', ' would', ' really'])\n",
      "(tensor([0.1463, 0.0695, 0.0694, 0.0662, 0.0488], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' can', ' wanted', ' feel'])\n",
      "(tensor([0.5565, 0.3120, 0.0442, 0.0282, 0.0125], grad_fn=<ToCopyBackward0>), [' to', ' my', ' the', ' it', ' that'])\n",
      "(tensor([0.1942, 0.0927, 0.0654, 0.0612, 0.0534], grad_fn=<ToCopyBackward0>), [' shoot', ' walk', ' see', ' kill', ' turn'])\n",
      "(tensor([0.3308, 0.1476, 0.1010, 0.1009, 0.0831], grad_fn=<ToCopyBackward0>), [' myself', ' my', ' it', ' the', ' someone'])\n",
      "(tensor([0.1959, 0.1364, 0.1128, 0.0872, 0.0211], grad_fn=<ToCopyBackward0>), [' eye', ' agent', ' eyes', ' mouth', ' ears'])\n",
      "(tensor([0.1593, 0.1575, 0.0619, 0.0337, 0.0288], grad_fn=<ToCopyBackward0>), [' because', ' in', ' so', ' with', ' out'])\n",
      "(tensor([9.7885e-01, 1.1714e-02, 2.5943e-03, 1.0962e-03, 7.2072e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' his', ' her'])\n",
      "(tensor([0.7031, 0.0892, 0.0344, 0.0269, 0.0187], grad_fn=<ToCopyBackward0>), [' head', ' face', ' eye', ' forehead', ' mouth'])\n",
      "(tensor([0.2391, 0.1420, 0.1076, 0.0841, 0.0735], grad_fn=<ToCopyBackward0>), [' when', ' with', '.', ' because', ' and'])\n",
      "(tensor([0.7721, 0.1039, 0.0310, 0.0309, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' he', ' she', ' i', ' this'])\n",
      "(tensor([0.5831, 0.0762, 0.0654, 0.0533, 0.0339], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' found', ' was', ' read'])\n",
      "(tensor([0.4915, 0.1959, 0.0947, 0.0598, 0.0377], grad_fn=<ToCopyBackward0>), [' this', ' the', ' that', ' it', ' how'])\n",
      "(tensor([0.6824, 0.0909, 0.0838, 0.0364, 0.0099], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', '!'])\n",
      "(tensor([0.6370, 0.1045, 0.0553, 0.0341, 0.0195], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', '!', ' and'])\n",
      "(tensor([0.3818, 0.1183, 0.1071, 0.0336, 0.0155], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.0637, 0.0625, 0.0577, 0.0564, 0.0482], grad_fn=<ToCopyBackward0>), [' really', ' have', ' think', ' was', ' thought'])\n",
      "(tensor([0.1302, 0.0938, 0.0894, 0.0668, 0.0542], grad_fn=<ToCopyBackward0>), [' so', ' really', ' actually', ' in', ' a'])\n",
      "(tensor([0.1183, 0.0968, 0.0723, 0.0384, 0.0349], grad_fn=<ToCopyBackward0>), [' really', ' in', ' very', ' so', ' a'])\n",
      "(tensor([0.1662, 0.1241, 0.0998, 0.0790, 0.0724], grad_fn=<ToCopyBackward0>), [' surprised', ' looking', ' disappointed', ' angry', ' excited'])\n",
      "(tensor([0.2770, 0.2220, 0.1122, 0.1040, 0.0461], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', '.', ' that'])\n",
      "(tensor([0.3944, 0.1145, 0.1088, 0.0470, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.0799, 0.0773, 0.0765, 0.0553, 0.0450], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' have', ' think'])\n",
      "(tensor([0.5118, 0.1307, 0.0812, 0.0802, 0.0601], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' that'])\n",
      "(tensor([0.3352, 0.2146, 0.0929, 0.0778, 0.0186], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' I', ' after'])\n",
      "(tensor([0.1532, 0.1481, 0.0905, 0.0739, 0.0656], grad_fn=<ToCopyBackward0>), [' I', ' this', ' watching', ' the', ' all'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this film was very well made. There were a number of interesting characters in it and I thought the story was fairly original. However, the acting wasn't very convincing. I thought that it was very difficult for the actors to convincingly portray the\n",
      "(tensor([0.3848, 0.1713, 0.0895, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3311, 0.2372, 0.0583, 0.0558, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4154, 0.1961, 0.1687, 0.0497, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.6385, 0.0588, 0.0580, 0.0560, 0.0176], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' could'])\n",
      "(tensor([0.1008, 0.0717, 0.0698, 0.0440, 0.0338], grad_fn=<ToCopyBackward0>), [' a', ' very', ' pretty', ' so', ' really'])\n",
      "(tensor([0.3670, 0.0837, 0.0667, 0.0521, 0.0297], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' much', ' interesting'])\n",
      "(tensor([0.5672, 0.1879, 0.1421, 0.0151, 0.0128], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.3627, 0.2162, 0.1839, 0.0307, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' but', '...'])\n",
      "(tensor([0.2094, 0.2067, 0.1897, 0.0286, 0.0239], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' Unfortunately', ' There'])\n",
      "(tensor([0.3174, 0.2879, 0.1537, 0.1083, 0.0749], grad_fn=<ToCopyBackward0>), [' were', ' was', ' are', ' is', \"'s\"])\n",
      "(tensor([0.2105, 0.2078, 0.0640, 0.0626, 0.0487], grad_fn=<ToCopyBackward0>), [' a', ' some', ' no', ' so', ' very'])\n",
      "(tensor([0.4469, 0.2553, 0.1881, 0.0797, 0.0079], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' number', ' bunch'])\n",
      "(tensor([9.9910e-01, 2.2869e-04, 1.1043e-04, 7.0587e-05, 5.6856e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' that', ',', ' (', ' in'])\n",
      "(tensor([0.0654, 0.0621, 0.0575, 0.0512, 0.0448], grad_fn=<ToCopyBackward0>), [' well', ' interesting', ' good', ' very', ' things'])\n",
      "(tensor([0.1549, 0.0834, 0.0608, 0.0559, 0.0489], grad_fn=<ToCopyBackward0>), [' characters', ' points', ' and', ' character', ' themes'])\n",
      "(tensor([0.2162, 0.1782, 0.1185, 0.1129, 0.0486], grad_fn=<ToCopyBackward0>), [' and', ' in', '.', ',', ' that'])\n",
      "(tensor([0.5383, 0.2156, 0.1917, 0.0173, 0.0157], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' there', ' here'])\n",
      "(tensor([0.4584, 0.2721, 0.1309, 0.0270, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' -'])\n",
      "(tensor([0.4492, 0.1279, 0.0865, 0.0576, 0.0336], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' a', ' some'])\n",
      "(tensor([0.2252, 0.1756, 0.0734, 0.0627, 0.0586], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' enjoyed', ' found'])\n",
      "(tensor([0.3926, 0.2492, 0.2440, 0.0136, 0.0071], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' there'])\n",
      "(tensor([0.1805, 0.1187, 0.0605, 0.0445, 0.0339], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' director', ' film', ' plot'])\n",
      "(tensor([0.5580, 0.0433, 0.0413, 0.0328, 0.0221], grad_fn=<ToCopyBackward0>), [' was', ' itself', ' line', ' did', ' could'])\n",
      "(tensor([0.2352, 0.1357, 0.0847, 0.0652, 0.0584], grad_fn=<ToCopyBackward0>), [' interesting', ' pretty', ' very', ' fairly', ' good'])\n",
      "(tensor([0.1957, 0.0913, 0.0870, 0.0769, 0.0620], grad_fn=<ToCopyBackward0>), [' interesting', ' original', ' well', ' good', ' compelling'])\n",
      "(tensor([0.4931, 0.3427, 0.0441, 0.0212, 0.0184], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' in', ' but'])\n",
      "(tensor([0.1401, 0.1294, 0.1152, 0.0977, 0.0627], grad_fn=<ToCopyBackward0>), [' I', ' The', ' However', ' It', ' Unfortunately'])\n",
      "(tensor([0.7572, 0.0781, 0.0447, 0.0428, 0.0086], grad_fn=<ToCopyBackward0>), [',', ' the', ' I', ' it', ' there'])\n",
      "(tensor([0.2900, 0.2403, 0.1116, 0.0354, 0.0321], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' this', ' there'])\n",
      "(tensor([0.0828, 0.0713, 0.0639, 0.0485, 0.0476], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' story', ' problem', ' main'])\n",
      "(tensor([0.4503, 0.1451, 0.0814, 0.0380, 0.0307], grad_fn=<ToCopyBackward0>), [' was', ' and', ' in', ' of', ' wasn'])\n",
      "(tensor([9.9465e-01, 2.0984e-03, 4.4084e-04, 4.2545e-04, 2.4315e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", ','])\n",
      "(tensor([0.1990, 0.1034, 0.0839, 0.0753, 0.0719], grad_fn=<ToCopyBackward0>), [' very', ' good', ' great', ' as', ' all'])\n",
      "(tensor([0.7355, 0.1590, 0.0294, 0.0098, 0.0072], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' believable', ' well', ' appealing'])\n",
      "(tensor([0.5208, 0.1867, 0.1063, 0.0437, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' to'])\n",
      "(tensor([0.2417, 0.2087, 0.0805, 0.0609, 0.0416], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.1094, 0.1021, 0.0695, 0.0606, 0.0451], grad_fn=<ToCopyBackward0>), [' think', ' thought', ' found', ' don', \"'m\"])\n",
      "(tensor([0.2448, 0.2334, 0.1150, 0.0258, 0.0194], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' they', ' there'])\n",
      "(tensor([0.3648, 0.0704, 0.0413, 0.0292, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' it', ' they', ' there', ' I'])\n",
      "(tensor([0.3607, 0.0732, 0.0564, 0.0414, 0.0400], grad_fn=<ToCopyBackward0>), [' was', ' would', ' needed', ' just', ' could'])\n",
      "(tensor([0.2206, 0.1913, 0.0415, 0.0327, 0.0283], grad_fn=<ToCopyBackward0>), [' a', ' very', ' all', ' pretty', ' just'])\n",
      "(tensor([0.1776, 0.0940, 0.0563, 0.0540, 0.0267], grad_fn=<ToCopyBackward0>), [' hard', ' difficult', ' over', ' disappointing', ' obvious'])\n",
      "(tensor([8.4589e-01, 1.3891e-01, 2.8570e-03, 1.0559e-03, 7.9835e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' and', ',', ' trying'])\n",
      "(tensor([0.6026, 0.0425, 0.0385, 0.0195, 0.0193], grad_fn=<ToCopyBackward0>), [' the', ' some', ' most', ' a', ' them'])\n",
      "(tensor([0.4434, 0.0722, 0.0664, 0.0422, 0.0384], grad_fn=<ToCopyBackward0>), [' actors', ' actor', ' characters', ' leading', ' director'])\n",
      "(tensor([0.9606, 0.0091, 0.0049, 0.0036, 0.0036], grad_fn=<ToCopyBackward0>), [' to', '.', ' in', ',', ' and'])\n",
      "(tensor([0.0892, 0.0748, 0.0716, 0.0620, 0.0546], grad_fn=<ToCopyBackward0>), [' convinc', ' convince', ' portray', ' convey', ' deliver'])\n",
      "(tensor([0.8609, 0.0425, 0.0160, 0.0059, 0.0032], grad_fn=<ToCopyBackward0>), ['ingly', 'er', 'ie', 'e', '.'])\n",
      "(tensor([0.6777, 0.0748, 0.0374, 0.0135, 0.0122], grad_fn=<ToCopyBackward0>), [' portray', ' play', ' deliver', ' give', ' carry'])\n",
      "(tensor([0.5741, 0.0763, 0.0525, 0.0367, 0.0136], grad_fn=<ToCopyBackward0>), [' the', ' their', ' a', ' characters', ' these'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of the worst films I've ever had the misfortune to watch. I can honestly say that I have never been so embarrassed to be a member of the public. I was actually in a theater when I first saw this. My friend and\n",
      "(tensor([0.3822, 0.1731, 0.0907, 0.0773, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7145, 0.1159, 0.0392, 0.0100, 0.0087], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1832, 0.1475, 0.0505, 0.0450, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9172, 0.0123, 0.0065, 0.0065, 0.0040], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7577, 0.1963, 0.0169, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.5698, 0.1209, 0.1093, 0.0263, 0.0262], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' most', ' funn', ' dumb'])\n",
      "(tensor([0.7940, 0.1565, 0.0050, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ',', ' film'])\n",
      "(tensor([0.7825, 0.0621, 0.0524, 0.0439, 0.0123], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.3754, 0.3511, 0.1365, 0.0552, 0.0519], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', \"'d\", ' ever'])\n",
      "(tensor([0.8660, 0.1132, 0.0071, 0.0051, 0.0016], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' been'])\n",
      "(tensor([0.8431, 0.0435, 0.0255, 0.0199, 0.0145], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' made', ' been'])\n",
      "(tensor([0.8355, 0.0723, 0.0221, 0.0172, 0.0107], grad_fn=<ToCopyBackward0>), [' the', ' to', ' a', ' in', ','])\n",
      "(tensor([0.5293, 0.3830, 0.0232, 0.0217, 0.0078], grad_fn=<ToCopyBackward0>), [' misfortune', ' displeasure', ' privilege', ' pleasure', ' fortune'])\n",
      "(tensor([9.3510e-01, 6.3590e-02, 1.0335e-04, 8.5632e-05, 8.0572e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' of', ' in', ' for', ' with'])\n",
      "(tensor([0.4180, 0.2731, 0.1555, 0.0303, 0.0236], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' sit', ' have', ' be'])\n",
      "(tensor([0.6919, 0.1304, 0.0651, 0.0133, 0.0127], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', '!', '...'])\n",
      "(tensor([0.1733, 0.1665, 0.1440, 0.0234, 0.0212], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'The'])\n",
      "(tensor([0.1062, 0.0883, 0.0746, 0.0419, 0.0418], grad_fn=<ToCopyBackward0>), [' was', ' can', \"'m\", ' really', ' don'])\n",
      "(tensor([0.4775, 0.2032, 0.0529, 0.0391, 0.0223], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' only', ' remember', ' say'])\n",
      "(tensor([0.9639, 0.0085, 0.0037, 0.0025, 0.0020], grad_fn=<ToCopyBackward0>), [' say', ' tell', ' remember', ' only', ' not'])\n",
      "(tensor([0.4867, 0.2665, 0.1070, 0.0468, 0.0142], grad_fn=<ToCopyBackward0>), [' that', ' I', ' it', ' this', ' there'])\n",
      "(tensor([0.3957, 0.1355, 0.1017, 0.0389, 0.0318], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' the', ' even'])\n",
      "(tensor([0.1778, 0.0722, 0.0638, 0.0583, 0.0489], grad_fn=<ToCopyBackward0>), [' have', ' was', \"'m\", \"'ve\", ' am'])\n",
      "(tensor([0.3813, 0.1585, 0.0621, 0.0527, 0.0424], grad_fn=<ToCopyBackward0>), [' never', ' seen', ' not', ' absolutely', ' a'])\n",
      "(tensor([0.3137, 0.1344, 0.0639, 0.0631, 0.0420], grad_fn=<ToCopyBackward0>), [' seen', ' been', ',', ' in', ' had'])\n",
      "(tensor([0.6435, 0.1530, 0.0566, 0.0235, 0.0210], grad_fn=<ToCopyBackward0>), [' so', ' more', ' as', ' a', ' able'])\n",
      "(tensor([0.2865, 0.2110, 0.2104, 0.0440, 0.0347], grad_fn=<ToCopyBackward0>), [' disappointed', ' bored', ' angry', ' embarrassed', ' disgusted'])\n",
      "(tensor([0.5857, 0.1718, 0.0663, 0.0332, 0.0210], grad_fn=<ToCopyBackward0>), [' to', ' in', ' by', ' as', ' for'])\n",
      "(tensor([0.7155, 0.1070, 0.0401, 0.0237, 0.0129], grad_fn=<ToCopyBackward0>), [' be', ' have', ' see', ' watch', ' sit'])\n",
      "(tensor([0.2478, 0.2250, 0.1504, 0.0366, 0.0355], grad_fn=<ToCopyBackward0>), [' a', ' from', ' in', ' on', ' watching'])\n",
      "(tensor([0.3993, 0.2715, 0.0347, 0.0314, 0.0258], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' part', ' member', ' cinema'])\n",
      "(tensor([9.9913e-01, 3.5760e-04, 9.7097e-05, 5.6114e-05, 3.2319e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' in', ' or', ' on', ' and'])\n",
      "(tensor([0.8146, 0.1035, 0.0214, 0.0192, 0.0110], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' this', ' an'])\n",
      "(tensor([0.2249, 0.1523, 0.0210, 0.0170, 0.0164], grad_fn=<ToCopyBackward0>), [' public', ' human', ' film', ' Canadian', ' British'])\n",
      "(tensor([0.3857, 0.1010, 0.0913, 0.0628, 0.0476], grad_fn=<ToCopyBackward0>), ['.', ' as', ' to', ' at', ' in'])\n",
      "(tensor([0.2246, 0.1185, 0.0924, 0.0392, 0.0328], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1200, 0.0925, 0.0764, 0.0562, 0.0532], grad_fn=<ToCopyBackward0>), [' was', ' can', \"'m\", ' actually', ' have'])\n",
      "(tensor([0.1151, 0.0804, 0.0757, 0.0622, 0.0469], grad_fn=<ToCopyBackward0>), [' in', ' actually', ' so', ' embarrassed', ' a'])\n",
      "(tensor([0.1542, 0.0600, 0.0448, 0.0359, 0.0350], grad_fn=<ToCopyBackward0>), [' in', ' embarrassed', ' a', ' surprised', ' so'])\n",
      "(tensor([0.5130, 0.3380, 0.0141, 0.0127, 0.0106], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' such', ' shock'])\n",
      "(tensor([0.0559, 0.0502, 0.0298, 0.0277, 0.0250], grad_fn=<ToCopyBackward0>), [' theater', ' public', ' state', ' dark', ' deep'])\n",
      "(tensor([0.1815, 0.1029, 0.0619, 0.0567, 0.0423], grad_fn=<ToCopyBackward0>), [' when', ' with', ' the', ' in', ' about'])\n",
      "(tensor([0.4737, 0.3313, 0.0762, 0.0555, 0.0109], grad_fn=<ToCopyBackward0>), [' I', ' this', ' the', ' it', ' they'])\n",
      "(tensor([0.1868, 0.1488, 0.1194, 0.0667, 0.0460], grad_fn=<ToCopyBackward0>), [' saw', ' first', ' found', ' watched', ' was'])\n",
      "(tensor([0.6111, 0.0973, 0.0387, 0.0248, 0.0221], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' heard', ' viewed', ' came'])\n",
      "(tensor([0.5237, 0.4200, 0.0361, 0.0048, 0.0025], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "(tensor([0.4209, 0.1286, 0.0939, 0.0807, 0.0654], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ',', ' and'])\n",
      "(tensor([0.4566, 0.0905, 0.0655, 0.0284, 0.0154], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' My'])\n",
      "(tensor([0.2357, 0.1731, 0.0683, 0.0376, 0.0309], grad_fn=<ToCopyBackward0>), [' friend', ' friends', ' girlfriend', ' eyes', ' jaw'])\n",
      "(tensor([0.4013, 0.0673, 0.0408, 0.0314, 0.0274], grad_fn=<ToCopyBackward0>), [' and', ' was', ' who', ' wanted', ' rented'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this is a movie that should be watched by the entire world. I really enjoyed the movie because I think that there is a lot of truth in it. It's not a great movie, but I really enjoyed it. I think that it's\n",
      "(tensor([0.3841, 0.1717, 0.0898, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3313, 0.2367, 0.0582, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4151, 0.1965, 0.1688, 0.0496, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.3749, 0.2203, 0.0978, 0.0616, 0.0314], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' not', ' an'])\n",
      "(tensor([0.3299, 0.0729, 0.0613, 0.0572, 0.0408], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' good', ' sequel', ' film'])\n",
      "(tensor([0.5388, 0.0724, 0.0629, 0.0533, 0.0296], grad_fn=<ToCopyBackward0>), [' that', ' for', ' about', ' where', ' which'])\n",
      "(tensor([0.1164, 0.0797, 0.0478, 0.0421, 0.0402], grad_fn=<ToCopyBackward0>), [' is', ' should', ' could', ' I', ' the'])\n",
      "(tensor([0.5458, 0.1669, 0.1145, 0.0414, 0.0144], grad_fn=<ToCopyBackward0>), [' be', ' not', ' have', ' never', ' stay'])\n",
      "(tensor([0.2781, 0.0994, 0.0811, 0.0438, 0.0397], grad_fn=<ToCopyBackward0>), [' watched', ' in', ' shown', ' on', ' made'])\n",
      "(tensor([0.3190, 0.1602, 0.0848, 0.0463, 0.0395], grad_fn=<ToCopyBackward0>), [' by', ' in', ' on', ' with', ' at'])\n",
      "(tensor([0.1552, 0.1096, 0.0862, 0.0738, 0.0680], grad_fn=<ToCopyBackward0>), [' everyone', ' people', ' the', ' a', ' everybody'])\n",
      "(tensor([0.1123, 0.0728, 0.0722, 0.0427, 0.0377], grad_fn=<ToCopyBackward0>), [' public', ' people', ' entire', ' whole', ' masses'])\n",
      "(tensor([0.3600, 0.2712, 0.0401, 0.0250, 0.0205], grad_fn=<ToCopyBackward0>), [' family', ' world', ' population', ' universe', ' city'])\n",
      "(tensor([0.4169, 0.2394, 0.0930, 0.0282, 0.0265], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' because'])\n",
      "(tensor([0.3109, 0.1288, 0.0675, 0.0573, 0.0452], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' So'])\n",
      "(tensor([0.1317, 0.0985, 0.0762, 0.0541, 0.0387], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' am', \"'m\"])\n",
      "(tensor([0.2304, 0.0569, 0.0510, 0.0503, 0.0434], grad_fn=<ToCopyBackward0>), [' wanted', ' did', ' enjoyed', ' hope', ' thought'])\n",
      "(tensor([0.4221, 0.2810, 0.1164, 0.0193, 0.0116], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' this', ' all'])\n",
      "(tensor([0.2365, 0.1352, 0.1071, 0.0601, 0.0447], grad_fn=<ToCopyBackward0>), [' first', ' story', ' movie', ' fact', ' book'])\n",
      "(tensor([0.2106, 0.1974, 0.0648, 0.0632, 0.0603], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', '...'])\n",
      "(tensor([0.3528, 0.1896, 0.1649, 0.0587, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' there'])\n",
      "(tensor([0.0812, 0.0785, 0.0713, 0.0621, 0.0554], grad_fn=<ToCopyBackward0>), [' was', ' think', ' really', ' am', ' thought'])\n",
      "(tensor([0.4589, 0.1805, 0.0631, 0.0418, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' it', ' the', ' this', ' I'])\n",
      "(tensor([0.2474, 0.1108, 0.1012, 0.0500, 0.0335], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.4079, 0.3070, 0.0937, 0.0796, 0.0404], grad_fn=<ToCopyBackward0>), [' is', ' are', \"'s\", ' was', ' were'])\n",
      "(tensor([0.2145, 0.1693, 0.1563, 0.1415, 0.0661], grad_fn=<ToCopyBackward0>), [' something', ' a', ' no', ' nothing', ' not'])\n",
      "(tensor([0.4417, 0.1880, 0.0501, 0.0285, 0.0217], grad_fn=<ToCopyBackward0>), [' lot', ' message', ' very', ' good', ' great'])\n",
      "(tensor([0.4342, 0.3928, 0.0596, 0.0368, 0.0189], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ' in', ' more'])\n",
      "(tensor([0.1372, 0.1071, 0.0548, 0.0426, 0.0389], grad_fn=<ToCopyBackward0>), [' humor', ' truth', ' art', ' value', ' good'])\n",
      "(tensor([0.5304, 0.1661, 0.0956, 0.0624, 0.0218], grad_fn=<ToCopyBackward0>), [' in', ' to', ' behind', ' and', '.'])\n",
      "(tensor([0.6098, 0.2487, 0.0767, 0.0098, 0.0072], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' what', ' there'])\n",
      "(tensor([0.5343, 0.1646, 0.0606, 0.0250, 0.0236], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' that'])\n",
      "(tensor([0.5035, 0.0744, 0.0407, 0.0393, 0.0370], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.5478, 0.1936, 0.0361, 0.0330, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' has', ' touches'])\n",
      "(tensor([0.3517, 0.1142, 0.0921, 0.0746, 0.0237], grad_fn=<ToCopyBackward0>), [' not', ' a', ' like', ' about', ' just'])\n",
      "(tensor([0.1806, 0.1585, 0.1312, 0.1128, 0.0825], grad_fn=<ToCopyBackward0>), [' like', ' the', ' a', ' just', ' that'])\n",
      "(tensor([0.1259, 0.1164, 0.1098, 0.1068, 0.0382], grad_fn=<ToCopyBackward0>), [' great', ' Hollywood', ' movie', ' perfect', ' good'])\n",
      "(tensor([0.7840, 0.1089, 0.0317, 0.0087, 0.0064], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' plot', ' Hollywood'])\n",
      "(tensor([0.3360, 0.1497, 0.0955, 0.0655, 0.0535], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' in', ' for'])\n",
      "(tensor([0.7731, 0.0823, 0.0192, 0.0188, 0.0107], grad_fn=<ToCopyBackward0>), [' but', ' it', ' I', ' and', ' not'])\n",
      "(tensor([0.3494, 0.3109, 0.0301, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' at', ' if', ' the'])\n",
      "(tensor([0.2245, 0.1829, 0.0758, 0.0370, 0.0277], grad_fn=<ToCopyBackward0>), [' really', ' think', ' thought', ' enjoyed', ' can'])\n",
      "(tensor([0.1905, 0.1580, 0.1075, 0.0897, 0.0725], grad_fn=<ToCopyBackward0>), [' enjoyed', ' think', ' liked', ' thought', ' enjoy'])\n",
      "(tensor([0.4871, 0.4118, 0.0485, 0.0102, 0.0062], grad_fn=<ToCopyBackward0>), [' watching', ' it', ' the', ' seeing', ' this'])\n",
      "(tensor([0.5445, 0.1392, 0.0872, 0.0681, 0.0305], grad_fn=<ToCopyBackward0>), ['.', ' because', ' and', ',', ' for'])\n",
      "(tensor([0.5591, 0.0898, 0.0441, 0.0304, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' If'])\n",
      "(tensor([0.1576, 0.1325, 0.0611, 0.0441, 0.0378], grad_fn=<ToCopyBackward0>), [' really', ' think', ' was', ' thought', ' don'])\n",
      "(tensor([0.6127, 0.1133, 0.0645, 0.0515, 0.0174], grad_fn=<ToCopyBackward0>), [' that', ' it', ' this', ' the', ' if'])\n",
      "(tensor([0.2485, 0.2209, 0.1007, 0.0395, 0.0315], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' people', ' if'])\n",
      "(tensor([0.4834, 0.1266, 0.0833, 0.0417, 0.0330], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' should', ' has', ' was'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was going to be sick from the whole thing. I actually thought that it might have been something to do with the polio vaccine. But I was wrong. It was actually a brain tumor.\"The tumor was so small, it did not cause any\n",
      "(tensor([0.3833, 0.1720, 0.0900, 0.0772, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2249, 0.1947, 0.1571, 0.0695, 0.0537], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4600, 0.2366, 0.0587, 0.0169, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.9598, 0.0135, 0.0028, 0.0028, 0.0026], grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' for', ' mad', ' blind'])\n",
      "(tensor([0.5421, 0.1264, 0.0628, 0.0560, 0.0285], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' have', ' get', ' see'])\n",
      "(tensor([0.0963, 0.0800, 0.0554, 0.0389, 0.0333], grad_fn=<ToCopyBackward0>), [' in', ' sick', ' a', ' locked', ' watching'])\n",
      "(tensor([0.1477, 0.1077, 0.0986, 0.0836, 0.0735], grad_fn=<ToCopyBackward0>), ['.', ' to', ' when', ',', ' from'])\n",
      "(tensor([0.3125, 0.2954, 0.0765, 0.0557, 0.0309], grad_fn=<ToCopyBackward0>), [' this', ' the', ' watching', ' it', ' all'])\n",
      "(tensor([0.0386, 0.0317, 0.0279, 0.0203, 0.0203], grad_fn=<ToCopyBackward0>), [' idea', ' whole', ' first', ' entire', ' pain'])\n",
      "(tensor([0.0669, 0.0595, 0.0419, 0.0325, 0.0209], grad_fn=<ToCopyBackward0>), [' thing', ' movie', ' stupid', ' episode', ' filming'])\n",
      "(tensor([0.4907, 0.2822, 0.0282, 0.0239, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '!', ' and'])\n",
      "(tensor([0.3821, 0.0754, 0.0703, 0.0283, 0.0212], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' And', ' This'])\n",
      "(tensor([0.1660, 0.1316, 0.0950, 0.0496, 0.0411], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', \"'m\", ' actually'])\n",
      "(tensor([0.3067, 0.0771, 0.0317, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' thought', ' felt', ' had', ' started', ' think'])\n",
      "(tensor([0.4951, 0.1444, 0.0654, 0.0440, 0.0348], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' that', ' this'])\n",
      "(tensor([0.3278, 0.2179, 0.0781, 0.0739, 0.0341], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' my'])\n",
      "(tensor([0.5979, 0.1674, 0.1066, 0.0427, 0.0300], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' had', ' could'])\n",
      "(tensor([0.4059, 0.3761, 0.1029, 0.0182, 0.0178], grad_fn=<ToCopyBackward0>), [' have', ' be', ' kill', ' not', ' make'])\n",
      "(tensor([0.5003, 0.1803, 0.0420, 0.0342, 0.0172], grad_fn=<ToCopyBackward0>), [' been', ' something', ' to', ' had', ' come'])\n",
      "(tensor([0.1696, 0.1344, 0.0607, 0.0471, 0.0438], grad_fn=<ToCopyBackward0>), [' the', ' a', ' something', ' better', ' contagious'])\n",
      "(tensor([0.3005, 0.1040, 0.0889, 0.0459, 0.0435], grad_fn=<ToCopyBackward0>), [' to', ' in', ' that', ' more', ' I'])\n",
      "(tensor([0.9369, 0.0189, 0.0085, 0.0017, 0.0012], grad_fn=<ToCopyBackward0>), [' do', ' with', ' be', ' get', ' make'])\n",
      "(tensor([9.9277e-01, 8.1472e-04, 5.9251e-04, 5.3873e-04, 3.9956e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' to', ' or', ' more', ' in'])\n",
      "(tensor([0.4328, 0.0249, 0.0186, 0.0149, 0.0115], grad_fn=<ToCopyBackward0>), [' the', ' my', ' this', ' it', ' a'])\n",
      "(tensor([0.0593, 0.0315, 0.0306, 0.0199, 0.0124], grad_fn=<ToCopyBackward0>), [' fact', ' movie', ' drugs', ' LSD', ' polio'])\n",
      "(tensor([0.5429, 0.0644, 0.0541, 0.0353, 0.0301], grad_fn=<ToCopyBackward0>), [' vaccine', ' virus', ' vaccines', '.', ','])\n",
      "(tensor([0.2946, 0.2918, 0.0907, 0.0489, 0.0428], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' I', ' that'])\n",
      "(tensor([0.3334, 0.0669, 0.0635, 0.0495, 0.0313], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.1887, 0.1522, 0.0771, 0.0705, 0.0501], grad_fn=<ToCopyBackward0>), [' it', ' I', ' after', ' when', ' no'])\n",
      "(tensor([0.2996, 0.0950, 0.0597, 0.0587, 0.0407], grad_fn=<ToCopyBackward0>), [' was', ' just', ' didn', \"'m\", ' guess'])\n",
      "(tensor([0.4583, 0.0304, 0.0301, 0.0192, 0.0190], grad_fn=<ToCopyBackward0>), [' wrong', ' feeling', ' not', ' so', ' told'])\n",
      "(tensor([0.6993, 0.0809, 0.0418, 0.0324, 0.0263], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' because', ' on'])\n",
      "(tensor([0.2825, 0.1513, 0.1478, 0.0709, 0.0176], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' This', ' There'])\n",
      "(tensor([0.6620, 0.0971, 0.0806, 0.0271, 0.0257], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' had', ' turned'])\n",
      "(tensor([0.1104, 0.1081, 0.0789, 0.0531, 0.0464], grad_fn=<ToCopyBackward0>), [' not', ' the', ' actually', ' a', ' really'])\n",
      "(tensor([0.2725, 0.0941, 0.0896, 0.0724, 0.0228], grad_fn=<ToCopyBackward0>), [' the', ' something', ' a', ' from', ' because'])\n",
      "(tensor([0.1471, 0.0806, 0.0243, 0.0181, 0.0180], grad_fn=<ToCopyBackward0>), [' brain', ' virus', ' little', ' bad', ' big'])\n",
      "(tensor([0.1612, 0.1318, 0.0684, 0.0634, 0.0346], grad_fn=<ToCopyBackward0>), [' tumor', ' an', ' infection', ' injury', ' cancer'])\n",
      "(tensor([0.3290, 0.1595, 0.0642, 0.0541, 0.0440], grad_fn=<ToCopyBackward0>), ['.', '.\"', '\"', ' called', ','])\n",
      "(tensor([0.1151, 0.0650, 0.0449, 0.0317, 0.0300], grad_fn=<ToCopyBackward0>), ['The', 'A', 'I', ' The', 'In'])\n",
      "(tensor([0.5183, 0.0596, 0.0421, 0.0155, 0.0120], grad_fn=<ToCopyBackward0>), [' tumor', ' brain', ' cancer', ' doctor', ' tum'])\n",
      "(tensor([0.2324, 0.0773, 0.0601, 0.0430, 0.0213], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' grew'])\n",
      "(tensor([0.2080, 0.0736, 0.0595, 0.0457, 0.0412], grad_fn=<ToCopyBackward0>), [' so', ' removed', ' benign', ' in', ' surg'])\n",
      "(tensor([0.2552, 0.1141, 0.0872, 0.0505, 0.0409], grad_fn=<ToCopyBackward0>), [' large', ' big', ' small', ' bad', ' massive'])\n",
      "(tensor([0.3104, 0.2296, 0.1722, 0.0685, 0.0371], grad_fn=<ToCopyBackward0>), [' that', ',', ' it', ' and', ' ('])\n",
      "(tensor([0.2329, 0.1158, 0.0566, 0.0548, 0.0518], grad_fn=<ToCopyBackward0>), [' it', ' he', ' in', ' that', ' \"'])\n",
      "(tensor([0.2462, 0.1200, 0.1071, 0.0834, 0.0444], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' did', ' could', ' would'])\n",
      "(tensor([0.9525, 0.0220, 0.0021, 0.0020, 0.0018], grad_fn=<ToCopyBackward0>), [' not', ' nothing', ' something', ' NOT', ' no'])\n",
      "(tensor([0.1364, 0.0663, 0.0424, 0.0419, 0.0418], grad_fn=<ToCopyBackward0>), [' cause', ' show', ' require', ' go', ' even'])\n",
      "(tensor([0.3839, 0.1809, 0.1008, 0.0605, 0.0368], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' much', ' life'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was a good movie. It was a really bad premise, but it was good to see some good actors in the movie. I was really surprised at the acting ability of Glover and Glover. They really pulled it all off. I really didn\n",
      "(tensor([0.3836, 0.1722, 0.0900, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5006, 0.0599, 0.0341, 0.0152, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6239, 0.0399, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0596, 0.0527, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1137, 0.0606, 0.0595, 0.0563, 0.0371], grad_fn=<ToCopyBackward0>), [' good', ' very', ' pretty', ' great', ' little'])\n",
      "(tensor([0.5097, 0.1118, 0.0925, 0.0333, 0.0246], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' idea', ' story', ' comedy'])\n",
      "(tensor([0.3205, 0.1456, 0.1100, 0.0728, 0.0374], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', '!'])\n",
      "(tensor([0.3114, 0.1755, 0.0739, 0.0373, 0.0188], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.4288, 0.1223, 0.0817, 0.0345, 0.0339], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' has'])\n",
      "(tensor([0.0963, 0.0860, 0.0836, 0.0606, 0.0423], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' entertaining', ' interesting', ' fun'])\n",
      "(tensor([0.3645, 0.0420, 0.0371, 0.0325, 0.0316], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' very', ' really', ' pretty'])\n",
      "(tensor([0.4547, 0.1473, 0.0599, 0.0497, 0.0240], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' boring', ' funny', ','])\n",
      "(tensor([0.6282, 0.0685, 0.0392, 0.0234, 0.0233], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ' premise', ' comedy'])\n",
      "(tensor([0.2821, 0.1623, 0.1620, 0.0864, 0.0857], grad_fn=<ToCopyBackward0>), [' for', ',', ' and', '.', ' to'])\n",
      "(tensor([0.4056, 0.0905, 0.0489, 0.0489, 0.0247], grad_fn=<ToCopyBackward0>), [' but', ' and', ' a', ' the', ' I'])\n",
      "(tensor([0.2259, 0.2216, 0.1345, 0.0337, 0.0269], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' at', ' a'])\n",
      "(tensor([0.4818, 0.1095, 0.0661, 0.0525, 0.0314], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' could', ' seemed'])\n",
      "(tensor([0.1807, 0.1231, 0.0999, 0.0897, 0.0579], grad_fn=<ToCopyBackward0>), [' a', ' good', ' interesting', ' funny', ' entertaining'])\n",
      "(tensor([0.3099, 0.1086, 0.0907, 0.0611, 0.0568], grad_fn=<ToCopyBackward0>), [' in', ' to', ' as', ' for', '.'])\n",
      "(tensor([0.6005, 0.1141, 0.0931, 0.0392, 0.0231], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' be', ' have', ' make'])\n",
      "(tensor([0.0676, 0.0658, 0.0393, 0.0228, 0.0161], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' something', ' some'])\n",
      "(tensor([0.1101, 0.0766, 0.0690, 0.0687, 0.0587], grad_fn=<ToCopyBackward0>), [' real', ' good', ' funny', ' of', ' pretty'])\n",
      "(tensor([0.2401, 0.1124, 0.0612, 0.0385, 0.0362], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' looking', ' performances', ' action'])\n",
      "(tensor([0.7471, 0.0927, 0.0306, 0.0123, 0.0113], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' and', ' on'])\n",
      "(tensor([0.3595, 0.2037, 0.1801, 0.0687, 0.0340], grad_fn=<ToCopyBackward0>), [' it', ' the', ' a', ' there', ' this'])\n",
      "(tensor([0.7283, 0.0385, 0.0232, 0.0158, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' lead', ' cast', ' same'])\n",
      "(tensor([0.7831, 0.1437, 0.0138, 0.0082, 0.0077], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' like'])\n",
      "(tensor([0.2558, 0.1634, 0.0480, 0.0479, 0.0454], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', 'I', ' The'])\n",
      "(tensor([0.1372, 0.1039, 0.0772, 0.0691, 0.0519], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', \"'m\"])\n",
      "(tensor([0.4591, 0.1385, 0.0266, 0.0234, 0.0213], grad_fn=<ToCopyBackward0>), [' really', ' very', ' not', ' just', ' a'])\n",
      "(tensor([0.4133, 0.1651, 0.1042, 0.0277, 0.0270], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' looking', ' excited', ' impressed'])\n",
      "(tensor([0.2180, 0.1530, 0.1299, 0.1250, 0.1010], grad_fn=<ToCopyBackward0>), [' by', ' when', ' that', ' at', ' with'])\n",
      "(tensor([0.6890, 0.1754, 0.0362, 0.0136, 0.0132], grad_fn=<ToCopyBackward0>), [' the', ' how', ' some', ' what', ' that'])\n",
      "(tensor([0.2400, 0.1228, 0.0427, 0.0426, 0.0347], grad_fn=<ToCopyBackward0>), [' acting', ' ending', ' end', ' plot', ' number'])\n",
      "(tensor([0.4156, 0.0906, 0.0486, 0.0418, 0.0361], grad_fn=<ToCopyBackward0>), [' in', ' ability', ' of', ' and', ','])\n",
      "(tensor([0.7791, 0.1185, 0.0231, 0.0125, 0.0116], grad_fn=<ToCopyBackward0>), [' of', ' in', ' on', ' and', ','])\n",
      "(tensor([0.1000, 0.0463, 0.0400, 0.0254, 0.0213], grad_fn=<ToCopyBackward0>), [' the', ' Michael', ' Glover', ' Adam', ' Kevin'])\n",
      "(tensor([0.4530, 0.1587, 0.0984, 0.0709, 0.0635], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' as', ' in'])\n",
      "(tensor([0.3132, 0.0615, 0.0520, 0.0262, 0.0219], grad_fn=<ToCopyBackward0>), [' Glover', ' Mc', ' the', ' I', ' G'])\n",
      "(tensor([0.3503, 0.1753, 0.0995, 0.0770, 0.0310], grad_fn=<ToCopyBackward0>), [' was', ' alone', ' is', '.', ' in'])\n",
      "(tensor([0.3140, 0.1032, 0.0548, 0.0502, 0.0493], grad_fn=<ToCopyBackward0>), [' I', ' It', ' They', ' And', ' The'])\n",
      "(tensor([0.2994, 0.1120, 0.0909, 0.0668, 0.0511], grad_fn=<ToCopyBackward0>), [' were', ' did', ' really', \"'re\", ' just'])\n",
      "(tensor([0.0928, 0.0517, 0.0504, 0.0472, 0.0462], grad_fn=<ToCopyBackward0>), [' did', ' made', ' were', ' pulled', ' do'])\n",
      "(tensor([0.7279, 0.0479, 0.0390, 0.0299, 0.0186], grad_fn=<ToCopyBackward0>), [' it', ' through', ' the', ' off', ' all'])\n",
      "(tensor([0.9071, 0.0450, 0.0158, 0.0136, 0.0126], grad_fn=<ToCopyBackward0>), [' off', ' together', ' all', ' through', ' out'])\n",
      "(tensor([0.6062, 0.3596, 0.0194, 0.0045, 0.0019], grad_fn=<ToCopyBackward0>), [' together', ' off', ' out', ' through', ' of'])\n",
      "(tensor([0.6600, 0.1195, 0.0497, 0.0230, 0.0191], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' well'])\n",
      "(tensor([0.2759, 0.1225, 0.0715, 0.0587, 0.0462], grad_fn=<ToCopyBackward0>), [' I', ' It', 'I', ' The', ' And'])\n",
      "(tensor([0.1703, 0.1253, 0.0888, 0.0703, 0.0671], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', ' think', \"'m\"])\n",
      "(tensor([0.1475, 0.1340, 0.1194, 0.0822, 0.0574], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' like', ' enjoyed', ' didn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie sucked. I thought it was a big waste of time and money. The movie was not scary or interesting or scary or interesting in any way. The characters were not believable. The acting was terrible. I think it was just a bad movie\n",
      "(tensor([0.3829, 0.1723, 0.0904, 0.0773, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2447, 0.1958, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6529, 0.0595, 0.0362, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.2484, 0.0999, 0.0783, 0.0380, 0.0303], grad_fn=<ToCopyBackward0>), [' big', ' so', '.', ' when', ' as'])\n",
      "(tensor([0.2363, 0.1336, 0.1131, 0.0224, 0.0217], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.1029, 0.0853, 0.0813, 0.0754, 0.0484], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.3712, 0.3150, 0.1754, 0.0372, 0.0108], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' I'])\n",
      "(tensor([0.7296, 0.0739, 0.0200, 0.0170, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' had', ' would', \"'s\"])\n",
      "(tensor([0.1462, 0.0534, 0.0521, 0.0507, 0.0499], grad_fn=<ToCopyBackward0>), [' a', ' boring', ' the', ' stupid', ' terrible'])\n",
      "(tensor([0.1765, 0.0729, 0.0599, 0.0410, 0.0296], grad_fn=<ToCopyBackward0>), [' big', ' bad', ' piece', ' waste', ' complete'])\n",
      "(tensor([0.1718, 0.0994, 0.0732, 0.0703, 0.0334], grad_fn=<ToCopyBackward0>), [' joke', ' disappointment', ',', ' waste', ' fat'])\n",
      "(tensor([9.9048e-01, 4.8734e-03, 6.2508e-04, 5.9900e-04, 4.1277e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' to'])\n",
      "(tensor([0.5318, 0.1461, 0.0535, 0.0346, 0.0238], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' a', ' talent'])\n",
      "(tensor([0.4538, 0.3886, 0.0965, 0.0082, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' for'])\n",
      "(tensor([0.4413, 0.1013, 0.0931, 0.0916, 0.0241], grad_fn=<ToCopyBackward0>), [' money', ' effort', ' a', ' I', ' my'])\n",
      "(tensor([0.7789, 0.0841, 0.0465, 0.0313, 0.0107], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' ('])\n",
      "(tensor([0.3552, 0.0909, 0.0745, 0.0335, 0.0301], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.1957, 0.1079, 0.0510, 0.0417, 0.0246], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' special'])\n",
      "(tensor([0.2865, 0.1451, 0.0366, 0.0292, 0.0292], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' starts', ' just'])\n",
      "(tensor([0.1103, 0.0931, 0.0636, 0.0474, 0.0370], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' very', ' about'])\n",
      "(tensor([0.4948, 0.1383, 0.0847, 0.0788, 0.0166], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' entertaining', ' fun'])\n",
      "(tensor([0.2527, 0.1792, 0.1387, 0.1292, 0.0581], grad_fn=<ToCopyBackward0>), [',', ' at', ' or', '.', ' enough'])\n",
      "(tensor([0.2251, 0.1634, 0.1294, 0.0948, 0.0555], grad_fn=<ToCopyBackward0>), [' interesting', ' funny', ' entertaining', ' suspense', ' scary'])\n",
      "(tensor([0.2979, 0.1575, 0.1162, 0.0946, 0.0691], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' at', ' enough'])\n",
      "(tensor([0.1689, 0.1465, 0.1371, 0.0646, 0.0541], grad_fn=<ToCopyBackward0>), [' entertaining', ' interesting', ' funny', ' scary', ' suspense'])\n",
      "(tensor([0.4163, 0.1599, 0.0944, 0.0697, 0.0692], grad_fn=<ToCopyBackward0>), [' or', ' at', ' in', '.', ' enough'])\n",
      "(tensor([0.8241, 0.0348, 0.0291, 0.0193, 0.0190], grad_fn=<ToCopyBackward0>), [' interesting', ' even', ' funny', ' entertaining', ' scary'])\n",
      "(tensor([0.3106, 0.2526, 0.1002, 0.0679, 0.0610], grad_fn=<ToCopyBackward0>), [' at', '.', ' or', ' to', ' in'])\n",
      "(tensor([0.5104, 0.1943, 0.1936, 0.0347, 0.0277], grad_fn=<ToCopyBackward0>), [' a', ' any', ' the', ' an', ' some'])\n",
      "(tensor([0.9458, 0.0301, 0.0025, 0.0018, 0.0016], grad_fn=<ToCopyBackward0>), [' way', ' other', ' manner', ' conceivable', ' of'])\n",
      "(tensor([0.6263, 0.0731, 0.0566, 0.0411, 0.0188], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' whatsoever', '....'])\n",
      "(tensor([0.2458, 0.1966, 0.1884, 0.0255, 0.0169], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' There', 'I'])\n",
      "(tensor([0.3918, 0.0896, 0.0524, 0.0520, 0.0438], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' only', ' movie'])\n",
      "(tensor([0.6534, 0.0485, 0.0484, 0.0328, 0.0265], grad_fn=<ToCopyBackward0>), [' were', ' are', ' in', ' weren', ' had'])\n",
      "(tensor([0.2217, 0.0521, 0.0466, 0.0374, 0.0318], grad_fn=<ToCopyBackward0>), [' not', ' boring', ' so', ' annoying', ' un'])\n",
      "(tensor([0.2594, 0.1136, 0.1027, 0.0922, 0.0652], grad_fn=<ToCopyBackward0>), [' interesting', ' lik', ' even', ' scary', ' believable'])\n",
      "(tensor([0.5898, 0.1158, 0.1124, 0.0883, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', ' in'])\n",
      "(tensor([0.6487, 0.0917, 0.0513, 0.0284, 0.0212], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.3177, 0.1306, 0.0624, 0.0623, 0.0479], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' dialogue', ' actors'])\n",
      "(tensor([0.8872, 0.0277, 0.0126, 0.0076, 0.0065], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ',', ' is'])\n",
      "(tensor([0.2048, 0.1159, 0.0708, 0.0658, 0.0573], grad_fn=<ToCopyBackward0>), [' not', ' terrible', ' horrible', ' bad', ' awful'])\n",
      "(tensor([0.7820, 0.1152, 0.0442, 0.0074, 0.0046], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' in'])\n",
      "(tensor([0.5924, 0.1026, 0.0795, 0.0454, 0.0158], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0805, 0.0668, 0.0647, 0.0599, 0.0429], grad_fn=<ToCopyBackward0>), [' was', ' don', \"'m\", ' think', ' thought'])\n",
      "(tensor([0.3039, 0.1568, 0.1240, 0.0977, 0.0571], grad_fn=<ToCopyBackward0>), [' the', ' that', ' it', ' this', ' I'])\n",
      "(tensor([0.3724, 0.1621, 0.1053, 0.0618, 0.0272], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' would', ' is', ' could'])\n",
      "(tensor([0.2139, 0.0961, 0.0684, 0.0516, 0.0493], grad_fn=<ToCopyBackward0>), [' a', ' shot', ' filmed', ' the', ' just'])\n",
      "(tensor([0.4824, 0.0391, 0.0350, 0.0242, 0.0242], grad_fn=<ToCopyBackward0>), [' a', ' an', ' terrible', ' plain', ' one'])\n",
      "(tensor([0.2104, 0.2040, 0.0746, 0.0276, 0.0267], grad_fn=<ToCopyBackward0>), [' waste', ' big', ' bad', ' bunch', ' stupid'])\n",
      "(tensor([0.5415, 0.1103, 0.0326, 0.0307, 0.0208], grad_fn=<ToCopyBackward0>), [' movie', ' script', ',', ' film', ' copy'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was an opportunity for the entire world to see the film. It was an opportunity to bring together all the people involved in this movie and the world to be able to see the movie in all its beauty. It was a very emotional movie for me\n",
      "(tensor([0.3830, 0.1726, 0.0904, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4368, 0.2455, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4796, 0.1351, 0.1036, 0.0551, 0.0255], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1496, 0.0706, 0.0648, 0.0337, 0.0313], grad_fn=<ToCopyBackward0>), [' awful', ' OK', ' atro', ' insult', ' opportunity'])\n",
      "(tensor([0.8052, 0.1397, 0.0074, 0.0073, 0.0068], grad_fn=<ToCopyBackward0>), [' to', ' for', ' that', '.', '...'])\n",
      "(tensor([0.2372, 0.1658, 0.1656, 0.1001, 0.0321], grad_fn=<ToCopyBackward0>), [' me', ' the', ' us', ' a', ' all'])\n",
      "(tensor([0.0534, 0.0392, 0.0370, 0.0317, 0.0252], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' producers', ' entire', ' creators'])\n",
      "(tensor([0.1447, 0.1045, 0.0414, 0.0386, 0.0348], grad_fn=<ToCopyBackward0>), [' film', ' world', ' community', ' movie', ' universe'])\n",
      "(tensor([0.9591, 0.0094, 0.0044, 0.0038, 0.0036], grad_fn=<ToCopyBackward0>), [' to', \"'s\", ' -', ' of', '.'])\n",
      "(tensor([0.3939, 0.1929, 0.1168, 0.0383, 0.0341], grad_fn=<ToCopyBackward0>), [' see', ' learn', ' get', ' be', ' say'])\n",
      "(tensor([0.0739, 0.0725, 0.0668, 0.0582, 0.0182], grad_fn=<ToCopyBackward0>), [' what', ' this', ' how', ' the', ' Cuba'])\n",
      "(tensor([0.0238, 0.0184, 0.0172, 0.0148, 0.0094], grad_fn=<ToCopyBackward0>), [' real', ' artwork', ' movie', ' film', ' trailer'])\n",
      "(tensor([0.3545, 0.1496, 0.0350, 0.0258, 0.0229], grad_fn=<ToCopyBackward0>), ['.', ' in', ' because', ' without', ' for'])\n",
      "(tensor([0.3270, 0.0991, 0.0810, 0.0463, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' So', ' And'])\n",
      "(tensor([0.4361, 0.1680, 0.1137, 0.0285, 0.0272], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' has'])\n",
      "(tensor([0.1826, 0.1056, 0.0512, 0.0356, 0.0312], grad_fn=<ToCopyBackward0>), [' a', ' like', ' an', ' so', ' the'])\n",
      "(tensor([0.6675, 0.0959, 0.0180, 0.0164, 0.0126], grad_fn=<ToCopyBackward0>), [' opportunity', ' embarrassment', ' insult', ' awful', ' amazing'])\n",
      "(tensor([0.5177, 0.4492, 0.0060, 0.0045, 0.0029], grad_fn=<ToCopyBackward0>), [' to', ' for', ' that', '...', ','])\n",
      "(tensor([0.1954, 0.0722, 0.0565, 0.0541, 0.0455], grad_fn=<ToCopyBackward0>), [' show', ' bring', ' be', ' tell', ' get'])\n",
      "(tensor([0.1914, 0.1591, 0.0750, 0.0719, 0.0545], grad_fn=<ToCopyBackward0>), [' to', ' the', ' out', ' together', ' a'])\n",
      "(tensor([0.2085, 0.1837, 0.1199, 0.0681, 0.0332], grad_fn=<ToCopyBackward0>), [' the', ' all', ' people', ' a', ' great'])\n",
      "(tensor([0.2724, 0.1992, 0.0995, 0.0315, 0.0285], grad_fn=<ToCopyBackward0>), [' the', ' those', ' people', ' these', ' of'])\n",
      "(tensor([0.1354, 0.0613, 0.0342, 0.0185, 0.0156], grad_fn=<ToCopyBackward0>), [' different', ' people', ' great', ' talented', ' world'])\n",
      "(tensor([0.5826, 0.2385, 0.0548, 0.0183, 0.0176], grad_fn=<ToCopyBackward0>), [' involved', ' who', ' in', ' that', ' from'])\n",
      "(tensor([0.7053, 0.0860, 0.0710, 0.0416, 0.0411], grad_fn=<ToCopyBackward0>), [' in', '.', ' with', ',', ' and'])\n",
      "(tensor([0.4383, 0.3114, 0.1281, 0.0208, 0.0101], grad_fn=<ToCopyBackward0>), [' the', ' this', ' making', ' it', ' a'])\n",
      "(tensor([0.3784, 0.1634, 0.1266, 0.0305, 0.0257], grad_fn=<ToCopyBackward0>), [' film', ' project', ' movie', '.', ' and'])\n",
      "(tensor([0.4965, 0.1260, 0.0965, 0.0692, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' to'])\n",
      "(tensor([0.6454, 0.0586, 0.0294, 0.0194, 0.0173], grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' make', ' get'])\n",
      "(tensor([0.0959, 0.0368, 0.0345, 0.0260, 0.0252], grad_fn=<ToCopyBackward0>), [' people', ' entire', ' world', ' rest', ' fact'])\n",
      "(tensor([0.2177, 0.1600, 0.1596, 0.0518, 0.0460], grad_fn=<ToCopyBackward0>), [' to', '.', ' of', ' in', ' at'])\n",
      "(tensor([0.3193, 0.0645, 0.0571, 0.0564, 0.0401], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' watch', ' enjoy'])\n",
      "(tensor([0.4389, 0.0526, 0.0409, 0.0352, 0.0338], grad_fn=<ToCopyBackward0>), [' able', ' together', ' amazed', ' a', ' inspired'])\n",
      "(tensor([9.9745e-01, 1.0900e-03, 3.9776e-04, 5.8682e-05, 5.7716e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' see', ' for', ' really', ' in'])\n",
      "(tensor([0.4183, 0.0676, 0.0474, 0.0339, 0.0333], grad_fn=<ToCopyBackward0>), [' see', ' enjoy', ' say', ' get', ' learn'])\n",
      "(tensor([0.3446, 0.2335, 0.1536, 0.0456, 0.0419], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' what'])\n",
      "(tensor([0.2694, 0.0884, 0.0805, 0.0343, 0.0220], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' result', ' complete', ' trailer'])\n",
      "(tensor([0.2769, 0.1611, 0.1566, 0.0464, 0.0354], grad_fn=<ToCopyBackward0>), [' in', ' together', '.', ' for', ' on'])\n",
      "(tensor([0.1786, 0.1633, 0.0904, 0.0873, 0.0377], grad_fn=<ToCopyBackward0>), [' all', ' a', ' the', ' its', ' an'])\n",
      "(tensor([0.6090, 0.0441, 0.0343, 0.0342, 0.0291], grad_fn=<ToCopyBackward0>), [' its', ' kinds', ' sorts', ' of', ' forms'])\n",
      "(tensor([0.4506, 0.0643, 0.0301, 0.0225, 0.0092], grad_fn=<ToCopyBackward0>), [' glory', ' spl', ' original', ' beauty', ' weird'])\n",
      "(tensor([0.6031, 0.1867, 0.0701, 0.0420, 0.0125], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' -'])\n",
      "(tensor([0.2164, 0.1154, 0.0811, 0.0593, 0.0548], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' So'])\n",
      "(tensor([0.3405, 0.2573, 0.0790, 0.0438, 0.0388], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' seemed', ' wasn'])\n",
      "(tensor([0.3397, 0.3301, 0.0366, 0.0225, 0.0219], grad_fn=<ToCopyBackward0>), [' an', ' a', ' the', ' not', ' like'])\n",
      "(tensor([0.3230, 0.1080, 0.0520, 0.0495, 0.0413], grad_fn=<ToCopyBackward0>), [' chance', ' very', ' way', ' great', ' good'])\n",
      "(tensor([0.1592, 0.0825, 0.0463, 0.0277, 0.0270], grad_fn=<ToCopyBackward0>), [' emotional', ' sad', ' difficult', ' unfortunate', ' emotionally'])\n",
      "(tensor([0.5218, 0.1121, 0.0920, 0.0417, 0.0303], grad_fn=<ToCopyBackward0>), [' movie', ' experience', ' film', ' time', ' day'])\n",
      "(tensor([0.4117, 0.1881, 0.1494, 0.0604, 0.0383], grad_fn=<ToCopyBackward0>), [' for', '.', ' to', ' and', ','])\n",
      "(tensor([0.8910, 0.0334, 0.0211, 0.0117, 0.0085], grad_fn=<ToCopyBackward0>), [' me', ' the', ' all', ' us', ' everyone'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a waste of my time. It was so predictable and predictable, I was actually surprised when I found out that it was not going to be the same movie as the preview I saw. The movie was slow and boring, the plot was\n",
      "(tensor([0.3841, 0.1716, 0.0897, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2433, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6518, 0.0597, 0.0363, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1371, 0.0702, 0.0660, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0613, 0.0530, 0.0426, 0.0405], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.9688, 0.0080, 0.0021, 0.0019, 0.0014], grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' and', ' because'])\n",
      "(tensor([0.5765, 0.0886, 0.0884, 0.0432, 0.0165], grad_fn=<ToCopyBackward0>), [' time', ' my', ' money', ' 90', ' 2'])\n",
      "(tensor([0.7810, 0.1434, 0.0491, 0.0054, 0.0042], grad_fn=<ToCopyBackward0>), [' time', ' money', ' life', ' $', ' precious'])\n",
      "(tensor([0.6273, 0.1341, 0.0512, 0.0329, 0.0165], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', '....'])\n",
      "(tensor([0.2251, 0.2047, 0.1471, 0.0253, 0.0204], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.2725, 0.1747, 0.0663, 0.0551, 0.0430], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' seemed', ' wasn'])\n",
      "(tensor([0.1132, 0.0943, 0.0837, 0.0792, 0.0764], grad_fn=<ToCopyBackward0>), [' slow', ' so', ' not', ' boring', ' a'])\n",
      "(tensor([0.3177, 0.1152, 0.0660, 0.0341, 0.0285], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' bad', ' slow', ' stupid'])\n",
      "(tensor([0.4359, 0.1536, 0.1456, 0.0862, 0.0349], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' that', ' I'])\n",
      "(tensor([0.1219, 0.0709, 0.0584, 0.0525, 0.0485], grad_fn=<ToCopyBackward0>), [' predictable', ' cliché', ' the', ' so', ' I'])\n",
      "(tensor([0.1529, 0.1163, 0.0910, 0.0856, 0.0795], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', '.', ' it'])\n",
      "(tensor([0.3079, 0.1449, 0.1240, 0.0648, 0.0636], grad_fn=<ToCopyBackward0>), [' I', ' that', ' and', ' the', ' it'])\n",
      "(tensor([0.1368, 0.1225, 0.0957, 0.0671, 0.0653], grad_fn=<ToCopyBackward0>), [' just', ' was', ' could', ' couldn', ' almost'])\n",
      "(tensor([0.0687, 0.0520, 0.0459, 0.0426, 0.0366], grad_fn=<ToCopyBackward0>), [' almost', ' disappointed', ' really', ' actually', ' getting'])\n",
      "(tensor([0.1330, 0.0802, 0.0798, 0.0661, 0.0388], grad_fn=<ToCopyBackward0>), [' surprised', ' disappointed', ' bored', ' embarrassed', ' looking'])\n",
      "(tensor([0.3050, 0.1628, 0.1392, 0.0829, 0.0680], grad_fn=<ToCopyBackward0>), [' when', ' at', ' that', ' by', ' to'])\n",
      "(tensor([0.5326, 0.2043, 0.1197, 0.0368, 0.0287], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' they', ' this'])\n",
      "(tensor([0.1718, 0.1502, 0.0666, 0.0598, 0.0587], grad_fn=<ToCopyBackward0>), [' saw', ' found', ' was', ' got', ' watched'])\n",
      "(tensor([0.7774, 0.0600, 0.0593, 0.0408, 0.0259], grad_fn=<ToCopyBackward0>), [' out', ' that', ' it', ' myself', ' the'])\n",
      "(tensor([0.3597, 0.3228, 0.0737, 0.0350, 0.0343], grad_fn=<ToCopyBackward0>), [' it', ' that', ' this', ' how', ' the'])\n",
      "(tensor([0.2565, 0.1515, 0.1428, 0.1207, 0.0508], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' I', ' there'])\n",
      "(tensor([0.6645, 0.1206, 0.0396, 0.0230, 0.0219], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' had', \"'s\", ' would'])\n",
      "(tensor([0.3038, 0.0938, 0.0815, 0.0505, 0.0470], grad_fn=<ToCopyBackward0>), [' a', ' based', ' not', ' about', ' supposed'])\n",
      "(tensor([0.1802, 0.1173, 0.0963, 0.0769, 0.0756], grad_fn=<ToCopyBackward0>), [' a', ' even', ' only', ' based', ' going'])\n",
      "(tensor([9.8531e-01, 1.9698e-03, 1.0393e-03, 9.9717e-04, 9.3773e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' straight', ' in', ' anywhere'])\n",
      "(tensor([0.5306, 0.3210, 0.0470, 0.0172, 0.0124], grad_fn=<ToCopyBackward0>), [' be', ' end', ' get', ' have', ' make'])\n",
      "(tensor([0.2334, 0.0967, 0.0631, 0.0481, 0.0450], grad_fn=<ToCopyBackward0>), [' a', ' rated', ' released', ' the', ' in'])\n",
      "(tensor([0.1056, 0.1018, 0.0795, 0.0780, 0.0662], grad_fn=<ToCopyBackward0>), [' last', ' worst', ' same', ' best', ' \"'])\n",
      "(tensor([0.2564, 0.1385, 0.1020, 0.0620, 0.0319], grad_fn=<ToCopyBackward0>), [' as', '.', ' movie', ' ending', ' in'])\n",
      "(tensor([0.4740, 0.0989, 0.0930, 0.0554, 0.0324], grad_fn=<ToCopyBackward0>), ['.', ' I', ' as', ' at', ' in'])\n",
      "(tensor([0.5233, 0.1455, 0.0360, 0.0246, 0.0241], grad_fn=<ToCopyBackward0>), [' the', ' in', ' before', ' it', ' \"'])\n",
      "(tensor([0.3187, 0.1766, 0.0748, 0.0654, 0.0436], grad_fn=<ToCopyBackward0>), [' previews', ' preview', ' one', ' trailer', ' book'])\n",
      "(tensor([0.3795, 0.1664, 0.0526, 0.0379, 0.0378], grad_fn=<ToCopyBackward0>), ['.', ' I', ',', ' reel', ' showed'])\n",
      "(tensor([0.6419, 0.1853, 0.0385, 0.0141, 0.0121], grad_fn=<ToCopyBackward0>), [' saw', ' watched', ' had', ' viewed', ' was'])\n",
      "(tensor([0.7594, 0.0578, 0.0316, 0.0190, 0.0144], grad_fn=<ToCopyBackward0>), ['.', ' at', ' in', ' back', ' on'])\n",
      "(tensor([0.2109, 0.1837, 0.1055, 0.0440, 0.0219], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'The'])\n",
      "(tensor([0.1230, 0.1222, 0.0691, 0.0396, 0.0380], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' acting', ' plot', ' first'])\n",
      "(tensor([0.2879, 0.1131, 0.0577, 0.0444, 0.0434], grad_fn=<ToCopyBackward0>), [' was', ' is', ' starts', ' started', ' had'])\n",
      "(tensor([0.1004, 0.0748, 0.0712, 0.0546, 0.0489], grad_fn=<ToCopyBackward0>), [' so', ' not', ' about', ' slow', ' predictable'])\n",
      "(tensor([0.4299, 0.2991, 0.1306, 0.0516, 0.0142], grad_fn=<ToCopyBackward0>), [' and', ' moving', ',', ' paced', ' going'])\n",
      "(tensor([0.3217, 0.1281, 0.0567, 0.0323, 0.0289], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' had', ' was', ' the'])\n",
      "(tensor([0.3538, 0.2887, 0.1993, 0.0320, 0.0196], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' with'])\n",
      "(tensor([0.2258, 0.1824, 0.0900, 0.0870, 0.0322], grad_fn=<ToCopyBackward0>), [' and', ' the', ' I', ' with', ' it'])\n",
      "(tensor([0.5502, 0.0491, 0.0484, 0.0373, 0.0306], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' plot', ' story'])\n",
      "(tensor([0.8093, 0.0187, 0.0098, 0.0091, 0.0080], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' seemed', ' wasn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that this was a really dumb movie. It's not even that I think it's dumb. It's just dumb in a way that makes no sense, that has nothing to do with what's going to happen in the story. There's a reason\n",
      "(tensor([0.3834, 0.1723, 0.0903, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3325, 0.2358, 0.0582, 0.0562, 0.0227], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.4146, 0.1972, 0.1689, 0.0494, 0.0165], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' would'])\n",
      "(tensor([0.4996, 0.1496, 0.0852, 0.0601, 0.0126], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' probably'])\n",
      "(tensor([0.1224, 0.0881, 0.0844, 0.0760, 0.0756], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' really', ' pretty', ' sequel'])\n",
      "(tensor([0.3680, 0.1323, 0.0573, 0.0366, 0.0229], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' stupid', ' cheesy', ' good'])\n",
      "(tensor([0.8759, 0.0411, 0.0138, 0.0094, 0.0070], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' story', ','])\n",
      "(tensor([0.6346, 0.0805, 0.0349, 0.0335, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', '!', ' when'])\n",
      "(tensor([0.2260, 0.1299, 0.0868, 0.0229, 0.0219], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.3113, 0.2136, 0.0516, 0.0408, 0.0365], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' seemed', ' had', ' is'])\n",
      "(tensor([0.2884, 0.0886, 0.0792, 0.0744, 0.0629], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' really', ' just'])\n",
      "(tensor([0.5607, 0.0942, 0.0666, 0.0471, 0.0298], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' as', ' that', ' a'])\n",
      "(tensor([0.4015, 0.1233, 0.0469, 0.0466, 0.0284], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' good', ' that', ' the'])\n",
      "(tensor([0.3763, 0.1177, 0.1014, 0.0784, 0.0782], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' bad', ' I', ' it'])\n",
      "(tensor([0.1673, 0.1355, 0.1205, 0.0669, 0.0641], grad_fn=<ToCopyBackward0>), [' don', \"'m\", ' think', ' didn', ' hate'])\n",
      "(tensor([0.5513, 0.1169, 0.0616, 0.0438, 0.0261], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' they'])\n",
      "(tensor([0.6749, 0.1165, 0.0961, 0.0133, 0.0118], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' sucks'])\n",
      "(tensor([0.2645, 0.2257, 0.1232, 0.0651, 0.0278], grad_fn=<ToCopyBackward0>), [' a', ' dumb', ' bad', ' stupid', ' good'])\n",
      "(tensor([0.4139, 0.2195, 0.0998, 0.0294, 0.0242], grad_fn=<ToCopyBackward0>), [',', '.', ';', ' --', ' to'])\n",
      "(tensor([0.4056, 0.2668, 0.0436, 0.0256, 0.0188], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.8336, 0.1105, 0.0178, 0.0097, 0.0049], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' is', ' was', ' has'])\n",
      "(tensor([0.6881, 0.0866, 0.0775, 0.0244, 0.0148], grad_fn=<ToCopyBackward0>), [' just', ' that', ' not', ' dumb', ' the'])\n",
      "(tensor([0.2391, 0.2278, 0.1040, 0.0521, 0.0369], grad_fn=<ToCopyBackward0>), [' that', ' dumb', ' stupid', ' really', ' not'])\n",
      "(tensor([0.3213, 0.2503, 0.1065, 0.0631, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ' in', ' that', ' as', ' for'])\n",
      "(tensor([0.3958, 0.3468, 0.0370, 0.0326, 0.0315], grad_fn=<ToCopyBackward0>), [' a', ' the', ' that', ' an', ' its'])\n",
      "(tensor([0.1315, 0.1134, 0.0829, 0.0798, 0.0491], grad_fn=<ToCopyBackward0>), [' way', ' really', ' different', ' very', ' movie'])\n",
      "(tensor([0.9010, 0.0400, 0.0127, 0.0092, 0.0020], grad_fn=<ToCopyBackward0>), [' that', ' I', ' you', ' where', ' thats'])\n",
      "(tensor([0.1514, 0.1443, 0.0834, 0.0624, 0.0539], grad_fn=<ToCopyBackward0>), [' is', ' makes', ' I', \"'s\", ' doesn'])\n",
      "(tensor([0.2911, 0.2513, 0.1799, 0.1128, 0.0173], grad_fn=<ToCopyBackward0>), [' you', ' no', ' it', ' me', ' the'])\n",
      "(tensor([0.8830, 0.0242, 0.0109, 0.0051, 0.0037], grad_fn=<ToCopyBackward0>), [' sense', ' damn', ' logical', ' goddamn', ' obvious'])\n",
      "(tensor([0.6111, 0.1002, 0.0893, 0.0713, 0.0304], grad_fn=<ToCopyBackward0>), ['.', ' whatsoever', ',', ' at', ' to'])\n",
      "(tensor([0.1808, 0.0739, 0.0560, 0.0470, 0.0410], grad_fn=<ToCopyBackward0>), [' and', ' like', ' even', ' except', ' that'])\n",
      "(tensor([0.2600, 0.1419, 0.1074, 0.0889, 0.0640], grad_fn=<ToCopyBackward0>), [' makes', \"'s\", ' is', ' doesn', ' has'])\n",
      "(tensor([0.6658, 0.2310, 0.0137, 0.0133, 0.0104], grad_fn=<ToCopyBackward0>), [' no', ' nothing', ' absolutely', ' been', ' a'])\n",
      "(tensor([0.9457, 0.0149, 0.0074, 0.0064, 0.0025], grad_fn=<ToCopyBackward0>), [' to', ' at', ' in', ' even', ' but'])\n",
      "(tensor([0.9279, 0.0169, 0.0111, 0.0109, 0.0043], grad_fn=<ToCopyBackward0>), [' do', ' say', ' recommend', ' offer', ' contribute'])\n",
      "(tensor([0.9742, 0.0068, 0.0035, 0.0018, 0.0011], grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' the', ' other', ' even'])\n",
      "(tensor([0.6771, 0.0723, 0.0270, 0.0241, 0.0230], grad_fn=<ToCopyBackward0>), [' the', ' any', ' anything', ' what', ' reality'])\n",
      "(tensor([0.2453, 0.2237, 0.0764, 0.0724, 0.0694], grad_fn=<ToCopyBackward0>), [' the', ' it', ' is', ' you', \"'s\"])\n",
      "(tensor([0.5848, 0.0972, 0.0727, 0.0503, 0.0376], grad_fn=<ToCopyBackward0>), [' going', ' happening', ' actually', ' supposed', ' on'])\n",
      "(tensor([9.8944e-01, 4.3143e-03, 2.1990e-03, 7.6308e-04, 5.7327e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' on', ' to', ' in', ' through', ' down'])\n",
      "(tensor([0.8962, 0.0384, 0.0195, 0.0063, 0.0049], grad_fn=<ToCopyBackward0>), [' happen', ' be', ' come', ' make', ' actually'])\n",
      "(tensor([0.4009, 0.2458, 0.0889, 0.0533, 0.0482], grad_fn=<ToCopyBackward0>), [' in', '.', ',', ' to', ' next'])\n",
      "(tensor([0.7764, 0.0823, 0.0599, 0.0169, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' any', ' a'])\n",
      "(tensor([0.6158, 0.1291, 0.0476, 0.0219, 0.0182], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' end', ' film', ' future'])\n",
      "(tensor([0.5903, 0.2297, 0.0525, 0.0380, 0.0131], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', ' at'])\n",
      "(tensor([0.1588, 0.1274, 0.1253, 0.0487, 0.0279], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.4129, 0.2121, 0.1953, 0.0877, 0.0240], grad_fn=<ToCopyBackward0>), [\"'s\", ' are', ' is', ' was', ' were'])\n",
      "(tensor([0.4259, 0.2229, 0.0693, 0.0374, 0.0344], grad_fn=<ToCopyBackward0>), [' nothing', ' no', ' a', ' not', ' just'])\n",
      "(tensor([0.1248, 0.0937, 0.0735, 0.0599, 0.0465], grad_fn=<ToCopyBackward0>), [' guy', ' really', ' lot', ' plot', ' reason'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I was going to watch this movie just because it's a good movie. I had a pretty good time and it was well made. I don't think I was expecting much, but it was pretty funny too. I really don't like to be\n",
      "(tensor([0.3838, 0.1715, 0.0896, 0.0771, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2234, 0.1943, 0.1577, 0.0694, 0.0541], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.4603, 0.2361, 0.0590, 0.0169, 0.0133], grad_fn=<ToCopyBackward0>), [' going', ' in', ' watching', ' so', ' the'])\n",
      "(tensor([0.9597, 0.0135, 0.0028, 0.0028, 0.0027], grad_fn=<ToCopyBackward0>), [' to', ' crazy', ' for', ' mad', ' blind'])\n",
      "(tensor([0.5428, 0.1262, 0.0626, 0.0561, 0.0284], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' have', ' get', ' see'])\n",
      "(tensor([0.5129, 0.1303, 0.1064, 0.0797, 0.0164], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' paint'])\n",
      "(tensor([0.5605, 0.1600, 0.0330, 0.0154, 0.0148], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' because', ' one', ' for'])\n",
      "(tensor([0.1312, 0.1176, 0.1099, 0.0805, 0.0551], grad_fn=<ToCopyBackward0>), [' all', ' just', ' for', ' in', ' and'])\n",
      "(tensor([0.5996, 0.1270, 0.1042, 0.0256, 0.0235], grad_fn=<ToCopyBackward0>), [' to', ' for', ' because', ' the', ' a'])\n",
      "(tensor([0.3071, 0.2041, 0.1212, 0.0694, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' of', ' its'])\n",
      "(tensor([0.4787, 0.1985, 0.0762, 0.0286, 0.0255], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' is', ' stars'])\n",
      "(tensor([0.2103, 0.1098, 0.0698, 0.0326, 0.0259], grad_fn=<ToCopyBackward0>), [' a', ' so', ' the', ' fun', ' about'])\n",
      "(tensor([0.1998, 0.1789, 0.1110, 0.0433, 0.0398], grad_fn=<ToCopyBackward0>), [' remake', ' good', ' really', ' pretty', ' classic'])\n",
      "(tensor([0.7560, 0.0302, 0.0250, 0.0248, 0.0167], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' comedy', ' horror', ' thriller'])\n",
      "(tensor([0.6662, 0.1708, 0.0328, 0.0151, 0.0129], grad_fn=<ToCopyBackward0>), ['.', ',', '...', '....', ' and'])\n",
      "(tensor([0.3613, 0.1099, 0.0874, 0.0481, 0.0411], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' And', ' Then'])\n",
      "(tensor([0.2217, 0.0626, 0.0553, 0.0478, 0.0329], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' didn', ' thought', ' had'])\n",
      "(tensor([0.1565, 0.1178, 0.0989, 0.0749, 0.0468], grad_fn=<ToCopyBackward0>), [' a', ' no', ' to', ' very', ' absolutely'])\n",
      "(tensor([0.3200, 0.1848, 0.0715, 0.0631, 0.0475], grad_fn=<ToCopyBackward0>), [' feeling', ' really', ' pretty', ' great', ' few'])\n",
      "(tensor([0.8883, 0.0189, 0.0076, 0.0056, 0.0042], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' high', ' decent', ' easy'])\n",
      "(tensor([0.3737, 0.3160, 0.0563, 0.0388, 0.0237], grad_fn=<ToCopyBackward0>), [' time', ' idea', ' laugh', ' feeling', ' imagination'])\n",
      "(tensor([0.3678, 0.2051, 0.1097, 0.0314, 0.0220], grad_fn=<ToCopyBackward0>), [' watching', '.', ',', ' and', ' with'])\n",
      "(tensor([0.4583, 0.0936, 0.0371, 0.0335, 0.0254], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' was'])\n",
      "(tensor([0.4381, 0.1360, 0.0398, 0.0319, 0.0291], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' wasn', ' had'])\n",
      "(tensor([0.1431, 0.0863, 0.0655, 0.0643, 0.0375], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' kind', ' funny', ' well'])\n",
      "(tensor([0.5275, 0.1425, 0.0982, 0.0941, 0.0264], grad_fn=<ToCopyBackward0>), [' acted', ' made', '-', ' done', ' directed'])\n",
      "(tensor([0.5510, 0.1099, 0.0932, 0.0831, 0.0337], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.2159, 0.1506, 0.1056, 0.0926, 0.0263], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' So'])\n",
      "(tensor([0.1032, 0.0867, 0.0773, 0.0601, 0.0427], grad_fn=<ToCopyBackward0>), [' was', ' just', ' really', \"'m\", ' don'])\n",
      "(tensor([9.9645e-01, 9.3083e-04, 5.8273e-04, 2.2626e-04, 1.6898e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2740, 0.2340, 0.1850, 0.0465, 0.0282], grad_fn=<ToCopyBackward0>), [' know', ' think', ' really', ' mind', ' like'])\n",
      "(tensor([0.2678, 0.2239, 0.0959, 0.0667, 0.0629], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' there', ' anyone'])\n",
      "(tensor([0.1228, 0.1214, 0.0746, 0.0743, 0.0607], grad_fn=<ToCopyBackward0>), [\"'m\", \"'ll\", ' would', ' was', \"'ve\"])\n",
      "(tensor([0.4698, 0.0507, 0.0406, 0.0377, 0.0306], grad_fn=<ToCopyBackward0>), [' expecting', ' looking', ' watching', ' disappointed', ' really'])\n",
      "(tensor([0.3810, 0.1643, 0.0997, 0.0939, 0.0280], grad_fn=<ToCopyBackward0>), [' much', ' a', ' to', ' it', ' anything'])\n",
      "(tensor([0.2108, 0.1990, 0.1643, 0.1260, 0.0778], grad_fn=<ToCopyBackward0>), [',', '.', ' from', ' more', ' in'])\n",
      "(tensor([0.8228, 0.0218, 0.0161, 0.0143, 0.0113], grad_fn=<ToCopyBackward0>), [' but', ' except', ' though', ' I', ' and'])\n",
      "(tensor([0.4350, 0.2142, 0.0348, 0.0308, 0.0216], grad_fn=<ToCopyBackward0>), [' I', ' it', ' then', ' the', ' what'])\n",
      "(tensor([0.4695, 0.0518, 0.0340, 0.0319, 0.0319], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' exceeded', ' didn', ' turned'])\n",
      "(tensor([0.2030, 0.0887, 0.0518, 0.0452, 0.0372], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' a', ' well', ' funny'])\n",
      "(tensor([0.5251, 0.0785, 0.0770, 0.0582, 0.0382], grad_fn=<ToCopyBackward0>), [' funny', ' cool', ' good', ' entertaining', ' boring'])\n",
      "(tensor([0.4664, 0.3543, 0.0563, 0.0134, 0.0121], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' too', '!'])\n",
      "(tensor([0.8990, 0.0374, 0.0121, 0.0106, 0.0092], grad_fn=<ToCopyBackward0>), ['.', ',', ' so', '!', ' and'])\n",
      "(tensor([0.3409, 0.1470, 0.1289, 0.0254, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'I', 'The'])\n",
      "(tensor([0.0878, 0.0839, 0.0640, 0.0576, 0.0550], grad_fn=<ToCopyBackward0>), [' was', ' really', ' don', \"'m\", ' think'])\n",
      "(tensor([0.1453, 0.1337, 0.0734, 0.0562, 0.0552], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' enjoyed', ' liked', ' wanted'])\n",
      "(tensor([9.9809e-01, 2.6486e-04, 2.4119e-04, 1.6472e-04, 8.4986e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', ';'])\n",
      "(tensor([0.3648, 0.2364, 0.0834, 0.0524, 0.0515], grad_fn=<ToCopyBackward0>), [' think', ' know', ' understand', ' like', ' mind'])\n",
      "(tensor([0.1292, 0.1236, 0.1119, 0.0463, 0.0405], grad_fn=<ToCopyBackward0>), [' to', ' this', ' the', ' it', ' movies'])\n",
      "(tensor([0.1623, 0.1195, 0.0965, 0.0723, 0.0543], grad_fn=<ToCopyBackward0>), [' be', ' see', ' watch', ' give', ' think'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty boring. I mean, I've seen a few bad bad movies before, but this was just boring. It wasn't funny, it wasn't entertaining, the plot was stupid, and the acting was terrible. I mean, how\n",
      "(tensor([0.3838, 0.1721, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2443, 0.1964, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7769, 0.0502, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1266, 0.1052, 0.0741, 0.0604], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.3245, 0.2565, 0.1515, 0.0371, 0.0155], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' at'])\n",
      "(tensor([0.1947, 0.1705, 0.1468, 0.0341, 0.0178], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.0962, 0.0702, 0.0618, 0.0559, 0.0475], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', ' really', \"'m\"])\n",
      "(tensor([0.3914, 0.1454, 0.0984, 0.0431, 0.0221], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' this'])\n",
      "(tensor([0.1754, 0.1474, 0.1263, 0.0264, 0.0229], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' there', ' this'])\n",
      "(tensor([0.0896, 0.0599, 0.0529, 0.0453, 0.0441], grad_fn=<ToCopyBackward0>), [\"'m\", ' like', \"'ve\", ' was', ' didn'])\n",
      "(tensor([0.4643, 0.1149, 0.0646, 0.0609, 0.0213], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' watched', ' never', ' got'])\n",
      "(tensor([0.1043, 0.0776, 0.0516, 0.0413, 0.0391], grad_fn=<ToCopyBackward0>), [' some', ' lots', ' a', ' pretty', ' more'])\n",
      "(tensor([0.8063, 0.0550, 0.0417, 0.0210, 0.0154], grad_fn=<ToCopyBackward0>), [' lot', ' few', ' couple', ' bunch', ' ton'])\n",
      "(tensor([0.1447, 0.0964, 0.0732, 0.0507, 0.0375], grad_fn=<ToCopyBackward0>), [' bad', ' other', ' films', ' movies', ' more'])\n",
      "(tensor([0.3159, 0.1607, 0.1246, 0.0531, 0.0259], grad_fn=<ToCopyBackward0>), [' movies', ' horror', ' films', ' bad', ' ones'])\n",
      "(tensor([0.6875, 0.0739, 0.0669, 0.0450, 0.0114], grad_fn=<ToCopyBackward0>), [' movies', ' bad', ' films', ' horror', ' movie'])\n",
      "(tensor([0.4783, 0.2503, 0.0732, 0.0394, 0.0385], grad_fn=<ToCopyBackward0>), [' in', ',', '.', ' before', ' and'])\n",
      "(tensor([0.5854, 0.0915, 0.0795, 0.0712, 0.0336], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', '...'])\n",
      "(tensor([0.7266, 0.1107, 0.0242, 0.0175, 0.0145], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' I', ' like'])\n",
      "(tensor([0.6231, 0.1408, 0.0701, 0.0091, 0.0090], grad_fn=<ToCopyBackward0>), [' this', ' I', ' not', ' they', ' it'])\n",
      "(tensor([0.5097, 0.2067, 0.0675, 0.0349, 0.0246], grad_fn=<ToCopyBackward0>), [' one', ' was', ' is', ' wasn', ' movie'])\n",
      "(tensor([0.3059, 0.0669, 0.0654, 0.0609, 0.0576], grad_fn=<ToCopyBackward0>), [' just', ' one', ' really', ' the', ' pretty'])\n",
      "(tensor([0.2657, 0.0475, 0.0399, 0.0387, 0.0379], grad_fn=<ToCopyBackward0>), [' boring', ' bad', ' a', ' plain', ' lame'])\n",
      "(tensor([0.4701, 0.2543, 0.0608, 0.0359, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' to'])\n",
      "(tensor([0.1983, 0.1834, 0.1386, 0.0376, 0.0252], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2879, 0.1205, 0.1091, 0.0803, 0.0778], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' wasn', ' didn'])\n",
      "(tensor([9.9769e-01, 6.3103e-04, 2.9513e-04, 1.3166e-04, 1.2387e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.4173, 0.2790, 0.0971, 0.0347, 0.0206], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' even', ' bad', ' that'])\n",
      "(tensor([0.4189, 0.1629, 0.1242, 0.1043, 0.0504], grad_fn=<ToCopyBackward0>), [',', '.', ' at', ' or', ' and'])\n",
      "(tensor([0.3235, 0.0836, 0.0829, 0.0746, 0.0582], grad_fn=<ToCopyBackward0>), [' it', ' or', ' and', ' the', ' I'])\n",
      "(tensor([0.7300, 0.1345, 0.0962, 0.0129, 0.0064], grad_fn=<ToCopyBackward0>), [' wasn', ' didn', ' was', ' had', ' just'])\n",
      "(tensor([9.9786e-01, 4.1826e-04, 3.1893e-04, 1.6936e-04, 1.0430e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '.'])\n",
      "(tensor([0.4437, 0.1345, 0.0509, 0.0508, 0.0436], grad_fn=<ToCopyBackward0>), [' interesting', ' scary', ' exciting', ' entertaining', ' suspense'])\n",
      "(tensor([0.5704, 0.2109, 0.0616, 0.0450, 0.0139], grad_fn=<ToCopyBackward0>), [',', '.', '...', ' and', ' or'])\n",
      "(tensor([0.6652, 0.1572, 0.0426, 0.0230, 0.0214], grad_fn=<ToCopyBackward0>), [' it', ' and', ' the', ' I', ' there'])\n",
      "(tensor([0.4567, 0.0651, 0.0562, 0.0538, 0.0269], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' plot', ' actors', ' story'])\n",
      "(tensor([0.8029, 0.0406, 0.0158, 0.0116, 0.0107], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' didn', ' had', ' made'])\n",
      "(tensor([0.0979, 0.0769, 0.0625, 0.0482, 0.0427], grad_fn=<ToCopyBackward0>), [' predictable', ' stupid', ' boring', ' so', ' weak'])\n",
      "(tensor([0.3984, 0.3353, 0.1420, 0.0404, 0.0107], grad_fn=<ToCopyBackward0>), [' and', ',', '.', '...', ' as'])\n",
      "(tensor([0.4356, 0.3420, 0.0578, 0.0253, 0.0230], grad_fn=<ToCopyBackward0>), [' and', ' the', ' it', ' I', ' there'])\n",
      "(tensor([0.5496, 0.1175, 0.0978, 0.0363, 0.0113], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' even'])\n",
      "(tensor([0.7286, 0.0458, 0.0353, 0.0321, 0.0120], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' characters', ' ending', ' movie'])\n",
      "(tensor([0.7525, 0.0968, 0.0436, 0.0150, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', '...', ' just'])\n",
      "(tensor([0.0891, 0.0763, 0.0425, 0.0375, 0.0368], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' even', ' horrible', ' worse'])\n",
      "(tensor([0.8823, 0.0281, 0.0159, 0.0155, 0.0089], grad_fn=<ToCopyBackward0>), ['.', '!', '...', ',', ' ('])\n",
      "(tensor([0.1906, 0.1733, 0.1000, 0.0341, 0.0226], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "(tensor([0.1008, 0.0782, 0.0635, 0.0555, 0.0495], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' don', ' really', ' mean'])\n",
      "(tensor([0.8010, 0.0312, 0.0216, 0.0166, 0.0104], grad_fn=<ToCopyBackward0>), [',', ' the', ' I', ' it', ' this'])\n",
      "(tensor([0.2188, 0.0573, 0.0560, 0.0314, 0.0220], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' how', ' there'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the worst movie I have ever seen! It is not even funny! I am a Christian and a Christian has no business making a movie like this. If you want to see a movie that is FUNNY you should see The Naked Gun with\n",
      "(tensor([0.3834, 0.1721, 0.0904, 0.0772, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4368, 0.2450, 0.1961, 0.0166, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4797, 0.1353, 0.1035, 0.0552, 0.0255], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7418, 0.0314, 0.0260, 0.0243, 0.0177], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.6909, 0.0867, 0.0160, 0.0158, 0.0132], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' Christmas', ' horror'])\n",
      "(tensor([0.6251, 0.2458, 0.0754, 0.0089, 0.0069], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' in'])\n",
      "(tensor([0.4412, 0.2257, 0.1737, 0.0719, 0.0536], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([9.2860e-01, 6.2850e-02, 2.0397e-03, 1.6488e-03, 9.0628e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' EVER', ' watched'])\n",
      "(tensor([0.8896, 0.0225, 0.0169, 0.0107, 0.0106], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' wasted', ' made'])\n",
      "(tensor([0.5637, 0.1221, 0.1023, 0.0780, 0.0212], grad_fn=<ToCopyBackward0>), ['.', '!', ' in', ',', '...'])\n",
      "(tensor([0.3399, 0.1364, 0.1149, 0.0301, 0.0201], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' No'])\n",
      "(tensor([0.4375, 0.1171, 0.0820, 0.0376, 0.0338], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' had', ' made'])\n",
      "(tensor([0.1811, 0.1624, 0.0938, 0.0508, 0.0338], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' the', ' just'])\n",
      "(tensor([0.6197, 0.2367, 0.0128, 0.0125, 0.0118], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' a', ' as', ' only'])\n",
      "(tensor([0.5610, 0.0643, 0.0451, 0.0434, 0.0396], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' worthy', ' a', ' worth'])\n",
      "(tensor([0.2785, 0.2502, 0.1081, 0.0787, 0.0404], grad_fn=<ToCopyBackward0>), ['!', '.', ',', ' to', ' at'])\n",
      "(tensor([0.2609, 0.1575, 0.1521, 0.0186, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' No'])\n",
      "(tensor([0.0863, 0.0710, 0.0594, 0.0561, 0.0457], grad_fn=<ToCopyBackward0>), [' have', ' am', ' was', ' don', ' would'])\n",
      "(tensor([0.1958, 0.1008, 0.0652, 0.0617, 0.0354], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' speech', ' surprised'])\n",
      "(tensor([0.0914, 0.0838, 0.0376, 0.0376, 0.0338], grad_fn=<ToCopyBackward0>), [' Christian', ' big', ' family', ' huge', ' fan'])\n",
      "(tensor([0.3647, 0.1158, 0.0431, 0.0305, 0.0219], grad_fn=<ToCopyBackward0>), [' and', ',', ' &', ' I', '!'])\n",
      "(tensor([0.5247, 0.0667, 0.0395, 0.0329, 0.0303], grad_fn=<ToCopyBackward0>), [' I', ' this', ' a', ' my', ' am'])\n",
      "(tensor([0.3024, 0.0851, 0.0184, 0.0148, 0.0147], grad_fn=<ToCopyBackward0>), [' Christian', ' family', ' Catholic', ' Family', ' very'])\n",
      "(tensor([0.0689, 0.0482, 0.0366, 0.0308, 0.0263], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' does', ' doesn', ' has'])\n",
      "(tensor([0.1509, 0.0764, 0.0700, 0.0678, 0.0624], grad_fn=<ToCopyBackward0>), [' a', ' no', ' always', ' never', ' to'])\n",
      "(tensor([0.3190, 0.1138, 0.0702, 0.0427, 0.0302], grad_fn=<ToCopyBackward0>), [' right', ' sense', ' business', ' taste', ' part'])\n",
      "(tensor([0.4466, 0.0497, 0.0431, 0.0395, 0.0295], grad_fn=<ToCopyBackward0>), [' making', ' saying', ' being', ' telling', ' ever'])\n",
      "(tensor([0.4763, 0.2654, 0.0814, 0.0465, 0.0108], grad_fn=<ToCopyBackward0>), [' a', ' movies', ' such', ' this', ' an'])\n",
      "(tensor([0.7535, 0.1247, 0.0325, 0.0085, 0.0055], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' mockery', ' stupid'])\n",
      "(tensor([0.8260, 0.0711, 0.0183, 0.0174, 0.0164], grad_fn=<ToCopyBackward0>), [' like', ' that', ' about', ' this', ' with'])\n",
      "(tensor([9.7466e-01, 2.2093e-02, 7.8636e-04, 4.4078e-04, 3.8438e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' this', ' that', ' the', ' \"', ' it'])\n",
      "(tensor([0.3209, 0.2831, 0.0705, 0.0606, 0.0590], grad_fn=<ToCopyBackward0>), ['.', '!', '!\"', ' one', ','])\n",
      "(tensor([0.3181, 0.0997, 0.0946, 0.0246, 0.0189], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' They'])\n",
      "(tensor([0.4186, 0.1740, 0.1162, 0.0445, 0.0372], grad_fn=<ToCopyBackward0>), [' you', ' I', ' they', ' this', ' it'])\n",
      "(tensor([0.2526, 0.2004, 0.0653, 0.0407, 0.0354], grad_fn=<ToCopyBackward0>), [' want', ' are', ' have', ' do', ' like'])\n",
      "(tensor([0.8618, 0.0951, 0.0067, 0.0053, 0.0031], grad_fn=<ToCopyBackward0>), [' to', ' a', ' an', ' comedy', ' the'])\n",
      "(tensor([0.2983, 0.1816, 0.0809, 0.0657, 0.0500], grad_fn=<ToCopyBackward0>), [' see', ' make', ' be', ' watch', ' go'])\n",
      "(tensor([0.6165, 0.0491, 0.0446, 0.0338, 0.0256], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' more', ' how'])\n",
      "(tensor([0.3593, 0.1586, 0.1428, 0.0918, 0.0386], grad_fn=<ToCopyBackward0>), [' movie', ' good', ' comedy', ' laugh', ' funny'])\n",
      "(tensor([0.5758, 0.1229, 0.0856, 0.0295, 0.0216], grad_fn=<ToCopyBackward0>), [' that', ' about', ' like', ' with', ' you'])\n",
      "(tensor([0.2238, 0.1060, 0.0535, 0.0274, 0.0267], grad_fn=<ToCopyBackward0>), [' is', ' will', ' has', ' teaches', \"'s\"])\n",
      "(tensor([0.5012, 0.1006, 0.0867, 0.0584, 0.0321], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' FUN', ' not', ' fun'])\n",
      "(tensor([9.7089e-01, 6.1290e-03, 5.6744e-03, 4.8085e-03, 8.1983e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['NY', 'N', 'ny', 'D', '-'])\n",
      "(tensor([0.3898, 0.1358, 0.0831, 0.0593, 0.0349], grad_fn=<ToCopyBackward0>), [' then', ' you', ',', ' and', ' this'])\n",
      "(tensor([0.2337, 0.1733, 0.1063, 0.0884, 0.0692], grad_fn=<ToCopyBackward0>), [' should', ' can', ' have', ' go', ' will'])\n",
      "(tensor([0.3955, 0.3530, 0.0780, 0.0213, 0.0144], grad_fn=<ToCopyBackward0>), [' go', ' see', ' watch', ' rent', ' get'])\n",
      "(tensor([0.0307, 0.0254, 0.0168, 0.0114, 0.0110], grad_fn=<ToCopyBackward0>), [' the', ' The', ' Dumb', ' \"', ' Billy'])\n",
      "(tensor([0.3987, 0.0271, 0.0230, 0.0203, 0.0163], grad_fn=<ToCopyBackward0>), [' Naked', ' Simpsons', ' Nut', ' Incredible', ' Ring'])\n",
      "(tensor([0.8582, 0.0382, 0.0141, 0.0069, 0.0040], grad_fn=<ToCopyBackward0>), [' Gun', ' Brothers', ' Man', ' Brother', ' Mile'])\n",
      "(tensor([0.1880, 0.1075, 0.0744, 0.0592, 0.0531], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' with', ' the'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd seen it all. I was wrong. This movie is really funny. It's not as bad as I thought it could be. It's not as bad as I thought it could be. I was really looking forward to seeing it. This\n",
      "(tensor([0.3834, 0.1722, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2252, 0.1946, 0.1574, 0.0695, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2081, 0.1049, 0.0618, 0.0395, 0.0298], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.4483, 0.0926, 0.0914, 0.0736, 0.0531], grad_fn=<ToCopyBackward0>), [' it', ' everything', ' the', ' all', ' a'])\n",
      "(tensor([9.9687e-01, 5.3489e-04, 3.8403e-04, 2.1002e-04, 1.7362e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', '.', ',', ' done', ' in'])\n",
      "(tensor([0.2495, 0.2374, 0.1108, 0.0868, 0.0486], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', ' in', ' before'])\n",
      "(tensor([0.4638, 0.0460, 0.0439, 0.0386, 0.0208], grad_fn=<ToCopyBackward0>), [' I', ' This', ' The', ' It', ' But'])\n",
      "(tensor([0.1886, 0.1414, 0.0792, 0.0623, 0.0295], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', \"'m\", \"'ve\"])\n",
      "(tensor([0.8307, 0.0477, 0.0147, 0.0082, 0.0058], grad_fn=<ToCopyBackward0>), [' wrong', ' so', ' really', ' in', ' a'])\n",
      "(tensor([0.8309, 0.0975, 0.0101, 0.0099, 0.0068], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' on', '!!'])\n",
      "(tensor([0.3055, 0.2609, 0.0551, 0.0472, 0.0385], grad_fn=<ToCopyBackward0>), [' This', ' I', ' The', ' It', 'This'])\n",
      "(tensor([0.3767, 0.1608, 0.0961, 0.0861, 0.0415], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.4154, 0.1871, 0.0378, 0.0359, 0.0319], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' really'])\n",
      "(tensor([0.0927, 0.0614, 0.0559, 0.0457, 0.0412], grad_fn=<ToCopyBackward0>), [' just', ' about', ' so', ' really', ' not'])\n",
      "(tensor([0.2096, 0.1168, 0.0829, 0.0709, 0.0487], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ',', ' funny', ' something'])\n",
      "(tensor([0.5458, 0.1687, 0.1187, 0.0611, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' as'])\n",
      "(tensor([0.2225, 0.2065, 0.1217, 0.0218, 0.0196], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.4573, 0.1644, 0.1432, 0.0216, 0.0193], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' really'])\n",
      "(tensor([0.1980, 0.0816, 0.0597, 0.0546, 0.0398], grad_fn=<ToCopyBackward0>), [' not', ' really', ' got', ' like', ' funny'])\n",
      "(tensor([0.1114, 0.1046, 0.0918, 0.0648, 0.0597], grad_fn=<ToCopyBackward0>), [' even', ' as', ' just', ' a', ' that'])\n",
      "(tensor([0.2531, 0.0522, 0.0367, 0.0297, 0.0206], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' dumb', ' bad', ' much'])\n",
      "(tensor([0.9722, 0.0102, 0.0027, 0.0017, 0.0014], grad_fn=<ToCopyBackward0>), [' as', ' a', ',', ' or', ' for'])\n",
      "(tensor([0.4552, 0.0722, 0.0673, 0.0424, 0.0372], grad_fn=<ToCopyBackward0>), [' I', ' \"', ' it', ' you', ' the'])\n",
      "(tensor([0.4993, 0.0920, 0.0866, 0.0405, 0.0356], grad_fn=<ToCopyBackward0>), [' thought', ' expected', ' was', ' think', ' first'])\n",
      "(tensor([9.8347e-01, 5.6156e-03, 4.2148e-03, 9.1651e-04, 6.0574e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' I', ' the'])\n",
      "(tensor([0.6812, 0.2497, 0.0361, 0.0162, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', \"'d\", ' might'])\n",
      "(tensor([0.9217, 0.0342, 0.0149, 0.0063, 0.0043], grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', ' or', ' possibly'])\n",
      "(tensor([0.5176, 0.3562, 0.0177, 0.0130, 0.0094], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.2348, 0.2061, 0.1224, 0.0223, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.5983, 0.1164, 0.0561, 0.0211, 0.0166], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' really'])\n",
      "(tensor([0.2385, 0.1490, 0.0616, 0.0383, 0.0358], grad_fn=<ToCopyBackward0>), [' not', ' funny', ' just', ' a', ' really'])\n",
      "(tensor([0.7490, 0.0910, 0.0240, 0.0156, 0.0136], grad_fn=<ToCopyBackward0>), [' as', ' even', ' that', ' funny', ' the'])\n",
      "(tensor([0.3582, 0.2635, 0.0712, 0.0411, 0.0127], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' good', ' boring', ' scary'])\n",
      "(tensor([0.9755, 0.0049, 0.0046, 0.0022, 0.0011], grad_fn=<ToCopyBackward0>), [' as', ',', ' a', ' or', ' it'])\n",
      "(tensor([0.8397, 0.0182, 0.0105, 0.0089, 0.0083], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' it', ' you'])\n",
      "(tensor([0.8584, 0.0396, 0.0284, 0.0227, 0.0032], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' expected', ' would'])\n",
      "(tensor([0.6622, 0.1293, 0.0597, 0.0260, 0.0094], grad_fn=<ToCopyBackward0>), [' it', ' I', '.', ' the', ','])\n",
      "(tensor([0.8906, 0.0384, 0.0343, 0.0071, 0.0044], grad_fn=<ToCopyBackward0>), [' could', ' would', ' was', ' should', ' might'])\n",
      "(tensor([9.9225e-01, 1.8182e-03, 1.4155e-03, 4.1104e-04, 3.6075e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' get', ' have', '.', ' become'])\n",
      "(tensor([0.8265, 0.0677, 0.0202, 0.0165, 0.0076], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' because', ' either'])\n",
      "(tensor([0.3619, 0.2088, 0.1067, 0.0284, 0.0239], grad_fn=<ToCopyBackward0>), [' It', ' I', ' This', ' The', ' You'])\n",
      "(tensor([0.1361, 0.1058, 0.1018, 0.0744, 0.0376], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' think'])\n",
      "(tensor([0.4432, 0.1290, 0.0635, 0.0579, 0.0395], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' so', ' very', ' actually'])\n",
      "(tensor([0.2566, 0.1647, 0.1084, 0.0829, 0.0432], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' surprised', ' wrong', ','])\n",
      "(tensor([9.9557e-01, 2.8094e-03, 5.6005e-04, 3.5118e-04, 2.3215e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' at', ' forwards'])\n",
      "(tensor([9.9206e-01, 1.8473e-03, 1.1525e-03, 1.0936e-03, 7.0513e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' it', '.', ','])\n",
      "(tensor([0.3173, 0.2053, 0.1913, 0.1337, 0.0286], grad_fn=<ToCopyBackward0>), [' this', ' watching', ' seeing', ' it', ' the'])\n",
      "(tensor([0.5159, 0.2295, 0.0274, 0.0206, 0.0115], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' some', ' a'])\n",
      "(tensor([0.2753, 0.2594, 0.0852, 0.0702, 0.0587], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' and', ' but'])\n",
      "(tensor([0.5408, 0.1355, 0.0374, 0.0244, 0.0241], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' This', ' The'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a good movie. It's not the best movie I've ever seen, but it's not the worst movie I've ever seen. I thought it was pretty cool and I really wanted to like it. The problem I had with this\n",
      "(tensor([0.3844, 0.1717, 0.0898, 0.0771, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2437, 0.1962, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6521, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0661, 0.0547, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0851, 0.0615, 0.0527, 0.0428, 0.0406], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.3100, 0.1901, 0.0865, 0.0350, 0.0293], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' story', ' premise'])\n",
      "(tensor([0.3249, 0.1267, 0.0883, 0.0762, 0.0577], grad_fn=<ToCopyBackward0>), ['.', '...', '....', '!', ','])\n",
      "(tensor([0.3263, 0.2303, 0.0792, 0.0279, 0.0153], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.3805, 0.1241, 0.1186, 0.0543, 0.0362], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' has', ' wasn'])\n",
      "(tensor([0.1638, 0.1083, 0.0621, 0.0547, 0.0532], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' one', ' got'])\n",
      "(tensor([0.1194, 0.0989, 0.0939, 0.0782, 0.0691], grad_fn=<ToCopyBackward0>), [' as', ' even', ' great', ' that', ' the'])\n",
      "(tensor([0.5115, 0.3804, 0.0435, 0.0132, 0.0042], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' most', ' first'])\n",
      "(tensor([0.4744, 0.0762, 0.0647, 0.0352, 0.0279], grad_fn=<ToCopyBackward0>), [' movie', ' I', ' picture', ' comedy', ' film'])\n",
      "(tensor([0.7010, 0.1190, 0.0320, 0.0266, 0.0168], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ',', ' in'])\n",
      "(tensor([0.7021, 0.2336, 0.0267, 0.0077, 0.0076], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' think', ' saw'])\n",
      "(tensor([0.7489, 0.2309, 0.0046, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' made'])\n",
      "(tensor([0.7672, 0.1278, 0.0238, 0.0205, 0.0166], grad_fn=<ToCopyBackward0>), [' seen', ' made', ' watched', ' had', ' been'])\n",
      "(tensor([0.7132, 0.0813, 0.0660, 0.0552, 0.0115], grad_fn=<ToCopyBackward0>), [',', '.', ' in', ' but', ' ('])\n",
      "(tensor([0.8433, 0.0286, 0.0228, 0.0157, 0.0097], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' not', ' I'])\n",
      "(tensor([0.6709, 0.0929, 0.0145, 0.0139, 0.0132], grad_fn=<ToCopyBackward0>), [' it', ' I', ' for', ' its', ' if'])\n",
      "(tensor([0.5882, 0.2051, 0.0386, 0.0225, 0.0215], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.4788, 0.0868, 0.0754, 0.0460, 0.0306], grad_fn=<ToCopyBackward0>), [' not', ' good', ' a', ' worth', ' pretty'])\n",
      "(tensor([0.6972, 0.0812, 0.0511, 0.0299, 0.0171], grad_fn=<ToCopyBackward0>), [' the', ' a', ' as', ' bad', ' one'])\n",
      "(tensor([9.8363e-01, 4.2912e-03, 1.1746e-03, 8.3616e-04, 8.0905e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' worst', ' worse', ' bad', ' second', ' most'])\n",
      "(tensor([0.3258, 0.3116, 0.1535, 0.0695, 0.0429], grad_fn=<ToCopyBackward0>), [' either', ' movie', ' one', ',', '.'])\n",
      "(tensor([0.9497, 0.0157, 0.0127, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' either', '.', ','])\n",
      "(tensor([9.7937e-01, 1.3476e-02, 2.7501e-03, 7.8985e-04, 7.0895e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' ever', ' think', \"'m\"])\n",
      "(tensor([9.6931e-01, 2.7118e-02, 1.1365e-03, 4.0801e-04, 3.8085e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' even', ' EVER'])\n",
      "(tensor([0.9290, 0.0521, 0.0039, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' been', ' had', ' made'])\n",
      "(tensor([0.4102, 0.4083, 0.1003, 0.0251, 0.0062], grad_fn=<ToCopyBackward0>), [',', '.', ' either', ' and', ' as'])\n",
      "(tensor([0.2905, 0.2722, 0.0675, 0.0339, 0.0194], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'It'])\n",
      "(tensor([0.0768, 0.0719, 0.0663, 0.0569, 0.0448], grad_fn=<ToCopyBackward0>), [\"'m\", ' thought', ' think', ' just', ' don'])\n",
      "(tensor([0.5461, 0.1687, 0.1337, 0.0495, 0.0116], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.7773, 0.0458, 0.0209, 0.0207, 0.0130], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' did'])\n",
      "(tensor([0.2009, 0.1233, 0.0590, 0.0563, 0.0546], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' watch', ' interesting', ' funny'])\n",
      "(tensor([0.2956, 0.2564, 0.1447, 0.0355, 0.0270], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' entertaining', ' cool', ' interesting'])\n",
      "(tensor([0.3407, 0.2319, 0.1780, 0.0363, 0.0170], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' when'])\n",
      "(tensor([0.2909, 0.1302, 0.0686, 0.0577, 0.0426], grad_fn=<ToCopyBackward0>), [' I', ' interesting', ' the', ' pretty', ' well'])\n",
      "(tensor([0.1737, 0.1196, 0.1065, 0.0634, 0.0506], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' really', ' enjoyed', ' was'])\n",
      "(tensor([0.2483, 0.2109, 0.1023, 0.0582, 0.0578], grad_fn=<ToCopyBackward0>), [' liked', ' enjoyed', ' wanted', ' like', ' thought'])\n",
      "(tensor([0.9357, 0.0242, 0.0111, 0.0086, 0.0050], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' my', ' a'])\n",
      "(tensor([0.5812, 0.2714, 0.0722, 0.0077, 0.0063], grad_fn=<ToCopyBackward0>), [' see', ' like', ' watch', ' be', ' go'])\n",
      "(tensor([0.8788, 0.0732, 0.0280, 0.0025, 0.0014], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' some'])\n",
      "(tensor([0.5487, 0.2721, 0.0168, 0.0167, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' more', ' a', ' but'])\n",
      "(tensor([0.2504, 0.1233, 0.0940, 0.0725, 0.0352], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', 'I'])\n",
      "(tensor([0.2437, 0.1207, 0.0784, 0.0588, 0.0548], grad_fn=<ToCopyBackward0>), [' problem', ' only', ' acting', ' main', ' movie'])\n",
      "(tensor([0.6431, 0.2365, 0.0437, 0.0396, 0.0066], grad_fn=<ToCopyBackward0>), [' is', ' was', ' I', ' with', ','])\n",
      "(tensor([0.5313, 0.3638, 0.0215, 0.0116, 0.0072], grad_fn=<ToCopyBackward0>), [' have', ' had', ' found', ' think', ' really'])\n",
      "(tensor([0.6511, 0.2034, 0.0760, 0.0171, 0.0053], grad_fn=<ToCopyBackward0>), [' was', ' with', ' is', ',', ' in'])\n",
      "(tensor([0.5265, 0.3908, 0.0705, 0.0021, 0.0009], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie had very little to do with New York. I think the city has an enormous amount to do with it, but this movie was more about the city than the city was about the movie. So I really didn't think it could be a\n",
      "(tensor([0.3836, 0.1720, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5002, 0.0601, 0.0341, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6239, 0.0400, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2579, 0.0922, 0.0640, 0.0438, 0.0428], grad_fn=<ToCopyBackward0>), [' a', ' to', ' some', ' the', ' very'])\n",
      "(tensor([0.3123, 0.2931, 0.0420, 0.0280, 0.0204], grad_fn=<ToCopyBackward0>), [' little', ' good', ' few', ' well', ' interesting'])\n",
      "(tensor([0.4918, 0.1670, 0.0240, 0.0216, 0.0190], grad_fn=<ToCopyBackward0>), [' to', ' plot', ' humor', ' in', ','])\n",
      "(tensor([0.3500, 0.2445, 0.1909, 0.0744, 0.0535], grad_fn=<ToCopyBackward0>), [' do', ' recommend', ' say', ' offer', ' comment'])\n",
      "(tensor([0.8996, 0.0346, 0.0088, 0.0044, 0.0042], grad_fn=<ToCopyBackward0>), [' with', ' about', ' in', ' other', ','])\n",
      "(tensor([0.2603, 0.0343, 0.0291, 0.0234, 0.0220], grad_fn=<ToCopyBackward0>), [' the', ' Little', ' this', ' New', ' any'])\n",
      "(tensor([0.7220, 0.0445, 0.0389, 0.0341, 0.0187], grad_fn=<ToCopyBackward0>), [' York', ' Zealand', ' Zeal', ' Age', ' Jersey'])\n",
      "(tensor([0.4524, 0.2682, 0.1022, 0.0254, 0.0212], grad_fn=<ToCopyBackward0>), [' City', '.', ',', ' and', ' in'])\n",
      "(tensor([0.1619, 0.1454, 0.1341, 0.0365, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.1489, 0.0981, 0.0713, 0.0595, 0.0503], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' mean', \"'m\"])\n",
      "(tensor([0.3492, 0.1443, 0.1238, 0.0537, 0.0330], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' New'])\n",
      "(tensor([0.1029, 0.0855, 0.0607, 0.0371, 0.0361], grad_fn=<ToCopyBackward0>), [' movie', ' city', ' director', ' only', ' New'])\n",
      "(tensor([0.3081, 0.1576, 0.0869, 0.0796, 0.0434], grad_fn=<ToCopyBackward0>), [' is', ' was', ' itself', ' of', ' has'])\n",
      "(tensor([0.2364, 0.1358, 0.1029, 0.0590, 0.0377], grad_fn=<ToCopyBackward0>), [' a', ' to', ' been', ' always', ' an'])\n",
      "(tensor([0.1268, 0.0772, 0.0486, 0.0366, 0.0261], grad_fn=<ToCopyBackward0>), [' incredible', ' enormous', ' amazing', ' important', ' incredibly'])\n",
      "(tensor([0.4083, 0.0620, 0.0360, 0.0272, 0.0270], grad_fn=<ToCopyBackward0>), [' amount', ' impact', ' cultural', ' role', ' part'])\n",
      "(tensor([0.8750, 0.0990, 0.0063, 0.0045, 0.0027], grad_fn=<ToCopyBackward0>), [' to', ' of', ' in', ' more', ' going'])\n",
      "(tensor([0.4099, 0.3440, 0.0637, 0.0457, 0.0399], grad_fn=<ToCopyBackward0>), [' say', ' do', ' tell', ' offer', ' be'])\n",
      "(tensor([9.8107e-01, 2.7768e-03, 1.1748e-03, 1.0938e-03, 7.7522e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' in', ' not', ' as', ' and'])\n",
      "(tensor([0.5058, 0.2429, 0.0464, 0.0278, 0.0249], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' why', ' New'])\n",
      "(tensor([0.4355, 0.3471, 0.0267, 0.0251, 0.0222], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' but', ' in'])\n",
      "(tensor([0.6785, 0.0731, 0.0355, 0.0175, 0.0145], grad_fn=<ToCopyBackward0>), [' but', ' and', ' the', ' as', ' especially'])\n",
      "(tensor([0.2419, 0.1313, 0.1065, 0.0529, 0.0328], grad_fn=<ToCopyBackward0>), [' the', ' this', ' I', ' it', ' that'])\n",
      "(tensor([0.4430, 0.1911, 0.1087, 0.0373, 0.0325], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.1499, 0.1443, 0.1057, 0.0998, 0.0633], grad_fn=<ToCopyBackward0>), [' was', ' is', ' has', ' had', ' seemed'])\n",
      "(tensor([0.1067, 0.0806, 0.0586, 0.0578, 0.0537], grad_fn=<ToCopyBackward0>), [' about', ' more', ' just', ' very', ' really'])\n",
      "(tensor([0.6478, 0.0986, 0.0401, 0.0248, 0.0173], grad_fn=<ToCopyBackward0>), [' about', ' like', ' a', ' of', ' along'])\n",
      "(tensor([0.3448, 0.1865, 0.0395, 0.0189, 0.0169], grad_fn=<ToCopyBackward0>), [' the', ' New', ' Manhattan', ' a', ' how'])\n",
      "(tensor([0.0964, 0.0378, 0.0359, 0.0289, 0.0281], grad_fn=<ToCopyBackward0>), [' city', ' idea', ' people', ' story', ' relationship'])\n",
      "(tensor([0.1767, 0.1436, 0.1421, 0.1359, 0.0912], grad_fn=<ToCopyBackward0>), [' than', ' in', '.', ' as', ' of'])\n",
      "(tensor([0.5686, 0.1031, 0.0957, 0.0738, 0.0376], grad_fn=<ToCopyBackward0>), [' the', ' New', ' about', ' it', ' any'])\n",
      "(tensor([0.4618, 0.0721, 0.0421, 0.0317, 0.0194], grad_fn=<ToCopyBackward0>), [' city', ' rest', ' people', ' characters', ' borough'])\n",
      "(tensor([0.3328, 0.2496, 0.1390, 0.0756, 0.0328], grad_fn=<ToCopyBackward0>), [' was', '.', ' had', ' did', ' itself'])\n",
      "(tensor([0.8311, 0.0270, 0.0204, 0.0187, 0.0087], grad_fn=<ToCopyBackward0>), [' about', '.', ' in', ' the', ' with'])\n",
      "(tensor([0.9696, 0.0092, 0.0072, 0.0025, 0.0014], grad_fn=<ToCopyBackward0>), [' the', ' it', ' New', ' this', ' its'])\n",
      "(tensor([0.2022, 0.1838, 0.1151, 0.0631, 0.0592], grad_fn=<ToCopyBackward0>), [' characters', ' movie', ' city', ' character', ' people'])\n",
      "(tensor([0.9673, 0.0055, 0.0049, 0.0032, 0.0029], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', '....'])\n",
      "(tensor([0.1578, 0.0989, 0.0965, 0.0425, 0.0401], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' So', ' There'])\n",
      "(tensor([0.2344, 0.1368, 0.0869, 0.0807, 0.0544], grad_fn=<ToCopyBackward0>), [' I', ',', ' it', ' the', ' when'])\n",
      "(tensor([0.1467, 0.0929, 0.0647, 0.0580, 0.0560], grad_fn=<ToCopyBackward0>), [' was', ' think', \"'m\", ' really', ' don'])\n",
      "(tensor([0.1370, 0.0882, 0.0739, 0.0651, 0.0633], grad_fn=<ToCopyBackward0>), [' wanted', ' tried', ' don', ' was', ' didn'])\n",
      "(tensor([9.9630e-01, 1.2156e-03, 4.7450e-04, 2.8445e-04, 1.7229e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2788, 0.1010, 0.0932, 0.0654, 0.0467], grad_fn=<ToCopyBackward0>), [' think', ' know', ' like', ' have', ' see'])\n",
      "(tensor([0.2241, 0.1639, 0.1498, 0.1323, 0.0426], grad_fn=<ToCopyBackward0>), [' that', ' it', ' much', ' the', ' there'])\n",
      "(tensor([0.4410, 0.4293, 0.0423, 0.0266, 0.0089], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', \"'d\"])\n",
      "(tensor([0.6993, 0.0608, 0.0290, 0.0277, 0.0257], grad_fn=<ToCopyBackward0>), [' be', ' work', ' possibly', ' get', ' have'])\n",
      "(tensor([0.2707, 0.1042, 0.0446, 0.0431, 0.0422], grad_fn=<ToCopyBackward0>), [' a', ' as', ' very', ' funny', ' much'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a sequel to this movie. I can't even describe how bad that movie was, but this movie, I could not watch after it. The only part of that movie that was entertaining was the guy that plays a doctor that gets bitten by\n",
      "(tensor([0.3847, 0.1712, 0.0895, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1165, 0.0399, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1844, 0.1450, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1649, 0.1162, 0.0572, 0.0492, 0.0452], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8524, 0.0311, 0.0139, 0.0121, 0.0085], grad_fn=<ToCopyBackward0>), [' to', ' of', ',', '...', ' that'])\n",
      "(tensor([0.1700, 0.0655, 0.0591, 0.0368, 0.0355], grad_fn=<ToCopyBackward0>), [' the', ' \"', ' The', ' a', ' this'])\n",
      "(tensor([0.7994, 0.1092, 0.0049, 0.0037, 0.0026], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', ' movies', ','])\n",
      "(tensor([0.7300, 0.0735, 0.0192, 0.0153, 0.0136], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' because', ' and'])\n",
      "(tensor([0.3158, 0.0944, 0.0636, 0.0300, 0.0265], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1478, 0.1220, 0.0558, 0.0521, 0.0453], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' can', ' really'])\n",
      "(tensor([0.6263, 0.0736, 0.0483, 0.0435, 0.0301], grad_fn=<ToCopyBackward0>), [\"'t\", ' see', ' only', ' tell', ' remember'])\n",
      "(tensor([0.2940, 0.2651, 0.1040, 0.0712, 0.0344], grad_fn=<ToCopyBackward0>), [' remember', ' believe', ' even', ' really', ' say'])\n",
      "(tensor([0.4470, 0.1006, 0.0909, 0.0719, 0.0436], grad_fn=<ToCopyBackward0>), [' remember', ' describe', ' believe', ' tell', ' explain'])\n",
      "(tensor([0.4927, 0.2466, 0.0865, 0.0490, 0.0346], grad_fn=<ToCopyBackward0>), [' it', ' how', ' the', ' to', ' this'])\n",
      "(tensor([0.2302, 0.1709, 0.1292, 0.0596, 0.0593], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' awful', ' horrible', ' terrible'])\n",
      "(tensor([0.6927, 0.2170, 0.0406, 0.0236, 0.0153], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' that'])\n",
      "(tensor([0.6049, 0.1787, 0.0407, 0.0334, 0.0245], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' is', ' film', ' one'])\n",
      "(tensor([0.7836, 0.1625, 0.0218, 0.0075, 0.0019], grad_fn=<ToCopyBackward0>), [' was', ' is', ' really', ' actually', ' got'])\n",
      "(tensor([0.7469, 0.1264, 0.0352, 0.0153, 0.0072], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', ' but'])\n",
      "(tensor([0.3820, 0.0782, 0.0648, 0.0518, 0.0356], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' I', ' it'])\n",
      "(tensor([0.3944, 0.1338, 0.0998, 0.0376, 0.0326], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' the', ' at'])\n",
      "(tensor([0.3894, 0.1507, 0.1266, 0.1035, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' one', ' just'])\n",
      "(tensor([0.3232, 0.2174, 0.0618, 0.0524, 0.0331], grad_fn=<ToCopyBackward0>), [' is', ' was', '?', ',', ' just'])\n",
      "(tensor([0.2539, 0.1725, 0.0798, 0.0543, 0.0300], grad_fn=<ToCopyBackward0>), [' I', ' it', ' oh', ' well', ' not'])\n",
      "(tensor([0.1465, 0.1314, 0.0709, 0.0609, 0.0537], grad_fn=<ToCopyBackward0>), [' can', ' thought', ' think', ' mean', ' could'])\n",
      "(tensor([0.4413, 0.1420, 0.0655, 0.0324, 0.0319], grad_fn=<ToCopyBackward0>), [' not', ' barely', ' see', ' describe', ' watch'])\n",
      "(tensor([0.2830, 0.1239, 0.1232, 0.0750, 0.0412], grad_fn=<ToCopyBackward0>), [' even', ' stop', ' believe', ' get', ' watch'])\n",
      "(tensor([0.5944, 0.0536, 0.0529, 0.0352, 0.0273], grad_fn=<ToCopyBackward0>), [' it', '.', ' for', ' the', ' after'])\n",
      "(tensor([0.1633, 0.1127, 0.0967, 0.0769, 0.0748], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' it', ' watching', ' I'])\n",
      "(tensor([0.3386, 0.2336, 0.0551, 0.0430, 0.0235], grad_fn=<ToCopyBackward0>), ['.', ' was', ',', ' even', ' finished'])\n",
      "(tensor([0.3817, 0.1597, 0.1164, 0.0238, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.2399, 0.1741, 0.0576, 0.0480, 0.0390], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' movie', ' story'])\n",
      "(tensor([0.3273, 0.1516, 0.1167, 0.0471, 0.0348], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' part', ' way'])\n",
      "(tensor([0.4372, 0.2440, 0.1942, 0.0325, 0.0104], grad_fn=<ToCopyBackward0>), [' that', ' I', ' of', ' where', ' about'])\n",
      "(tensor([0.4649, 0.3114, 0.1983, 0.0179, 0.0031], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' that', ' my'])\n",
      "(tensor([9.8548e-01, 5.5043e-03, 1.8111e-03, 1.0708e-03, 5.2016e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' film', ' that', ' I', ' stupid'])\n",
      "(tensor([0.8833, 0.0905, 0.0048, 0.0033, 0.0021], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' which', ' was'])\n",
      "(tensor([0.5024, 0.1664, 0.0338, 0.0337, 0.0284], grad_fn=<ToCopyBackward0>), [' was', ' I', ' made', ' kept', ' could'])\n",
      "(tensor([0.2737, 0.1786, 0.1758, 0.0478, 0.0344], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' entertaining', ' toler', ' enjoyable'])\n",
      "(tensor([0.7961, 0.0622, 0.0346, 0.0153, 0.0144], grad_fn=<ToCopyBackward0>), [' was', ' were', ' to', ' for', ','])\n",
      "(tensor([0.5947, 0.1735, 0.0186, 0.0068, 0.0062], grad_fn=<ToCopyBackward0>), [' the', ' when', ' that', ' a', ' seeing'])\n",
      "(tensor([0.0497, 0.0492, 0.0327, 0.0283, 0.0267], grad_fn=<ToCopyBackward0>), [' ending', ' car', ' last', ' bit', ' guy'])\n",
      "(tensor([0.1395, 0.1361, 0.1015, 0.0700, 0.0583], grad_fn=<ToCopyBackward0>), [' that', ' with', ' who', ' falling', ' getting'])\n",
      "(tensor([0.3548, 0.1034, 0.0604, 0.0460, 0.0321], grad_fn=<ToCopyBackward0>), [' played', ' plays', ' was', ' did', ' got'])\n",
      "(tensor([0.4822, 0.0417, 0.0122, 0.0085, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' a', ' his', ' that', ' Mr'])\n",
      "(tensor([0.0781, 0.0579, 0.0309, 0.0260, 0.0212], grad_fn=<ToCopyBackward0>), [' cop', ' guy', ' doctor', ' little', ' really'])\n",
      "(tensor([0.2247, 0.2124, 0.0795, 0.0689, 0.0382], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' in', ' who'])\n",
      "(tensor([0.0677, 0.0348, 0.0343, 0.0330, 0.0322], grad_fn=<ToCopyBackward0>), [' gets', ' is', ' was', ' plays', ' has'])\n",
      "(tensor([0.0861, 0.0764, 0.0673, 0.0620, 0.0591], grad_fn=<ToCopyBackward0>), [' the', ' bit', ' bitten', ' to', ' his'])\n",
      "(tensor([0.8676, 0.0389, 0.0260, 0.0177, 0.0116], grad_fn=<ToCopyBackward0>), [' by', ' in', ' and', ' on', '.'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch it in a theater because the premise was so ridiculous and ridiculous, but I was wrong. I was really wrong. I actually found this to be a pretty enjoyable film with a pretty good cast. I was actually surprised by how well\n",
      "(tensor([0.3848, 0.1712, 0.0896, 0.0769, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2244, 0.1943, 0.1573, 0.0694, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6388, 0.1358, 0.0519, 0.0220, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2275, 0.1012, 0.0992, 0.0795, 0.0620], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6470, 0.1892, 0.0580, 0.0090, 0.0080], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.1713, 0.1294, 0.1087, 0.0934, 0.0925], grad_fn=<ToCopyBackward0>), [' because', ' all', ',', ' in', ' to'])\n",
      "(tensor([0.1654, 0.1384, 0.0754, 0.0570, 0.0374], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' order', ' Japanese'])\n",
      "(tensor([0.2793, 0.0366, 0.0291, 0.0270, 0.0247], grad_fn=<ToCopyBackward0>), [' theater', ' certain', ' theatre', ' language', ' special'])\n",
      "(tensor([0.1767, 0.1169, 0.0856, 0.0707, 0.0549], grad_fn=<ToCopyBackward0>), [' because', '.', ' so', ' like', ' to'])\n",
      "(tensor([0.4483, 0.1711, 0.0522, 0.0274, 0.0246], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' of'])\n",
      "(tensor([0.1176, 0.0705, 0.0302, 0.0261, 0.0242], grad_fn=<ToCopyBackward0>), [' movie', ' premise', ' sound', ' cover', ' story'])\n",
      "(tensor([0.3833, 0.2281, 0.1187, 0.0928, 0.0476], grad_fn=<ToCopyBackward0>), [' was', ' is', ' sounded', ' of', ' seemed'])\n",
      "(tensor([0.6501, 0.0233, 0.0182, 0.0156, 0.0153], grad_fn=<ToCopyBackward0>), [' so', ' pretty', ' ridiculous', ' terrible', ' bad'])\n",
      "(tensor([0.1482, 0.0997, 0.0504, 0.0408, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' ridiculous', ' terrible', ' bizarre', ' absurd'])\n",
      "(tensor([0.4179, 0.1485, 0.1105, 0.1040, 0.0241], grad_fn=<ToCopyBackward0>), ['.', ' and', ' that', ',', ' it'])\n",
      "(tensor([0.1342, 0.0662, 0.0478, 0.0401, 0.0272], grad_fn=<ToCopyBackward0>), [' the', ' I', ' predictable', ' so', ' ridiculous'])\n",
      "(tensor([0.2088, 0.1718, 0.1454, 0.1062, 0.0463], grad_fn=<ToCopyBackward0>), [' and', ' that', '.', ',', ' it'])\n",
      "(tensor([0.4853, 0.1601, 0.0525, 0.0466, 0.0375], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' the', ' that'])\n",
      "(tensor([0.3251, 0.1895, 0.0681, 0.0577, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' this', ' after'])\n",
      "(tensor([0.1021, 0.0841, 0.0779, 0.0767, 0.0598], grad_fn=<ToCopyBackward0>), [' actually', ' really', ' just', ' was', ' rented'])\n",
      "(tensor([0.2230, 0.1376, 0.1323, 0.0659, 0.0398], grad_fn=<ToCopyBackward0>), [' really', ' wrong', ' actually', ' so', ' pleasantly'])\n",
      "(tensor([0.8673, 0.0531, 0.0135, 0.0101, 0.0090], grad_fn=<ToCopyBackward0>), ['.', '!', ' because', ' on', ','])\n",
      "(tensor([0.4405, 0.1518, 0.0943, 0.0643, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.0956, 0.0695, 0.0691, 0.0649, 0.0539], grad_fn=<ToCopyBackward0>), [' watched', ' was', ' really', ' actually', ' just'])\n",
      "(tensor([0.3114, 0.1485, 0.0730, 0.0717, 0.0476], grad_fn=<ToCopyBackward0>), [' wrong', ' really', ' actually', ' very', ' so'])\n",
      "(tensor([0.2406, 0.2155, 0.1440, 0.0800, 0.0415], grad_fn=<ToCopyBackward0>), [' disappointed', ' wrong', ' bored', ' looking', ' surprised'])\n",
      "(tensor([0.7940, 0.0408, 0.0319, 0.0260, 0.0232], grad_fn=<ToCopyBackward0>), ['.', ' about', ' in', ' on', ' because'])\n",
      "(tensor([0.4002, 0.1505, 0.0869, 0.0612, 0.0268], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1029, 0.0937, 0.0689, 0.0569, 0.0423], grad_fn=<ToCopyBackward0>), [' was', ' really', ' actually', \"'m\", ' watched'])\n",
      "(tensor([0.1850, 0.0804, 0.0764, 0.0542, 0.0332], grad_fn=<ToCopyBackward0>), [' found', ' rented', ' really', ' thought', ' enjoyed'])\n",
      "(tensor([0.3610, 0.2300, 0.0985, 0.0793, 0.0679], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' myself', ' watching'])\n",
      "(tensor([0.2604, 0.2261, 0.0753, 0.0526, 0.0378], grad_fn=<ToCopyBackward0>), [' to', ' movie', ' film', ' on', ' one'])\n",
      "(tensor([0.9760, 0.0050, 0.0037, 0.0019, 0.0017], grad_fn=<ToCopyBackward0>), [' be', ' a', ' actually', ' really', ' have'])\n",
      "(tensor([0.2303, 0.1186, 0.0929, 0.0770, 0.0657], grad_fn=<ToCopyBackward0>), [' a', ' one', ' pretty', ' very', ' an'])\n",
      "(tensor([0.3878, 0.2312, 0.1218, 0.0183, 0.0163], grad_fn=<ToCopyBackward0>), [' very', ' pretty', ' really', ' fairly', ' great'])\n",
      "(tensor([0.1871, 0.1269, 0.0854, 0.0543, 0.0392], grad_fn=<ToCopyBackward0>), [' entertaining', ' enjoyable', ' interesting', ' accurate', ' funny'])\n",
      "(tensor([0.5066, 0.3178, 0.0368, 0.0095, 0.0089], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' watch', ' 90', ','])\n",
      "(tensor([0.7274, 0.0791, 0.0290, 0.0248, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', ' with'])\n",
      "(tensor([0.3159, 0.0495, 0.0450, 0.0420, 0.0383], grad_fn=<ToCopyBackward0>), [' a', ' some', ' very', ' an', ' pretty'])\n",
      "(tensor([0.1179, 0.1153, 0.0795, 0.0631, 0.0629], grad_fn=<ToCopyBackward0>), [' very', ' few', ' pretty', ' good', ' lot'])\n",
      "(tensor([0.3297, 0.0882, 0.0866, 0.0295, 0.0288], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' entertaining', ' funny', ' decent'])\n",
      "(tensor([0.2092, 0.1418, 0.0902, 0.0752, 0.0700], grad_fn=<ToCopyBackward0>), [' cast', ' plot', ' budget', ' acting', ' supporting'])\n",
      "(tensor([0.6368, 0.1565, 0.0933, 0.0147, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' of', '.\"'])\n",
      "(tensor([0.1969, 0.1371, 0.1246, 0.0384, 0.0357], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', 'The'])\n",
      "(tensor([0.0877, 0.0864, 0.0532, 0.0491, 0.0485], grad_fn=<ToCopyBackward0>), [' really', ' was', ' thought', \"'m\", ' just'])\n",
      "(tensor([0.4854, 0.1079, 0.0414, 0.0214, 0.0163], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' very', ' surprised', ' not'])\n",
      "(tensor([0.4752, 0.0689, 0.0684, 0.0528, 0.0367], grad_fn=<ToCopyBackward0>), [' really', ' very', ' pretty', ' looking', ' surprised'])\n",
      "(tensor([0.2947, 0.2283, 0.1677, 0.0700, 0.0648], grad_fn=<ToCopyBackward0>), [' by', ' at', ' that', ' to', ' how'])\n",
      "(tensor([0.4877, 0.1704, 0.0799, 0.0673, 0.0368], grad_fn=<ToCopyBackward0>), [' the', ' how', ' this', ' it', ' some'])\n",
      "(tensor([0.1735, 0.1232, 0.0965, 0.0724, 0.0677], grad_fn=<ToCopyBackward0>), [' much', ' good', ' bad', ' many', ' well'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch this because I was really disappointed with this movie. I'm not even gonna waste my time telling anyone about it because it's not funny...not even a little bit. I'm not even kidding...it's not even even funny\n",
      "(tensor([0.3836, 0.1721, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2256, 0.1946, 0.1571, 0.0694, 0.0535], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6388, 0.1358, 0.0516, 0.0221, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2255, 0.1012, 0.0998, 0.0803, 0.0619], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6463, 0.1897, 0.0579, 0.0089, 0.0081], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3172, 0.2491, 0.0943, 0.0425, 0.0238], grad_fn=<ToCopyBackward0>), [' movie', ' because', ' film', ' one', ' so'])\n",
      "(tensor([0.3611, 0.1971, 0.0618, 0.0535, 0.0213], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' i'])\n",
      "(tensor([0.1337, 0.1295, 0.1167, 0.0980, 0.0473], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', \"'m\", ' have'])\n",
      "(tensor([0.1674, 0.1248, 0.1205, 0.0420, 0.0406], grad_fn=<ToCopyBackward0>), [' so', ' really', ' a', ' in', ' going'])\n",
      "(tensor([0.3061, 0.1541, 0.0755, 0.0484, 0.0244], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ' bored', ' interested', ' hoping'])\n",
      "(tensor([0.3101, 0.2897, 0.2593, 0.0235, 0.0196], grad_fn=<ToCopyBackward0>), [' in', ' with', ' by', '.', ' when'])\n",
      "(tensor([0.5692, 0.1476, 0.0206, 0.0190, 0.0150], grad_fn=<ToCopyBackward0>), [' this', ' the', ' how', ' it', ' \"'])\n",
      "(tensor([0.6199, 0.1961, 0.0350, 0.0093, 0.0090], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' episode', ' sequel', ' show'])\n",
      "(tensor([0.7872, 0.0507, 0.0216, 0.0181, 0.0135], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' in'])\n",
      "(tensor([0.2433, 0.1769, 0.1353, 0.0318, 0.0269], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1192, 0.0869, 0.0608, 0.0590, 0.0589], grad_fn=<ToCopyBackward0>), [' was', ' really', \"'m\", ' have', ' thought'])\n",
      "(tensor([0.3126, 0.2496, 0.0476, 0.0290, 0.0230], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' an', ' sure'])\n",
      "(tensor([0.1899, 0.1432, 0.1351, 0.1094, 0.0953], grad_fn=<ToCopyBackward0>), [' a', ' sure', ' even', ' one', ' going'])\n",
      "(tensor([0.3505, 0.2493, 0.1040, 0.0379, 0.0267], grad_fn=<ToCopyBackward0>), [' going', ' gonna', ' sure', ' talking', ' kidding'])\n",
      "(tensor([0.0862, 0.0826, 0.0825, 0.0772, 0.0552], grad_fn=<ToCopyBackward0>), [' call', ' waste', ' go', ' talk', ' bother'])\n",
      "(tensor([0.2966, 0.2441, 0.0845, 0.0738, 0.0659], grad_fn=<ToCopyBackward0>), [' my', ' time', ' more', ' any', ' the'])\n",
      "(tensor([0.9021, 0.0467, 0.0058, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' time', ' money', ' words', ' precious', ' 10'])\n",
      "(tensor([0.2961, 0.1595, 0.1346, 0.0472, 0.0430], grad_fn=<ToCopyBackward0>), [' commenting', ' writing', ' talking', ' telling', ' describing'])\n",
      "(tensor([0.6997, 0.0618, 0.0441, 0.0389, 0.0307], grad_fn=<ToCopyBackward0>), [' you', ' anyone', ' the', ' others', ' people'])\n",
      "(tensor([0.1817, 0.1147, 0.1004, 0.0949, 0.0921], grad_fn=<ToCopyBackward0>), [' about', ' else', ' how', ' to', ' what'])\n",
      "(tensor([0.5582, 0.3360, 0.0477, 0.0282, 0.0105], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' how', ' that'])\n",
      "(tensor([0.4888, 0.1798, 0.1383, 0.0141, 0.0112], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' or', ' but'])\n",
      "(tensor([0.3841, 0.2102, 0.0721, 0.0653, 0.0283], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' there', ' this'])\n",
      "(tensor([0.5210, 0.0868, 0.0681, 0.0657, 0.0379], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' doesn'])\n",
      "(tensor([0.5065, 0.1032, 0.0570, 0.0336, 0.0291], grad_fn=<ToCopyBackward0>), [' not', ' really', ' so', ' just', ' a'])\n",
      "(tensor([0.3681, 0.2592, 0.1130, 0.0841, 0.0317], grad_fn=<ToCopyBackward0>), [' worth', ' even', ' funny', ' really', ' worthy'])\n",
      "(tensor([0.3242, 0.1982, 0.1485, 0.0990, 0.0570], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' or', '...'])\n",
      "(tensor([0.0948, 0.0776, 0.0691, 0.0662, 0.0437], grad_fn=<ToCopyBackward0>), ['not', 'it', 'so', 'and', ' It'])\n",
      "(tensor([0.5857, 0.1565, 0.0638, 0.0336, 0.0294], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' really', ' in', ' at'])\n",
      "(tensor([0.1678, 0.1374, 0.0916, 0.0543, 0.0405], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' close', ' the', ' in'])\n",
      "(tensor([0.9449, 0.0235, 0.0065, 0.0049, 0.0011], grad_fn=<ToCopyBackward0>), [' little', ' bit', ' tiny', ' tad', ' few'])\n",
      "(tensor([0.9439, 0.0343, 0.0061, 0.0043, 0.0015], grad_fn=<ToCopyBackward0>), [' bit', '.', '...', ' funny', ' little'])\n",
      "(tensor([0.6997, 0.1208, 0.0486, 0.0236, 0.0133], grad_fn=<ToCopyBackward0>), ['.', '...', '!', '....', ','])\n",
      "(tensor([0.1927, 0.1903, 0.1310, 0.0421, 0.0288], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.2077, 0.0662, 0.0650, 0.0403, 0.0370], grad_fn=<ToCopyBackward0>), [\"'m\", ' don', ' was', ' can', ' really'])\n",
      "(tensor([0.3464, 0.0618, 0.0479, 0.0384, 0.0381], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' just', ' a', ' gonna'])\n",
      "(tensor([0.6334, 0.0646, 0.0502, 0.0464, 0.0394], grad_fn=<ToCopyBackward0>), [' even', ' gonna', ' one', ' sure', ' going'])\n",
      "(tensor([0.9482, 0.0247, 0.0080, 0.0029, 0.0018], grad_fn=<ToCopyBackward0>), [' gonna', ' going', ' kidding', ' sure', ' even'])\n",
      "(tensor([0.4731, 0.2211, 0.0779, 0.0400, 0.0324], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', '...', '!'])\n",
      "(tensor([0.1837, 0.1830, 0.1390, 0.1109, 0.0495], grad_fn=<ToCopyBackward0>), ['it', 'I', 'this', 'not', 'the'])\n",
      "(tensor([0.8237, 0.0370, 0.0216, 0.0164, 0.0137], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' wasn', ' is', ' really'])\n",
      "(tensor([0.6238, 0.0884, 0.0315, 0.0213, 0.0192], grad_fn=<ToCopyBackward0>), [' not', ' just', ' so', ' the', ' a'])\n",
      "(tensor([0.6782, 0.2934, 0.0070, 0.0054, 0.0016], grad_fn=<ToCopyBackward0>), [' even', ' funny', '.', ' a', ' amusing'])\n",
      "(tensor([0.6446, 0.1099, 0.0500, 0.0175, 0.0139], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' worth', ' entertaining', ' even'])\n",
      "(tensor([0.6554, 0.1026, 0.0411, 0.0203, 0.0136], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' worth', ' entertaining', ' amusing'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a good movie. I think that the idea of the two families is a pretty good one. And the movie is pretty well-acted, too. The movie is pretty well-acted. It's not a great movie, but I think\n",
      "(tensor([0.3841, 0.1713, 0.0897, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1165, 0.0398, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1842, 0.1451, 0.0509, 0.0455, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1648, 0.1161, 0.0572, 0.0491, 0.0452], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8846, 0.0467, 0.0214, 0.0081, 0.0078], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' idea', ' premise', ' sequel'])\n",
      "(tensor([0.3984, 0.1174, 0.0714, 0.0431, 0.0330], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' for'])\n",
      "(tensor([0.3423, 0.1963, 0.0491, 0.0222, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1429, 0.1428, 0.0653, 0.0406, 0.0394], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' liked'])\n",
      "(tensor([0.4278, 0.1287, 0.0867, 0.0375, 0.0280], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.2660, 0.1055, 0.0697, 0.0329, 0.0219], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' if', ' there'])\n",
      "(tensor([0.1002, 0.0693, 0.0575, 0.0550, 0.0490], grad_fn=<ToCopyBackward0>), [' movie', ' director', ' acting', ' story', ' idea'])\n",
      "(tensor([0.5093, 0.1263, 0.0955, 0.0880, 0.0811], grad_fn=<ToCopyBackward0>), [' of', ' was', ' that', ' behind', ' is'])\n",
      "(tensor([0.1566, 0.1298, 0.0849, 0.0476, 0.0291], grad_fn=<ToCopyBackward0>), [' seeing', ' the', ' a', ' having', ' being'])\n",
      "(tensor([0.0233, 0.0176, 0.0173, 0.0171, 0.0144], grad_fn=<ToCopyBackward0>), [' family', ' two', ' movie', ' guy', ' \"'])\n",
      "(tensor([0.3209, 0.0403, 0.0340, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' main', ' families', ' brothers', ' of', ' sides'])\n",
      "(tensor([0.1663, 0.1062, 0.0586, 0.0546, 0.0435], grad_fn=<ToCopyBackward0>), [' is', ' being', ' (', ' in', ','])\n",
      "(tensor([0.2350, 0.1218, 0.0981, 0.0761, 0.0336], grad_fn=<ToCopyBackward0>), [' a', ' very', ' interesting', ' good', ' pretty'])\n",
      "(tensor([0.6368, 0.0667, 0.0591, 0.0510, 0.0208], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' very', ' really', ' great'])\n",
      "(tensor([0.3563, 0.2085, 0.0348, 0.0303, 0.0248], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' original', ' strong', ' intriguing'])\n",
      "(tensor([0.4013, 0.2100, 0.0634, 0.0573, 0.0367], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' plot', ' idea', ' story'])\n",
      "(tensor([0.4572, 0.2492, 0.1196, 0.0326, 0.0176], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' for', ' to'])\n",
      "(tensor([0.1613, 0.1138, 0.0948, 0.0678, 0.0605], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' And'])\n",
      "(tensor([0.2683, 0.1736, 0.0836, 0.0434, 0.0362], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ',', ' that'])\n",
      "(tensor([0.1783, 0.1190, 0.0956, 0.0314, 0.0204], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' movie', ' fact', ' story'])\n",
      "(tensor([0.3240, 0.1174, 0.0701, 0.0620, 0.0340], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' does', ' did'])\n",
      "(tensor([0.3355, 0.0647, 0.0439, 0.0425, 0.0268], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' good', ' a', ' very'])\n",
      "(tensor([0.3029, 0.0877, 0.0767, 0.0751, 0.0729], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' entertaining', ' well', ' funny'])\n",
      "(tensor([0.6500, 0.1397, 0.0999, 0.0350, 0.0085], grad_fn=<ToCopyBackward0>), [' acted', ' done', '-', ' made', ' scripted'])\n",
      "(tensor([0.7526, 0.0895, 0.0577, 0.0318, 0.0194], grad_fn=<ToCopyBackward0>), ['acted', 'made', 'written', 'crafted', 'directed'])\n",
      "(tensor([0.4055, 0.2376, 0.1236, 0.0550, 0.0240], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' too', ' as'])\n",
      "(tensor([0.2342, 0.1308, 0.0818, 0.0729, 0.0609], grad_fn=<ToCopyBackward0>), [' too', ' but', ' I', ' and', ' although'])\n",
      "(tensor([0.6995, 0.2388, 0.0113, 0.0092, 0.0072], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' (', ';'])\n",
      "(tensor([0.1720, 0.1325, 0.1232, 0.0613, 0.0451], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' So', ' The'])\n",
      "(tensor([0.0942, 0.0835, 0.0791, 0.0558, 0.0256], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' problem', ' only', ' acting'])\n",
      "(tensor([0.4526, 0.0960, 0.0611, 0.0430, 0.0377], grad_fn=<ToCopyBackward0>), [' is', ' has', ' was', ' just', ' does'])\n",
      "(tensor([0.2660, 0.0426, 0.0393, 0.0356, 0.0313], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' a', ' good', ' really'])\n",
      "(tensor([0.2397, 0.1902, 0.1018, 0.0875, 0.0754], grad_fn=<ToCopyBackward0>), [' well', ' good', ' entertaining', ' funny', ' interesting'])\n",
      "(tensor([0.6581, 0.2732, 0.0233, 0.0118, 0.0046], grad_fn=<ToCopyBackward0>), ['-', ' acted', ' done', ' made', ' directed'])\n",
      "(tensor([0.6916, 0.0895, 0.0781, 0.0708, 0.0211], grad_fn=<ToCopyBackward0>), ['acted', 'made', 'written', 'directed', 'crafted'])\n",
      "(tensor([0.3338, 0.2828, 0.1061, 0.0498, 0.0231], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' in', ' by'])\n",
      "(tensor([0.1496, 0.1216, 0.1129, 0.0730, 0.0702], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' And', ' The'])\n",
      "(tensor([0.5738, 0.1000, 0.0497, 0.0468, 0.0374], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' just', ' is', ' was'])\n",
      "(tensor([0.1629, 0.1495, 0.0621, 0.0539, 0.0405], grad_fn=<ToCopyBackward0>), [' a', ' not', ' good', ' pretty', ' one'])\n",
      "(tensor([0.1085, 0.1013, 0.0706, 0.0693, 0.0691], grad_fn=<ToCopyBackward0>), [' a', ' great', ' very', ' one', ' the'])\n",
      "(tensor([0.3793, 0.0999, 0.0781, 0.0422, 0.0414], grad_fn=<ToCopyBackward0>), [' great', ' big', ' good', ' movie', ' very'])\n",
      "(tensor([0.7684, 0.0535, 0.0286, 0.0232, 0.0188], grad_fn=<ToCopyBackward0>), [' movie', ' picture', ' film', ' one', ' acting'])\n",
      "(tensor([0.4969, 0.2992, 0.0538, 0.0212, 0.0205], grad_fn=<ToCopyBackward0>), [',', '.', ' by', ' in', ' but'])\n",
      "(tensor([0.8796, 0.0211, 0.0140, 0.0110, 0.0096], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' it', ' by'])\n",
      "(tensor([0.5821, 0.1406, 0.0437, 0.0220, 0.0141], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' that', ' then'])\n",
      "(tensor([0.3081, 0.1248, 0.0528, 0.0517, 0.0364], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' liked', ' really', \"'m\"])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was a good movie. I thought it was a good movie. But I was wrong. I was so wrong. I thought it was a good movie.I think that the first one is better than the second one, but it's not as\n",
      "(tensor([0.3834, 0.1720, 0.0902, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7138, 0.1163, 0.0395, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1838, 0.1463, 0.0507, 0.0451, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1641, 0.1159, 0.0580, 0.0491, 0.0446], grad_fn=<ToCopyBackward0>), [' good', ' sequel', ' really', ' great', ' remake'])\n",
      "(tensor([0.8839, 0.0469, 0.0218, 0.0081, 0.0078], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' idea', ' premise', ' sequel'])\n",
      "(tensor([0.3974, 0.1174, 0.0722, 0.0432, 0.0331], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' for'])\n",
      "(tensor([0.3431, 0.1962, 0.0490, 0.0222, 0.0163], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' And'])\n",
      "(tensor([0.1432, 0.1430, 0.0653, 0.0404, 0.0395], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', ' think', ' liked'])\n",
      "(tensor([0.7154, 0.1126, 0.0338, 0.0170, 0.0153], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.8197, 0.0260, 0.0181, 0.0150, 0.0105], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' represented'])\n",
      "(tensor([0.2276, 0.0864, 0.0769, 0.0574, 0.0520], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' pretty', ' interesting', ' entertaining'])\n",
      "(tensor([0.4335, 0.1036, 0.0634, 0.0415, 0.0356], grad_fn=<ToCopyBackward0>), [' good', ' really', ' very', ' bad', ' pretty'])\n",
      "(tensor([0.9155, 0.0207, 0.0204, 0.0047, 0.0034], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' idea', ' acting'])\n",
      "(tensor([0.7024, 0.0423, 0.0320, 0.0283, 0.0227], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' for', ' when'])\n",
      "(tensor([0.3076, 0.0863, 0.0728, 0.0319, 0.0304], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' This', ' And'])\n",
      "(tensor([0.1487, 0.1237, 0.0902, 0.0900, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' the', ' what'])\n",
      "(tensor([0.1110, 0.0690, 0.0658, 0.0602, 0.0582], grad_fn=<ToCopyBackward0>), [' was', ' can', ' really', ' didn', ' don'])\n",
      "(tensor([0.1674, 0.0853, 0.0838, 0.0604, 0.0560], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' not', ' so', ' disappointed'])\n",
      "(tensor([0.5825, 0.0910, 0.0606, 0.0369, 0.0353], grad_fn=<ToCopyBackward0>), ['.', ' on', ' in', ',', '!'])\n",
      "(tensor([0.2357, 0.1437, 0.0789, 0.0519, 0.0447], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', 'I'])\n",
      "(tensor([0.4080, 0.0767, 0.0447, 0.0332, 0.0315], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' should', ' am'])\n",
      "(tensor([0.6850, 0.0745, 0.0708, 0.0233, 0.0108], grad_fn=<ToCopyBackward0>), [' wrong', ' very', ' so', ' really', ' not'])\n",
      "(tensor([0.9453, 0.0107, 0.0037, 0.0023, 0.0022], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ',', ' mistaken', ' far'])\n",
      "(tensor([0.4469, 0.1037, 0.0788, 0.0725, 0.0498], grad_fn=<ToCopyBackward0>), ['.', ' in', ' about', ' I', '!'])\n",
      "(tensor([0.3527, 0.1052, 0.0740, 0.0635, 0.0420], grad_fn=<ToCopyBackward0>), [' I', ' It', 'I', ' This', ' The'])\n",
      "(tensor([0.2875, 0.0701, 0.0646, 0.0553, 0.0397], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' thought', ' can'])\n",
      "(tensor([0.6316, 0.1359, 0.0509, 0.0420, 0.0317], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' that'])\n",
      "(tensor([0.9152, 0.0350, 0.0069, 0.0051, 0.0041], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', \"'s\", ' could'])\n",
      "(tensor([0.4144, 0.1010, 0.0483, 0.0395, 0.0246], grad_fn=<ToCopyBackward0>), [' a', ' so', ' funny', ' boring', ' terrible'])\n",
      "(tensor([0.6749, 0.0949, 0.0743, 0.0284, 0.0137], grad_fn=<ToCopyBackward0>), [' good', ' great', ' really', ' very', ' bad'])\n",
      "(tensor([9.8789e-01, 2.0343e-03, 1.8766e-03, 1.3441e-03, 8.8246e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' story', ','])\n",
      "(tensor([0.6868, 0.0954, 0.0484, 0.0201, 0.0163], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', '!', ' but'])\n",
      "(tensor([0.5496, 0.0667, 0.0631, 0.0352, 0.0314], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', 'I', ' And'])\n",
      "(tensor([0.1380, 0.1376, 0.0699, 0.0669, 0.0568], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', \"'m\", ' think'])\n",
      "(tensor([0.2541, 0.1808, 0.0986, 0.0804, 0.0789], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' that', ' the'])\n",
      "(tensor([0.1607, 0.1270, 0.1245, 0.0423, 0.0397], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' if', \"'s\"])\n",
      "(tensor([0.0582, 0.0579, 0.0558, 0.0499, 0.0497], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' first', ' biggest', ' acting'])\n",
      "(tensor([0.1214, 0.0545, 0.0455, 0.0353, 0.0315], grad_fn=<ToCopyBackward0>), [' one', ' time', ' movie', ' 10', ' half'])\n",
      "(tensor([0.5275, 0.1284, 0.0275, 0.0208, 0.0156], grad_fn=<ToCopyBackward0>), [' was', ' is', ' that', ' I', ' had'])\n",
      "(tensor([0.1790, 0.0846, 0.0653, 0.0562, 0.0544], grad_fn=<ToCopyBackward0>), [' a', ' better', ' the', ' more', ' probably'])\n",
      "(tensor([0.4978, 0.3383, 0.0756, 0.0161, 0.0120], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' because', ' but'])\n",
      "(tensor([0.7837, 0.1453, 0.0078, 0.0061, 0.0052], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' The', ' I'])\n",
      "(tensor([0.9018, 0.0156, 0.0156, 0.0129, 0.0129], grad_fn=<ToCopyBackward0>), [' second', ' sequel', ' last', ' third', ' first'])\n",
      "(tensor([0.8879, 0.0572, 0.0185, 0.0135, 0.0049], grad_fn=<ToCopyBackward0>), [' one', '.', ' movie', ',', ' and'])\n",
      "(tensor([0.6060, 0.2101, 0.0560, 0.0192, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', '...', ' in'])\n",
      "(tensor([0.5359, 0.0642, 0.0540, 0.0275, 0.0267], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' so', ' because'])\n",
      "(tensor([0.3558, 0.1339, 0.1253, 0.0640, 0.0370], grad_fn=<ToCopyBackward0>), [' I', ' this', ' the', ' it', ' then'])\n",
      "(tensor([0.3869, 0.0924, 0.0605, 0.0567, 0.0308], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.3674, 0.1069, 0.0671, 0.0376, 0.0375], grad_fn=<ToCopyBackward0>), [' not', ' a', ' all', ' funny', ' still'])\n",
      "(tensor([0.2866, 0.0795, 0.0668, 0.0653, 0.0632], grad_fn=<ToCopyBackward0>), [' as', ' even', ' the', '.', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad it could not be worse.The acting is bad, the script is bad.I'm not saying this because I'm a fan of the actor.I'm saying this because the script is bad.The acting is bad.\n",
      "(tensor([0.3842, 0.1718, 0.0899, 0.0769, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2435, 0.1968, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6527, 0.0596, 0.0360, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1371, 0.0701, 0.0661, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5192, 0.0474, 0.0456, 0.0419, 0.0297], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2383, 0.2246, 0.2054, 0.0698, 0.0469], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.6809, 0.0473, 0.0332, 0.0264, 0.0222], grad_fn=<ToCopyBackward0>), [' was', ' could', ' had', ' would', ' should'])\n",
      "(tensor([0.5128, 0.1678, 0.1119, 0.0563, 0.0469], grad_fn=<ToCopyBackward0>), [' not', ' have', ' only', ' be', ' never'])\n",
      "(tensor([0.4398, 0.1177, 0.0930, 0.0891, 0.0495], grad_fn=<ToCopyBackward0>), [' be', ' possibly', ' even', ' have', ' get'])\n",
      "(tensor([0.2926, 0.2058, 0.1235, 0.0225, 0.0221], grad_fn=<ToCopyBackward0>), [' funny', ' worse', ' good', ' less', ' more'])\n",
      "(tensor([0.7945, 0.0852, 0.0383, 0.0190, 0.0119], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', '!', '...'])\n",
      "(tensor([0.2039, 0.1968, 0.1169, 0.0353, 0.0197], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.1981, 0.1341, 0.0664, 0.0588, 0.0549], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' story', ' movie'])\n",
      "(tensor([0.7102, 0.1395, 0.0253, 0.0233, 0.0174], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' and', ' in'])\n",
      "(tensor([0.1453, 0.0902, 0.0721, 0.0501, 0.0501], grad_fn=<ToCopyBackward0>), [' so', ' terrible', ' bad', ' horrible', ' awful'])\n",
      "(tensor([0.3283, 0.1784, 0.1652, 0.0387, 0.0252], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' at'])\n",
      "(tensor([0.4927, 0.1552, 0.0707, 0.0488, 0.0245], grad_fn=<ToCopyBackward0>), [' the', 'the', ' and', ' but', 'and'])\n",
      "(tensor([0.2666, 0.2009, 0.1190, 0.0296, 0.0246], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' directing', ' dialog'])\n",
      "(tensor([0.7435, 0.1108, 0.0288, 0.0267, 0.0070], grad_fn=<ToCopyBackward0>), [' is', ' was', ' and', ',', '...'])\n",
      "(tensor([0.5194, 0.0813, 0.0403, 0.0325, 0.0276], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' worse', ' horrible'])\n",
      "(tensor([0.5465, 0.2516, 0.1274, 0.0109, 0.0066], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.2061, 0.1484, 0.0497, 0.0488, 0.0393], grad_fn=<ToCopyBackward0>), [' The', 'The', ' I', 'I', ' It'])\n",
      "(tensor([0.0593, 0.0571, 0.0533, 0.0526, 0.0493], grad_fn=<ToCopyBackward0>), [' think', ' don', ' have', \"'m\", ' am'])\n",
      "(tensor([0.3367, 0.0973, 0.0479, 0.0429, 0.0362], grad_fn=<ToCopyBackward0>), [' not', ' a', ' giving', ' sure', ' so'])\n",
      "(tensor([0.2599, 0.2141, 0.0993, 0.0793, 0.0438], grad_fn=<ToCopyBackward0>), [' sure', ' even', ' going', ' a', ' saying'])\n",
      "(tensor([0.3198, 0.1697, 0.1627, 0.1488, 0.0463], grad_fn=<ToCopyBackward0>), [' that', ' this', ' it', ' the', ' I'])\n",
      "(tensor([0.3321, 0.2919, 0.2494, 0.0153, 0.0119], grad_fn=<ToCopyBackward0>), [' because', ' movie', ' is', ' was', ' for'])\n",
      "(tensor([0.5286, 0.1110, 0.0815, 0.0804, 0.0443], grad_fn=<ToCopyBackward0>), [' I', ' the', ' of', ' i', ' it'])\n",
      "(tensor([0.4840, 0.0604, 0.0539, 0.0440, 0.0403], grad_fn=<ToCopyBackward0>), [\"'m\", ' think', ' have', ' like', ' don'])\n",
      "(tensor([0.5993, 0.0749, 0.0365, 0.0336, 0.0217], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' some', ' in'])\n",
      "(tensor([0.1724, 0.0875, 0.0582, 0.0508, 0.0270], grad_fn=<ToCopyBackward0>), [' fan', ' movie', ' big', ' critic', ' huge'])\n",
      "(tensor([0.8399, 0.0516, 0.0364, 0.0273, 0.0121], grad_fn=<ToCopyBackward0>), [' of', ',', ' or', '.', ' but'])\n",
      "(tensor([0.2291, 0.2259, 0.0812, 0.0104, 0.0095], grad_fn=<ToCopyBackward0>), [' the', ' this', ' any', ' John', ' it'])\n",
      "(tensor([0.1944, 0.1303, 0.0718, 0.0312, 0.0270], grad_fn=<ToCopyBackward0>), [' actors', ' movie', ' actor', ' book', ' original'])\n",
      "(tensor([0.3107, 0.2393, 0.1768, 0.0349, 0.0218], grad_fn=<ToCopyBackward0>), [',', ' or', '.', ' in', '/'])\n",
      "(tensor([0.2699, 0.2279, 0.0460, 0.0399, 0.0388], grad_fn=<ToCopyBackward0>), [' I', 'I', ' It', 'This', 'The'])\n",
      "(tensor([0.7638, 0.0281, 0.0250, 0.0192, 0.0184], grad_fn=<ToCopyBackward0>), [\"'m\", ' just', ' don', ' am', ' think'])\n",
      "(tensor([0.7328, 0.0948, 0.0731, 0.0268, 0.0064], grad_fn=<ToCopyBackward0>), [' saying', ' not', ' just', ' a', ' only'])\n",
      "(tensor([0.5864, 0.3675, 0.0230, 0.0095, 0.0025], grad_fn=<ToCopyBackward0>), [' this', ' it', ' because', ' that', ' the'])\n",
      "(tensor([0.9501, 0.0072, 0.0059, 0.0056, 0.0053], grad_fn=<ToCopyBackward0>), [' because', ' for', ' as', ' so', ' to'])\n",
      "(tensor([0.2690, 0.2237, 0.1363, 0.0524, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' it', ' there'])\n",
      "(tensor([0.1921, 0.1871, 0.0429, 0.0407, 0.0349], grad_fn=<ToCopyBackward0>), [' movie', ' script', ' only', ' plot', ' story'])\n",
      "(tensor([0.4155, 0.1552, 0.0797, 0.0368, 0.0319], grad_fn=<ToCopyBackward0>), [' is', ' was', ' and', ' sucks', ' makes'])\n",
      "(tensor([0.6527, 0.1148, 0.0169, 0.0158, 0.0153], grad_fn=<ToCopyBackward0>), [' so', ' bad', ' just', ' not', ' terrible'])\n",
      "(tensor([0.6988, 0.1175, 0.1081, 0.0144, 0.0061], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2198, 0.1460, 0.0371, 0.0336, 0.0316], grad_fn=<ToCopyBackward0>), ['The', 'I', 'It', ' The', 'This'])\n",
      "(tensor([0.2229, 0.0738, 0.0668, 0.0397, 0.0381], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' actors', ' plot'])\n",
      "(tensor([0.5489, 0.1117, 0.0516, 0.0476, 0.0392], grad_fn=<ToCopyBackward0>), [' is', ' was', '.', ',', ' in'])\n",
      "(tensor([0.5896, 0.0463, 0.0354, 0.0225, 0.0201], grad_fn=<ToCopyBackward0>), [' bad', ' so', ' not', ' terrible', ' poor'])\n",
      "(tensor([0.4279, 0.3503, 0.0507, 0.0486, 0.0131], grad_fn=<ToCopyBackward0>), [',', '.', ' because', ' and', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was very boring to me. It was very disappointing. The acting is very bad. The script was not very good. The story was not interesting. The editing was not good. I was very disappointed.I was very disappointed with that movie\n",
      "(tensor([0.3831, 0.1723, 0.0903, 0.0773, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2450, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7767, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1275, 0.0770, 0.0660, 0.0556, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.3631, 0.1015, 0.0651, 0.0282, 0.0276], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' much', ' poorly'])\n",
      "(tensor([0.4531, 0.2327, 0.1050, 0.0393, 0.0141], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' as'])\n",
      "(tensor([0.7539, 0.0509, 0.0288, 0.0209, 0.0168], grad_fn=<ToCopyBackward0>), [' watch', ' the', ' be', ' me', ' sit'])\n",
      "(tensor([0.6854, 0.0786, 0.0399, 0.0272, 0.0252], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' at'])\n",
      "(tensor([0.2298, 0.1764, 0.1569, 0.0282, 0.0211], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.2713, 0.1609, 0.1176, 0.0762, 0.0377], grad_fn=<ToCopyBackward0>), [' was', ' seemed', \"'s\", ' had', ' didn'])\n",
      "(tensor([0.1868, 0.1021, 0.0719, 0.0649, 0.0460], grad_fn=<ToCopyBackward0>), [' boring', ' very', ' just', ' not', ' so'])\n",
      "(tensor([0.2466, 0.0551, 0.0530, 0.0384, 0.0222], grad_fn=<ToCopyBackward0>), [' boring', ' preach', ' predictable', ' disappointing', ' hard'])\n",
      "(tensor([0.5633, 0.1312, 0.0924, 0.0543, 0.0415], grad_fn=<ToCopyBackward0>), ['.', ' to', ' and', ' because', ','])\n",
      "(tensor([0.3043, 0.1696, 0.1069, 0.0336, 0.0263], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1065, 0.0940, 0.0619, 0.0444, 0.0442], grad_fn=<ToCopyBackward0>), [' story', ' acting', ' only', ' first', ' movie'])\n",
      "(tensor([0.8023, 0.0457, 0.0451, 0.0155, 0.0123], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.2071, 0.1185, 0.0500, 0.0410, 0.0350], grad_fn=<ToCopyBackward0>), [' not', ' very', ' terrible', ' so', ' poor'])\n",
      "(tensor([0.1465, 0.1022, 0.0545, 0.0506, 0.0498], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' weak', ' poor', ','])\n",
      "(tensor([0.3444, 0.2333, 0.2222, 0.0583, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.2884, 0.1640, 0.1563, 0.0375, 0.0227], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2370, 0.2129, 0.1418, 0.0375, 0.0286], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' only'])\n",
      "(tensor([0.5126, 0.3767, 0.0128, 0.0123, 0.0066], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' and', ' had'])\n",
      "(tensor([0.3472, 0.0766, 0.0689, 0.0457, 0.0332], grad_fn=<ToCopyBackward0>), [' very', ' not', ' bad', ' weak', ' terrible'])\n",
      "(tensor([0.2282, 0.1925, 0.0866, 0.0704, 0.0432], grad_fn=<ToCopyBackward0>), [' good', ' very', ' funny', ' interesting', ' even'])\n",
      "(tensor([0.7658, 0.0343, 0.0324, 0.0309, 0.0207], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' interesting', ' well', ' entertaining'])\n",
      "(tensor([0.4843, 0.1585, 0.0864, 0.0724, 0.0515], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' either'])\n",
      "(tensor([0.3360, 0.1681, 0.1378, 0.0385, 0.0293], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.1049, 0.0562, 0.0519, 0.0491, 0.0469], grad_fn=<ToCopyBackward0>), [' story', ' cinem', ' acting', ' director', ' plot'])\n",
      "(tensor([0.6246, 0.1970, 0.0337, 0.0099, 0.0098], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' itself', ' just'])\n",
      "(tensor([0.4292, 0.2245, 0.0545, 0.0221, 0.0206], grad_fn=<ToCopyBackward0>), [' not', ' very', ' weak', ' stupid', ' boring'])\n",
      "(tensor([0.5563, 0.0808, 0.0718, 0.0640, 0.0399], grad_fn=<ToCopyBackward0>), [' very', ' interesting', ' really', ' that', ' good'])\n",
      "(tensor([0.3102, 0.2981, 0.1300, 0.0536, 0.0493], grad_fn=<ToCopyBackward0>), ['.', ' to', ' at', ',', ' and'])\n",
      "(tensor([0.3720, 0.1669, 0.1552, 0.0322, 0.0261], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0938, 0.0586, 0.0494, 0.0464, 0.0333], grad_fn=<ToCopyBackward0>), [' acting', ' editing', ' cinem', ' movie', ' story'])\n",
      "(tensor([0.7975, 0.0905, 0.0182, 0.0139, 0.0133], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ' of', ' wasn'])\n",
      "(tensor([0.4001, 0.2455, 0.0408, 0.0292, 0.0254], grad_fn=<ToCopyBackward0>), [' not', ' very', ' poor', ' terrible', ' bad'])\n",
      "(tensor([0.3851, 0.3268, 0.0534, 0.0232, 0.0183], grad_fn=<ToCopyBackward0>), [' very', ' good', ' interesting', ' convincing', ' well'])\n",
      "(tensor([0.7721, 0.0686, 0.0588, 0.0177, 0.0098], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', '...'])\n",
      "(tensor([0.4206, 0.1688, 0.1125, 0.0426, 0.0224], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0911, 0.0826, 0.0778, 0.0714, 0.0642], grad_fn=<ToCopyBackward0>), [' really', ' was', ' think', ' just', ' don'])\n",
      "(tensor([0.5652, 0.1245, 0.0634, 0.0498, 0.0216], grad_fn=<ToCopyBackward0>), [' very', ' not', ' expecting', ' really', ' so'])\n",
      "(tensor([0.9213, 0.0164, 0.0123, 0.0053, 0.0038], grad_fn=<ToCopyBackward0>), [' disappointed', ',', ' surprised', ' dis', ' critical'])\n",
      "(tensor([0.4449, 0.2757, 0.1183, 0.0297, 0.0244], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' and'])\n",
      "(tensor([0.4654, 0.1238, 0.0794, 0.0537, 0.0250], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.0942, 0.0914, 0.0794, 0.0634, 0.0517], grad_fn=<ToCopyBackward0>), [' was', ' really', ' think', ' have', \"'m\"])\n",
      "(tensor([0.4115, 0.1619, 0.1067, 0.0610, 0.0244], grad_fn=<ToCopyBackward0>), [' very', ' expecting', ' really', ' not', ' hoping'])\n",
      "(tensor([0.9439, 0.0174, 0.0037, 0.0026, 0.0023], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ',', ' dis', ' much'])\n",
      "(tensor([0.4053, 0.2046, 0.1180, 0.0555, 0.0499], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' because', ' when'])\n",
      "(tensor([0.7443, 0.1446, 0.0898, 0.0062, 0.0013], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' \"'])\n",
      "(tensor([0.6119, 0.2576, 0.0233, 0.0197, 0.0100], grad_fn=<ToCopyBackward0>), [' film', ' movie', '.', ' one', ' because'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was a great way to get the word out about the film festival. I'm really excited to be in it. I was very disappointed with the film in the end. I really wanted to like it, because I'm a fan of the\n",
      "(tensor([0.3839, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4377, 0.2435, 0.1962, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7760, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1274, 0.0769, 0.0660, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.0785, 0.0620, 0.0525, 0.0453, 0.0446], grad_fn=<ToCopyBackward0>), [' very', ' waste', ' good', ' great', ' really'])\n",
      "(tensor([0.1965, 0.1800, 0.0849, 0.0515, 0.0262], grad_fn=<ToCopyBackward0>), [' idea', ' way', ' concept', ' example', ' film'])\n",
      "(tensor([9.3412e-01, 4.2374e-02, 1.5473e-02, 1.0977e-03, 6.5257e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' of', ' in', ' into'])\n",
      "(tensor([0.1653, 0.1429, 0.0414, 0.0359, 0.0318], grad_fn=<ToCopyBackward0>), [' bring', ' get', ' introduce', ' see', ' be'])\n",
      "(tensor([0.3420, 0.1178, 0.0625, 0.0555, 0.0495], grad_fn=<ToCopyBackward0>), [' the', ' into', ' to', ' a', ' my'])\n",
      "(tensor([0.1282, 0.0643, 0.0386, 0.0329, 0.0306], grad_fn=<ToCopyBackward0>), [' word', ' fans', ' kids', ' people', ' message'])\n",
      "(tensor([0.9833, 0.0030, 0.0021, 0.0021, 0.0019], grad_fn=<ToCopyBackward0>), [' out', '-', ' about', ' across', ' to'])\n",
      "(tensor([0.4441, 0.1607, 0.0989, 0.0729, 0.0417], grad_fn=<ToCopyBackward0>), [' about', ' on', ' to', '.', ' in'])\n",
      "(tensor([0.4247, 0.0646, 0.0569, 0.0236, 0.0146], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' what', ' our'])\n",
      "(tensor([0.0960, 0.0802, 0.0248, 0.0178, 0.0174], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' Nightmare', ' festival', ' films'])\n",
      "(tensor([0.5358, 0.2359, 0.0430, 0.0268, 0.0098], grad_fn=<ToCopyBackward0>), [' festival', '.', ' industry', ' festivals', ' because'])\n",
      "(tensor([0.5758, 0.2069, 0.0647, 0.0174, 0.0111], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' in', ' that'])\n",
      "(tensor([0.3932, 0.1173, 0.0823, 0.0546, 0.0225], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' We', ' So'])\n",
      "(tensor([0.1553, 0.0767, 0.0653, 0.0534, 0.0494], grad_fn=<ToCopyBackward0>), [' was', ' thought', \"'m\", ' think', ' had'])\n",
      "(tensor([0.1917, 0.1209, 0.0802, 0.0684, 0.0585], grad_fn=<ToCopyBackward0>), [' a', ' really', ' sure', ' very', ' not'])\n",
      "(tensor([0.3750, 0.1173, 0.0788, 0.0426, 0.0412], grad_fn=<ToCopyBackward0>), [' looking', ' excited', ' surprised', ' disappointed', ','])\n",
      "(tensor([0.4706, 0.2756, 0.0654, 0.0511, 0.0285], grad_fn=<ToCopyBackward0>), [' to', ' for', ' by', ' about', ' that'])\n",
      "(tensor([0.5180, 0.1699, 0.0925, 0.0265, 0.0227], grad_fn=<ToCopyBackward0>), [' see', ' be', ' have', ' get', ' start'])\n",
      "(tensor([0.1341, 0.1205, 0.0781, 0.0676, 0.0619], grad_fn=<ToCopyBackward0>), [' a', ' part', ' able', ' in', ' involved'])\n",
      "(tensor([0.1463, 0.0725, 0.0543, 0.0343, 0.0281], grad_fn=<ToCopyBackward0>), [' the', ' it', ' a', ' front', ' New'])\n",
      "(tensor([0.1942, 0.1636, 0.1619, 0.1008, 0.0731], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' as'])\n",
      "(tensor([0.5615, 0.1587, 0.0443, 0.0198, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' And'])\n",
      "(tensor([0.1436, 0.0887, 0.0798, 0.0657, 0.0618], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' think', ' really'])\n",
      "(tensor([0.4464, 0.0893, 0.0282, 0.0264, 0.0250], grad_fn=<ToCopyBackward0>), [' really', ' very', ' a', ' so', ' actually'])\n",
      "(tensor([0.2794, 0.0862, 0.0704, 0.0611, 0.0571], grad_fn=<ToCopyBackward0>), [' excited', ' skeptical', ' surprised', ' disappointed', ' much'])\n",
      "(tensor([0.4814, 0.1935, 0.0785, 0.0653, 0.0596], grad_fn=<ToCopyBackward0>), [' with', ' when', ' in', ' that', ' by'])\n",
      "(tensor([0.3789, 0.2200, 0.0512, 0.0269, 0.0232], grad_fn=<ToCopyBackward0>), [' the', ' this', ' \"', ' how', ' my'])\n",
      "(tensor([0.1489, 0.1400, 0.0462, 0.0421, 0.0400], grad_fn=<ToCopyBackward0>), [' film', ' original', ' first', ' movie', ' last'])\n",
      "(tensor([0.0862, 0.0730, 0.0672, 0.0593, 0.0522], grad_fn=<ToCopyBackward0>), [' last', ' festival', ' in', ' I', ' that'])\n",
      "(tensor([0.3917, 0.0510, 0.0495, 0.0383, 0.0364], grad_fn=<ToCopyBackward0>), [' the', ' its', ' my', ' some', ' terms'])\n",
      "(tensor([0.1694, 0.0971, 0.0825, 0.0758, 0.0521], grad_fn=<ToCopyBackward0>), [' first', ' end', ' beginning', ' theater', ' U'])\n",
      "(tensor([0.4418, 0.2014, 0.0910, 0.0881, 0.0402], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' because', ' and'])\n",
      "(tensor([0.3355, 0.1677, 0.1158, 0.0261, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.1808, 0.0898, 0.0838, 0.0790, 0.0666], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' really', ' think', \"'m\"])\n",
      "(tensor([0.1640, 0.1148, 0.0673, 0.0536, 0.0500], grad_fn=<ToCopyBackward0>), [' wanted', ' thought', ' didn', ' was', ' don'])\n",
      "(tensor([0.8968, 0.0604, 0.0196, 0.0058, 0.0029], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' a', ' my'])\n",
      "(tensor([0.8427, 0.0441, 0.0270, 0.0081, 0.0068], grad_fn=<ToCopyBackward0>), [' like', ' see', ' be', ' give', ' get'])\n",
      "(tensor([0.8469, 0.0803, 0.0426, 0.0045, 0.0014], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' what'])\n",
      "(tensor([0.3441, 0.2518, 0.1931, 0.0411, 0.0359], grad_fn=<ToCopyBackward0>), [',', '.', ' but', ' more', ' a'])\n",
      "(tensor([0.8537, 0.0244, 0.0238, 0.0167, 0.0107], grad_fn=<ToCopyBackward0>), [' but', ' I', ' and', ' because', ' so'])\n",
      "(tensor([0.4950, 0.1546, 0.1060, 0.0230, 0.0207], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' as', ' when'])\n",
      "(tensor([0.1477, 0.1436, 0.1354, 0.1130, 0.0947], grad_fn=<ToCopyBackward0>), [' really', ' think', \"'m\", ' thought', ' like'])\n",
      "(tensor([0.6591, 0.0789, 0.0410, 0.0293, 0.0218], grad_fn=<ToCopyBackward0>), [' a', ' an', ' from', ' not', ' such'])\n",
      "(tensor([0.4186, 0.2027, 0.0988, 0.0506, 0.0312], grad_fn=<ToCopyBackward0>), [' fan', ' big', ' film', ' huge', ' movie'])\n",
      "(tensor([0.9673, 0.0127, 0.0068, 0.0029, 0.0013], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' and', ' but'])\n",
      "(tensor([0.1540, 0.0269, 0.0227, 0.0176, 0.0148], grad_fn=<ToCopyBackward0>), [' the', ' low', ' \"', ' all', ' independent'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was the most boring movie I have ever seen. It was not even funny, not even interesting. It was boring. It was just boring and stupid. I was very disappointed. This movie is a big disappointment. It was a big waste of\n",
      "(tensor([0.3846, 0.1712, 0.0896, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4386, 0.2429, 0.1959, 0.0166, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4800, 0.1358, 0.1037, 0.0555, 0.0252], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.7397, 0.0315, 0.0258, 0.0250, 0.0178], grad_fn=<ToCopyBackward0>), [' worst', ' Worst', ' most', ' WOR', ' movie'])\n",
      "(tensor([0.3615, 0.0476, 0.0350, 0.0334, 0.0330], grad_fn=<ToCopyBackward0>), [' boring', ' god', ' atro', ' cl', ' pointless'])\n",
      "(tensor([0.6474, 0.2181, 0.0185, 0.0158, 0.0123], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' documentary', ' and', ','])\n",
      "(tensor([0.6466, 0.1980, 0.0677, 0.0168, 0.0143], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' in', ' of'])\n",
      "(tensor([0.4355, 0.2992, 0.1242, 0.0669, 0.0418], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' had', ' ever', \"'d\"])\n",
      "(tensor([0.9360, 0.0492, 0.0038, 0.0027, 0.0014], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' EVER'])\n",
      "(tensor([0.7284, 0.1010, 0.0348, 0.0210, 0.0192], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' wasted'])\n",
      "(tensor([0.5013, 0.1793, 0.0683, 0.0632, 0.0594], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', '...', '!'])\n",
      "(tensor([0.2046, 0.1693, 0.1313, 0.0281, 0.0257], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' Not'])\n",
      "(tensor([0.2714, 0.0981, 0.0764, 0.0709, 0.0566], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' is', ' seemed'])\n",
      "(tensor([0.1438, 0.1385, 0.1284, 0.0860, 0.0322], grad_fn=<ToCopyBackward0>), [' so', ' not', ' slow', ' boring', ' like'])\n",
      "(tensor([0.6167, 0.0626, 0.0618, 0.0368, 0.0323], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' entertaining', ' boring', ' scary'])\n",
      "(tensor([0.4243, 0.1960, 0.0545, 0.0258, 0.0246], grad_fn=<ToCopyBackward0>), [' funny', ' entertaining', ' interesting', ' scary', ' suspense'])\n",
      "(tensor([0.5101, 0.0708, 0.0656, 0.0608, 0.0267], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' to', ' in'])\n",
      "(tensor([0.2141, 0.1146, 0.0814, 0.0757, 0.0596], grad_fn=<ToCopyBackward0>), [' it', ' the', ' not', ' just', ' but'])\n",
      "(tensor([0.5758, 0.1502, 0.0560, 0.0318, 0.0289], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' interesting', ' entertaining', ' one'])\n",
      "(tensor([0.3973, 0.0888, 0.0759, 0.0348, 0.0317], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' scary', ' suspense', ' funny'])\n",
      "(tensor([0.3733, 0.3220, 0.0501, 0.0436, 0.0357], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' to'])\n",
      "(tensor([0.2223, 0.2026, 0.1725, 0.0221, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' This'])\n",
      "(tensor([0.5157, 0.0706, 0.0634, 0.0393, 0.0311], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', \"'s\", ' wasn'])\n",
      "(tensor([0.4035, 0.1447, 0.0604, 0.0322, 0.0294], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' so', ' simply'])\n",
      "(tensor([0.3261, 0.2870, 0.1258, 0.0706, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' as'])\n",
      "(tensor([0.1809, 0.1517, 0.1294, 0.0401, 0.0255], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.5118, 0.0886, 0.0415, 0.0398, 0.0324], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' wasn', ' didn'])\n",
      "(tensor([0.1548, 0.1292, 0.0790, 0.0522, 0.0468], grad_fn=<ToCopyBackward0>), [' boring', ' not', ' just', ' so', ' stupid'])\n",
      "(tensor([0.3156, 0.1149, 0.0769, 0.0397, 0.0359], grad_fn=<ToCopyBackward0>), [' boring', ' a', ' plain', ' stupid', ' one'])\n",
      "(tensor([0.7783, 0.0564, 0.0254, 0.0230, 0.0219], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.1701, 0.0517, 0.0463, 0.0455, 0.0311], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' I', ' not', ' boring'])\n",
      "(tensor([0.6793, 0.2088, 0.0333, 0.0097, 0.0092], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.2460, 0.1236, 0.1154, 0.0344, 0.0264], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.0724, 0.0664, 0.0541, 0.0437, 0.0390], grad_fn=<ToCopyBackward0>), [' was', ' don', ' have', ' really', \"'m\"])\n",
      "(tensor([0.1452, 0.0678, 0.0478, 0.0423, 0.0399], grad_fn=<ToCopyBackward0>), [' not', ' so', ' very', ' actually', ' really'])\n",
      "(tensor([0.8418, 0.0189, 0.0176, 0.0109, 0.0082], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' angry', ' confused', ' dis'])\n",
      "(tensor([0.5616, 0.1588, 0.1207, 0.0351, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' and'])\n",
      "(tensor([0.3319, 0.1155, 0.1035, 0.0420, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.4806, 0.2765, 0.1239, 0.0217, 0.0163], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' has'])\n",
      "(tensor([0.2774, 0.1119, 0.0751, 0.0679, 0.0443], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' made', ' has'])\n",
      "(tensor([0.2724, 0.0904, 0.0858, 0.0671, 0.0511], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' only'])\n",
      "(tensor([0.2317, 0.1332, 0.0610, 0.0447, 0.0445], grad_fn=<ToCopyBackward0>), [' big', ' complete', ' total', ' waste', ' perfect'])\n",
      "(tensor([0.1943, 0.1734, 0.0742, 0.0574, 0.0264], grad_fn=<ToCopyBackward0>), [' joke', ' disappointment', ' waste', ' insult', ' disaster'])\n",
      "(tensor([0.4329, 0.1386, 0.1077, 0.0623, 0.0582], grad_fn=<ToCopyBackward0>), ['.', ' to', ' for', ' because', ' and'])\n",
      "(tensor([0.2414, 0.1457, 0.1197, 0.0312, 0.0303], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' This'])\n",
      "(tensor([0.2423, 0.1785, 0.1473, 0.0613, 0.0404], grad_fn=<ToCopyBackward0>), [' is', ' was', \"'s\", ' has', ' really'])\n",
      "(tensor([0.2816, 0.1433, 0.0611, 0.0572, 0.0505], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' so', ' very'])\n",
      "(tensor([0.5605, 0.1247, 0.0327, 0.0294, 0.0172], grad_fn=<ToCopyBackward0>), [' big', ' waste', ' huge', ' complete', ' real'])\n",
      "(tensor([0.4300, 0.2582, 0.0311, 0.0305, 0.0220], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' let', ',', ' joke'])\n",
      "(tensor([9.9384e-01, 3.7865e-03, 7.8365e-04, 2.3617e-04, 1.6310e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', ' for', ' to', ' and'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was a sequel, not a remake. The first one was very good, but it's just not the same. The first one was a very good movie. The original movie was very good. This is a remake that is not the same as\n",
      "(tensor([0.3838, 0.1719, 0.0899, 0.0770, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4376, 0.2440, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4798, 0.1356, 0.1036, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1341, 0.1095, 0.0993, 0.0923, 0.0466], grad_fn=<ToCopyBackward0>), [' really', ' sequel', ' good', ' great', ' movie'])\n",
      "(tensor([0.7063, 0.0560, 0.0492, 0.0204, 0.0171], grad_fn=<ToCopyBackward0>), [' to', ' of', ' that', ',', ' in'])\n",
      "(tensor([0.2275, 0.2080, 0.0467, 0.0458, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' but', ' and', ' I', ' then'])\n",
      "(tensor([0.8808, 0.0317, 0.0256, 0.0229, 0.0054], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' another', ' even'])\n",
      "(tensor([0.5868, 0.1769, 0.0529, 0.0391, 0.0322], grad_fn=<ToCopyBackward0>), [' remake', ' pre', ' re', ' sequel', ' rem'])\n",
      "(tensor([0.7945, 0.0574, 0.0409, 0.0172, 0.0126], grad_fn=<ToCopyBackward0>), ['.', ' of', ',', '!', ' ('])\n",
      "(tensor([0.2404, 0.1765, 0.0503, 0.0466, 0.0185], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' But'])\n",
      "(tensor([0.1746, 0.1080, 0.0580, 0.0504, 0.0337], grad_fn=<ToCopyBackward0>), [' original', ' first', ' movie', ' acting', ' only'])\n",
      "(tensor([0.3690, 0.1282, 0.0746, 0.0341, 0.0286], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' film', ' sequel', ' was'])\n",
      "(tensor([0.7065, 0.0574, 0.0523, 0.0093, 0.0092], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' I'])\n",
      "(tensor([0.2250, 0.0733, 0.0549, 0.0514, 0.0499], grad_fn=<ToCopyBackward0>), [' great', ' good', ' very', ' a', ' pretty'])\n",
      "(tensor([0.4885, 0.0570, 0.0560, 0.0469, 0.0391], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' well', ' much', ' interesting'])\n",
      "(tensor([0.3461, 0.3089, 0.1599, 0.0377, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' in'])\n",
      "(tensor([0.2639, 0.1541, 0.0933, 0.0799, 0.0700], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' so', ' very'])\n",
      "(tensor([0.6470, 0.0598, 0.0562, 0.0456, 0.0136], grad_fn=<ToCopyBackward0>), [' this', ' it', ' I', ' the', ' not'])\n",
      "(tensor([0.4153, 0.0841, 0.0716, 0.0533, 0.0504], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' had', ' didn'])\n",
      "(tensor([0.2902, 0.1213, 0.0733, 0.0413, 0.0404], grad_fn=<ToCopyBackward0>), [' not', ' a', ' been', ' hard', ' just'])\n",
      "(tensor([0.3311, 0.2439, 0.0429, 0.0355, 0.0311], grad_fn=<ToCopyBackward0>), [' not', ' a', ' the', ' been', ' too'])\n",
      "(tensor([0.2649, 0.2061, 0.0581, 0.0439, 0.0387], grad_fn=<ToCopyBackward0>), [' as', ' the', ' funny', ' a', ' that'])\n",
      "(tensor([0.9782, 0.0047, 0.0034, 0.0016, 0.0010], grad_fn=<ToCopyBackward0>), [' same', ' movie', ' story', ' sequel', ' original'])\n",
      "(tensor([0.4873, 0.1343, 0.0799, 0.0697, 0.0219], grad_fn=<ToCopyBackward0>), ['.', ' as', ' movie', ' story', ','])\n",
      "(tensor([0.2632, 0.1751, 0.0757, 0.0651, 0.0230], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'I'])\n",
      "(tensor([0.2694, 0.0865, 0.0604, 0.0536, 0.0408], grad_fn=<ToCopyBackward0>), [' first', ' original', ' story', ' second', ' acting'])\n",
      "(tensor([0.8536, 0.0315, 0.0161, 0.0078, 0.0075], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' was', ' time', ' film'])\n",
      "(tensor([0.7195, 0.1180, 0.0412, 0.0107, 0.0099], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ',', ' has'])\n",
      "(tensor([0.1262, 0.1184, 0.1067, 0.0493, 0.0345], grad_fn=<ToCopyBackward0>), [' a', ' very', ' more', ' good', ' interesting'])\n",
      "(tensor([0.3078, 0.0988, 0.0625, 0.0523, 0.0419], grad_fn=<ToCopyBackward0>), [' good', ' great', ' very', ' remake', ' little'])\n",
      "(tensor([0.5743, 0.0724, 0.0586, 0.0157, 0.0108], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' interesting', ' creepy', ' clever'])\n",
      "(tensor([0.5026, 0.1297, 0.0617, 0.0616, 0.0496], grad_fn=<ToCopyBackward0>), [' movie', ' action', ' comedy', ' film', ' story'])\n",
      "(tensor([0.3753, 0.3287, 0.0438, 0.0373, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' in', ' with', ' but'])\n",
      "(tensor([0.2629, 0.1588, 0.1081, 0.1049, 0.0991], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' But', ' It'])\n",
      "(tensor([0.3767, 0.3262, 0.0336, 0.0221, 0.0217], grad_fn=<ToCopyBackward0>), [' second', ' first', ' original', ' remake', ' only'])\n",
      "(tensor([0.3484, 0.1912, 0.0985, 0.0274, 0.0237], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' was', ' \"', ' is'])\n",
      "(tensor([0.6223, 0.1257, 0.0389, 0.0251, 0.0108], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ',', ' has'])\n",
      "(tensor([0.2075, 0.1401, 0.0995, 0.0503, 0.0434], grad_fn=<ToCopyBackward0>), [' very', ' great', ' a', ' good', ' pretty'])\n",
      "(tensor([0.6845, 0.0622, 0.0564, 0.0243, 0.0181], grad_fn=<ToCopyBackward0>), [' good', ' funny', ' entertaining', ' interesting', ' well'])\n",
      "(tensor([0.4261, 0.3810, 0.0342, 0.0200, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' too'])\n",
      "(tensor([0.2213, 0.1822, 0.1428, 0.0753, 0.0604], grad_fn=<ToCopyBackward0>), [' I', ' The', ' But', ' This', ' It'])\n",
      "(tensor([0.3500, 0.2435, 0.1870, 0.0649, 0.0169], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' is', ' was', ' sequel'])\n",
      "(tensor([0.3704, 0.2666, 0.1148, 0.0258, 0.0168], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' an', ' the'])\n",
      "(tensor([0.1927, 0.0964, 0.0649, 0.0549, 0.0435], grad_fn=<ToCopyBackward0>), [' remake', ' very', ' good', ' bad', ' totally'])\n",
      "(tensor([0.3125, 0.2285, 0.2044, 0.0600, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ' of', ',', ' that', ' and'])\n",
      "(tensor([0.1881, 0.1385, 0.0861, 0.0689, 0.0453], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' doesn', ' was', ' tries'])\n",
      "(tensor([0.3454, 0.1034, 0.0635, 0.0634, 0.0312], grad_fn=<ToCopyBackward0>), [' not', ' a', ' just', ' very', ' trying'])\n",
      "(tensor([0.1898, 0.1565, 0.1361, 0.0759, 0.0740], grad_fn=<ToCopyBackward0>), [' good', ' very', ' as', ' really', ' the'])\n",
      "(tensor([0.9246, 0.0589, 0.0022, 0.0017, 0.0015], grad_fn=<ToCopyBackward0>), [' same', ' original', ' first', ' quality', ' remake'])\n",
      "(tensor([0.4086, 0.1819, 0.1773, 0.0472, 0.0283], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' as', ' at', ','])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. The acting was pretty bad. The script was just terrible. I mean it's a comedy. It's like a comedy. It's not a drama. It's a comedy. It's like a comedy. It's\n",
      "(tensor([0.3833, 0.1721, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4373, 0.2447, 0.1959, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7767, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0660, 0.0557, 0.0422], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1304, 0.1267, 0.1048, 0.0741, 0.0606], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5981, 0.0807, 0.0745, 0.0361, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2067, 0.1827, 0.1379, 0.0233, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.1615, 0.0781, 0.0753, 0.0724, 0.0317], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' only', ' script'])\n",
      "(tensor([0.7844, 0.0461, 0.0400, 0.0236, 0.0191], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ',', ' and'])\n",
      "(tensor([0.1184, 0.0918, 0.0767, 0.0738, 0.0545], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' pretty', ' horrible'])\n",
      "(tensor([0.2892, 0.0982, 0.0863, 0.0734, 0.0484], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' poor', ' terrible', ' lousy'])\n",
      "(tensor([0.2886, 0.2791, 0.1493, 0.1236, 0.0496], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' too', ' as'])\n",
      "(tensor([0.4702, 0.1028, 0.0861, 0.0320, 0.0242], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.2854, 0.1650, 0.1546, 0.0484, 0.0257], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' cinem', ' storyline'])\n",
      "(tensor([0.7548, 0.0743, 0.0196, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' is', ' wasn', ','])\n",
      "(tensor([0.3429, 0.0657, 0.0565, 0.0427, 0.0412], grad_fn=<ToCopyBackward0>), [' pretty', ' bad', ' really', ' just', ' terrible'])\n",
      "(tensor([0.1366, 0.1184, 0.0968, 0.0570, 0.0544], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' stupid', ' horrible', ' a'])\n",
      "(tensor([0.8222, 0.0831, 0.0395, 0.0091, 0.0040], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' -'])\n",
      "(tensor([0.3266, 0.1416, 0.0964, 0.0524, 0.0253], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0741, 0.0727, 0.0684, 0.0680, 0.0621], grad_fn=<ToCopyBackward0>), [' thought', ' don', ' was', ' mean', ' think'])\n",
      "(tensor([0.3794, 0.1691, 0.0962, 0.0678, 0.0344], grad_fn=<ToCopyBackward0>), [',', ' the', ' it', ' I', ' this'])\n",
      "(tensor([0.4202, 0.1779, 0.0527, 0.0404, 0.0338], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' could'])\n",
      "(tensor([0.2053, 0.1517, 0.0302, 0.0243, 0.0234], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' pretty', ' like'])\n",
      "(tensor([0.1377, 0.1043, 0.0628, 0.0499, 0.0365], grad_fn=<ToCopyBackward0>), [' comedy', ' really', ' good', ' movie', ' great'])\n",
      "(tensor([0.2008, 0.1640, 0.1347, 0.1272, 0.0473], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' and', ' so'])\n",
      "(tensor([0.2456, 0.1211, 0.0598, 0.0542, 0.0359], grad_fn=<ToCopyBackward0>), [' It', ' I', ' You', ' The', ' But'])\n",
      "(tensor([0.6926, 0.0781, 0.0361, 0.0221, 0.0201], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' was', ' doesn', ' has'])\n",
      "(tensor([0.2651, 0.2096, 0.1727, 0.0598, 0.0282], grad_fn=<ToCopyBackward0>), [' supposed', ' not', ' a', ' like', ' funny'])\n",
      "(tensor([0.2649, 0.1707, 0.0660, 0.0568, 0.0266], grad_fn=<ToCopyBackward0>), [' a', ' watching', ',', ' the', ' you'])\n",
      "(tensor([0.1575, 0.0846, 0.0530, 0.0442, 0.0404], grad_fn=<ToCopyBackward0>), [' comedy', ' bad', ' really', ' sitcom', ' drama'])\n",
      "(tensor([0.4452, 0.0693, 0.0434, 0.0373, 0.0355], grad_fn=<ToCopyBackward0>), ['.', ' with', ' that', '-', '!'])\n",
      "(tensor([0.2306, 0.1248, 0.0700, 0.0491, 0.0373], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' You', ' But'])\n",
      "(tensor([0.6252, 0.0740, 0.0337, 0.0326, 0.0315], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' has', ' doesn', ' was'])\n",
      "(tensor([0.2859, 0.2234, 0.1465, 0.0634, 0.0451], grad_fn=<ToCopyBackward0>), [' like', ' not', ' supposed', ' a', ' funny'])\n",
      "(tensor([0.1912, 0.1483, 0.1459, 0.0779, 0.0397], grad_fn=<ToCopyBackward0>), [' supposed', ' a', ' like', ' even', ' really'])\n",
      "(tensor([0.4211, 0.2453, 0.0276, 0.0258, 0.0256], grad_fn=<ToCopyBackward0>), [' drama', ' serious', ' tragedy', ' really', ' dramatic'])\n",
      "(tensor([0.6262, 0.0780, 0.0735, 0.0488, 0.0156], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', '!', ' at'])\n",
      "(tensor([0.2910, 0.1417, 0.0750, 0.0627, 0.0363], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' So'])\n",
      "(tensor([0.7538, 0.0526, 0.0230, 0.0229, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' doesn', ' just', ' has'])\n",
      "(tensor([0.3353, 0.1903, 0.1498, 0.0826, 0.0297], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' just', ' supposed'])\n",
      "(tensor([0.6419, 0.0644, 0.0388, 0.0265, 0.0136], grad_fn=<ToCopyBackward0>), [' comedy', ' stupid', ' silly', ' funny', ' very'])\n",
      "(tensor([0.7301, 0.0539, 0.0404, 0.0232, 0.0230], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' with', ' that'])\n",
      "(tensor([0.2157, 0.1302, 0.0690, 0.0642, 0.0435], grad_fn=<ToCopyBackward0>), [' It', ' I', ' And', ' The', ' But'])\n",
      "(tensor([0.6860, 0.0499, 0.0393, 0.0323, 0.0249], grad_fn=<ToCopyBackward0>), [\"'s\", ' should', ' has', ' was', ' just'])\n",
      "(tensor([0.2880, 0.2159, 0.0877, 0.0658, 0.0653], grad_fn=<ToCopyBackward0>), [' like', ' not', ' a', ' funny', ' supposed'])\n",
      "(tensor([0.4844, 0.0637, 0.0452, 0.0432, 0.0368], grad_fn=<ToCopyBackward0>), [' a', ',', ' watching', ' the', ' an'])\n",
      "(tensor([0.6670, 0.0264, 0.0193, 0.0177, 0.0158], grad_fn=<ToCopyBackward0>), [' comedy', ' drama', ' really', ' sitcom', ' funny'])\n",
      "(tensor([0.8093, 0.0261, 0.0205, 0.0195, 0.0138], grad_fn=<ToCopyBackward0>), ['.', '!', ' with', ',', ' that'])\n",
      "(tensor([0.2659, 0.0994, 0.0572, 0.0504, 0.0446], grad_fn=<ToCopyBackward0>), [' It', ' I', ' And', ' The', ' But'])\n",
      "(tensor([0.8669, 0.0189, 0.0157, 0.0136, 0.0118], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' is', ' just'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I had to watch this movie just to find out where the money came from to make such a terrible movie. I was so disappointed that I just turned it off after 30 minutes. I can't even believe that this movie is not in the IMDB\n",
      "(tensor([0.3843, 0.1718, 0.0899, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2251, 0.1947, 0.1573, 0.0695, 0.0535], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.6381, 0.1364, 0.0516, 0.0222, 0.0094], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' it'])\n",
      "(tensor([0.2257, 0.1009, 0.1000, 0.0802, 0.0619], grad_fn=<ToCopyBackward0>), [' watch', ' be', ' write', ' give', ' change'])\n",
      "(tensor([0.6466, 0.1896, 0.0578, 0.0089, 0.0081], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' some'])\n",
      "(tensor([0.3176, 0.2490, 0.0943, 0.0425, 0.0238], grad_fn=<ToCopyBackward0>), [' movie', ' because', ' film', ' one', ' so'])\n",
      "(tensor([0.4171, 0.0955, 0.0559, 0.0463, 0.0427], grad_fn=<ToCopyBackward0>), [' just', ' because', '.', ' to', ','])\n",
      "(tensor([0.7761, 0.0735, 0.0664, 0.0414, 0.0169], grad_fn=<ToCopyBackward0>), [' to', ' because', ' for', ' so', ' in'])\n",
      "(tensor([0.5381, 0.0910, 0.0825, 0.0371, 0.0205], grad_fn=<ToCopyBackward0>), [' see', ' get', ' be', ' find', ' understand'])\n",
      "(tensor([0.9666, 0.0088, 0.0039, 0.0025, 0.0020], grad_fn=<ToCopyBackward0>), [' out', ' the', ' a', ' that', ' something'])\n",
      "(tensor([0.4212, 0.2156, 0.1227, 0.0512, 0.0464], grad_fn=<ToCopyBackward0>), [' what', ' how', ' if', ' why', ' where'])\n",
      "(tensor([0.3907, 0.3219, 0.0740, 0.0366, 0.0229], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' they', ' to'])\n",
      "(tensor([0.0843, 0.0607, 0.0589, 0.0395, 0.0393], grad_fn=<ToCopyBackward0>), [' hell', ' money', ' title', ' ending', ' movie'])\n",
      "(tensor([0.3110, 0.1858, 0.1271, 0.0611, 0.0525], grad_fn=<ToCopyBackward0>), [' went', ' was', ' came', ' is', ' goes'])\n",
      "(tensor([0.8763, 0.0330, 0.0301, 0.0181, 0.0056], grad_fn=<ToCopyBackward0>), [' from', ' up', ' out', ' in', ' into'])\n",
      "(tensor([0.4492, 0.1620, 0.1236, 0.0573, 0.0474], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' for', ' and'])\n",
      "(tensor([0.6909, 0.0561, 0.0463, 0.0434, 0.0296], grad_fn=<ToCopyBackward0>), [' make', ' get', ' pay', ' create', ' buy'])\n",
      "(tensor([0.6291, 0.2157, 0.0503, 0.0322, 0.0194], grad_fn=<ToCopyBackward0>), [' it', ' this', ' such', ' the', ' a'])\n",
      "(tensor([0.9322, 0.0330, 0.0024, 0.0022, 0.0016], grad_fn=<ToCopyBackward0>), [' a', ' an', ' bad', ' terrible', ' great'])\n",
      "(tensor([0.1505, 0.1492, 0.0951, 0.0770, 0.0375], grad_fn=<ToCopyBackward0>), [' terrible', ' bad', ' crappy', ' lousy', ' horrible'])\n",
      "(tensor([0.8611, 0.0874, 0.0185, 0.0055, 0.0045], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' script', ',', ' sequel'])\n",
      "(tensor([0.8835, 0.0288, 0.0246, 0.0123, 0.0085], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', '..'])\n",
      "(tensor([0.2822, 0.1824, 0.0851, 0.0405, 0.0223], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.1529, 0.0760, 0.0666, 0.0447, 0.0318], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' have', ' can'])\n",
      "(tensor([0.2080, 0.1318, 0.0822, 0.0602, 0.0493], grad_fn=<ToCopyBackward0>), [' really', ' so', ' very', ' actually', ' wrong'])\n",
      "(tensor([0.6667, 0.0368, 0.0253, 0.0200, 0.0197], grad_fn=<ToCopyBackward0>), [' disappointed', ' wrong', ' surprised', ' upset', ' annoyed'])\n",
      "(tensor([0.2427, 0.1355, 0.0903, 0.0839, 0.0775], grad_fn=<ToCopyBackward0>), ['.', ' in', ' when', ' with', ' that'])\n",
      "(tensor([0.4235, 0.1206, 0.0885, 0.0593, 0.0527], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' they', ' it'])\n",
      "(tensor([0.1573, 0.1069, 0.0747, 0.0544, 0.0490], grad_fn=<ToCopyBackward0>), [' actually', ' had', ' could', ' just', ' couldn'])\n",
      "(tensor([0.3051, 0.0585, 0.0558, 0.0498, 0.0492], grad_fn=<ToCopyBackward0>), [' couldn', ' wasted', ' could', ' turned', ' had'])\n",
      "(tensor([0.5418, 0.3620, 0.0314, 0.0245, 0.0072], grad_fn=<ToCopyBackward0>), [' it', ' the', ' off', ' this', ' to'])\n",
      "(tensor([9.9747e-01, 6.1253e-04, 4.4795e-04, 2.9794e-04, 1.8187e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' off', ' on', ' OFF', ' over', ' down'])\n",
      "(tensor([0.5574, 0.1067, 0.0739, 0.0264, 0.0252], grad_fn=<ToCopyBackward0>), [' after', '.', ' halfway', ' in', ' and'])\n",
      "(tensor([0.1540, 0.0800, 0.0648, 0.0587, 0.0514], grad_fn=<ToCopyBackward0>), [' the', ' 10', ' 45', ' 30', ' 40'])\n",
      "(tensor([9.8023e-01, 9.5613e-03, 4.9778e-03, 1.3362e-03, 5.4700e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' minutes', ' seconds', ' mins', ' min', 'min'])\n",
      "(tensor([0.6706, 0.0620, 0.0555, 0.0420, 0.0347], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' thinking'])\n",
      "(tensor([0.3087, 0.2120, 0.0735, 0.0355, 0.0171], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' If'])\n",
      "(tensor([0.0931, 0.0708, 0.0689, 0.0684, 0.0680], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' really', ' can', ' would'])\n",
      "(tensor([0.6652, 0.0559, 0.0403, 0.0365, 0.0363], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' see', ' understand', ' honestly'])\n",
      "(tensor([0.5994, 0.1058, 0.0890, 0.0431, 0.0236], grad_fn=<ToCopyBackward0>), [' believe', ' imagine', ' even', ' understand', ' say'])\n",
      "(tensor([0.1530, 0.1036, 0.1017, 0.0784, 0.0644], grad_fn=<ToCopyBackward0>), [' describe', ' tell', ' believe', ' imagine', ' begin'])\n",
      "(tensor([0.5975, 0.1106, 0.0390, 0.0357, 0.0299], grad_fn=<ToCopyBackward0>), [' that', ' this', ' I', ' the', ' they'])\n",
      "(tensor([0.1327, 0.1080, 0.1007, 0.0932, 0.0767], grad_fn=<ToCopyBackward0>), [' I', ' the', ' this', ' a', ' so'])\n",
      "(tensor([0.8639, 0.0362, 0.0205, 0.0150, 0.0141], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' kind'])\n",
      "(tensor([0.4251, 0.1567, 0.0569, 0.0516, 0.0512], grad_fn=<ToCopyBackward0>), [' was', ' is', ' even', ' could', ' got'])\n",
      "(tensor([0.1836, 0.1203, 0.0842, 0.0736, 0.0731], grad_fn=<ToCopyBackward0>), [' actually', ' in', ' being', ' even', ' not'])\n",
      "(tensor([0.1670, 0.0895, 0.0793, 0.0668, 0.0540], grad_fn=<ToCopyBackward0>), [' in', ' a', ' even', ' at', ' more'])\n",
      "(tensor([0.7058, 0.0540, 0.0452, 0.0114, 0.0088], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' any', ' some'])\n",
      "(tensor([0.4225, 0.3351, 0.0163, 0.0107, 0.0102], grad_fn=<ToCopyBackward0>), [' IM', ' bottom', ' top', ' worst', ' Bottom'])\n",
      "(tensor([8.8444e-01, 1.0763e-01, 4.9939e-03, 5.3864e-04, 4.0474e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['DB', 'Db', 'db', 'BD', 'AX'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the ending was pretty cool too. I thought it was pretty violent. I thought the movie would have been better with no ending and only one or two more minutes of the movie. I thought the movie would have been better with no ending and only one\n",
      "(tensor([0.3834, 0.1720, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5010, 0.0600, 0.0340, 0.0152, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.5513, 0.0548, 0.0542, 0.0335, 0.0287], grad_fn=<ToCopyBackward0>), [' was', ' made', ' of', ' to', ' sucked'])\n",
      "(tensor([0.3874, 0.0509, 0.0385, 0.0363, 0.0336], grad_fn=<ToCopyBackward0>), [' pretty', ' really', ' very', ' weak', ' a'])\n",
      "(tensor([0.1634, 0.0785, 0.0572, 0.0493, 0.0354], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' cool', ' decent', ' weak'])\n",
      "(tensor([0.4667, 0.1455, 0.1344, 0.0591, 0.0289], grad_fn=<ToCopyBackward0>), [' too', ',', '.', ' and', ' as'])\n",
      "(tensor([0.7144, 0.1364, 0.0240, 0.0175, 0.0124], grad_fn=<ToCopyBackward0>), ['.', ',', '..', '!', ' but'])\n",
      "(tensor([0.2959, 0.0747, 0.0605, 0.0320, 0.0160], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Too', 'I'])\n",
      "(tensor([0.1313, 0.1037, 0.0907, 0.0608, 0.0396], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', ' think', ' mean'])\n",
      "(tensor([0.2998, 0.2213, 0.0402, 0.0289, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' this', ' I'])\n",
      "(tensor([0.5796, 0.0983, 0.0472, 0.0212, 0.0212], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' really', ' had'])\n",
      "(tensor([0.4781, 0.1048, 0.0371, 0.0341, 0.0212], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' the', ' really', ' very'])\n",
      "(tensor([0.1330, 0.0667, 0.0434, 0.0321, 0.0302], grad_fn=<ToCopyBackward0>), [' original', ' good', ' violent', ' well', ' intense'])\n",
      "(tensor([0.3423, 0.2174, 0.1248, 0.0613, 0.0471], grad_fn=<ToCopyBackward0>), [' too', ' and', ',', '.', ' but'])\n",
      "(tensor([0.4310, 0.0547, 0.0529, 0.0501, 0.0439], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' But'])\n",
      "(tensor([0.1900, 0.0896, 0.0794, 0.0692, 0.0547], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' think', ' mean', ' really'])\n",
      "(tensor([0.4377, 0.2454, 0.0386, 0.0367, 0.0225], grad_fn=<ToCopyBackward0>), [' it', ' the', ' there', ' that', ' I'])\n",
      "(tensor([0.3601, 0.0822, 0.0412, 0.0208, 0.0186], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' violence', ' film', ' ending'])\n",
      "(tensor([0.5681, 0.0379, 0.0373, 0.0271, 0.0256], grad_fn=<ToCopyBackward0>), [' was', ' did', ' had', ' could', ' would'])\n",
      "(tensor([0.5492, 0.2095, 0.0303, 0.0195, 0.0175], grad_fn=<ToCopyBackward0>), [' be', ' have', ' get', ' end', ' make'])\n",
      "(tensor([0.4564, 0.1936, 0.0614, 0.0288, 0.0276], grad_fn=<ToCopyBackward0>), [' been', ' a', ' more', ' to', ' ended'])\n",
      "(tensor([0.4741, 0.1638, 0.1209, 0.0256, 0.0211], grad_fn=<ToCopyBackward0>), [' better', ' more', ' a', ' much', ' really'])\n",
      "(tensor([0.6530, 0.0769, 0.0696, 0.0496, 0.0303], grad_fn=<ToCopyBackward0>), [' if', ' with', ' without', ' had', '.'])\n",
      "(tensor([0.2552, 0.1304, 0.0811, 0.0576, 0.0530], grad_fn=<ToCopyBackward0>), [' a', ' the', ' more', ' some', ' no'])\n",
      "(tensor([0.1774, 0.0872, 0.0448, 0.0357, 0.0226], grad_fn=<ToCopyBackward0>), [' ending', ' violence', ' plot', ' resolution', ' real'])\n",
      "(tensor([0.4074, 0.1930, 0.1848, 0.0317, 0.0254], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', ' because', ' and'])\n",
      "(tensor([0.1335, 0.1187, 0.0823, 0.0727, 0.0478], grad_fn=<ToCopyBackward0>), [' no', ' just', ' only', ' it', ' maybe'])\n",
      "(tensor([0.2513, 0.1873, 0.1085, 0.1006, 0.0760], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' some', ' two'])\n",
      "(tensor([0.0708, 0.0696, 0.0695, 0.0405, 0.0291], grad_fn=<ToCopyBackward0>), [' or', ' part', ' scene', ' more', ' person'])\n",
      "(tensor([0.8822, 0.0663, 0.0141, 0.0042, 0.0034], grad_fn=<ToCopyBackward0>), [' two', ' maybe', ' no', ' a', ' the'])\n",
      "(tensor([0.5275, 0.0498, 0.0408, 0.0218, 0.0114], grad_fn=<ToCopyBackward0>), [' more', ' people', ' of', ' scenes', ' parts'])\n",
      "(tensor([0.1342, 0.0957, 0.0573, 0.0336, 0.0267], grad_fn=<ToCopyBackward0>), [' minutes', ' scenes', ' people', ' hours', ' things'])\n",
      "(tensor([0.3557, 0.2225, 0.1557, 0.0637, 0.0293], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' left', ' to'])\n",
      "(tensor([0.3919, 0.0726, 0.0384, 0.0359, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' it', ' story', ' footage', ' people'])\n",
      "(tensor([0.4569, 0.0555, 0.0328, 0.0164, 0.0160], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' guy', ' same'])\n",
      "(tensor([0.5158, 0.2277, 0.0354, 0.0313, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' that', ' and'])\n",
      "(tensor([0.2589, 0.1440, 0.0637, 0.0541, 0.0369], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' That'])\n",
      "(tensor([0.1827, 0.0920, 0.0904, 0.0729, 0.0505], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' was', ' really', ' just'])\n",
      "(tensor([0.4084, 0.2537, 0.0962, 0.0351, 0.0208], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' there', ' this'])\n",
      "(tensor([0.5643, 0.0277, 0.0251, 0.0198, 0.0155], grad_fn=<ToCopyBackward0>), [' movie', ' ending', ' acting', ' story', ' violence'])\n",
      "(tensor([0.3123, 0.2820, 0.0819, 0.0454, 0.0228], grad_fn=<ToCopyBackward0>), [' would', ' was', ' could', ' had', ' should'])\n",
      "(tensor([0.7250, 0.2107, 0.0096, 0.0086, 0.0067], grad_fn=<ToCopyBackward0>), [' have', ' be', ' not', ' of', ' make'])\n",
      "(tensor([0.8729, 0.0316, 0.0176, 0.0140, 0.0084], grad_fn=<ToCopyBackward0>), [' been', ' made', ' ended', ' had', ' worked'])\n",
      "(tensor([0.6376, 0.1421, 0.0458, 0.0279, 0.0237], grad_fn=<ToCopyBackward0>), [' better', ' more', ' a', ' much', ' even'])\n",
      "(tensor([0.3891, 0.3780, 0.1580, 0.0137, 0.0092], grad_fn=<ToCopyBackward0>), [' if', ' with', ' without', ' had', '.'])\n",
      "(tensor([0.3899, 0.1216, 0.0680, 0.0646, 0.0618], grad_fn=<ToCopyBackward0>), [' no', ' a', ' the', ' only', ' one'])\n",
      "(tensor([0.8264, 0.0327, 0.0117, 0.0103, 0.0094], grad_fn=<ToCopyBackward0>), [' ending', ' end', ' beginning', ' one', ' violence'])\n",
      "(tensor([0.7515, 0.0740, 0.0668, 0.0202, 0.0143], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' at', ' but'])\n",
      "(tensor([0.9658, 0.0094, 0.0044, 0.0032, 0.0019], grad_fn=<ToCopyBackward0>), [' only', ' just', ' no', ' one', ' the'])\n",
      "(tensor([0.9183, 0.0442, 0.0117, 0.0080, 0.0018], grad_fn=<ToCopyBackward0>), [' one', ' two', ' three', ' a', ' five'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty boring. The plot was weak and I didn't care one bit about any of the actors in this movie. The acting was bad as was the story. The movie was not funny. I don't see why they made it in\n",
      "(tensor([0.3831, 0.1724, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4996, 0.0602, 0.0341, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6238, 0.0399, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2584, 0.0595, 0.0528, 0.0428, 0.0412], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1970, 0.1240, 0.1045, 0.0785, 0.0679], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3189, 0.2552, 0.2075, 0.0207, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' but'])\n",
      "(tensor([0.1796, 0.1512, 0.1356, 0.0298, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' Not'])\n",
      "(tensor([0.1918, 0.0887, 0.0816, 0.0536, 0.0417], grad_fn=<ToCopyBackward0>), [' acting', ' movie', ' only', ' story', ' plot'])\n",
      "(tensor([0.7251, 0.0570, 0.0207, 0.0147, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' had', ' seemed'])\n",
      "(tensor([0.1200, 0.1158, 0.1128, 0.0570, 0.0451], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' pretty', ' not', ' very'])\n",
      "(tensor([0.5405, 0.2286, 0.1045, 0.0195, 0.0082], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' at', ' as'])\n",
      "(tensor([0.3853, 0.1681, 0.0259, 0.0238, 0.0172], grad_fn=<ToCopyBackward0>), [' the', ' predictable', ' was', ' I', ' seemed'])\n",
      "(tensor([0.1752, 0.1353, 0.0762, 0.0594, 0.0587], grad_fn=<ToCopyBackward0>), [' didn', ' was', ' just', ' really', ' found'])\n",
      "(tensor([9.9574e-01, 1.7313e-03, 6.2601e-04, 3.9112e-04, 1.4406e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2654, 0.2113, 0.1468, 0.0684, 0.0614], grad_fn=<ToCopyBackward0>), [' care', ' really', ' understand', ' get', ' enjoy'])\n",
      "(tensor([0.3628, 0.2962, 0.0625, 0.0497, 0.0373], grad_fn=<ToCopyBackward0>), [' for', ' about', ' one', ' at', ' who'])\n",
      "(tensor([0.9347, 0.0270, 0.0197, 0.0026, 0.0017], grad_fn=<ToCopyBackward0>), [' bit', ' way', ' little', ' dime', ' i'])\n",
      "(tensor([0.7863, 0.1023, 0.0241, 0.0179, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' for', ' what', '.', ' when'])\n",
      "(tensor([0.5547, 0.2923, 0.0354, 0.0124, 0.0092], grad_fn=<ToCopyBackward0>), [' any', ' the', ' it', ' what', ' anyone'])\n",
      "(tensor([0.9261, 0.0265, 0.0148, 0.0100, 0.0025], grad_fn=<ToCopyBackward0>), [' of', ' character', ' one', ' characters', ' part'])\n",
      "(tensor([0.9787, 0.0069, 0.0041, 0.0035, 0.0035], grad_fn=<ToCopyBackward0>), [' the', ' these', ' it', ' those', ' them'])\n",
      "(tensor([9.7924e-01, 1.3323e-02, 1.6588e-03, 6.1666e-04, 4.9950e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' characters', ' actors', ' character', ' main', ' people'])\n",
      "(tensor([0.5727, 0.1264, 0.0853, 0.0824, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ' or', ' in', ',', ' that'])\n",
      "(tensor([0.4257, 0.2949, 0.2351, 0.0155, 0.0057], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' any', ' that'])\n",
      "(tensor([0.6769, 0.1018, 0.0871, 0.0200, 0.0190], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' film', ' flick', '.'])\n",
      "(tensor([0.7186, 0.1014, 0.0170, 0.0166, 0.0152], grad_fn=<ToCopyBackward0>), ['.', ',', ' except', ' (', '...'])\n",
      "(tensor([0.2484, 0.2182, 0.1019, 0.0321, 0.0264], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1675, 0.1453, 0.0924, 0.0640, 0.0427], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' acting', ' plot', ' story'])\n",
      "(tensor([0.7288, 0.0626, 0.0435, 0.0282, 0.0148], grad_fn=<ToCopyBackward0>), [' was', ' in', ' wasn', ' is', ' and'])\n",
      "(tensor([0.0818, 0.0632, 0.0438, 0.0435, 0.0435], grad_fn=<ToCopyBackward0>), [' so', ' terrible', ' awful', ' pretty', ' bad'])\n",
      "(tensor([0.3924, 0.1480, 0.1039, 0.0707, 0.0668], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' too', ' as'])\n",
      "(tensor([0.5701, 0.0632, 0.0351, 0.0304, 0.0220], grad_fn=<ToCopyBackward0>), [' well', ' was', ' it', ' they', ' usual'])\n",
      "(tensor([0.8662, 0.0338, 0.0174, 0.0110, 0.0065], grad_fn=<ToCopyBackward0>), [' the', ' all', ' everything', ' most', ' acting'])\n",
      "(tensor([0.5632, 0.1466, 0.0468, 0.0323, 0.0229], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' directing'])\n",
      "(tensor([0.3457, 0.3131, 0.1048, 0.0466, 0.0357], grad_fn=<ToCopyBackward0>), [' line', '.', ' telling', ' and', ','])\n",
      "(tensor([0.2738, 0.2155, 0.0658, 0.0536, 0.0206], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.2176, 0.1904, 0.0618, 0.0387, 0.0250], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' plot', ' whole', ' story'])\n",
      "(tensor([0.2726, 0.0878, 0.0634, 0.0506, 0.0359], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' is', ' didn'])\n",
      "(tensor([0.1219, 0.0943, 0.0814, 0.0691, 0.0614], grad_fn=<ToCopyBackward0>), [' not', ' just', ' about', ' so', ' a'])\n",
      "(tensor([0.2464, 0.2287, 0.0776, 0.0738, 0.0664], grad_fn=<ToCopyBackward0>), [' funny', ' scary', ' even', ' worth', ' entertaining'])\n",
      "(tensor([0.3538, 0.1471, 0.1011, 0.0958, 0.0632], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' and', ' or'])\n",
      "(tensor([0.2878, 0.2238, 0.1210, 0.0267, 0.0239], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.0842, 0.0675, 0.0608, 0.0571, 0.0570], grad_fn=<ToCopyBackward0>), [' don', ' think', ' really', ' was', \"'m\"])\n",
      "(tensor([9.9418e-01, 1.7918e-03, 1.1144e-03, 5.1999e-04, 3.1525e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3292, 0.1849, 0.1174, 0.0962, 0.0567], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' see'])\n",
      "(tensor([0.5254, 0.2862, 0.0414, 0.0320, 0.0283], grad_fn=<ToCopyBackward0>), [' how', ' why', ' the', ' any', ' what'])\n",
      "(tensor([0.1802, 0.1582, 0.1167, 0.0786, 0.0692], grad_fn=<ToCopyBackward0>), [' people', ' they', ' anyone', ' this', ' it'])\n",
      "(tensor([0.2605, 0.1060, 0.0936, 0.0508, 0.0351], grad_fn=<ToCopyBackward0>), [' made', ' even', ' would', ' had', ' put'])\n",
      "(tensor([0.3806, 0.3091, 0.1276, 0.1040, 0.0572], grad_fn=<ToCopyBackward0>), [' it', ' this', ' a', ' the', ' such'])\n",
      "(tensor([0.1634, 0.0986, 0.0739, 0.0526, 0.0507], grad_fn=<ToCopyBackward0>), ['.', ' a', ' so', ' in', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film would be a real disappointment to me. It wasn't. It was a total waste of time. I was very disappointed. The story had potential. The acting was very bad and the story was not interesting to begin with. The plot was\n",
      "(tensor([0.3835, 0.1719, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2443, 0.1960, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7766, 0.0503, 0.0283, 0.0263, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.8312, 0.0781, 0.0099, 0.0060, 0.0057], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' get', ' never'])\n",
      "(tensor([0.1771, 0.1268, 0.1050, 0.0669, 0.0600], grad_fn=<ToCopyBackward0>), [' a', ' more', ' interesting', ' great', ' better'])\n",
      "(tensor([0.3916, 0.1457, 0.0485, 0.0477, 0.0141], grad_fn=<ToCopyBackward0>), [' good', ' great', ' lot', ' real', ' big'])\n",
      "(tensor([0.2457, 0.0336, 0.0335, 0.0310, 0.0275], grad_fn=<ToCopyBackward0>), [' disappointment', ' hit', ' challenge', ' waste', ' chore'])\n",
      "(tensor([0.4516, 0.1511, 0.0810, 0.0430, 0.0367], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' after', ' for'])\n",
      "(tensor([0.2676, 0.2024, 0.0863, 0.0764, 0.0398], grad_fn=<ToCopyBackward0>), [' the', ' me', ' see', ' those', ' everyone'])\n",
      "(tensor([0.6207, 0.0974, 0.0520, 0.0318, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' when', ' if'])\n",
      "(tensor([0.2263, 0.1775, 0.1383, 0.0259, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' Not'])\n",
      "(tensor([0.2142, 0.1402, 0.0828, 0.0602, 0.0488], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' seemed', ' wasn'])\n",
      "(tensor([9.9631e-01, 1.2934e-03, 3.8290e-04, 2.5885e-04, 1.7858e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.3257, 0.1189, 0.0970, 0.0584, 0.0358], grad_fn=<ToCopyBackward0>), ['.', ' even', ' at', ' as', ' that'])\n",
      "(tensor([0.2800, 0.2389, 0.1015, 0.0449, 0.0362], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' This', 'I'])\n",
      "(tensor([0.7112, 0.0418, 0.0352, 0.0269, 0.0121], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' really'])\n",
      "(tensor([0.1312, 0.0984, 0.0691, 0.0390, 0.0379], grad_fn=<ToCopyBackward0>), [' a', ' actually', ' boring', ' just', ' pretty'])\n",
      "(tensor([0.1726, 0.0741, 0.0735, 0.0525, 0.0480], grad_fn=<ToCopyBackward0>), [' real', ' disappointment', ' very', ' complete', ' total'])\n",
      "(tensor([0.4780, 0.2878, 0.0145, 0.0117, 0.0106], grad_fn=<ToCopyBackward0>), [' disappointment', ' waste', ' hit', ' bomb', ' blast'])\n",
      "(tensor([9.9482e-01, 3.4314e-03, 2.6195e-04, 2.1596e-04, 1.4832e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', '.', '!', ' and', ','])\n",
      "(tensor([0.6273, 0.1490, 0.0388, 0.0198, 0.0186], grad_fn=<ToCopyBackward0>), [' time', ' my', ' 90', ' money', ' 2'])\n",
      "(tensor([0.5714, 0.2558, 0.0915, 0.0110, 0.0080], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', '....'])\n",
      "(tensor([0.2758, 0.2002, 0.1187, 0.0241, 0.0217], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' This'])\n",
      "(tensor([0.1058, 0.0727, 0.0449, 0.0388, 0.0374], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' really', ' don'])\n",
      "(tensor([0.2002, 0.1141, 0.0614, 0.0474, 0.0345], grad_fn=<ToCopyBackward0>), [' very', ' really', ' so', ' actually', ' extremely'])\n",
      "(tensor([0.9002, 0.0116, 0.0088, 0.0071, 0.0060], grad_fn=<ToCopyBackward0>), [' disappointed', ' surprised', ' skeptical', ' much', ' dis'])\n",
      "(tensor([0.3952, 0.2544, 0.1556, 0.0390, 0.0196], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' when'])\n",
      "(tensor([0.2764, 0.1627, 0.1558, 0.0262, 0.0199], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1590, 0.0966, 0.0729, 0.0433, 0.0411], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' actors', ' plot'])\n",
      "(tensor([0.5112, 0.1550, 0.0713, 0.0192, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' just', ' had'])\n",
      "(tensor([0.1351, 0.1006, 0.0921, 0.0831, 0.0716], grad_fn=<ToCopyBackward0>), [' so', ' no', ' a', ' potential', ' some'])\n",
      "(tensor([0.5851, 0.1531, 0.0810, 0.0594, 0.0404], grad_fn=<ToCopyBackward0>), [',', '.', ' to', ' and', ' but'])\n",
      "(tensor([0.3767, 0.1684, 0.1072, 0.0603, 0.0282], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' But', ' There'])\n",
      "(tensor([0.3310, 0.0749, 0.0381, 0.0287, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' idea', ' characters', ' main'])\n",
      "(tensor([0.8046, 0.0398, 0.0250, 0.0182, 0.0177], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' and', ' is', ','])\n",
      "(tensor([0.0802, 0.0636, 0.0513, 0.0434, 0.0425], grad_fn=<ToCopyBackward0>), [' good', ' okay', ' mediocre', ' very', ' not'])\n",
      "(tensor([0.5819, 0.0441, 0.0375, 0.0269, 0.0170], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' weak', ' poor', ' well'])\n",
      "(tensor([0.6289, 0.1597, 0.1209, 0.0131, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' but', ' in'])\n",
      "(tensor([0.5678, 0.1041, 0.0323, 0.0248, 0.0193], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' very'])\n",
      "(tensor([0.3261, 0.1557, 0.1011, 0.0289, 0.0234], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' storyline'])\n",
      "(tensor([0.5558, 0.0472, 0.0274, 0.0228, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' had', ' seemed', ' just', ' line'])\n",
      "(tensor([0.1148, 0.0848, 0.0617, 0.0367, 0.0364], grad_fn=<ToCopyBackward0>), [' very', ' not', ' weak', ' predictable', ' just'])\n",
      "(tensor([0.0995, 0.0953, 0.0885, 0.0514, 0.0443], grad_fn=<ToCopyBackward0>), [' interesting', ' very', ' even', ' convincing', ' really'])\n",
      "(tensor([0.4380, 0.2086, 0.1009, 0.0720, 0.0582], grad_fn=<ToCopyBackward0>), ['.', ' at', ' to', ' enough', ' or'])\n",
      "(tensor([0.3200, 0.2243, 0.1111, 0.0752, 0.0320], grad_fn=<ToCopyBackward0>), [' me', ' say', ' begin', ' watch', ' tell'])\n",
      "(tensor([9.9247e-01, 3.4191e-03, 6.4950e-04, 4.7071e-04, 3.9782e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', '.', ' to', ',', ' and'])\n",
      "(tensor([0.9371, 0.0207, 0.0049, 0.0040, 0.0028], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' (', '!'])\n",
      "(tensor([0.2455, 0.2372, 0.1134, 0.0243, 0.0229], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.1148, 0.0741, 0.0731, 0.0453, 0.0401], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' movie', ' plot', ' actors'])\n",
      "(tensor([0.6887, 0.0414, 0.0387, 0.0165, 0.0092], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' made', ' wasn'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty boring. It was not funny or interesting at all. It seemed like it was just a bunch of people running around. And that is not at all scary. It seemed like it was more of a drama than an horror movie.\n",
      "(tensor([0.3840, 0.1715, 0.0896, 0.0771, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2431, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6514, 0.0598, 0.0363, 0.0355, 0.0264], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0660, 0.0547, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1760, 0.1519, 0.1083, 0.0953, 0.0896], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.3315, 0.2171, 0.1737, 0.0373, 0.0158], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' compared'])\n",
      "(tensor([0.1853, 0.1705, 0.1501, 0.0311, 0.0179], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.2595, 0.1409, 0.1160, 0.0920, 0.0480], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' seemed', ' had', ' wasn'])\n",
      "(tensor([0.1292, 0.0917, 0.0496, 0.0485, 0.0460], grad_fn=<ToCopyBackward0>), [' boring', ' not', ' slow', ' just', ' more'])\n",
      "(tensor([0.4357, 0.0837, 0.0780, 0.0436, 0.0369], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' entertaining', ' as'])\n",
      "(tensor([0.3969, 0.1991, 0.1338, 0.0504, 0.0503], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' or', ' in'])\n",
      "(tensor([0.4691, 0.1962, 0.0334, 0.0332, 0.0247], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' funny', ' exciting', ' dramatic'])\n",
      "(tensor([0.2743, 0.2568, 0.0970, 0.0917, 0.0662], grad_fn=<ToCopyBackward0>), ['.', ' at', ' to', ',', ' or'])\n",
      "(tensor([0.9860, 0.0053, 0.0030, 0.0013, 0.0012], grad_fn=<ToCopyBackward0>), [' all', ' any', ' least', ' the', ' ALL'])\n",
      "(tensor([0.7413, 0.1121, 0.0425, 0.0147, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' to'])\n",
      "(tensor([0.2559, 0.1838, 0.1404, 0.0306, 0.0268], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', 'The', ' There'])\n",
      "(tensor([0.4229, 0.1004, 0.0852, 0.0507, 0.0333], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' had', \"'s\", ' wasn'])\n",
      "(tensor([0.3584, 0.2981, 0.1505, 0.0678, 0.0282], grad_fn=<ToCopyBackward0>), [' like', ' to', ' more', ' as', ' that'])\n",
      "(tensor([0.3573, 0.2436, 0.1995, 0.0772, 0.0150], grad_fn=<ToCopyBackward0>), [' it', ' they', ' a', ' the', ' an'])\n",
      "(tensor([0.5866, 0.1036, 0.0560, 0.0369, 0.0287], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' just', ' could'])\n",
      "(tensor([0.3001, 0.1220, 0.0780, 0.0446, 0.0273], grad_fn=<ToCopyBackward0>), [' trying', ' made', ' just', ' written', ' a'])\n",
      "(tensor([0.3404, 0.0778, 0.0708, 0.0408, 0.0311], grad_fn=<ToCopyBackward0>), [' a', ' made', ' trying', ' an', ' showing'])\n",
      "(tensor([0.3831, 0.0217, 0.0144, 0.0133, 0.0120], grad_fn=<ToCopyBackward0>), [' bunch', ' movie', ' remake', ' big', ' commercial'])\n",
      "(tensor([9.9119e-01, 1.0040e-03, 5.0070e-04, 3.7770e-04, 3.7692e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' people', ' guys', ' o', ' more'])\n",
      "(tensor([0.1689, 0.1144, 0.0673, 0.0503, 0.0422], grad_fn=<ToCopyBackward0>), [' guys', ' people', ' kids', ' random', ' scenes'])\n",
      "(tensor([0.0635, 0.0532, 0.0452, 0.0384, 0.0374], grad_fn=<ToCopyBackward0>), [' in', ' running', ' sitting', ' that', ' trying'])\n",
      "(tensor([0.7970, 0.0715, 0.0299, 0.0185, 0.0130], grad_fn=<ToCopyBackward0>), [' around', ' through', ' in', ' from', ' into'])\n",
      "(tensor([0.3123, 0.1409, 0.1027, 0.0525, 0.0401], grad_fn=<ToCopyBackward0>), ['.', ' and', ' in', ',', ' trying'])\n",
      "(tensor([0.1872, 0.1601, 0.1173, 0.0799, 0.0358], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' And'])\n",
      "(tensor([0.1278, 0.1061, 0.1051, 0.0809, 0.0756], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' that', ' when'])\n",
      "(tensor([0.4905, 0.2369, 0.1322, 0.0103, 0.0064], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' would'])\n",
      "(tensor([0.5251, 0.0649, 0.0566, 0.0272, 0.0230], grad_fn=<ToCopyBackward0>), [' not', ' NOT', ' the', ' a', ' what'])\n",
      "(tensor([0.5246, 0.1305, 0.0270, 0.0269, 0.0243], grad_fn=<ToCopyBackward0>), [' a', ' the', ' at', ' an', ' fun'])\n",
      "(tensor([9.9639e-01, 1.2583e-03, 8.5548e-04, 4.7020e-04, 2.1735e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' the', ' least', 'yp'])\n",
      "(tensor([0.3067, 0.1378, 0.1303, 0.0907, 0.0832], grad_fn=<ToCopyBackward0>), [' a', ' the', ' what', ' entertaining', ' scary'])\n",
      "(tensor([0.4224, 0.2162, 0.1566, 0.0315, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ' or', ',', ' to', '!'])\n",
      "(tensor([0.2602, 0.1532, 0.0500, 0.0392, 0.0343], grad_fn=<ToCopyBackward0>), [' I', ' It', ' That', ' The', ' And'])\n",
      "(tensor([0.2578, 0.1991, 0.1541, 0.0796, 0.0621], grad_fn=<ToCopyBackward0>), [' was', ' is', \"'s\", ' just', ' seemed'])\n",
      "(tensor([0.4936, 0.2804, 0.0530, 0.0271, 0.0179], grad_fn=<ToCopyBackward0>), [' like', ' more', ' to', ' as', ' very'])\n",
      "(tensor([0.4227, 0.2191, 0.1145, 0.0510, 0.0234], grad_fn=<ToCopyBackward0>), [' it', ' they', ' a', ' the', ' people'])\n",
      "(tensor([0.7638, 0.0649, 0.0463, 0.0174, 0.0151], grad_fn=<ToCopyBackward0>), [' was', ' would', ' just', ' could', ' had'])\n",
      "(tensor([0.3585, 0.1064, 0.0841, 0.0242, 0.0242], grad_fn=<ToCopyBackward0>), [' just', ' more', ' a', ' all', ' trying'])\n",
      "(tensor([0.7186, 0.0938, 0.0330, 0.0204, 0.0170], grad_fn=<ToCopyBackward0>), [' like', ' of', ' for', ' a', ' boring'])\n",
      "(tensor([0.9000, 0.0540, 0.0248, 0.0057, 0.0034], grad_fn=<ToCopyBackward0>), [' a', ' an', ' the', ' just', ' like'])\n",
      "(tensor([0.1003, 0.0987, 0.0602, 0.0519, 0.0425], grad_fn=<ToCopyBackward0>), [' drama', ' comedy', ' chore', ' \"', ' boring'])\n",
      "(tensor([0.4074, 0.1494, 0.1017, 0.0440, 0.0310], grad_fn=<ToCopyBackward0>), [' than', '.', ' about', '/', ' with'])\n",
      "(tensor([0.5218, 0.2485, 0.0880, 0.0231, 0.0188], grad_fn=<ToCopyBackward0>), [' a', ' anything', ' it', ' horror', ' an'])\n",
      "(tensor([0.3052, 0.0922, 0.0886, 0.0563, 0.0332], grad_fn=<ToCopyBackward0>), [' action', ' actual', ' adventure', ' horror', ' thriller'])\n",
      "(tensor([0.7813, 0.1040, 0.0828, 0.0121, 0.0025], grad_fn=<ToCopyBackward0>), [' movie', '.', ' film', ' flick', ','])\n",
      "(tensor([0.8663, 0.0316, 0.0289, 0.0241, 0.0049], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', ' at'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty funny, but the idea that a movie like this has a 5.2 rating on this site, it's pretty ridiculous that someone actually thinks that that movie is funny. I'm a huge Adam Sandler fan, I even like\n",
      "(tensor([0.3843, 0.1714, 0.0898, 0.0769, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5015, 0.0597, 0.0340, 0.0150, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6234, 0.0401, 0.0381, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2578, 0.0595, 0.0526, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1975, 0.1232, 0.1053, 0.0782, 0.0676], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.3150, 0.1799, 0.1288, 0.0504, 0.0371], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', ' in'])\n",
      "(tensor([0.4151, 0.0505, 0.0410, 0.0382, 0.0347], grad_fn=<ToCopyBackward0>), [' but', ' especially', ' and', ' so', ' the'])\n",
      "(tensor([0.1366, 0.1269, 0.1008, 0.0824, 0.0431], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' not'])\n",
      "(tensor([0.1646, 0.1428, 0.0621, 0.0597, 0.0498], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' plot', ' idea', ' acting'])\n",
      "(tensor([0.5256, 0.2047, 0.0921, 0.0451, 0.0279], grad_fn=<ToCopyBackward0>), [' of', ' that', ' was', ' behind', ' is'])\n",
      "(tensor([0.1464, 0.0969, 0.0802, 0.0522, 0.0428], grad_fn=<ToCopyBackward0>), [' it', ' the', ' a', ' this', ' they'])\n",
      "(tensor([0.4508, 0.0571, 0.0453, 0.0149, 0.0115], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' bunch', ' comedy', ' group'])\n",
      "(tensor([0.1417, 0.1189, 0.0810, 0.0614, 0.0553], grad_fn=<ToCopyBackward0>), [' could', ' with', ' is', ' like', ' has'])\n",
      "(tensor([0.7340, 0.1062, 0.0366, 0.0185, 0.0063], grad_fn=<ToCopyBackward0>), [' this', ' that', ' \"', ' it', ' The'])\n",
      "(tensor([0.3007, 0.1829, 0.1060, 0.0658, 0.0601], grad_fn=<ToCopyBackward0>), [' is', ' could', ' can', ' has', ' was'])\n",
      "(tensor([0.4603, 0.1364, 0.1157, 0.0906, 0.0174], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' been', ' so'])\n",
      "(tensor([0.0497, 0.0481, 0.0463, 0.0422, 0.0334], grad_fn=<ToCopyBackward0>), [' 1', ' 5', ' $', ' 2', ' 6'])\n",
      "(tensor([0.3135, 0.2709, 0.1054, 0.0291, 0.0281], grad_fn=<ToCopyBackward0>), [' star', '.', '-', ',', ' rating'])\n",
      "(tensor([0.4456, 0.2416, 0.0665, 0.0623, 0.0525], grad_fn=<ToCopyBackward0>), ['1', '2', '0', '5', '7'])\n",
      "(tensor([0.5120, 0.0485, 0.0198, 0.0188, 0.0084], grad_fn=<ToCopyBackward0>), [' rating', ' score', 'M', ' IM', ' here'])\n",
      "(tensor([0.2243, 0.1782, 0.1468, 0.1435, 0.0493], grad_fn=<ToCopyBackward0>), ['?', ' on', ' is', ',', ' in'])\n",
      "(tensor([0.4517, 0.2663, 0.0730, 0.0365, 0.0193], grad_fn=<ToCopyBackward0>), [' IM', ' this', ' R', ' the', ' here'])\n",
      "(tensor([0.5675, 0.2024, 0.0109, 0.0106, 0.0089], grad_fn=<ToCopyBackward0>), [' site', ' website', ' movie', ' page', ' board'])\n",
      "(tensor([0.4543, 0.0892, 0.0670, 0.0367, 0.0276], grad_fn=<ToCopyBackward0>), [' is', ',', ' was', '?', ' makes'])\n",
      "(tensor([0.1843, 0.1432, 0.0819, 0.0707, 0.0665], grad_fn=<ToCopyBackward0>), [' I', ' is', ' that', ' and', ' it'])\n",
      "(tensor([0.6957, 0.0373, 0.0300, 0.0276, 0.0238], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' just', ' makes', ' seems'])\n",
      "(tensor([0.1741, 0.1224, 0.0953, 0.0734, 0.0650], grad_fn=<ToCopyBackward0>), [' not', ' a', ' ridiculous', ' kind', ' pretty'])\n",
      "(tensor([0.1411, 0.1007, 0.0684, 0.0583, 0.0528], grad_fn=<ToCopyBackward0>), [' ridiculous', ' crazy', ' hard', ' scary', ' funny'])\n",
      "(tensor([0.4448, 0.0929, 0.0850, 0.0846, 0.0630], grad_fn=<ToCopyBackward0>), ['.', ' to', ',', ' that', ' and'])\n",
      "(tensor([0.2393, 0.1387, 0.0663, 0.0655, 0.0649], grad_fn=<ToCopyBackward0>), [' they', ' a', ' something', ' someone', ' that'])\n",
      "(tensor([0.3489, 0.1352, 0.0799, 0.0430, 0.0251], grad_fn=<ToCopyBackward0>), [' rated', ' actually', ' thought', ' would', ' could'])\n",
      "(tensor([0.3946, 0.0828, 0.0331, 0.0291, 0.0182], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' thinks', ' rated', ' put'])\n",
      "(tensor([0.4318, 0.3424, 0.0977, 0.0572, 0.0148], grad_fn=<ToCopyBackward0>), [' that', ' this', ' it', ' they', ' like'])\n",
      "(tensor([0.1841, 0.1375, 0.1346, 0.1190, 0.0990], grad_fn=<ToCopyBackward0>), [' this', ' it', '.', ' that', ' a'])\n",
      "(tensor([0.2964, 0.1280, 0.1007, 0.0473, 0.0408], grad_fn=<ToCopyBackward0>), [' movie', \"'s\", ' is', ' kind', ' sort'])\n",
      "(tensor([0.6830, 0.0643, 0.0294, 0.0277, 0.0273], grad_fn=<ToCopyBackward0>), [' is', ' was', ' deserves', \"'s\", ' should'])\n",
      "(tensor([0.2469, 0.1413, 0.0630, 0.0598, 0.0462], grad_fn=<ToCopyBackward0>), [' funny', ' worth', ' a', ' good', ' that'])\n",
      "(tensor([0.8131, 0.0340, 0.0323, 0.0260, 0.0213], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '!', '....'])\n",
      "(tensor([0.3101, 0.0815, 0.0609, 0.0321, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1699, 0.0867, 0.0668, 0.0652, 0.0484], grad_fn=<ToCopyBackward0>), [' mean', ' was', ' don', \"'m\", ' can'])\n",
      "(tensor([0.3552, 0.1091, 0.0729, 0.0600, 0.0280], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' really'])\n",
      "(tensor([0.2341, 0.0781, 0.0756, 0.0341, 0.0329], grad_fn=<ToCopyBackward0>), [' big', ' huge', ' movie', ' fan', ' little'])\n",
      "(tensor([0.2766, 0.1158, 0.0351, 0.0297, 0.0151], grad_fn=<ToCopyBackward0>), [' Adam', ' Van', ' fan', ' Michael', ' Conan'])\n",
      "(tensor([0.9638, 0.0060, 0.0036, 0.0036, 0.0021], grad_fn=<ToCopyBackward0>), [' Sand', ' sand', ' \"', ' West', ' S'])\n",
      "(tensor([9.9627e-01, 1.1917e-03, 3.8068e-04, 3.7768e-04, 1.4575e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ler', 'lers', 'lin', 'ra', 'erson'])\n",
      "(tensor([9.9301e-01, 2.2343e-03, 8.0861e-04, 6.4420e-04, 5.9018e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' fan', ' fans', ' movie', ' guy', ' Fan'])\n",
      "(tensor([0.7116, 0.1170, 0.0473, 0.0217, 0.0140], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', ' so'])\n",
      "(tensor([0.4004, 0.3288, 0.1231, 0.0657, 0.0094], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' so', ' he'])\n",
      "(tensor([0.1405, 0.1034, 0.0922, 0.0840, 0.0768], grad_fn=<ToCopyBackward0>), [' even', ' think', ' love', \"'m\", \"'ve\"])\n",
      "(tensor([0.0617, 0.0572, 0.0570, 0.0537, 0.0469], grad_fn=<ToCopyBackward0>), [' like', ' have', ' made', ' thought', ' liked'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would watch this movie, since it's a remake of a great movie I loved growing up. The original was about a boy and his dog that get lost in a forest for a month, and then he meets a talking bear. It was pretty\n",
      "(tensor([0.3844, 0.1717, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2246, 0.1945, 0.1575, 0.0694, 0.0537], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2255, 0.1212, 0.0844, 0.0370, 0.0234], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.7081, 0.0898, 0.0597, 0.0410, 0.0110], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.5523, 0.1864, 0.0686, 0.0169, 0.0090], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' because', ' one', ' with'])\n",
      "(tensor([0.5507, 0.0745, 0.0373, 0.0258, 0.0238], grad_fn=<ToCopyBackward0>), [' just', ' because', ' for', ' as', ','])\n",
      "(tensor([0.1466, 0.1459, 0.0724, 0.0565, 0.0344], grad_fn=<ToCopyBackward0>), [' since', ' but', ' because', ' and', ' after'])\n",
      "(tensor([0.3019, 0.2567, 0.0599, 0.0269, 0.0237], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' its', ' i'])\n",
      "(tensor([0.2344, 0.1581, 0.1156, 0.1103, 0.0404], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' stars', ' seemed'])\n",
      "(tensor([0.1639, 0.1400, 0.0527, 0.0526, 0.0518], grad_fn=<ToCopyBackward0>), [' a', ' been', ' about', ' the', ' rated'])\n",
      "(tensor([0.2077, 0.2073, 0.0977, 0.0463, 0.0323], grad_fn=<ToCopyBackward0>), [' remake', ' good', ' classic', ' really', ' great'])\n",
      "(tensor([0.8299, 0.0612, 0.0216, 0.0176, 0.0140], grad_fn=<ToCopyBackward0>), [' of', '.', ',', ' (', ' and'])\n",
      "(tensor([0.4135, 0.1839, 0.0651, 0.0529, 0.0180], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' one', \" '\"])\n",
      "(tensor([0.3892, 0.1169, 0.0720, 0.0614, 0.0584], grad_fn=<ToCopyBackward0>), [' classic', ' great', ' really', ' good', ' movie'])\n",
      "(tensor([0.4204, 0.0513, 0.0413, 0.0312, 0.0196], grad_fn=<ToCopyBackward0>), [' movie', ' film', ',', ' classic', ' story'])\n",
      "(tensor([0.3703, 0.1944, 0.1155, 0.0582, 0.0311], grad_fn=<ToCopyBackward0>), ['.', ' from', ',', ' I', ' that'])\n",
      "(tensor([0.1038, 0.0993, 0.0875, 0.0798, 0.0691], grad_fn=<ToCopyBackward0>), [' loved', \"'ve\", ' saw', ' watched', ' used'])\n",
      "(tensor([0.2345, 0.1370, 0.0831, 0.0780, 0.0733], grad_fn=<ToCopyBackward0>), [' as', ' years', '.', ' when', ' growing'])\n",
      "(tensor([9.9977e-01, 4.7357e-05, 2.3040e-05, 1.7642e-05, 1.6753e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' up', '-', ' Up', ' out', 'up'])\n",
      "(tensor([0.6562, 0.1132, 0.0428, 0.0283, 0.0150], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ' in', ' and'])\n",
      "(tensor([0.2217, 0.1085, 0.0913, 0.0752, 0.0371], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' Unfortunately'])\n",
      "(tensor([0.1932, 0.0985, 0.0928, 0.0541, 0.0434], grad_fn=<ToCopyBackward0>), [' original', ' remake', ' movie', ' first', ' story'])\n",
      "(tensor([0.2746, 0.1084, 0.0984, 0.0792, 0.0375], grad_fn=<ToCopyBackward0>), [' \"', ' movie', ' was', \" '\", ' film'])\n",
      "(tensor([0.1450, 0.1148, 0.0499, 0.0435, 0.0356], grad_fn=<ToCopyBackward0>), [' a', ' great', ' about', ' very', ' good'])\n",
      "(tensor([0.5981, 0.0708, 0.0534, 0.0190, 0.0164], grad_fn=<ToCopyBackward0>), [' a', ' the', ' an', ' two', ' this'])\n",
      "(tensor([0.0786, 0.0749, 0.0680, 0.0631, 0.0273], grad_fn=<ToCopyBackward0>), [' guy', ' little', ' group', ' man', ' boy'])\n",
      "(tensor([0.3954, 0.0758, 0.0529, 0.0509, 0.0491], grad_fn=<ToCopyBackward0>), [' who', ' (', ' and', ',', ' with'])\n",
      "(tensor([0.8219, 0.1207, 0.0298, 0.0075, 0.0032], grad_fn=<ToCopyBackward0>), [' his', ' a', ' girl', ' the', ' an'])\n",
      "(tensor([0.2597, 0.0668, 0.0574, 0.0383, 0.0359], grad_fn=<ToCopyBackward0>), [' dog', ' robot', ' father', ' dad', ' mother'])\n",
      "(tensor([0.2479, 0.2459, 0.1736, 0.1694, 0.0256], grad_fn=<ToCopyBackward0>), [',', ' who', '.', ' that', ' and'])\n",
      "(tensor([0.0903, 0.0617, 0.0598, 0.0430, 0.0409], grad_fn=<ToCopyBackward0>), [' move', ' get', ' go', ' fight', ' have'])\n",
      "(tensor([0.1061, 0.0829, 0.0483, 0.0412, 0.0378], grad_fn=<ToCopyBackward0>), [' caught', ' stranded', ' lost', ' mixed', ' into'])\n",
      "(tensor([0.7444, 0.0920, 0.0287, 0.0163, 0.0158], grad_fn=<ToCopyBackward0>), [' in', ' on', ' and', '.', ','])\n",
      "(tensor([0.6869, 0.1640, 0.0200, 0.0148, 0.0120], grad_fn=<ToCopyBackward0>), [' the', ' a', ' some', ' an', ' space'])\n",
      "(tensor([0.1271, 0.0554, 0.0356, 0.0333, 0.0272], grad_fn=<ToCopyBackward0>), [' cave', ' forest', ' maze', ' tunnel', ' small'])\n",
      "(tensor([0.2773, 0.2519, 0.1015, 0.0282, 0.0253], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' for', ' where'])\n",
      "(tensor([0.2590, 0.0976, 0.0555, 0.0510, 0.0411], grad_fn=<ToCopyBackward0>), [' a', ' days', ' over', ' weeks', ' years'])\n",
      "(tensor([0.3336, 0.2643, 0.1020, 0.0748, 0.0619], grad_fn=<ToCopyBackward0>), [' week', ' while', ' day', ' month', ' few'])\n",
      "(tensor([0.3644, 0.3021, 0.2009, 0.0179, 0.0090], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' while', ' with'])\n",
      "(tensor([0.4867, 0.0947, 0.0402, 0.0248, 0.0212], grad_fn=<ToCopyBackward0>), [' and', ' then', ' but', ' only', ' surviving'])\n",
      "(tensor([0.3195, 0.0424, 0.0387, 0.0327, 0.0325], grad_fn=<ToCopyBackward0>), [' then', ' he', ' the', ' when', ' while'])\n",
      "(tensor([0.1496, 0.1111, 0.0892, 0.0515, 0.0486], grad_fn=<ToCopyBackward0>), [' he', ' find', ' they', ' the', ' get'])\n",
      "(tensor([0.1842, 0.1127, 0.0699, 0.0450, 0.0418], grad_fn=<ToCopyBackward0>), [' meets', ' finds', ' gets', ' and', ' comes'])\n",
      "(tensor([0.6482, 0.0567, 0.0475, 0.0437, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' the', ' his', ' this', ' an'])\n",
      "(tensor([0.1513, 0.0712, 0.0597, 0.0412, 0.0285], grad_fn=<ToCopyBackward0>), [' girl', ' talking', ' little', ' friendly', ' dog'])\n",
      "(tensor([0.2333, 0.1770, 0.1231, 0.0683, 0.0258], grad_fn=<ToCopyBackward0>), [' dog', ' bear', ' dinosaur', ' wolf', ' tree'])\n",
      "(tensor([0.5689, 0.1266, 0.0632, 0.0546, 0.0265], grad_fn=<ToCopyBackward0>), ['.', ' that', ' who', ',', ' named'])\n",
      "(tensor([0.2413, 0.2228, 0.0729, 0.0484, 0.0341], grad_fn=<ToCopyBackward0>), [' The', ' I', ' This', ' It', ' That'])\n",
      "(tensor([0.4220, 0.3927, 0.0729, 0.0122, 0.0105], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' had', ' has'])\n",
      "(tensor([0.0903, 0.0844, 0.0727, 0.0460, 0.0440], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' pretty', ' very'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty awful. The story was pretty awful. The acting wasn't even good. The script was even worse. I can see why the studio went with this movie. I mean, it was pretty much a perfect movie to make. It\n",
      "(tensor([0.3824, 0.1731, 0.0906, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4366, 0.2456, 0.1960, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6539, 0.0593, 0.0361, 0.0355, 0.0261], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1378, 0.0699, 0.0666, 0.0547, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1750, 0.1515, 0.1089, 0.0938, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.5987, 0.0756, 0.0562, 0.0512, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' when'])\n",
      "(tensor([0.2088, 0.1740, 0.1364, 0.0208, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.2133, 0.0823, 0.0630, 0.0625, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' story', ' movie'])\n",
      "(tensor([0.5354, 0.1347, 0.0406, 0.0251, 0.0215], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' had', ' sucked'])\n",
      "(tensor([0.1051, 0.0608, 0.0553, 0.0486, 0.0379], grad_fn=<ToCopyBackward0>), [' pretty', ' weak', ' stupid', ' so', ' just'])\n",
      "(tensor([0.1295, 0.0569, 0.0554, 0.0444, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' awful', ' boring', ' ridiculous'])\n",
      "(tensor([0.4885, 0.1568, 0.1263, 0.1101, 0.0574], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' and', ' as'])\n",
      "(tensor([0.5614, 0.0912, 0.0673, 0.0339, 0.0220], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' But'])\n",
      "(tensor([0.7266, 0.0355, 0.0211, 0.0164, 0.0143], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' movie', ' characters', ' special'])\n",
      "(tensor([0.8566, 0.0404, 0.0178, 0.0174, 0.0126], grad_fn=<ToCopyBackward0>), [' was', ' wasn', '...', ' pretty', ','])\n",
      "(tensor([9.9614e-01, 1.5514e-03, 3.6072e-04, 3.0056e-04, 1.7085e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.1602, 0.1009, 0.0821, 0.0819, 0.0674], grad_fn=<ToCopyBackward0>), [' even', ' very', ' that', ' good', ' too'])\n",
      "(tensor([0.2585, 0.1721, 0.0708, 0.0497, 0.0419], grad_fn=<ToCopyBackward0>), [' good', ' that', ' bad', ' close', ' all'])\n",
      "(tensor([0.3737, 0.3524, 0.0958, 0.0176, 0.0174], grad_fn=<ToCopyBackward0>), [' enough', '.', ',', ' and', '...'])\n",
      "(tensor([0.3196, 0.1405, 0.0991, 0.0416, 0.0313], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.1581, 0.0880, 0.0703, 0.0688, 0.0547], grad_fn=<ToCopyBackward0>), [' plot', ' special', ' only', ' script', ' cinem'])\n",
      "(tensor([0.6514, 0.1017, 0.0972, 0.0179, 0.0179], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' sucked', ' didn', ','])\n",
      "(tensor([0.2535, 0.0849, 0.0662, 0.0588, 0.0479], grad_fn=<ToCopyBackward0>), [' pretty', ' not', ' just', ' even', ' bad'])\n",
      "(tensor([0.8601, 0.0549, 0.0319, 0.0082, 0.0029], grad_fn=<ToCopyBackward0>), [' worse', ' more', ' less', ' worst', ' u'])\n",
      "(tensor([0.9005, 0.0347, 0.0273, 0.0107, 0.0064], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', '!', ' and'])\n",
      "(tensor([0.2713, 0.1444, 0.0650, 0.0480, 0.0391], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' But'])\n",
      "(tensor([0.0979, 0.0944, 0.0590, 0.0519, 0.0514], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' can', ' really', ' don'])\n",
      "(tensor([0.5209, 0.1428, 0.0614, 0.0356, 0.0271], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' see', ' tell', ' only'])\n",
      "(tensor([0.3475, 0.1210, 0.0903, 0.0695, 0.0678], grad_fn=<ToCopyBackward0>), [' why', ' how', ' the', ' where', ' that'])\n",
      "(tensor([0.1733, 0.1479, 0.1455, 0.1446, 0.1196], grad_fn=<ToCopyBackward0>), [' people', ' they', ' it', ' the', ' this'])\n",
      "(tensor([0.2386, 0.1340, 0.0784, 0.0421, 0.0412], grad_fn=<ToCopyBackward0>), [' movie', ' people', ' studio', ' guy', ' director'])\n",
      "(tensor([0.2000, 0.0880, 0.0835, 0.0499, 0.0452], grad_fn=<ToCopyBackward0>), [' made', ' gave', ' was', ' put', ' went'])\n",
      "(tensor([0.1136, 0.1033, 0.0995, 0.0834, 0.0822], grad_fn=<ToCopyBackward0>), [' for', ' down', ' with', ' to', ' back'])\n",
      "(tensor([0.5110, 0.2728, 0.0527, 0.0182, 0.0179], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' such'])\n",
      "(tensor([0.2011, 0.1998, 0.1195, 0.0814, 0.0340], grad_fn=<ToCopyBackward0>), [' one', ' script', ' movie', '.', ','])\n",
      "(tensor([0.4814, 0.1916, 0.0836, 0.0386, 0.0279], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' to', ' and'])\n",
      "(tensor([0.2235, 0.2162, 0.1130, 0.0466, 0.0424], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' They'])\n",
      "(tensor([0.2902, 0.1226, 0.0640, 0.0513, 0.0449], grad_fn=<ToCopyBackward0>), [' can', ' mean', ' just', ' was', ' really'])\n",
      "(tensor([0.4913, 0.1069, 0.0974, 0.0257, 0.0189], grad_fn=<ToCopyBackward0>), [',', ' the', ' it', ' this', ' I'])\n",
      "(tensor([0.1875, 0.1739, 0.0743, 0.0374, 0.0308], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' they', ' what'])\n",
      "(tensor([0.3886, 0.2018, 0.0731, 0.0341, 0.0328], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' has', ' is'])\n",
      "(tensor([0.1526, 0.0736, 0.0532, 0.0507, 0.0387], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' directed', ' made', ' about'])\n",
      "(tensor([0.1660, 0.0578, 0.0480, 0.0444, 0.0361], grad_fn=<ToCopyBackward0>), [' much', ' funny', ' bad', ' cool', '.'])\n",
      "(tensor([0.3708, 0.1413, 0.0603, 0.0599, 0.0365], grad_fn=<ToCopyBackward0>), [' a', ' the', ' all', ' just', ' like'])\n",
      "(tensor([0.0877, 0.0375, 0.0350, 0.0231, 0.0222], grad_fn=<ToCopyBackward0>), [' complete', ' big', ' perfect', ' remake', ' re'])\n",
      "(tensor([0.3987, 0.0996, 0.0811, 0.0215, 0.0182], grad_fn=<ToCopyBackward0>), [' movie', ' script', ' story', ' storm', ' choice'])\n",
      "(tensor([0.3724, 0.0815, 0.0713, 0.0707, 0.0481], grad_fn=<ToCopyBackward0>), ['.', ' for', ' to', ',', ' in'])\n",
      "(tensor([0.1955, 0.1866, 0.0367, 0.0260, 0.0258], grad_fn=<ToCopyBackward0>), [' be', ' make', ' promote', ' go', ' get'])\n",
      "(tensor([0.3973, 0.1622, 0.0615, 0.0328, 0.0280], grad_fn=<ToCopyBackward0>), [' a', '.', ',', ' it', ' for'])\n",
      "(tensor([0.1157, 0.1093, 0.1036, 0.0813, 0.0234], grad_fn=<ToCopyBackward0>), [' It', ' But', ' The', ' I', ' You'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd never see another movie as bad as this one. The story is a joke, the acting is awful, the editing is so sloppy, and there is no plot. The only reason I watched this movie was because the box looked cool. I\n",
      "(tensor([0.3843, 0.1712, 0.0897, 0.0770, 0.0475], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2245, 0.1943, 0.1574, 0.0694, 0.0540], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2082, 0.1041, 0.0617, 0.0395, 0.0300], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.5491, 0.0457, 0.0369, 0.0366, 0.0281], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' find', ' seen'])\n",
      "(tensor([0.6783, 0.0966, 0.0656, 0.0337, 0.0211], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' another', ' a'])\n",
      "(tensor([0.6361, 0.0763, 0.0225, 0.0186, 0.0097], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' horror', ' version', ' Freddy'])\n",
      "(tensor([0.4054, 0.1573, 0.0510, 0.0464, 0.0396], grad_fn=<ToCopyBackward0>), [' as', ' with', ' like', ' where', ' in'])\n",
      "(tensor([0.8531, 0.0333, 0.0227, 0.0105, 0.0082], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' horrible', ' boring'])\n",
      "(tensor([0.9746, 0.0075, 0.0042, 0.0030, 0.0025], grad_fn=<ToCopyBackward0>), [' as', ' in', ' or', ',', ' and'])\n",
      "(tensor([0.8915, 0.0255, 0.0182, 0.0099, 0.0098], grad_fn=<ToCopyBackward0>), [' this', ' that', ' \"', ' it', ' the'])\n",
      "(tensor([0.5515, 0.2454, 0.0675, 0.0399, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ' one', ' again', ',', '...'])\n",
      "(tensor([0.9086, 0.0264, 0.0115, 0.0078, 0.0056], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' in', '..'])\n",
      "(tensor([0.1876, 0.1528, 0.0914, 0.0539, 0.0206], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' Not'])\n",
      "(tensor([0.1375, 0.0990, 0.0886, 0.0749, 0.0440], grad_fn=<ToCopyBackward0>), [' only', ' plot', ' story', ' acting', ' script'])\n",
      "(tensor([0.3620, 0.2852, 0.0286, 0.0282, 0.0199], grad_fn=<ToCopyBackward0>), [' is', ' was', ' line', ' has', ' had'])\n",
      "(tensor([0.1155, 0.0484, 0.0440, 0.0377, 0.0325], grad_fn=<ToCopyBackward0>), [' so', ' about', ' predictable', ' a', ' ridiculous'])\n",
      "(tensor([0.1361, 0.0531, 0.0509, 0.0485, 0.0456], grad_fn=<ToCopyBackward0>), [' joke', ' big', ' cliché', ' mess', ' bad'])\n",
      "(tensor([0.2999, 0.2071, 0.1850, 0.0361, 0.0304], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' from', ' with'])\n",
      "(tensor([0.7539, 0.0904, 0.0154, 0.0125, 0.0072], grad_fn=<ToCopyBackward0>), [' the', ' and', ' acting', ' with', ' it'])\n",
      "(tensor([0.6216, 0.0497, 0.0440, 0.0283, 0.0177], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' actors', ' dialogue', ' casting'])\n",
      "(tensor([0.6048, 0.0302, 0.0249, 0.0248, 0.0236], grad_fn=<ToCopyBackward0>), [' is', ' awful', ' (', ',', ' a'])\n",
      "(tensor([0.1104, 0.0920, 0.0904, 0.0461, 0.0317], grad_fn=<ToCopyBackward0>), [' laughable', ' terrible', ' awful', ' bad', ' a'])\n",
      "(tensor([0.6057, 0.2243, 0.0646, 0.0538, 0.0077], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.4692, 0.4362, 0.0161, 0.0134, 0.0115], grad_fn=<ToCopyBackward0>), [' the', ' and', ' there', ' it', ' even'])\n",
      "(tensor([0.1152, 0.1008, 0.0862, 0.0386, 0.0323], grad_fn=<ToCopyBackward0>), [' special', ' script', ' plot', ' effects', ' editing'])\n",
      "(tensor([0.7559, 0.0418, 0.0265, 0.0109, 0.0091], grad_fn=<ToCopyBackward0>), [' is', ' and', ',', ' was', ' ('])\n",
      "(tensor([0.0590, 0.0569, 0.0539, 0.0515, 0.0436], grad_fn=<ToCopyBackward0>), [' even', ' so', ' awful', ' terrible', ' ridiculous'])\n",
      "(tensor([0.1691, 0.0713, 0.0467, 0.0456, 0.0433], grad_fn=<ToCopyBackward0>), [' bad', ' sloppy', ' awful', ' cho', ' poor'])\n",
      "(tensor([0.2923, 0.1563, 0.1498, 0.1237, 0.0973], grad_fn=<ToCopyBackward0>), [' that', ' it', ' you', ',', ' and'])\n",
      "(tensor([0.4479, 0.3693, 0.0326, 0.0194, 0.0155], grad_fn=<ToCopyBackward0>), [' the', ' and', ' it', ' that', ' even'])\n",
      "(tensor([0.7634, 0.0364, 0.0289, 0.0282, 0.0134], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' I', ' even'])\n",
      "(tensor([0.4395, 0.2378, 0.2086, 0.0248, 0.0219], grad_fn=<ToCopyBackward0>), [\"'s\", ' are', ' is', ' was', ' isn'])\n",
      "(tensor([0.4079, 0.1387, 0.0774, 0.0652, 0.0348], grad_fn=<ToCopyBackward0>), [' no', ' absolutely', ' nothing', ' so', ' just'])\n",
      "(tensor([0.2542, 0.0785, 0.0687, 0.0406, 0.0370], grad_fn=<ToCopyBackward0>), [' plot', ' point', ' real', ' logic', ' reason'])\n",
      "(tensor([0.6858, 0.0766, 0.0403, 0.0361, 0.0319], grad_fn=<ToCopyBackward0>), ['.', ' to', ' -', ' at', ','])\n",
      "(tensor([0.2329, 0.0983, 0.0877, 0.0351, 0.0264], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.3554, 0.0713, 0.0277, 0.0274, 0.0198], grad_fn=<ToCopyBackward0>), [' only', ' movie', ' plot', ' whole', ' acting'])\n",
      "(tensor([0.2718, 0.0931, 0.0753, 0.0421, 0.0393], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' mystery', ' redeem'])\n",
      "(tensor([0.3458, 0.2657, 0.1220, 0.0432, 0.0362], grad_fn=<ToCopyBackward0>), [' I', ' to', ' this', ' for', ' anyone'])\n",
      "(tensor([0.1693, 0.1071, 0.0838, 0.0740, 0.0610], grad_fn=<ToCopyBackward0>), [' gave', ' watched', ' even', \"'m\", ' didn'])\n",
      "(tensor([0.4272, 0.4235, 0.1275, 0.0050, 0.0020], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' that', ' all'])\n",
      "(tensor([0.6115, 0.0938, 0.0830, 0.0546, 0.0261], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.5394, 0.3125, 0.0272, 0.0225, 0.0078], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' in', ' for'])\n",
      "(tensor([0.6634, 0.1491, 0.0364, 0.0354, 0.0263], grad_fn=<ToCopyBackward0>), [' because', ' to', ' so', ' for', ' that'])\n",
      "(tensor([0.4564, 0.1071, 0.0856, 0.0608, 0.0570], grad_fn=<ToCopyBackward0>), [' I', ' the', ' of', ' it', ' my'])\n",
      "(tensor([0.2140, 0.1159, 0.0559, 0.0249, 0.0215], grad_fn=<ToCopyBackward0>), [' cover', ' box', ' trailer', ' movie', ' cast'])\n",
      "(tensor([0.0690, 0.0607, 0.0542, 0.0476, 0.0441], grad_fn=<ToCopyBackward0>), [' looked', ' of', ' art', ' said', ' set'])\n",
      "(tensor([0.1768, 0.1214, 0.1168, 0.1166, 0.0773], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' cool', ' so', ' kinda'])\n",
      "(tensor([0.5574, 0.2086, 0.0843, 0.0304, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' on', ' so'])\n",
      "(tensor([0.2863, 0.1474, 0.0607, 0.0533, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' If', ' This'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was one of the worst movies I have ever seen. The plot was stupid, I don't think it would have made a good movie. The acting was terrible. The plot was stupid, it was so stupid I could not even describe it.\n",
      "(tensor([0.3849, 0.1711, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7131, 0.1167, 0.0397, 0.0101, 0.0084], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1845, 0.1451, 0.0510, 0.0454, 0.0439], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.9167, 0.0124, 0.0066, 0.0065, 0.0040], grad_fn=<ToCopyBackward0>), [' of', ' more', ' big', ' too', ' or'])\n",
      "(tensor([0.7612, 0.1932, 0.0167, 0.0051, 0.0030], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' his', ' them'])\n",
      "(tensor([0.5680, 0.1212, 0.1092, 0.0266, 0.0260], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' most', ' dumb', ' funn'])\n",
      "(tensor([0.7935, 0.1556, 0.0051, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ',', ' film'])\n",
      "(tensor([0.7384, 0.0886, 0.0755, 0.0330, 0.0126], grad_fn=<ToCopyBackward0>), [' I', ' i', ' ever', ' of', ' that'])\n",
      "(tensor([0.3616, 0.3547, 0.1208, 0.0773, 0.0568], grad_fn=<ToCopyBackward0>), [\"'ve\", ' have', ' had', ' ever', \"'d\"])\n",
      "(tensor([9.1591e-01, 7.6439e-02, 1.6304e-03, 1.6197e-03, 4.4365e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' had', ' watched', ' EVER'])\n",
      "(tensor([0.9145, 0.0250, 0.0141, 0.0074, 0.0044], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' been', ' wasted'])\n",
      "(tensor([0.5943, 0.1393, 0.0928, 0.0542, 0.0171], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', '!', '...'])\n",
      "(tensor([0.2659, 0.1401, 0.1377, 0.0241, 0.0229], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', 'I'])\n",
      "(tensor([0.1311, 0.1121, 0.1081, 0.0994, 0.0410], grad_fn=<ToCopyBackward0>), [' only', ' story', ' acting', ' plot', ' movie'])\n",
      "(tensor([0.7369, 0.1022, 0.0117, 0.0113, 0.0095], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' sounded', ','])\n",
      "(tensor([0.1135, 0.1039, 0.1011, 0.0383, 0.0368], grad_fn=<ToCopyBackward0>), [' so', ' stupid', ' ridiculous', ' terrible', ' just'])\n",
      "(tensor([0.5186, 0.2596, 0.1532, 0.0107, 0.0067], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' as', ';'])\n",
      "(tensor([0.5512, 0.1444, 0.0619, 0.0207, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' and', ' acting', ' I', ' it'])\n",
      "(tensor([0.1723, 0.0665, 0.0653, 0.0610, 0.0417], grad_fn=<ToCopyBackward0>), [' don', ' didn', ' think', ' couldn', ' mean'])\n",
      "(tensor([9.9080e-01, 3.7906e-03, 1.4428e-03, 9.5714e-04, 4.0188e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.2730, 0.1735, 0.1366, 0.0855, 0.0583], grad_fn=<ToCopyBackward0>), [' know', ' think', ' even', ' understand', ' get'])\n",
      "(tensor([0.2615, 0.1778, 0.0892, 0.0773, 0.0728], grad_fn=<ToCopyBackward0>), [' it', ' the', ' even', ' I', ' any'])\n",
      "(tensor([0.3305, 0.1266, 0.1033, 0.0874, 0.0417], grad_fn=<ToCopyBackward0>), [' was', ' had', ' could', ' would', ' even'])\n",
      "(tensor([0.4331, 0.1264, 0.0668, 0.0662, 0.0599], grad_fn=<ToCopyBackward0>), [' have', ' even', ' be', ' work', ' appeal'])\n",
      "(tensor([0.4682, 0.2311, 0.1043, 0.0323, 0.0155], grad_fn=<ToCopyBackward0>), [' worked', ' made', ' been', ' even', ' lasted'])\n",
      "(tensor([0.5128, 0.1491, 0.1225, 0.0450, 0.0285], grad_fn=<ToCopyBackward0>), [' a', ' any', ' it', ' much', ' an'])\n",
      "(tensor([0.7464, 0.0306, 0.0253, 0.0183, 0.0158], grad_fn=<ToCopyBackward0>), [' good', ' movie', ' great', ' decent', ' very'])\n",
      "(tensor([0.6766, 0.1742, 0.0327, 0.0197, 0.0091], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' story', ' comedy', ' family'])\n",
      "(tensor([0.3504, 0.3418, 0.0339, 0.0314, 0.0280], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' in', ' even'])\n",
      "(tensor([0.2642, 0.1484, 0.1005, 0.0318, 0.0310], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' And'])\n",
      "(tensor([0.5788, 0.0451, 0.0423, 0.0318, 0.0229], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' only', ' movie', ' characters'])\n",
      "(tensor([0.8406, 0.0415, 0.0131, 0.0126, 0.0104], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ',', ' sucked', ' and'])\n",
      "(tensor([0.1672, 0.1615, 0.0896, 0.0639, 0.0635], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' horrible', ' awful', ' not'])\n",
      "(tensor([0.3613, 0.2451, 0.2401, 0.0203, 0.0179], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' too'])\n",
      "(tensor([0.3143, 0.1992, 0.0910, 0.0579, 0.0198], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.1626, 0.1050, 0.0711, 0.0509, 0.0401], grad_fn=<ToCopyBackward0>), [' plot', ' only', ' script', ' story', ' movie'])\n",
      "(tensor([0.7811, 0.0215, 0.0177, 0.0154, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ' made', ' is', ' had', ','])\n",
      "(tensor([0.4319, 0.0649, 0.0472, 0.0467, 0.0409], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' just', ' not', ' ridiculous'])\n",
      "(tensor([0.3201, 0.3118, 0.2581, 0.0091, 0.0091], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' too'])\n",
      "(tensor([0.2780, 0.2290, 0.1065, 0.0577, 0.0365], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' it'])\n",
      "(tensor([0.2577, 0.1259, 0.0961, 0.0723, 0.0655], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' would', ' wasn', \"'s\"])\n",
      "(tensor([0.1743, 0.1170, 0.0974, 0.0580, 0.0537], grad_fn=<ToCopyBackward0>), [' not', ' just', ' stupid', ' a', ' so'])\n",
      "(tensor([0.3246, 0.2586, 0.0517, 0.0479, 0.0241], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' dumb', ' bad', ' ridiculous'])\n",
      "(tensor([0.2156, 0.1696, 0.1458, 0.1414, 0.1016], grad_fn=<ToCopyBackward0>), [' that', ' I', '.', ' it', ','])\n",
      "(tensor([0.1740, 0.1127, 0.1000, 0.0716, 0.0610], grad_fn=<ToCopyBackward0>), [' don', ' could', ' can', ' couldn', ' was'])\n",
      "(tensor([0.6924, 0.0409, 0.0394, 0.0366, 0.0284], grad_fn=<ToCopyBackward0>), [' not', ' see', ' only', ' have', ' barely'])\n",
      "(tensor([0.3175, 0.1896, 0.1216, 0.0614, 0.0274], grad_fn=<ToCopyBackward0>), [' even', ' believe', ' understand', ' follow', ' tell'])\n",
      "(tensor([0.2176, 0.1072, 0.0731, 0.0687, 0.0613], grad_fn=<ToCopyBackward0>), [' tell', ' describe', ' believe', ' explain', ' remember'])\n",
      "(tensor([0.8442, 0.0656, 0.0291, 0.0171, 0.0090], grad_fn=<ToCopyBackward0>), [' it', ' the', ' to', ' how', '.'])\n",
      "(tensor([0.6293, 0.1324, 0.0663, 0.0500, 0.0137], grad_fn=<ToCopyBackward0>), ['.', ' to', ' in', ',', ' properly'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very boring. The acting was terrible, and the story seemed pretty trite and obvious in its point. The acting was so bad that it seemed like the actors were reading from cue-cards, and they were not very good at it\n",
      "(tensor([0.3838, 0.1718, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4374, 0.2440, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6524, 0.0597, 0.0361, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1370, 0.0702, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5074, 0.0403, 0.0344, 0.0339, 0.0323], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ' funny', ','])\n",
      "(tensor([0.4164, 0.2417, 0.1165, 0.0408, 0.0139], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '!'])\n",
      "(tensor([0.1943, 0.1660, 0.1552, 0.0295, 0.0184], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' This'])\n",
      "(tensor([0.2563, 0.0652, 0.0595, 0.0574, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' movie', ' actors'])\n",
      "(tensor([0.8309, 0.0453, 0.0400, 0.0117, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ','])\n",
      "(tensor([0.0970, 0.0756, 0.0752, 0.0641, 0.0442], grad_fn=<ToCopyBackward0>), [' terrible', ' not', ' bad', ' very', ' awful'])\n",
      "(tensor([0.3136, 0.3107, 0.2519, 0.0114, 0.0105], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' as', '!'])\n",
      "(tensor([0.4345, 0.1754, 0.0434, 0.0330, 0.0246], grad_fn=<ToCopyBackward0>), [' the', ' and', ' but', ' especially', ' there'])\n",
      "(tensor([0.5765, 0.1257, 0.0416, 0.0384, 0.0235], grad_fn=<ToCopyBackward0>), [' the', ' I', ' there', ' it', ' even'])\n",
      "(tensor([0.3605, 0.2162, 0.0994, 0.0733, 0.0425], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' movie'])\n",
      "(tensor([0.7034, 0.0296, 0.0281, 0.0243, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ' sucked', ' line', ' seemed', ' just'])\n",
      "(tensor([0.5772, 0.0660, 0.0515, 0.0357, 0.0306], grad_fn=<ToCopyBackward0>), [' to', ' pretty', ' like', ' very', ' really'])\n",
      "(tensor([0.0713, 0.0495, 0.0439, 0.0398, 0.0391], grad_fn=<ToCopyBackward0>), [' weak', ' much', ' boring', ' stupid', ' tr'])\n",
      "(tensor([9.9883e-01, 1.2815e-04, 9.7740e-05, 7.7654e-05, 6.1930e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['ite', 'udging', 'ully', 'ites', 'ashed'])\n",
      "(tensor([0.4151, 0.3606, 0.0781, 0.0522, 0.0132], grad_fn=<ToCopyBackward0>), ['.', ' and', ' to', ',', ' at'])\n",
      "(tensor([0.0786, 0.0686, 0.0538, 0.0457, 0.0394], grad_fn=<ToCopyBackward0>), [' predictable', ' tr', ' obvious', ' un', ' old'])\n",
      "(tensor([0.6219, 0.0698, 0.0660, 0.0450, 0.0245], grad_fn=<ToCopyBackward0>), ['.', ' to', ' at', ',', ' in'])\n",
      "(tensor([0.1590, 0.1267, 0.0979, 0.0614, 0.0578], grad_fn=<ToCopyBackward0>), [' places', ' the', ' its', ' hindsight', ' retrospect'])\n",
      "(tensor([0.1148, 0.0729, 0.0305, 0.0197, 0.0178], grad_fn=<ToCopyBackward0>), [' message', ' point', ' plot', ' storyline', ' intentions'])\n",
      "(tensor([0.8913, 0.0275, 0.0175, 0.0139, 0.0129], grad_fn=<ToCopyBackward0>), ['.', ' of', '-', ' and', ','])\n",
      "(tensor([0.1788, 0.1708, 0.0764, 0.0296, 0.0276], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' This'])\n",
      "(tensor([0.1561, 0.1005, 0.0467, 0.0441, 0.0353], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' plot', ' acting', ' whole'])\n",
      "(tensor([0.6165, 0.0635, 0.0511, 0.0411, 0.0335], grad_fn=<ToCopyBackward0>), [' was', ' in', ' and', ' is', ','])\n",
      "(tensor([0.1305, 0.1259, 0.0716, 0.0476, 0.0378], grad_fn=<ToCopyBackward0>), [' so', ' bad', ' terrible', ' TER', ' very'])\n",
      "(tensor([0.7211, 0.0216, 0.0205, 0.0187, 0.0172], grad_fn=<ToCopyBackward0>), [' bad', ' wooden', '-', ' poor', ' terrible'])\n",
      "(tensor([0.5330, 0.1818, 0.1263, 0.0353, 0.0300], grad_fn=<ToCopyBackward0>), [' that', ',', ' I', ' it', ' in'])\n",
      "(tensor([0.4330, 0.1378, 0.0728, 0.0702, 0.0521], grad_fn=<ToCopyBackward0>), [' I', ' it', ' even', ' the', ' i'])\n",
      "(tensor([0.3313, 0.1133, 0.0908, 0.0640, 0.0290], grad_fn=<ToCopyBackward0>), [' was', ' made', ' seemed', \"'s\", ' wasn'])\n",
      "(tensor([0.5136, 0.1078, 0.0715, 0.0656, 0.0566], grad_fn=<ToCopyBackward0>), [' like', ' as', ' to', ' more', ' almost'])\n",
      "(tensor([0.2755, 0.2750, 0.1651, 0.0550, 0.0343], grad_fn=<ToCopyBackward0>), [' the', ' they', ' it', ' I', ' there'])\n",
      "(tensor([0.1800, 0.1780, 0.0771, 0.0599, 0.0491], grad_fn=<ToCopyBackward0>), [' director', ' actors', ' producers', ' writers', ' movie'])\n",
      "(tensor([0.6073, 0.0978, 0.0492, 0.0224, 0.0208], grad_fn=<ToCopyBackward0>), [' were', ' had', ' just', ' didn', ' weren'])\n",
      "(tensor([0.1183, 0.1104, 0.0996, 0.0798, 0.0322], grad_fn=<ToCopyBackward0>), [' just', ' actually', ' reading', ' trying', ' having'])\n",
      "(tensor([0.2696, 0.1799, 0.1787, 0.1329, 0.0489], grad_fn=<ToCopyBackward0>), [' their', ' from', ' the', ' cue', ' off'])\n",
      "(tensor([0.6832, 0.2007, 0.0608, 0.0101, 0.0059], grad_fn=<ToCopyBackward0>), [' cue', ' a', ' the', ' tele', ' an'])\n",
      "(tensor([9.7621e-01, 9.4137e-03, 3.6440e-03, 2.9430e-03, 7.1263e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' cards', 'cards', ' sheets', '-', ' card'])\n",
      "(tensor([0.6881, 0.0314, 0.0287, 0.0186, 0.0186], grad_fn=<ToCopyBackward0>), ['cards', 'card', 'rec', 'balls', 'm'])\n",
      "(tensor([0.1219, 0.0817, 0.0757, 0.0574, 0.0565], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' with', ' as'])\n",
      "(tensor([0.4389, 0.0661, 0.0614, 0.0540, 0.0488], grad_fn=<ToCopyBackward0>), [' and', ' not', ' which', ' rather', ' instead'])\n",
      "(tensor([0.2972, 0.0738, 0.0609, 0.0464, 0.0455], grad_fn=<ToCopyBackward0>), [' the', ' they', ' it', ' that', ' not'])\n",
      "(tensor([0.1904, 0.0748, 0.0725, 0.0702, 0.0670], grad_fn=<ToCopyBackward0>), [' were', ' had', ' just', ' didn', ' couldn'])\n",
      "(tensor([0.1652, 0.0962, 0.0509, 0.0463, 0.0288], grad_fn=<ToCopyBackward0>), [' all', ' reading', ' just', ' not', ' doing'])\n",
      "(tensor([0.1904, 0.0743, 0.0487, 0.0447, 0.0416], grad_fn=<ToCopyBackward0>), [' even', ' very', ' trying', ' good', ' doing'])\n",
      "(tensor([0.6173, 0.1989, 0.0187, 0.0161, 0.0154], grad_fn=<ToCopyBackward0>), [' good', ' convincing', ' attractive', ' believable', ' adept'])\n",
      "(tensor([0.7716, 0.1151, 0.0220, 0.0132, 0.0113], grad_fn=<ToCopyBackward0>), [' at', '.', ' actors', ' cue', ' in'])\n",
      "(tensor([0.7371, 0.0896, 0.0268, 0.0260, 0.0139], grad_fn=<ToCopyBackward0>), [' it', ' that', ' their', ' this', ' reading'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad that I actually felt compelled to register here.I was really looking forward to watching this movie because I thought it was going to be a funny comedy. The jokes were funny, the acting wasn't. The plot was predictable.\n",
      "(tensor([0.3840, 0.1720, 0.0900, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2439, 0.1964, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6530, 0.0596, 0.0360, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0661, 0.0548, 0.0469], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5191, 0.0473, 0.0457, 0.0419, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2378, 0.2248, 0.2056, 0.0699, 0.0469], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.4868, 0.1500, 0.1345, 0.0379, 0.0304], grad_fn=<ToCopyBackward0>), [' I', ' i', ' it', ' the', ' even'])\n",
      "(tensor([0.1725, 0.0621, 0.0589, 0.0403, 0.0340], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' had', ' couldn'])\n",
      "(tensor([0.1815, 0.1059, 0.0682, 0.0431, 0.0321], grad_fn=<ToCopyBackward0>), [' paid', ' made', ' thought', ' started', ' felt'])\n",
      "(tensor([0.1018, 0.0851, 0.0810, 0.0739, 0.0394], grad_fn=<ToCopyBackward0>), [' embarrassed', ' sorry', ' bad', ' compelled', ' sick'])\n",
      "(tensor([9.9807e-01, 2.6105e-04, 2.5682e-04, 1.9687e-04, 1.2933e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' by', ' (', ',', ' in'])\n",
      "(tensor([0.2130, 0.1909, 0.1008, 0.0814, 0.0390], grad_fn=<ToCopyBackward0>), [' register', ' write', ' comment', ' watch', ' give'])\n",
      "(tensor([0.1526, 0.0989, 0.0925, 0.0873, 0.0761], grad_fn=<ToCopyBackward0>), [' here', ' just', ' for', ' as', ' a'])\n",
      "(tensor([0.3093, 0.1483, 0.1094, 0.0668, 0.0584], grad_fn=<ToCopyBackward0>), ['.', ' on', ' to', ' just', ','])\n",
      "(tensor([0.3026, 0.1286, 0.0726, 0.0316, 0.0315], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', 'I', ' This'])\n",
      "(tensor([0.1211, 0.0753, 0.0567, 0.0449, 0.0416], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' am', ' rented', ' have'])\n",
      "(tensor([0.3209, 0.1396, 0.0447, 0.0300, 0.0205], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' so', ' very', ' in'])\n",
      "(tensor([0.4208, 0.1123, 0.0667, 0.0386, 0.0378], grad_fn=<ToCopyBackward0>), [' looking', ' disappointed', ',', ' really', ' surprised'])\n",
      "(tensor([9.8691e-01, 1.0353e-02, 1.6447e-03, 2.7100e-04, 2.3108e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' forward', ' for', ' to', ' forwards', ' at'])\n",
      "(tensor([9.9201e-01, 3.9046e-03, 8.2903e-04, 6.0997e-04, 3.2462e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' for', ' the', ' this', ' on'])\n",
      "(tensor([0.5046, 0.1924, 0.0877, 0.0761, 0.0136], grad_fn=<ToCopyBackward0>), [' this', ' seeing', ' the', ' watching', ' renting'])\n",
      "(tensor([0.8206, 0.0889, 0.0280, 0.0061, 0.0038], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' a', ' The'])\n",
      "(tensor([0.6860, 0.0821, 0.0790, 0.0342, 0.0201], grad_fn=<ToCopyBackward0>), [' movie', ',', ' film', ' because', '.'])\n",
      "(tensor([0.2153, 0.1065, 0.0828, 0.0679, 0.0539], grad_fn=<ToCopyBackward0>), [',', ' because', ' and', '.', ' as'])\n",
      "(tensor([0.4135, 0.1486, 0.1212, 0.1065, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' it', ' of', ' the', ' i'])\n",
      "(tensor([0.1271, 0.1167, 0.0985, 0.0926, 0.0819], grad_fn=<ToCopyBackward0>), [' really', ' thought', \"'m\", ' like', ' am'])\n",
      "(tensor([0.5494, 0.1209, 0.0735, 0.0551, 0.0429], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' that', ' this'])\n",
      "(tensor([0.4330, 0.3130, 0.0609, 0.0521, 0.0209], grad_fn=<ToCopyBackward0>), [' would', ' was', ' might', ' had', ' could'])\n",
      "(tensor([0.3907, 0.1079, 0.0438, 0.0398, 0.0378], grad_fn=<ToCopyBackward0>), [' going', ' a', ' gonna', ' so', ' pretty'])\n",
      "(tensor([9.9424e-01, 3.3225e-03, 2.3494e-04, 2.1219e-04, 1.5522e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' be', ' make', ' really', ' in'])\n",
      "(tensor([9.6905e-01, 6.5184e-03, 4.7499e-03, 1.5755e-03, 7.9938e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' get', ' provide'])\n",
      "(tensor([0.1658, 0.1251, 0.1245, 0.1161, 0.0492], grad_fn=<ToCopyBackward0>), [' a', ' really', ' pretty', ' funny', ' great'])\n",
      "(tensor([0.2073, 0.1620, 0.0764, 0.0707, 0.0491], grad_fn=<ToCopyBackward0>), [' good', ' really', ' great', ' funny', ' pretty'])\n",
      "(tensor([0.4936, 0.0931, 0.0814, 0.0246, 0.0200], grad_fn=<ToCopyBackward0>), [' movie', ',', ' comedy', ' film', ' little'])\n",
      "(tensor([0.5325, 0.1231, 0.0792, 0.0616, 0.0303], grad_fn=<ToCopyBackward0>), ['.', ',', ' about', ' with', ' that'])\n",
      "(tensor([0.2029, 0.1272, 0.1053, 0.0959, 0.0240], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' What'])\n",
      "(tensor([0.1150, 0.0935, 0.0636, 0.0601, 0.0579], grad_fn=<ToCopyBackward0>), [' plot', ' jokes', ' only', ' acting', ' movie'])\n",
      "(tensor([0.4343, 0.1008, 0.0905, 0.0723, 0.0318], grad_fn=<ToCopyBackward0>), [' were', ' weren', ' in', ' are', ' seemed'])\n",
      "(tensor([0.1810, 0.1276, 0.1115, 0.0820, 0.0302], grad_fn=<ToCopyBackward0>), [' funny', ' not', ' stupid', ' supposed', ' predictable'])\n",
      "(tensor([0.2590, 0.2069, 0.1377, 0.0686, 0.0578], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' but', ' in'])\n",
      "(tensor([0.3885, 0.2673, 0.0529, 0.0232, 0.0218], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' especially', ' I'])\n",
      "(tensor([0.4172, 0.0975, 0.0520, 0.0513, 0.0373], grad_fn=<ToCopyBackward0>), [' acting', ' actors', ' jokes', ' characters', ' story'])\n",
      "(tensor([0.8873, 0.0588, 0.0049, 0.0047, 0.0027], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' pretty', ' and', ' really'])\n",
      "(tensor([9.9639e-01, 1.0148e-03, 3.8504e-04, 2.6138e-04, 1.9379e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", ',', '´'])\n",
      "(tensor([0.2397, 0.1627, 0.1215, 0.0618, 0.0432], grad_fn=<ToCopyBackward0>), ['.', ' bad', ',', ' too', ' funny'])\n",
      "(tensor([0.3352, 0.1140, 0.0911, 0.0595, 0.0373], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' And'])\n",
      "(tensor([0.1504, 0.1470, 0.1261, 0.0745, 0.0595], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' jokes', ' story', ' only'])\n",
      "(tensor([0.7487, 0.0380, 0.0221, 0.0194, 0.0090], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' had', ' made'])\n",
      "(tensor([0.1361, 0.0886, 0.0499, 0.0464, 0.0432], grad_fn=<ToCopyBackward0>), [' stupid', ' predictable', ' ridiculous', ' not', ' so'])\n",
      "(tensor([0.3904, 0.2721, 0.2518, 0.0147, 0.0074], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' from', '...'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd never get that rated a movie. It's a really bad movie, but not as bad as this movie. I'm not sure that I've ever seen a movie with so many plot holes and so much bad acting in it. It's\n",
      "(tensor([0.3844, 0.1713, 0.0896, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2246, 0.1944, 0.1573, 0.0695, 0.0539], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2087, 0.1040, 0.0616, 0.0394, 0.0300], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.5495, 0.0456, 0.0369, 0.0365, 0.0282], grad_fn=<ToCopyBackward0>), [' see', ' be', ' get', ' find', ' seen'])\n",
      "(tensor([0.1280, 0.1220, 0.1010, 0.0685, 0.0363], grad_fn=<ToCopyBackward0>), [' through', ' to', ' that', ' out', ' there'])\n",
      "(tensor([0.0594, 0.0507, 0.0479, 0.0240, 0.0190], grad_fn=<ToCopyBackward0>), [' far', ' bored', ' chance', ' tired', ' rated'])\n",
      "(tensor([0.1722, 0.1123, 0.0854, 0.0730, 0.0274], grad_fn=<ToCopyBackward0>), [' a', ' as', '.', ' by', ' but'])\n",
      "(tensor([0.3611, 0.1372, 0.0551, 0.0530, 0.0461], grad_fn=<ToCopyBackward0>), [' 1', ' movie', ' 10', ' negative', ' one'])\n",
      "(tensor([0.4857, 0.0841, 0.0693, 0.0476, 0.0333], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' in', ' again'])\n",
      "(tensor([0.2572, 0.1296, 0.0532, 0.0382, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' But'])\n",
      "(tensor([0.4069, 0.2794, 0.0391, 0.0281, 0.0188], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' seemed', ' just'])\n",
      "(tensor([0.1552, 0.1169, 0.1107, 0.0693, 0.0661], grad_fn=<ToCopyBackward0>), [' not', ' so', ' a', ' just', ' one'])\n",
      "(tensor([0.2230, 0.1194, 0.0938, 0.0443, 0.0363], grad_fn=<ToCopyBackward0>), [' really', ' very', ' real', ' good', ' total'])\n",
      "(tensor([0.2870, 0.2029, 0.1049, 0.0452, 0.0367], grad_fn=<ToCopyBackward0>), [' bad', ' boring', ' cheesy', ' funny', ' dumb'])\n",
      "(tensor([0.7150, 0.0431, 0.0323, 0.0287, 0.0230], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ' script', ','])\n",
      "(tensor([0.6923, 0.1029, 0.0540, 0.0195, 0.0137], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' with', ' in'])\n",
      "(tensor([0.3531, 0.0772, 0.0635, 0.0387, 0.0324], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' the', ' a'])\n",
      "(tensor([0.2720, 0.2458, 0.0617, 0.0331, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' at', ' not'])\n",
      "(tensor([0.4593, 0.0757, 0.0638, 0.0523, 0.0425], grad_fn=<ToCopyBackward0>), [' as', ' in', ' a', ' funny', ' that'])\n",
      "(tensor([0.9625, 0.0044, 0.0040, 0.0034, 0.0028], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' stupid', ' much', ' awful'])\n",
      "(tensor([0.9722, 0.0121, 0.0029, 0.0020, 0.0010], grad_fn=<ToCopyBackward0>), [' as', ' a', ',', ' or', ' if'])\n",
      "(tensor([0.2093, 0.1837, 0.1061, 0.0902, 0.0677], grad_fn=<ToCopyBackward0>), [' this', ' I', ' it', ' the', ' \"'])\n",
      "(tensor([0.3906, 0.2445, 0.1238, 0.0162, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ' movie', ' one', '!', ' is'])\n",
      "(tensor([0.7781, 0.1214, 0.0151, 0.0121, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ' is', '!', ' was', '...'])\n",
      "(tensor([0.2326, 0.1841, 0.0840, 0.0676, 0.0192], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.0960, 0.0811, 0.0655, 0.0597, 0.0542], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' can', ' really'])\n",
      "(tensor([0.2059, 0.1396, 0.0918, 0.0603, 0.0400], grad_fn=<ToCopyBackward0>), [' a', ' not', ' really', ' just', ' very'])\n",
      "(tensor([0.2372, 0.1590, 0.1367, 0.1066, 0.0942], grad_fn=<ToCopyBackward0>), [' even', ' a', ' sure', ' going', ' one'])\n",
      "(tensor([0.3339, 0.1368, 0.1307, 0.1080, 0.0710], grad_fn=<ToCopyBackward0>), [' if', ' how', ' why', ' what', ' that'])\n",
      "(tensor([0.2038, 0.1070, 0.0901, 0.0887, 0.0660], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' the', ' any'])\n",
      "(tensor([0.1614, 0.1583, 0.1298, 0.0885, 0.0717], grad_fn=<ToCopyBackward0>), [' could', ' can', \"'ve\", ' would', \"'m\"])\n",
      "(tensor([0.6999, 0.2224, 0.0163, 0.0061, 0.0061], grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' been', ' made', ' really'])\n",
      "(tensor([0.4169, 0.1374, 0.0645, 0.0389, 0.0322], grad_fn=<ToCopyBackward0>), [' seen', ' been', ' rated', ' watched', ' had'])\n",
      "(tensor([0.7349, 0.0541, 0.0416, 0.0300, 0.0202], grad_fn=<ToCopyBackward0>), [' a', ' such', ' so', ' anything', ' an'])\n",
      "(tensor([0.5370, 0.2187, 0.0649, 0.0320, 0.0252], grad_fn=<ToCopyBackward0>), [' movie', ' worse', ' bad', ' really', ' more'])\n",
      "(tensor([0.1879, 0.1780, 0.1328, 0.1067, 0.1062], grad_fn=<ToCopyBackward0>), [' as', ' that', ' where', ' with', ' so'])\n",
      "(tensor([0.4713, 0.0960, 0.0824, 0.0794, 0.0671], grad_fn=<ToCopyBackward0>), [' so', ' such', ' more', ' as', ' a'])\n",
      "(tensor([0.5485, 0.3236, 0.0849, 0.0198, 0.0042], grad_fn=<ToCopyBackward0>), [' many', ' much', ' little', ' few', ' bad'])\n",
      "(tensor([0.1674, 0.0695, 0.0457, 0.0193, 0.0171], grad_fn=<ToCopyBackward0>), [' plot', ' bad', ' actors', ' big', ' impl'])\n",
      "(tensor([0.9679, 0.0049, 0.0047, 0.0031, 0.0027], grad_fn=<ToCopyBackward0>), [' holes', ' flaws', '-', ' points', ' twists'])\n",
      "(tensor([0.2165, 0.1830, 0.1770, 0.1612, 0.1044], grad_fn=<ToCopyBackward0>), [' as', ' in', ' and', '.', ','])\n",
      "(tensor([0.3228, 0.0318, 0.0308, 0.0247, 0.0180], grad_fn=<ToCopyBackward0>), [' so', ' plot', ' stupid', ' such', ' continuity'])\n",
      "(tensor([0.5687, 0.1719, 0.1713, 0.0525, 0.0032], grad_fn=<ToCopyBackward0>), [' many', ' little', ' much', ' few', ' bad'])\n",
      "(tensor([0.0895, 0.0473, 0.0450, 0.0308, 0.0277], grad_fn=<ToCopyBackward0>), [' stupidity', ' bad', ' that', ' wrong', ' inconsistency'])\n",
      "(tensor([0.8786, 0.0333, 0.0192, 0.0074, 0.0038], grad_fn=<ToCopyBackward0>), [' acting', ' taste', 'ness', ' dialogue', ' film'])\n",
      "(tensor([0.6330, 0.1621, 0.0876, 0.0433, 0.0224], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' and', ' as'])\n",
      "(tensor([0.3739, 0.3367, 0.1289, 0.0455, 0.0434], grad_fn=<ToCopyBackward0>), [' it', ' one', ' a', ' the', ' such'])\n",
      "(tensor([0.7321, 0.1185, 0.0413, 0.0358, 0.0114], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' that', ' before'])\n",
      "(tensor([0.1942, 0.1508, 0.1337, 0.0379, 0.0232], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.5716, 0.0607, 0.0482, 0.0249, 0.0244], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' looks', ' makes'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty funny. But then I got this movie with the same name. I think it's a good movie, but it's just not as good as the other movie I saw by the same name. It had a similar plot and it\n",
      "(tensor([0.3840, 0.1716, 0.0900, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2440, 0.1958, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6523, 0.0596, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1372, 0.0701, 0.0661, 0.0548, 0.0467], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1753, 0.1518, 0.1087, 0.0951, 0.0896], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.4494, 0.1295, 0.0921, 0.0257, 0.0246], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', ' in'])\n",
      "(tensor([0.2603, 0.1845, 0.0998, 0.0193, 0.0173], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.1212, 0.1051, 0.0638, 0.0624, 0.0616], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' this', ' then'])\n",
      "(tensor([0.5191, 0.0961, 0.0499, 0.0487, 0.0393], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' when', ','])\n",
      "(tensor([0.2076, 0.1172, 0.0641, 0.0610, 0.0531], grad_fn=<ToCopyBackward0>), [' found', ' watched', ' thought', ' got', ' saw'])\n",
      "(tensor([0.2361, 0.2138, 0.0762, 0.0625, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' the', ' to', ' this', ' really'])\n",
      "(tensor([0.3657, 0.2138, 0.0271, 0.0214, 0.0129], grad_fn=<ToCopyBackward0>), [' movie', ' DVD', ' feeling', ' really', ' film'])\n",
      "(tensor([0.1982, 0.1318, 0.0974, 0.0574, 0.0532], grad_fn=<ToCopyBackward0>), [' called', ' from', ' and', ' with', ' where'])\n",
      "(tensor([0.1176, 0.0785, 0.0341, 0.0264, 0.0199], grad_fn=<ToCopyBackward0>), [' the', ' this', ' Sha', ' Christopher', ' a'])\n",
      "(tensor([0.3175, 0.1410, 0.0309, 0.0296, 0.0202], grad_fn=<ToCopyBackward0>), [' same', ' guy', ' name', ' really', ' kid'])\n",
      "(tensor([0.6415, 0.1199, 0.0568, 0.0354, 0.0119], grad_fn=<ToCopyBackward0>), [' name', ' director', ' title', ' guy', ' plot'])\n",
      "(tensor([0.5914, 0.1351, 0.0860, 0.0243, 0.0200], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ' from'])\n",
      "(tensor([0.3676, 0.1116, 0.0894, 0.0658, 0.0477], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', ' This'])\n",
      "(tensor([0.1777, 0.1152, 0.0726, 0.0541, 0.0521], grad_fn=<ToCopyBackward0>), [' thought', ' was', \"'m\", ' think', ' really'])\n",
      "(tensor([0.2395, 0.2032, 0.1624, 0.0748, 0.0741], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' that'])\n",
      "(tensor([0.5947, 0.2105, 0.0453, 0.0199, 0.0154], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' might', ' has'])\n",
      "(tensor([0.1767, 0.1115, 0.0827, 0.0628, 0.0585], grad_fn=<ToCopyBackward0>), [' a', ' more', ' just', ' the', ' pretty'])\n",
      "(tensor([0.2425, 0.1704, 0.1156, 0.0816, 0.0525], grad_fn=<ToCopyBackward0>), [' good', ' really', ' lot', ' better', ' little'])\n",
      "(tensor([0.8985, 0.0551, 0.0269, 0.0037, 0.0012], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' idea', ' satire'])\n",
      "(tensor([0.3074, 0.2755, 0.0974, 0.0328, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' but', ' as'])\n",
      "(tensor([0.6731, 0.1308, 0.0510, 0.0299, 0.0184], grad_fn=<ToCopyBackward0>), [' but', ' too', ' I', ' so', ' though'])\n",
      "(tensor([0.2282, 0.1966, 0.0843, 0.0793, 0.0644], grad_fn=<ToCopyBackward0>), [' it', ' I', ' not', ' the', ' this'])\n",
      "(tensor([0.5498, 0.0977, 0.0566, 0.0398, 0.0342], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' doesn', ' is'])\n",
      "(tensor([0.4972, 0.0829, 0.0364, 0.0264, 0.0238], grad_fn=<ToCopyBackward0>), [' not', ' just', ' a', ' really', ' also'])\n",
      "(tensor([0.3655, 0.2150, 0.0386, 0.0372, 0.0357], grad_fn=<ToCopyBackward0>), [' not', ' a', ' too', ' the', ' so'])\n",
      "(tensor([0.6568, 0.2238, 0.0252, 0.0082, 0.0068], grad_fn=<ToCopyBackward0>), [' funny', ' as', ' the', ' that', ' very'])\n",
      "(tensor([0.9461, 0.0268, 0.0058, 0.0027, 0.0018], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' hilarious', ' fun', ' amusing'])\n",
      "(tensor([0.8803, 0.0726, 0.0068, 0.0064, 0.0041], grad_fn=<ToCopyBackward0>), [' as', '.', ' a', ',', ' because'])\n",
      "(tensor([0.2089, 0.1872, 0.0650, 0.0507, 0.0188], grad_fn=<ToCopyBackward0>), [' the', ' this', ' The', ' \"', ' I'])\n",
      "(tensor([0.2358, 0.1993, 0.1144, 0.0931, 0.0179], grad_fn=<ToCopyBackward0>), [' first', ' other', ' one', ' original', ' movie'])\n",
      "(tensor([0.8119, 0.0641, 0.0329, 0.0183, 0.0051], grad_fn=<ToCopyBackward0>), [' movie', ' one', ' movies', ' film', '.'])\n",
      "(tensor([0.4659, 0.2669, 0.0789, 0.0559, 0.0164], grad_fn=<ToCopyBackward0>), [' I', '.', ' with', ' that', ' in'])\n",
      "(tensor([0.2826, 0.1018, 0.0946, 0.0850, 0.0669], grad_fn=<ToCopyBackward0>), [' saw', \"'ve\", ' have', ' just', ' watched'])\n",
      "(tensor([0.3834, 0.1556, 0.0463, 0.0370, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ' with', ' (', ' last', ' by'])\n",
      "(tensor([0.6331, 0.1150, 0.0810, 0.0064, 0.0051], grad_fn=<ToCopyBackward0>), [' the', ' that', ' this', ' The', ' them'])\n",
      "(tensor([0.9664, 0.0145, 0.0030, 0.0026, 0.0019], grad_fn=<ToCopyBackward0>), [' same', ' name', ' guy', ' director', ' title'])\n",
      "(tensor([0.6718, 0.1963, 0.0916, 0.0082, 0.0072], grad_fn=<ToCopyBackward0>), [' name', ' title', ' director', ' guy', ' person'])\n",
      "(tensor([0.9420, 0.0107, 0.0060, 0.0058, 0.0042], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ' I', '!'])\n",
      "(tensor([0.3126, 0.0883, 0.0799, 0.0668, 0.0297], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' So'])\n",
      "(tensor([0.6138, 0.1331, 0.0356, 0.0215, 0.0214], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' has', ' is', ' had'])\n",
      "(tensor([0.2897, 0.1668, 0.0772, 0.0655, 0.0386], grad_fn=<ToCopyBackward0>), [' a', ' the', ' more', ' some', ' so'])\n",
      "(tensor([0.2013, 0.1450, 0.0717, 0.0682, 0.0632], grad_fn=<ToCopyBackward0>), [' lot', ' really', ' very', ' similar', ' pretty'])\n",
      "(tensor([0.6869, 0.1689, 0.0454, 0.0230, 0.0140], grad_fn=<ToCopyBackward0>), [' plot', ' storyline', ' theme', ' story', ' premise'])\n",
      "(tensor([0.3572, 0.2720, 0.1229, 0.0672, 0.0262], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', '.', ' with'])\n",
      "(tensor([0.2736, 0.0769, 0.0611, 0.0457, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' was', ' the', ' it', ' plot'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was pretty awful. The acting is awful. The plot is ridiculous. The acting. The plot. The acting. The plot was stupid. The plot was stupid. The acting. The plot. The plot was stupid. The plot was stupid\n",
      "(tensor([0.3845, 0.1715, 0.0896, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2431, 0.1963, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7761, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1273, 0.0769, 0.0659, 0.0556, 0.0420], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.1302, 0.1259, 0.1059, 0.0740, 0.0608], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' funny', ' boring', ' lame'])\n",
      "(tensor([0.5995, 0.0804, 0.0745, 0.0360, 0.0187], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' when'])\n",
      "(tensor([0.2067, 0.1826, 0.1377, 0.0234, 0.0169], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' There'])\n",
      "(tensor([0.1622, 0.0780, 0.0751, 0.0721, 0.0316], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' story', ' only', ' script'])\n",
      "(tensor([0.7843, 0.0463, 0.0401, 0.0236, 0.0189], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ',', ' and'])\n",
      "(tensor([0.0902, 0.0750, 0.0631, 0.0588, 0.0576], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' pretty', ' so'])\n",
      "(tensor([0.3659, 0.3247, 0.1867, 0.0115, 0.0096], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ';'])\n",
      "(tensor([0.6458, 0.0679, 0.0619, 0.0177, 0.0172], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.2785, 0.2062, 0.1277, 0.0493, 0.0242], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' cinem', ' directing'])\n",
      "(tensor([0.5438, 0.2283, 0.0212, 0.0144, 0.0139], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' -', ' makes'])\n",
      "(tensor([0.1525, 0.1091, 0.0609, 0.0531, 0.0383], grad_fn=<ToCopyBackward0>), [' awful', ' not', ' even', ' ridiculous', ' terrible'])\n",
      "(tensor([0.6754, 0.2082, 0.0538, 0.0073, 0.0069], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' ('])\n",
      "(tensor([0.6004, 0.0844, 0.0799, 0.0307, 0.0164], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.0871, 0.0725, 0.0617, 0.0559, 0.0432], grad_fn=<ToCopyBackward0>), [' special', ' editing', ' acting', ' ending', ' cinem'])\n",
      "(tensor([0.5560, 0.1467, 0.0725, 0.0444, 0.0407], grad_fn=<ToCopyBackward0>), [' is', '.', ' was', ',', ' and'])\n",
      "(tensor([0.4979, 0.0810, 0.0682, 0.0230, 0.0193], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' There'])\n",
      "(tensor([0.5641, 0.0817, 0.0226, 0.0205, 0.0199], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' directing', ' writing'])\n",
      "(tensor([0.7044, 0.0954, 0.0817, 0.0185, 0.0145], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', ',', ' and'])\n",
      "(tensor([0.4753, 0.0887, 0.0610, 0.0349, 0.0222], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' This'])\n",
      "(tensor([0.7964, 0.0353, 0.0119, 0.0108, 0.0097], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' casting', ' only', ' actors'])\n",
      "(tensor([0.9355, 0.0088, 0.0081, 0.0064, 0.0062], grad_fn=<ToCopyBackward0>), ['.', ' was', '...', ' is', '!'])\n",
      "(tensor([0.5894, 0.0746, 0.0433, 0.0291, 0.0230], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', ' This'])\n",
      "(tensor([0.7415, 0.0223, 0.0126, 0.0110, 0.0107], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' script', ' only', ' actors'])\n",
      "(tensor([0.8885, 0.0207, 0.0192, 0.0093, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ' is', ' was', '!', '?'])\n",
      "(tensor([0.2828, 0.0902, 0.0527, 0.0381, 0.0329], grad_fn=<ToCopyBackward0>), [' ridiculous', ' stupid', ' bad', ' silly', ' terrible'])\n",
      "(tensor([0.5092, 0.2115, 0.0756, 0.0474, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' as'])\n",
      "(tensor([0.4821, 0.0900, 0.0394, 0.0323, 0.0269], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3818, 0.2343, 0.0431, 0.0341, 0.0145], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' actors', ' script'])\n",
      "(tensor([0.7190, 0.0740, 0.0515, 0.0402, 0.0100], grad_fn=<ToCopyBackward0>), [' was', '.', ' sucked', ' is', ' had'])\n",
      "(tensor([0.6301, 0.0473, 0.0364, 0.0259, 0.0187], grad_fn=<ToCopyBackward0>), [' stupid', ' ridiculous', ' dumb', ' silly', ' not'])\n",
      "(tensor([0.8150, 0.0361, 0.0340, 0.0227, 0.0090], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', '!!'])\n",
      "(tensor([0.7084, 0.0524, 0.0229, 0.0189, 0.0141], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' And'])\n",
      "(tensor([0.4998, 0.4016, 0.0095, 0.0057, 0.0054], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' actors', ' only', ' script'])\n",
      "(tensor([0.6355, 0.2246, 0.0599, 0.0166, 0.0089], grad_fn=<ToCopyBackward0>), [' was', '.', ' is', '...', ' sucked'])\n",
      "(tensor([0.8703, 0.0243, 0.0112, 0.0101, 0.0081], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.9409, 0.0165, 0.0041, 0.0038, 0.0019], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' script', ' plotting', ' actors'])\n",
      "(tensor([0.6409, 0.2907, 0.0221, 0.0056, 0.0047], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', '?', ','])\n",
      "(tensor([0.8803, 0.0243, 0.0096, 0.0092, 0.0077], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.5046, 0.4145, 0.0070, 0.0039, 0.0039], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' plotting', ' script', ' actors'])\n",
      "(tensor([0.5436, 0.3897, 0.0165, 0.0076, 0.0047], grad_fn=<ToCopyBackward0>), ['.', ' was', ' is', ' sucked', '?'])\n",
      "(tensor([0.7748, 0.0301, 0.0212, 0.0203, 0.0117], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' ridiculous', ' silly', ' bad'])\n",
      "(tensor([0.9611, 0.0064, 0.0061, 0.0041, 0.0033], grad_fn=<ToCopyBackward0>), ['.', '!', '...', ',', ' and'])\n",
      "(tensor([0.8453, 0.0338, 0.0138, 0.0134, 0.0094], grad_fn=<ToCopyBackward0>), [' The', ' I', 'The', ' This', ' It'])\n",
      "(tensor([0.5765, 0.3458, 0.0072, 0.0052, 0.0044], grad_fn=<ToCopyBackward0>), [' plot', ' acting', ' actors', ' plotting', ' script'])\n",
      "(tensor([0.7713, 0.1810, 0.0142, 0.0053, 0.0032], grad_fn=<ToCopyBackward0>), [' was', '.', ' is', ' sucked', ' wasn'])\n",
      "(tensor([0.9414, 0.0099, 0.0041, 0.0032, 0.0021], grad_fn=<ToCopyBackward0>), [' stupid', ' dumb', ' silly', ' ridiculous', ' pointless'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I would watch a good one, since it was rated as the best horror movie of all time. The special effects were pretty good and the story was pretty good too. It is not the best film I have ever seen, it is not even the\n",
      "(tensor([0.3833, 0.1721, 0.0903, 0.0771, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2250, 0.1948, 0.1572, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2276, 0.1205, 0.0843, 0.0368, 0.0233], grad_fn=<ToCopyBackward0>), [' never', ' watch', ' like', ' give', ' be'])\n",
      "(tensor([0.7078, 0.0901, 0.0597, 0.0411, 0.0111], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.2565, 0.1036, 0.0933, 0.0652, 0.0495], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' good', ' scary', ' documentary'])\n",
      "(tensor([0.3691, 0.1582, 0.1381, 0.0532, 0.0263], grad_fn=<ToCopyBackward0>), [' movie', ' horror', ' film', ' one', ' documentary'])\n",
      "(tensor([0.1279, 0.1122, 0.1076, 0.0631, 0.0388], grad_fn=<ToCopyBackward0>), ['.', ' first', ',', ' and', ' to'])\n",
      "(tensor([0.1853, 0.1220, 0.1156, 0.0630, 0.0277], grad_fn=<ToCopyBackward0>), [' so', ' but', ' and', ' since', ' a'])\n",
      "(tensor([0.3770, 0.2577, 0.0644, 0.0389, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' it', ' this', ' the', ' i'])\n",
      "(tensor([0.3616, 0.1621, 0.0873, 0.0440, 0.0399], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' seemed'])\n",
      "(tensor([0.3470, 0.0894, 0.0572, 0.0382, 0.0320], grad_fn=<ToCopyBackward0>), [' on', ' rated', ' in', ' so', ' only'])\n",
      "(tensor([0.2035, 0.1399, 0.0820, 0.0785, 0.0475], grad_fn=<ToCopyBackward0>), [' as', ' a', ' at', ' so', ' by'])\n",
      "(tensor([0.3611, 0.0974, 0.0773, 0.0681, 0.0605], grad_fn=<ToCopyBackward0>), [' a', ' one', \" '\", ' \"', ' the'])\n",
      "(tensor([0.3083, 0.1907, 0.0680, 0.0656, 0.0481], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' \"', \" '\", ' #'])\n",
      "(tensor([0.2815, 0.1861, 0.0916, 0.0601, 0.0416], grad_fn=<ToCopyBackward0>), [' horror', ' film', ' one', ' movie', ' comedy'])\n",
      "(tensor([0.6246, 0.2592, 0.0102, 0.0093, 0.0088], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' on', ' in', '/'])\n",
      "(tensor([0.3041, 0.1656, 0.1327, 0.0848, 0.0542], grad_fn=<ToCopyBackward0>), [' of', ' in', ' ever', ' on', ' I'])\n",
      "(tensor([0.6045, 0.0750, 0.0705, 0.0241, 0.0219], grad_fn=<ToCopyBackward0>), [' all', ' 2001', ' the', ' 2002', ' 2006'])\n",
      "(tensor([9.9101e-01, 4.8782e-03, 2.2015e-03, 4.5589e-04, 3.1114e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' the', ' Time'])\n",
      "(tensor([0.6910, 0.1037, 0.0696, 0.0216, 0.0214], grad_fn=<ToCopyBackward0>), ['.', ' by', ',', ' in', ' on'])\n",
      "(tensor([0.3331, 0.1006, 0.0611, 0.0360, 0.0299], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' This'])\n",
      "(tensor([0.1188, 0.0732, 0.0408, 0.0386, 0.0320], grad_fn=<ToCopyBackward0>), [' acting', ' original', ' first', ' special', ' only'])\n",
      "(tensor([9.7318e-01, 2.0598e-02, 1.6782e-03, 1.6756e-03, 4.2081e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' effects', ' effect', ' features', ' affects', ' Effects'])\n",
      "(tensor([0.5219, 0.2761, 0.0263, 0.0241, 0.0151], grad_fn=<ToCopyBackward0>), [' were', ' are', ' and', ' in', ' make'])\n",
      "(tensor([0.1623, 0.0632, 0.0559, 0.0484, 0.0460], grad_fn=<ToCopyBackward0>), [' pretty', ' great', ' good', ' really', ' so'])\n",
      "(tensor([0.3897, 0.1256, 0.0907, 0.0710, 0.0176], grad_fn=<ToCopyBackward0>), [' good', ' cool', ' bad', ' impressive', ' cheesy'])\n",
      "(tensor([0.3574, 0.1346, 0.1304, 0.0797, 0.0785], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' too', ' for'])\n",
      "(tensor([0.6426, 0.0915, 0.0452, 0.0401, 0.0067], grad_fn=<ToCopyBackward0>), [' the', ' I', ' there', ' it', ' very'])\n",
      "(tensor([0.1922, 0.1618, 0.1139, 0.0534, 0.0494], grad_fn=<ToCopyBackward0>), [' story', ' acting', ' plot', ' cast', ' special'])\n",
      "(tensor([0.7089, 0.0283, 0.0281, 0.0187, 0.0149], grad_fn=<ToCopyBackward0>), [' was', ' had', ' itself', ' wasn', ' did'])\n",
      "(tensor([0.3042, 0.1163, 0.1113, 0.0488, 0.0286], grad_fn=<ToCopyBackward0>), [' pretty', ' interesting', ' good', ' decent', ' fairly'])\n",
      "(tensor([0.4159, 0.1130, 0.0684, 0.0546, 0.0539], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' decent', ' creepy', ' scary'])\n",
      "(tensor([0.3293, 0.2501, 0.1936, 0.0815, 0.0182], grad_fn=<ToCopyBackward0>), ['.', ',', ' too', ' as', ' but'])\n",
      "(tensor([0.6067, 0.2141, 0.0373, 0.0194, 0.0191], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' but', ' so'])\n",
      "(tensor([0.2542, 0.1343, 0.0872, 0.0795, 0.0392], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' However'])\n",
      "(tensor([0.3707, 0.1091, 0.0766, 0.0632, 0.0591], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' is', ' wasn'])\n",
      "(tensor([0.1867, 0.0521, 0.0509, 0.0395, 0.0366], grad_fn=<ToCopyBackward0>), [' a', ' not', ' pretty', ' very', ' one'])\n",
      "(tensor([0.2835, 0.1262, 0.0896, 0.0719, 0.0427], grad_fn=<ToCopyBackward0>), [' a', ' the', ' really', ' very', ' scary'])\n",
      "(tensor([0.4160, 0.3538, 0.0599, 0.0376, 0.0123], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' most', ' horror'])\n",
      "(tensor([0.2118, 0.1288, 0.0636, 0.0636, 0.0581], grad_fn=<ToCopyBackward0>), [' movie', ' horror', ' film', ',', ' of'])\n",
      "(tensor([0.2944, 0.2881, 0.0670, 0.0590, 0.0573], grad_fn=<ToCopyBackward0>), [' I', ' of', ',', ' in', ' but'])\n",
      "(tensor([0.8414, 0.0850, 0.0518, 0.0031, 0.0031], grad_fn=<ToCopyBackward0>), [' have', \"'ve\", ' ever', ' saw', ' had'])\n",
      "(tensor([7.7996e-01, 1.9597e-01, 1.7195e-02, 7.5933e-04, 4.1631e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' ever', ' seen', ' watched', ' had', ' see'])\n",
      "(tensor([0.9436, 0.0429, 0.0027, 0.0021, 0.0010], grad_fn=<ToCopyBackward0>), [' seen', ' watched', ' had', ' viewed', ' saw'])\n",
      "(tensor([0.6666, 0.1505, 0.0305, 0.0276, 0.0173], grad_fn=<ToCopyBackward0>), [',', ' but', ' in', '.', ' by'])\n",
      "(tensor([0.8063, 0.0366, 0.0145, 0.0133, 0.0116], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' since', ' or'])\n",
      "(tensor([0.4747, 0.1031, 0.0730, 0.0512, 0.0290], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' was', ' has', ' just'])\n",
      "(tensor([0.1933, 0.0729, 0.0665, 0.0572, 0.0458], grad_fn=<ToCopyBackward0>), [' not', ' just', ' good', ' a', ' pretty'])\n",
      "(tensor([0.3124, 0.2888, 0.0328, 0.0286, 0.0227], grad_fn=<ToCopyBackward0>), [' the', ' even', ' a', ' one', ' as'])\n",
      "(tensor([0.3598, 0.1289, 0.1235, 0.0803, 0.0385], grad_fn=<ToCopyBackward0>), [' the', ' in', ' good', ' close', ' a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie started well enough. It started off well enough. And then I just started to see the same thing over and over again. You get a little bit into the story, then you see something else. You get a little bit into the story\n",
      "(tensor([0.3841, 0.1717, 0.0898, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5015, 0.0598, 0.0340, 0.0151, 0.0145], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6234, 0.0400, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.4244, 0.3174, 0.0989, 0.0110, 0.0109], grad_fn=<ToCopyBackward0>), [' out', ' off', ' well', ' very', ' slow'])\n",
      "(tensor([0.4286, 0.0994, 0.0815, 0.0752, 0.0749], grad_fn=<ToCopyBackward0>), [' enough', ',', ' but', '...', '.'])\n",
      "(tensor([0.3443, 0.2038, 0.0711, 0.0701, 0.0659], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' with', ' but'])\n",
      "(tensor([0.1969, 0.1556, 0.1315, 0.0460, 0.0320], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' But', ' Then'])\n",
      "(tensor([0.2263, 0.0947, 0.0889, 0.0888, 0.0671], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' started', ' had', \"'s\"])\n",
      "(tensor([0.2994, 0.2675, 0.1698, 0.0278, 0.0179], grad_fn=<ToCopyBackward0>), [' well', ' off', ' out', ' with', ' very'])\n",
      "(tensor([0.2668, 0.0685, 0.0618, 0.0614, 0.0447], grad_fn=<ToCopyBackward0>), [' well', ' with', ' pretty', ' fine', ' very'])\n",
      "(tensor([0.9164, 0.0192, 0.0111, 0.0095, 0.0083], grad_fn=<ToCopyBackward0>), [' enough', ',', '.', ' but', ' in'])\n",
      "(tensor([0.2397, 0.1716, 0.1482, 0.1123, 0.0666], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', ' but', ' in'])\n",
      "(tensor([0.1411, 0.1317, 0.1055, 0.0965, 0.0579], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' And'])\n",
      "(tensor([0.2917, 0.1875, 0.1409, 0.0670, 0.0233], grad_fn=<ToCopyBackward0>), [' I', ' then', ' the', ' it', ','])\n",
      "(tensor([0.2869, 0.2269, 0.1407, 0.0357, 0.0291], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ',', ' you'])\n",
      "(tensor([0.0950, 0.0862, 0.0783, 0.0745, 0.0628], grad_fn=<ToCopyBackward0>), [' watched', ' just', ' thought', ' found', ' got'])\n",
      "(tensor([0.1536, 0.1186, 0.0741, 0.0628, 0.0476], grad_fn=<ToCopyBackward0>), [' watched', ' started', ' got', ' thought', ' couldn'])\n",
      "(tensor([0.2741, 0.2669, 0.0494, 0.0375, 0.0316], grad_fn=<ToCopyBackward0>), [' watching', ' to', ' getting', ' wondering', ' looking'])\n",
      "(tensor([0.1692, 0.1073, 0.1035, 0.0660, 0.0622], grad_fn=<ToCopyBackward0>), [' watch', ' feel', ' get', ' think', ' see'])\n",
      "(tensor([0.3473, 0.0872, 0.0684, 0.0428, 0.0397], grad_fn=<ToCopyBackward0>), [' the', ' more', ' it', ' that', ' a'])\n",
      "(tensor([0.0466, 0.0247, 0.0231, 0.0223, 0.0205], grad_fn=<ToCopyBackward0>), [' movie', ' changes', ' same', ' ending', ' teeth'])\n",
      "(tensor([0.1461, 0.0460, 0.0413, 0.0360, 0.0282], grad_fn=<ToCopyBackward0>), [' thing', ' scenes', ' things', ' old', ' acting'])\n",
      "(tensor([0.2692, 0.2358, 0.1085, 0.0997, 0.0404], grad_fn=<ToCopyBackward0>), [' over', ' again', '.', ' in', ','])\n",
      "(tensor([9.2566e-01, 6.8498e-02, 1.6779e-03, 8.3949e-04, 5.0333e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' and', ' again', ' the', ' &', ' in'])\n",
      "(tensor([9.9631e-01, 2.6969e-03, 3.9764e-04, 2.0183e-04, 5.7832e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' over', ' OVER', ' again', ' Over', 'over'])\n",
      "(tensor([0.8159, 0.1144, 0.0287, 0.0130, 0.0073], grad_fn=<ToCopyBackward0>), [' again', ' and', '.', ' in', ','])\n",
      "(tensor([0.3891, 0.2500, 0.1247, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ':'])\n",
      "(tensor([0.2475, 0.1737, 0.0615, 0.0523, 0.0344], grad_fn=<ToCopyBackward0>), [' I', ' And', ' It', ' The', ' You'])\n",
      "(tensor([0.4379, 0.0649, 0.0560, 0.0451, 0.0357], grad_fn=<ToCopyBackward0>), [' know', ' have', ' can', ' just', ' get'])\n",
      "(tensor([0.3011, 0.2000, 0.0561, 0.0425, 0.0275], grad_fn=<ToCopyBackward0>), [' a', ' the', ' to', ' so', ' into'])\n",
      "(tensor([0.1200, 0.1031, 0.0676, 0.0484, 0.0433], grad_fn=<ToCopyBackward0>), [' little', ' bunch', ' guy', ' really', ' group'])\n",
      "(tensor([0.5540, 0.0684, 0.0387, 0.0229, 0.0162], grad_fn=<ToCopyBackward0>), [' bit', ' bored', ' tired', ' more', ' too'])\n",
      "(tensor([0.2656, 0.1162, 0.0531, 0.0497, 0.0431], grad_fn=<ToCopyBackward0>), [' of', ' more', ' tired', ' bored', ' into'])\n",
      "(tensor([0.4623, 0.1139, 0.1041, 0.0952, 0.0384], grad_fn=<ToCopyBackward0>), [' the', ' a', ' this', ' it', ' your'])\n",
      "(tensor([0.2701, 0.1763, 0.0701, 0.0699, 0.0327], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' movie', ' script', ' storyline'])\n",
      "(tensor([0.3998, 0.2079, 0.0213, 0.0194, 0.0149], grad_fn=<ToCopyBackward0>), [' and', ',', '.', ' -', ' a'])\n",
      "(tensor([0.3067, 0.2722, 0.1172, 0.0925, 0.0178], grad_fn=<ToCopyBackward0>), [' and', ' you', ' then', ' but', ' the'])\n",
      "(tensor([0.5474, 0.1281, 0.0675, 0.0321, 0.0230], grad_fn=<ToCopyBackward0>), [' you', ' it', ' the', ' there', ' suddenly'])\n",
      "(tensor([0.2651, 0.1104, 0.1011, 0.1007, 0.0541], grad_fn=<ToCopyBackward0>), [' get', ' see', ' realize', ' just', ' have'])\n",
      "(tensor([0.3974, 0.1182, 0.0977, 0.0717, 0.0343], grad_fn=<ToCopyBackward0>), [' the', ' something', ' it', ' a', ' that'])\n",
      "(tensor([0.1310, 0.0879, 0.0826, 0.0677, 0.0612], grad_fn=<ToCopyBackward0>), [' else', ' like', ' different', ' that', ' and'])\n",
      "(tensor([0.2812, 0.2146, 0.1716, 0.0652, 0.0281], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' and', ' on'])\n",
      "(tensor([0.2656, 0.1913, 0.0647, 0.0642, 0.0584], grad_fn=<ToCopyBackward0>), [' You', ' And', ' Then', ' I', ' It'])\n",
      "(tensor([0.2660, 0.1460, 0.0866, 0.0706, 0.0379], grad_fn=<ToCopyBackward0>), [' get', ' see', ' start', ' just', ' don'])\n",
      "(tensor([0.8251, 0.0360, 0.0201, 0.0132, 0.0124], grad_fn=<ToCopyBackward0>), [' a', ' another', ' the', ' into', ' one'])\n",
      "(tensor([0.9661, 0.0019, 0.0017, 0.0017, 0.0017], grad_fn=<ToCopyBackward0>), [' little', ' bit', ' really', ' big', ' few'])\n",
      "(tensor([0.9747, 0.0053, 0.0037, 0.0020, 0.0017], grad_fn=<ToCopyBackward0>), [' bit', ' more', ' something', ' into', ' further'])\n",
      "(tensor([0.9277, 0.0364, 0.0097, 0.0068, 0.0043], grad_fn=<ToCopyBackward0>), [' into', ' in', ' more', ' further', ' deeper'])\n",
      "(tensor([0.9007, 0.0411, 0.0161, 0.0087, 0.0063], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' a', ' that'])\n",
      "(tensor([0.8801, 0.0453, 0.0140, 0.0109, 0.0093], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' character', ' plot', ' storyline'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this was a really bad movie. I thought this movie was so bad I actually paid money to see it. It was so terrible. It is one of those movies where you think \"Oh wow, that was so stupid\" and you just watch it\n",
      "(tensor([0.3841, 0.1716, 0.0899, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2440, 0.1957, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.4800, 0.1355, 0.1036, 0.0554, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' pretty'])\n",
      "(tensor([0.1340, 0.1093, 0.0992, 0.0921, 0.0467], grad_fn=<ToCopyBackward0>), [' really', ' sequel', ' good', ' great', ' movie'])\n",
      "(tensor([0.4369, 0.1023, 0.0498, 0.0395, 0.0183], grad_fn=<ToCopyBackward0>), [' bad', ' dumb', ' cheesy', ' stupid', ' good'])\n",
      "(tensor([0.8394, 0.0656, 0.0136, 0.0114, 0.0040], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' script', ' horror'])\n",
      "(tensor([0.7276, 0.0473, 0.0455, 0.0231, 0.0145], grad_fn=<ToCopyBackward0>), ['.', '!', ',', '...', ' when'])\n",
      "(tensor([0.3172, 0.1461, 0.0990, 0.0267, 0.0198], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' Not', ' This'])\n",
      "(tensor([0.1380, 0.0950, 0.0820, 0.0555, 0.0504], grad_fn=<ToCopyBackward0>), [' really', ' thought', ' was', \"'m\", ' mean'])\n",
      "(tensor([0.5645, 0.1621, 0.1216, 0.0568, 0.0190], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' I'])\n",
      "(tensor([0.6075, 0.2104, 0.1083, 0.0087, 0.0058], grad_fn=<ToCopyBackward0>), [' was', ' movie', ' is', ' one', ' film'])\n",
      "(tensor([0.5956, 0.0537, 0.0395, 0.0283, 0.0275], grad_fn=<ToCopyBackward0>), [' was', ' is', ' had', ' would', ' sucked'])\n",
      "(tensor([0.1380, 0.1211, 0.0584, 0.0558, 0.0406], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' just', ' the'])\n",
      "(tensor([0.3391, 0.0676, 0.0482, 0.0299, 0.0298], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' horrible', ','])\n",
      "(tensor([0.3827, 0.1591, 0.1528, 0.0943, 0.0840], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', '.'])\n",
      "(tensor([0.2307, 0.1095, 0.0808, 0.0597, 0.0383], grad_fn=<ToCopyBackward0>), [' actually', ' thought', ' was', ' could', ' would'])\n",
      "(tensor([0.1099, 0.0652, 0.0589, 0.0583, 0.0525], grad_fn=<ToCopyBackward0>), [' thought', ' made', ' paid', ' started', ' felt'])\n",
      "(tensor([0.1999, 0.1764, 0.0835, 0.0827, 0.0778], grad_fn=<ToCopyBackward0>), [' to', ' $', ' a', ' money', ' for'])\n",
      "(tensor([0.7925, 0.1627, 0.0039, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' to', ' for', ',', ' just', ' and'])\n",
      "(tensor([0.6870, 0.0792, 0.0783, 0.0585, 0.0153], grad_fn=<ToCopyBackward0>), [' see', ' rent', ' watch', ' get', ' go'])\n",
      "(tensor([0.7510, 0.1700, 0.0224, 0.0166, 0.0077], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' something'])\n",
      "(tensor([0.5951, 0.0707, 0.0657, 0.0633, 0.0394], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' in', ' just'])\n",
      "(tensor([0.4067, 0.0757, 0.0403, 0.0399, 0.0345], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', 'I'])\n",
      "(tensor([0.3397, 0.3053, 0.0772, 0.0214, 0.0200], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' looks'])\n",
      "(tensor([0.3902, 0.0874, 0.0543, 0.0400, 0.0345], grad_fn=<ToCopyBackward0>), [' so', ' like', ' a', ' not', ' just'])\n",
      "(tensor([0.7835, 0.0417, 0.0247, 0.0219, 0.0168], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' horrible', ' stupid', ' awful'])\n",
      "(tensor([0.3308, 0.1959, 0.1372, 0.1009, 0.0817], grad_fn=<ToCopyBackward0>), [' I', ' that', '.', ' it', ','])\n",
      "(tensor([0.3550, 0.1516, 0.0624, 0.0521, 0.0265], grad_fn=<ToCopyBackward0>), [' I', ' It', ' And', ' The', 'I'])\n",
      "(tensor([0.5275, 0.1418, 0.0451, 0.0236, 0.0231], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' had'])\n",
      "(tensor([0.4114, 0.0901, 0.0759, 0.0579, 0.0420], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' just', ' one'])\n",
      "(tensor([0.9225, 0.0191, 0.0089, 0.0085, 0.0041], grad_fn=<ToCopyBackward0>), [' of', ' bad', ' the', ' big', ' thing'])\n",
      "(tensor([0.5629, 0.3930, 0.0293, 0.0043, 0.0012], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' worst'])\n",
      "(tensor([9.5087e-01, 2.5294e-02, 3.5318e-03, 9.5185e-04, 9.3845e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' movies', ' films', ' movie', ' really', ' rare'])\n",
      "(tensor([0.5545, 0.2209, 0.0689, 0.0523, 0.0154], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', '.'])\n",
      "(tensor([0.2862, 0.2723, 0.1149, 0.0556, 0.0542], grad_fn=<ToCopyBackward0>), [' you', ' I', ' the', ' if', ' it'])\n",
      "(tensor([0.0755, 0.0750, 0.0619, 0.0475, 0.0465], grad_fn=<ToCopyBackward0>), [' can', ' think', ' just', ' are', \"'re\"])\n",
      "(tensor([0.3404, 0.1596, 0.0858, 0.0448, 0.0446], grad_fn=<ToCopyBackward0>), [' it', ',', ' you', ' \"', ' that'])\n",
      "(tensor([0.1207, 0.1128, 0.0709, 0.0523, 0.0492], grad_fn=<ToCopyBackward0>), ['Oh', 'oh', 'ok', 'OK', 'this'])\n",
      "(tensor([0.3296, 0.1025, 0.0832, 0.0655, 0.0553], grad_fn=<ToCopyBackward0>), [' my', ' God', ',', ' yeah', ' wow'])\n",
      "(tensor([0.3642, 0.1734, 0.1539, 0.0494, 0.0355], grad_fn=<ToCopyBackward0>), [',', ' this', '!', ' that', ' I'])\n",
      "(tensor([0.4193, 0.1581, 0.0992, 0.0377, 0.0361], grad_fn=<ToCopyBackward0>), [' I', ' this', ' that', ' what', ' they'])\n",
      "(tensor([0.3416, 0.2430, 0.1649, 0.0544, 0.0397], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' movie'])\n",
      "(tensor([0.6568, 0.0631, 0.0315, 0.0213, 0.0212], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' pretty', ' funny'])\n",
      "(tensor([0.3980, 0.1316, 0.1076, 0.0819, 0.0394], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' stupid', ' terrible', ' awful'])\n",
      "(tensor([0.1881, 0.1585, 0.1408, 0.0712, 0.0702], grad_fn=<ToCopyBackward0>), ['.\"', '\".', '\"', '.', ','])\n",
      "(tensor([0.3133, 0.2753, 0.0425, 0.0310, 0.0296], grad_fn=<ToCopyBackward0>), [' but', ' and', ' -', 'and', ' then'])\n",
      "(tensor([0.6937, 0.1311, 0.0355, 0.0272, 0.0218], grad_fn=<ToCopyBackward0>), [' then', ' you', ' it', ' \"', ' the'])\n",
      "(tensor([0.0976, 0.0737, 0.0720, 0.0587, 0.0523], grad_fn=<ToCopyBackward0>), [' actually', ' think', \"'re\", ' watch', ' just'])\n",
      "(tensor([0.2031, 0.1329, 0.1224, 0.0565, 0.0385], grad_fn=<ToCopyBackward0>), [' laugh', ' keep', ' watch', ' want', ' look'])\n",
      "(tensor([0.5580, 0.3349, 0.0164, 0.0111, 0.0080], grad_fn=<ToCopyBackward0>), [' the', ' it', ' a', ' and', ' to'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie was very well acted. The movie was funny, too, but the plot wasn't funny. I was expecting a funny movie with funny plot, but that was not it. I was expecting a funny movie with funny plot. It was\n",
      "(tensor([0.3832, 0.1720, 0.0903, 0.0773, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3315, 0.2364, 0.0583, 0.0562, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2624, 0.0540, 0.0272, 0.0220, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5314, 0.0603, 0.0539, 0.0231, 0.0205], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.1799, 0.0810, 0.0520, 0.0427, 0.0358], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' a', ' so', ' terrible'])\n",
      "(tensor([0.4255, 0.0824, 0.0428, 0.0316, 0.0266], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' disappointing', ',', ' funny'])\n",
      "(tensor([0.5640, 0.2573, 0.0836, 0.0228, 0.0175], grad_fn=<ToCopyBackward0>), [' acted', ' done', ' made', ' written', ' scripted'])\n",
      "(tensor([0.3237, 0.2061, 0.1855, 0.0461, 0.0290], grad_fn=<ToCopyBackward0>), [' and', ',', '.', '...', ' by'])\n",
      "(tensor([0.1585, 0.1583, 0.1259, 0.0379, 0.0358], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' But'])\n",
      "(tensor([0.2306, 0.1719, 0.0524, 0.0407, 0.0256], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' movie', ' acting', ' characters'])\n",
      "(tensor([0.3447, 0.2228, 0.0583, 0.0367, 0.0319], grad_fn=<ToCopyBackward0>), [' was', ' is', ' has', ' had', ' just'])\n",
      "(tensor([0.1412, 0.0813, 0.0460, 0.0414, 0.0399], grad_fn=<ToCopyBackward0>), [' very', ' pretty', ' good', ' funny', ' really'])\n",
      "(tensor([0.3552, 0.1737, 0.1228, 0.0592, 0.0486], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' too', ' in'])\n",
      "(tensor([0.2659, 0.1777, 0.0527, 0.0473, 0.0405], grad_fn=<ToCopyBackward0>), [' but', ' the', ' too', ' it', ' I'])\n",
      "(tensor([0.6852, 0.2267, 0.0108, 0.0084, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ',', ' (', ';', ' -'])\n",
      "(tensor([0.5109, 0.1022, 0.0429, 0.0368, 0.0224], grad_fn=<ToCopyBackward0>), [' but', ' and', ' I', ' so', ' although'])\n",
      "(tensor([0.2145, 0.1537, 0.1311, 0.0751, 0.0636], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' not', ' that'])\n",
      "(tensor([0.2091, 0.1462, 0.0890, 0.0646, 0.0555], grad_fn=<ToCopyBackward0>), [' story', ' movie', ' plot', ' acting', ' funny'])\n",
      "(tensor([0.8559, 0.0313, 0.0215, 0.0127, 0.0081], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' itself', ' had'])\n",
      "(tensor([9.9619e-01, 1.4111e-03, 4.2526e-04, 2.5777e-04, 1.8022e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ','])\n",
      "(tensor([0.2207, 0.1157, 0.1050, 0.0678, 0.0628], grad_fn=<ToCopyBackward0>), [' that', ' too', ' very', ' funny', ' as'])\n",
      "(tensor([0.5210, 0.1369, 0.0988, 0.0486, 0.0263], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' in', ' and'])\n",
      "(tensor([0.2450, 0.1894, 0.1603, 0.0314, 0.0284], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' That', ' There'])\n",
      "(tensor([0.1065, 0.0784, 0.0778, 0.0720, 0.0636], grad_fn=<ToCopyBackward0>), [' thought', ' don', ' think', ' just', ' was'])\n",
      "(tensor([0.1916, 0.1329, 0.0880, 0.0686, 0.0452], grad_fn=<ToCopyBackward0>), [' very', ' really', ' expecting', ' not', ' looking'])\n",
      "(tensor([0.2818, 0.1313, 0.1047, 0.0730, 0.0598], grad_fn=<ToCopyBackward0>), [' a', ' it', ' more', ' something', ' to'])\n",
      "(tensor([0.2974, 0.1833, 0.0555, 0.0468, 0.0324], grad_fn=<ToCopyBackward0>), [' lot', ' movie', ' comedy', ' funny', ' very'])\n",
      "(tensor([0.6471, 0.2721, 0.0339, 0.0101, 0.0041], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ',', ' film'])\n",
      "(tensor([0.3735, 0.2548, 0.0950, 0.0384, 0.0292], grad_fn=<ToCopyBackward0>), ['.', ',', ' with', ' because', ' to'])\n",
      "(tensor([0.5161, 0.1176, 0.0881, 0.0237, 0.0139], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' stupid', ' lots', ' bad'])\n",
      "(tensor([0.2839, 0.2067, 0.0899, 0.0640, 0.0436], grad_fn=<ToCopyBackward0>), [' plot', ' plots', ' characters', ' jokes', ' people'])\n",
      "(tensor([0.5923, 0.1939, 0.0450, 0.0232, 0.0169], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' because'])\n",
      "(tensor([0.5972, 0.0872, 0.0751, 0.0714, 0.0318], grad_fn=<ToCopyBackward0>), [' but', ' and', ' not', ' like', ' instead'])\n",
      "(tensor([0.2891, 0.2091, 0.1062, 0.0922, 0.0395], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' I', ' that'])\n",
      "(tensor([0.3510, 0.2186, 0.1448, 0.1281, 0.0680], grad_fn=<ToCopyBackward0>), [' wasn', ' was', ' didn', \"'s\", ' plot'])\n",
      "(tensor([0.9103, 0.0129, 0.0095, 0.0065, 0.0064], grad_fn=<ToCopyBackward0>), [' not', ' just', ' the', ' a', ' nowhere'])\n",
      "(tensor([0.5398, 0.1557, 0.0848, 0.0691, 0.0388], grad_fn=<ToCopyBackward0>), [' the', ' it', ' to', ' what', ' at'])\n",
      "(tensor([0.8954, 0.0452, 0.0141, 0.0136, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', '...', '!'])\n",
      "(tensor([0.2285, 0.1997, 0.1916, 0.0294, 0.0275], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.1422, 0.0895, 0.0699, 0.0580, 0.0571], grad_fn=<ToCopyBackward0>), [' was', ' think', ' don', ' thought', ' really'])\n",
      "(tensor([0.7116, 0.0518, 0.0403, 0.0302, 0.0214], grad_fn=<ToCopyBackward0>), [' expecting', ' very', ' not', ' really', ' hoping'])\n",
      "(tensor([0.5156, 0.0652, 0.0550, 0.0460, 0.0457], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' something', ' some', ' it'])\n",
      "(tensor([0.3259, 0.2854, 0.0830, 0.0458, 0.0157], grad_fn=<ToCopyBackward0>), [' funny', ' movie', ' comedy', ' good', ' really'])\n",
      "(tensor([0.8299, 0.0653, 0.0291, 0.0175, 0.0066], grad_fn=<ToCopyBackward0>), [' movie', ' plot', ' story', ' comedy', ','])\n",
      "(tensor([0.5304, 0.1607, 0.0859, 0.0729, 0.0182], grad_fn=<ToCopyBackward0>), [' with', ',', '.', ' that', ' about'])\n",
      "(tensor([0.8643, 0.0507, 0.0355, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' funny', ' stupid', ' a', ' good', ' some'])\n",
      "(tensor([0.8404, 0.0483, 0.0276, 0.0101, 0.0077], grad_fn=<ToCopyBackward0>), [' plot', ' plots', ' characters', ' story', ' actors'])\n",
      "(tensor([0.6931, 0.1157, 0.0454, 0.0265, 0.0235], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' with'])\n",
      "(tensor([0.1794, 0.1457, 0.1230, 0.1023, 0.0699], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' That', ' But'])\n",
      "(tensor([0.3787, 0.2773, 0.1017, 0.0600, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' didn', \"'s\", ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was so atrocious it was funny. I was actually really impressed with the fact that the film was so awful it was funny. It's one of those movies that you can sit through and enjoy. I'm sure it will be a classic\n",
      "(tensor([0.3845, 0.1715, 0.0897, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4378, 0.2432, 0.1963, 0.0167, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7759, 0.0505, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1273, 0.0769, 0.0659, 0.0556, 0.0420], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4445, 0.0487, 0.0405, 0.0301, 0.0287], grad_fn=<ToCopyBackward0>), [' bad', ' atro', ' awful', ' boring', ' terrible'])\n",
      "(tensor([9.9990e-01, 6.4446e-06, 6.0951e-06, 3.7859e-06, 3.6254e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'ct', 'etic', 'per', 'cul'])\n",
      "(tensor([0.3432, 0.1034, 0.1026, 0.0758, 0.0721], grad_fn=<ToCopyBackward0>), [' that', ' it', '.', ',', ' I'])\n",
      "(tensor([0.5417, 0.0787, 0.0516, 0.0412, 0.0329], grad_fn=<ToCopyBackward0>), [' was', ' would', ' could', ' should', ' must'])\n",
      "(tensor([0.5671, 0.0780, 0.0322, 0.0231, 0.0227], grad_fn=<ToCopyBackward0>), [' funny', ' almost', ' actually', ' hilarious', ' good'])\n",
      "(tensor([0.6847, 0.0745, 0.0438, 0.0195, 0.0177], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' to', '...'])\n",
      "(tensor([0.2294, 0.1389, 0.1027, 0.0259, 0.0162], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' Not'])\n",
      "(tensor([0.1093, 0.0615, 0.0550, 0.0517, 0.0432], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' thought', ' can', ' really'])\n",
      "(tensor([0.1796, 0.1361, 0.0849, 0.0644, 0.0361], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' so', ' in', ' very'])\n",
      "(tensor([0.2011, 0.0778, 0.0672, 0.0518, 0.0460], grad_fn=<ToCopyBackward0>), [' really', ' looking', ' very', ' in', ' surprised'])\n",
      "(tensor([0.4430, 0.1172, 0.0703, 0.0265, 0.0264], grad_fn=<ToCopyBackward0>), [' looking', ' surprised', ' disappointed', ' interested', ' impressed'])\n",
      "(tensor([0.5666, 0.1539, 0.0669, 0.0440, 0.0430], grad_fn=<ToCopyBackward0>), [' by', ' with', ' at', '.', ' that'])\n",
      "(tensor([0.4707, 0.2466, 0.1008, 0.0368, 0.0129], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' some'])\n",
      "(tensor([0.0548, 0.0478, 0.0472, 0.0452, 0.0368], grad_fn=<ToCopyBackward0>), [' original', ' fact', ' acting', ' story', ' premise'])\n",
      "(tensor([0.9181, 0.0212, 0.0108, 0.0061, 0.0051], grad_fn=<ToCopyBackward0>), [' that', ' the', ' it', ' I', ' they'])\n",
      "(tensor([0.1835, 0.1800, 0.1424, 0.0720, 0.0532], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' they', ' this'])\n",
      "(tensor([0.0775, 0.0721, 0.0627, 0.0609, 0.0544], grad_fn=<ToCopyBackward0>), [' movie', ' director', ' film', ' guy', ' people'])\n",
      "(tensor([0.4062, 0.1270, 0.0454, 0.0447, 0.0363], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' did', ' has'])\n",
      "(tensor([0.5338, 0.1722, 0.0204, 0.0187, 0.0151], grad_fn=<ToCopyBackward0>), [' so', ' made', ' able', ' as', ' even'])\n",
      "(tensor([0.3195, 0.0416, 0.0293, 0.0270, 0.0178], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' low', ' awful', ' terrible'])\n",
      "(tensor([0.2936, 0.2913, 0.0914, 0.0828, 0.0414], grad_fn=<ToCopyBackward0>), ['.', ' that', ',', ' it', ' and'])\n",
      "(tensor([0.7747, 0.0743, 0.0229, 0.0176, 0.0114], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' had', ' could', ' made'])\n",
      "(tensor([0.8211, 0.0294, 0.0233, 0.0118, 0.0083], grad_fn=<ToCopyBackward0>), [' funny', ' hilarious', ' actually', ' a', ' watch'])\n",
      "(tensor([0.8068, 0.0458, 0.0295, 0.0159, 0.0095], grad_fn=<ToCopyBackward0>), ['.', ',', '!', '...', ' ('])\n",
      "(tensor([0.2310, 0.1048, 0.0938, 0.0367, 0.0312], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' That'])\n",
      "(tensor([0.3205, 0.2379, 0.0508, 0.0439, 0.0290], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' is', ' seemed'])\n",
      "(tensor([0.1452, 0.1355, 0.0991, 0.0755, 0.0494], grad_fn=<ToCopyBackward0>), [' not', ' like', ' a', ' just', ' one'])\n",
      "(tensor([0.8030, 0.1301, 0.0126, 0.0106, 0.0034], grad_fn=<ToCopyBackward0>), [' of', ' thing', ' reason', ' big', ' the'])\n",
      "(tensor([0.8375, 0.1266, 0.0255, 0.0031, 0.0009], grad_fn=<ToCopyBackward0>), [' those', ' the', ' my', ' these', ' a'])\n",
      "(tensor([0.7762, 0.1796, 0.0034, 0.0033, 0.0020], grad_fn=<ToCopyBackward0>), [' films', ' movies', ' rare', ' film', ' cases'])\n",
      "(tensor([0.5947, 0.1560, 0.0991, 0.0800, 0.0144], grad_fn=<ToCopyBackward0>), [' that', ' where', ' you', ' I', ' like'])\n",
      "(tensor([0.1989, 0.1565, 0.1127, 0.0645, 0.0507], grad_fn=<ToCopyBackward0>), [' you', ' I', ' is', \"'s\", ' people'])\n",
      "(tensor([0.1442, 0.1332, 0.0670, 0.0403, 0.0351], grad_fn=<ToCopyBackward0>), [' can', ' watch', ' just', ' think', ' sit'])\n",
      "(tensor([0.3812, 0.1982, 0.0743, 0.0530, 0.0323], grad_fn=<ToCopyBackward0>), [' watch', \"'t\", ' tell', ' sit', ' see'])\n",
      "(tensor([0.5119, 0.1576, 0.1012, 0.0725, 0.0509], grad_fn=<ToCopyBackward0>), [' through', ' down', ' there', ' and', ' around'])\n",
      "(tensor([0.3782, 0.0681, 0.0530, 0.0454, 0.0408], grad_fn=<ToCopyBackward0>), [' and', ' in', ',', ' all', ' just'])\n",
      "(tensor([0.2731, 0.0679, 0.0634, 0.0498, 0.0489], grad_fn=<ToCopyBackward0>), [' enjoy', ' think', ' be', ' laugh', ' it'])\n",
      "(tensor([0.1647, 0.1242, 0.1228, 0.0735, 0.0398], grad_fn=<ToCopyBackward0>), ['.', ',', ' the', ' it', ' because'])\n",
      "(tensor([0.1965, 0.1699, 0.0612, 0.0610, 0.0476], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' You', ' But'])\n",
      "(tensor([0.1125, 0.0635, 0.0607, 0.0596, 0.0559], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' just', ' don', ' really'])\n",
      "(tensor([0.3603, 0.1499, 0.0909, 0.0449, 0.0313], grad_fn=<ToCopyBackward0>), [' not', ' sure', ' a', ' just', ' really'])\n",
      "(tensor([0.1545, 0.1473, 0.1324, 0.1321, 0.0409], grad_fn=<ToCopyBackward0>), [' there', ' that', ' it', ' the', ' I'])\n",
      "(tensor([0.5146, 0.1616, 0.1105, 0.0338, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' will', ' would', \"'ll\"])\n",
      "(tensor([0.3802, 0.0711, 0.0615, 0.0564, 0.0518], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' entertain', ' appeal'])\n",
      "(tensor([0.1165, 0.0931, 0.0544, 0.0438, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' on', ' nominated', ' remembered', ' shown'])\n",
      "(tensor([0.1029, 0.0892, 0.0662, 0.0528, 0.0459], grad_fn=<ToCopyBackward0>), [' hit', ' cult', ' big', ' little', ' classic'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty atrocious that I was subjected to this film in public. I have never been so disappointed to have been so disappointed to have been so disappointed to have been so disappointed. I can see that this is a very sensitive subject for the parents\n",
      "(tensor([0.3838, 0.1719, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7136, 0.1163, 0.0396, 0.0101, 0.0085], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1838, 0.1459, 0.0506, 0.0453, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1138, 0.0956, 0.0750, 0.0608, 0.0494], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([9.9941e-01, 5.8884e-05, 4.6899e-05, 2.4495e-05, 1.5529e-05],\n",
      "       grad_fn=<ToCopyBackward0>), ['cious', 'phy', 'etic', 'pic', 'c'])\n",
      "(tensor([0.1344, 0.0910, 0.0805, 0.0621, 0.0619], grad_fn=<ToCopyBackward0>), [' and', ' that', ',', ' how', '.'])\n",
      "(tensor([0.1816, 0.1330, 0.1084, 0.0768, 0.0357], grad_fn=<ToCopyBackward0>), [' I', ' they', ' the', ' this', ' a'])\n",
      "(tensor([0.1319, 0.1161, 0.1071, 0.0590, 0.0409], grad_fn=<ToCopyBackward0>), [' watched', ' had', ' was', \"'m\", ' even'])\n",
      "(tensor([0.1357, 0.0436, 0.0342, 0.0277, 0.0256], grad_fn=<ToCopyBackward0>), [' subjected', ' forced', ' the', ' made', ' actually'])\n",
      "(tensor([9.9461e-01, 8.7422e-04, 5.9257e-04, 3.8287e-04, 3.3444e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' to', ' in', ' as', ' the', ' this'])\n",
      "(tensor([0.6984, 0.1237, 0.0333, 0.0188, 0.0166], grad_fn=<ToCopyBackward0>), [' this', ' the', ' such', ' that', ' a'])\n",
      "(tensor([0.3116, 0.1786, 0.0189, 0.0141, 0.0093], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' atroc', ' torture', ' humiliation'])\n",
      "(tensor([0.4230, 0.1005, 0.0910, 0.0714, 0.0288], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' for', ' with'])\n",
      "(tensor([0.3126, 0.1739, 0.1428, 0.0380, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' my', ' a', ' school', ' public'])\n",
      "(tensor([0.3197, 0.2288, 0.0308, 0.0306, 0.0286], grad_fn=<ToCopyBackward0>), ['.', ' school', ' like', ' and', ' as'])\n",
      "(tensor([0.2636, 0.1293, 0.0762, 0.0456, 0.0206], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' This'])\n",
      "(tensor([0.1481, 0.0797, 0.0693, 0.0657, 0.0589], grad_fn=<ToCopyBackward0>), [' was', ' mean', \"'m\", ' have', ' don'])\n",
      "(tensor([0.2914, 0.1982, 0.0664, 0.0459, 0.0442], grad_fn=<ToCopyBackward0>), [' to', ' a', ' never', ' no', ' been'])\n",
      "(tensor([0.3339, 0.1859, 0.0474, 0.0400, 0.0399], grad_fn=<ToCopyBackward0>), [' been', ' seen', ' had', ' in', ' walked'])\n",
      "(tensor([0.5312, 0.0708, 0.0691, 0.0475, 0.0469], grad_fn=<ToCopyBackward0>), [' so', ' more', ' in', ' a', ' to'])\n",
      "(tensor([0.3574, 0.1062, 0.0802, 0.0693, 0.0597], grad_fn=<ToCopyBackward0>), [' embarrassed', ' disappointed', ' insulted', ' offended', ' bored'])\n",
      "(tensor([0.6948, 0.0753, 0.0704, 0.0591, 0.0228], grad_fn=<ToCopyBackward0>), [' in', ' with', ' to', ' by', '.'])\n",
      "(tensor([0.4021, 0.4010, 0.0690, 0.0376, 0.0166], grad_fn=<ToCopyBackward0>), [' be', ' see', ' watch', ' have', ' go'])\n",
      "(tensor([0.1835, 0.1498, 0.1213, 0.0679, 0.0556], grad_fn=<ToCopyBackward0>), [' to', ' a', ' been', ' my', ' so'])\n",
      "(tensor([0.3397, 0.1412, 0.1087, 0.0559, 0.0222], grad_fn=<ToCopyBackward0>), [' in', ' so', ' a', ' subjected', ' on'])\n",
      "(tensor([0.1334, 0.1086, 0.0309, 0.0275, 0.0249], grad_fn=<ToCopyBackward0>), [' wrong', ' disappointed', ' publicly', ' completely', ' mis'])\n",
      "(tensor([0.6601, 0.0983, 0.0793, 0.0512, 0.0240], grad_fn=<ToCopyBackward0>), [' in', '.', ' with', ' by', ' to'])\n",
      "(tensor([0.3650, 0.3297, 0.1183, 0.0816, 0.0166], grad_fn=<ToCopyBackward0>), [' have', ' be', ' see', ' watch', ' sit'])\n",
      "(tensor([0.5061, 0.1373, 0.0552, 0.0402, 0.0316], grad_fn=<ToCopyBackward0>), [' been', ' watched', ' to', ' had', ' so'])\n",
      "(tensor([0.4316, 0.1469, 0.1394, 0.0369, 0.0130], grad_fn=<ToCopyBackward0>), [' so', ' subjected', ' in', ' made', ' to'])\n",
      "(tensor([0.9162, 0.0076, 0.0041, 0.0034, 0.0029], grad_fn=<ToCopyBackward0>), [' disappointed', ' misled', ' dis', ' disgusted', ' frustrated'])\n",
      "(tensor([0.3489, 0.2883, 0.1429, 0.0534, 0.0315], grad_fn=<ToCopyBackward0>), [' in', ' to', '.', ' by', ' with'])\n",
      "(tensor([0.5344, 0.2460, 0.0745, 0.0690, 0.0128], grad_fn=<ToCopyBackward0>), [' have', ' be', ' watch', ' see', ' sit'])\n",
      "(tensor([0.6397, 0.1402, 0.0307, 0.0286, 0.0213], grad_fn=<ToCopyBackward0>), [' been', ' watched', ' to', ' seen', ' had'])\n",
      "(tensor([0.6551, 0.0544, 0.0442, 0.0213, 0.0185], grad_fn=<ToCopyBackward0>), [' so', ' in', ' subjected', ' made', ' to'])\n",
      "(tensor([9.8662e-01, 1.5189e-03, 1.1477e-03, 9.3187e-04, 8.2601e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' disappointed', ' disgusted', ' ashamed', ' frustrated', ' angry'])\n",
      "(tensor([0.4793, 0.1786, 0.1615, 0.0228, 0.0216], grad_fn=<ToCopyBackward0>), [' to', '.', ' in', '...', ' as'])\n",
      "(tensor([0.2364, 0.1238, 0.0730, 0.0664, 0.0404], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.1419, 0.1044, 0.0654, 0.0582, 0.0538], grad_fn=<ToCopyBackward0>), [' was', ' have', ' am', \"'m\", ' can'])\n",
      "(tensor([0.3086, 0.2562, 0.0952, 0.0427, 0.0255], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' honestly', ' see', ' not'])\n",
      "(tensor([0.1765, 0.1599, 0.1249, 0.1000, 0.0814], grad_fn=<ToCopyBackward0>), [' why', ' how', ' that', ' the', ' now'])\n",
      "(tensor([0.2423, 0.1087, 0.0797, 0.0736, 0.0507], grad_fn=<ToCopyBackward0>), [' the', ' there', ' it', ' in', ' this'])\n",
      "(tensor([0.3639, 0.2706, 0.1529, 0.0886, 0.0257], grad_fn=<ToCopyBackward0>), [' is', ' film', ' movie', ' was', ' has'])\n",
      "(tensor([0.4854, 0.0978, 0.0805, 0.0755, 0.0272], grad_fn=<ToCopyBackward0>), [' a', ' an', ' one', ' the', ' not'])\n",
      "(tensor([0.1426, 0.0690, 0.0366, 0.0277, 0.0193], grad_fn=<ToCopyBackward0>), [' very', ' movie', ' film', ' commercial', ' well'])\n",
      "(tensor([0.0338, 0.0336, 0.0324, 0.0319, 0.0253], grad_fn=<ToCopyBackward0>), [' well', ' low', ' difficult', ' personal', ' sensitive'])\n",
      "(tensor([0.2152, 0.1862, 0.1253, 0.0987, 0.0946], grad_fn=<ToCopyBackward0>), [' subject', ' and', ' film', ' movie', ' time'])\n",
      "(tensor([0.6036, 0.1674, 0.0523, 0.0333, 0.0202], grad_fn=<ToCopyBackward0>), [' for', ' and', ',', ' matter', ' to'])\n",
      "(tensor([0.1439, 0.0977, 0.0750, 0.0708, 0.0624], grad_fn=<ToCopyBackward0>), [' people', ' a', ' the', ' you', ' many'])\n",
      "(tensor([0.0647, 0.0635, 0.0379, 0.0339, 0.0336], grad_fn=<ToCopyBackward0>), [' parents', ' people', ' public', ' rest', ' many'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought the movie was pretty bad. This is a good movie for those of you who really enjoyed the original. I really like the original story and I think this one is better. I really like the casting for this one and I think it is a much\n",
      "(tensor([0.3836, 0.1717, 0.0901, 0.0771, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.5011, 0.0599, 0.0341, 0.0151, 0.0146], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' DVD', ' ending', ' whole'])\n",
      "(tensor([0.6235, 0.0400, 0.0382, 0.0354, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' started'])\n",
      "(tensor([0.2583, 0.0595, 0.0528, 0.0427, 0.0413], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' terrible', ' a', ' so'])\n",
      "(tensor([0.1974, 0.1237, 0.1049, 0.0783, 0.0677], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' lame', ' bad', ' awful'])\n",
      "(tensor([0.5078, 0.1546, 0.0558, 0.0269, 0.0237], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.2509, 0.1303, 0.1200, 0.0228, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', ' This'])\n",
      "(tensor([0.3734, 0.2332, 0.1453, 0.0430, 0.0253], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' one', ' film'])\n",
      "(tensor([0.1742, 0.1618, 0.1522, 0.0571, 0.0485], grad_fn=<ToCopyBackward0>), [' not', ' a', ' the', ' probably', ' one'])\n",
      "(tensor([0.2110, 0.1427, 0.0706, 0.0512, 0.0471], grad_fn=<ToCopyBackward0>), [' movie', ' really', ' good', ' pretty', ' very'])\n",
      "(tensor([0.7469, 0.0635, 0.0221, 0.0107, 0.0087], grad_fn=<ToCopyBackward0>), [' movie', ' cast', ' film', ' family', ' thing'])\n",
      "(tensor([0.1692, 0.1351, 0.1252, 0.0980, 0.0941], grad_fn=<ToCopyBackward0>), ['.', ' if', ' for', ',', ' to'])\n",
      "(tensor([0.6562, 0.0415, 0.0408, 0.0250, 0.0246], grad_fn=<ToCopyBackward0>), [' kids', ' the', ' teenagers', ' adults', ' those'])\n",
      "(tensor([0.3941, 0.3246, 0.0545, 0.0417, 0.0245], grad_fn=<ToCopyBackward0>), [' who', ' that', ' people', ' of', ' with'])\n",
      "(tensor([8.6014e-01, 1.3254e-01, 9.6678e-04, 7.2076e-04, 7.1414e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' us', ' those', ' my', ' the'])\n",
      "(tensor([0.6223, 0.2469, 0.0248, 0.0115, 0.0093], grad_fn=<ToCopyBackward0>), [' who', ' that', ' with', ' looking', ','])\n",
      "(tensor([0.2912, 0.1147, 0.0873, 0.0713, 0.0665], grad_fn=<ToCopyBackward0>), [' like', ' enjoy', ' have', ' really', ' want'])\n",
      "(tensor([0.2080, 0.2049, 0.1772, 0.0618, 0.0585], grad_fn=<ToCopyBackward0>), [' enjoy', ' want', ' like', ' enjoyed', ' hate'])\n",
      "(tensor([0.3158, 0.0810, 0.0446, 0.0324, 0.0195], grad_fn=<ToCopyBackward0>), [' the', ' it', ' \"', ' The', ' this'])\n",
      "(tensor([0.2141, 0.2081, 0.1964, 0.0269, 0.0198], grad_fn=<ToCopyBackward0>), [' original', ' first', ' book', ' movie', ' other'])\n",
      "(tensor([0.7359, 0.0372, 0.0352, 0.0203, 0.0109], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' movie', ' with'])\n",
      "(tensor([0.2762, 0.1324, 0.1206, 0.0398, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.0747, 0.0730, 0.0614, 0.0499, 0.0474], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' really', ' just', ' think'])\n",
      "(tensor([0.0957, 0.0907, 0.0847, 0.0774, 0.0613], grad_fn=<ToCopyBackward0>), [' like', ' don', ' enjoyed', ' didn', ' liked'])\n",
      "(tensor([0.4387, 0.0704, 0.0154, 0.0149, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' how', ' \"'])\n",
      "(tensor([0.0989, 0.0923, 0.0749, 0.0566, 0.0438], grad_fn=<ToCopyBackward0>), [' idea', ' first', ' story', ' new', ' original'])\n",
      "(tensor([0.1745, 0.0488, 0.0422, 0.0417, 0.0403], grad_fn=<ToCopyBackward0>), [' story', ' but', ' too', '.', ' movie'])\n",
      "(tensor([0.2864, 0.0850, 0.0707, 0.0607, 0.0416], grad_fn=<ToCopyBackward0>), [' and', ',', ' but', ' as', ' of'])\n",
      "(tensor([0.1106, 0.1068, 0.0823, 0.0389, 0.0354], grad_fn=<ToCopyBackward0>), [' I', ' the', ' movie', ' plot', ' director'])\n",
      "(tensor([0.1861, 0.1567, 0.0920, 0.0649, 0.0538], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' think', ' was', ' like'])\n",
      "(tensor([0.3805, 0.1655, 0.1550, 0.1066, 0.0187], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' that', ' they'])\n",
      "(tensor([0.3385, 0.2941, 0.1221, 0.0545, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' one', ' was', ' film'])\n",
      "(tensor([0.3746, 0.1438, 0.0575, 0.0488, 0.0368], grad_fn=<ToCopyBackward0>), [' is', ' was', ' could', ' has', ' does'])\n",
      "(tensor([0.4090, 0.1603, 0.0522, 0.0509, 0.0277], grad_fn=<ToCopyBackward0>), [' better', ' a', ' more', ' pretty', ' much'])\n",
      "(tensor([0.6762, 0.1645, 0.0276, 0.0261, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' than', ',', ' but', ' in'])\n",
      "(tensor([0.2489, 0.1184, 0.0995, 0.0517, 0.0285], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' But', ' There'])\n",
      "(tensor([0.1085, 0.0620, 0.0600, 0.0569, 0.0557], grad_fn=<ToCopyBackward0>), [' think', ' really', \"'m\", ' just', ' don'])\n",
      "(tensor([0.1888, 0.0978, 0.0680, 0.0640, 0.0596], grad_fn=<ToCopyBackward0>), [' like', ' don', ' wish', ' liked', ' think'])\n",
      "(tensor([0.6321, 0.0254, 0.0159, 0.0106, 0.0097], grad_fn=<ToCopyBackward0>), [' the', ' it', ' how', ' this', ' that'])\n",
      "(tensor([0.0875, 0.0633, 0.0573, 0.0537, 0.0457], grad_fn=<ToCopyBackward0>), [' casting', ' cast', ' story', ' acting', ' new'])\n",
      "(tensor([0.2738, 0.1318, 0.1007, 0.0566, 0.0465], grad_fn=<ToCopyBackward0>), [' of', ' in', ' and', ' too', ' for'])\n",
      "(tensor([0.3788, 0.1435, 0.0187, 0.0079, 0.0077], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' G', ' Jessica'])\n",
      "(tensor([0.7010, 0.1831, 0.0345, 0.0283, 0.0179], grad_fn=<ToCopyBackward0>), [' one', ' movie', ' and', '.', ','])\n",
      "(tensor([0.4327, 0.2300, 0.0798, 0.0694, 0.0642], grad_fn=<ToCopyBackward0>), [' too', '.', ' as', ',', ' and'])\n",
      "(tensor([0.4664, 0.2025, 0.0545, 0.0194, 0.0085], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' they', ' that'])\n",
      "(tensor([0.3213, 0.1939, 0.0997, 0.0439, 0.0276], grad_fn=<ToCopyBackward0>), [' think', ' really', ' thought', ' like', ' hope'])\n",
      "(tensor([0.3384, 0.3355, 0.0716, 0.0391, 0.0303], grad_fn=<ToCopyBackward0>), [' it', ' the', ' they', ' that', ' this'])\n",
      "(tensor([0.2850, 0.1752, 0.1660, 0.0734, 0.0211], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' really', ' will'])\n",
      "(tensor([0.2311, 0.2065, 0.1165, 0.0559, 0.0470], grad_fn=<ToCopyBackward0>), [' a', ' better', ' pretty', ' more', ' the'])\n",
      "(tensor([0.2433, 0.1814, 0.1118, 0.1057, 0.0571], grad_fn=<ToCopyBackward0>), [' good', ' better', ' great', ' pretty', ' much'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought it was pretty bad, but I just didn't think it was as bad as this movie. The acting is terrible, the plot is ridiculous, the script is ridiculous. This movie is just stupid. I was really surprised by it. It's just\n",
      "(tensor([0.3825, 0.1727, 0.0906, 0.0771, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.7139, 0.1161, 0.0394, 0.0101, 0.0086], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', ' could', ' sounded'])\n",
      "(tensor([0.1831, 0.1470, 0.0505, 0.0452, 0.0433], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' one', ' funny', ' the'])\n",
      "(tensor([0.1140, 0.0957, 0.0751, 0.0602, 0.0493], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' atro', ' boring'])\n",
      "(tensor([0.1290, 0.0995, 0.0796, 0.0609, 0.0476], grad_fn=<ToCopyBackward0>), [' when', ' that', '.', ',', ' news'])\n",
      "(tensor([0.3819, 0.0675, 0.0559, 0.0358, 0.0186], grad_fn=<ToCopyBackward0>), [' but', ' the', ' and', ' so', ' as'])\n",
      "(tensor([0.4010, 0.0784, 0.0411, 0.0390, 0.0324], grad_fn=<ToCopyBackward0>), [' I', ' it', ' after', ' then', ' the'])\n",
      "(tensor([0.1265, 0.0675, 0.0664, 0.0522, 0.0514], grad_fn=<ToCopyBackward0>), [' was', ' didn', ' guess', \"'m\", ' just'])\n",
      "(tensor([0.1612, 0.0840, 0.0643, 0.0513, 0.0460], grad_fn=<ToCopyBackward0>), [' thought', ' didn', ' don', ' couldn', ' laughed'])\n",
      "(tensor([9.9392e-01, 2.5724e-03, 7.3540e-04, 4.6992e-04, 2.1310e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', ';'])\n",
      "(tensor([0.2744, 0.1587, 0.0983, 0.0870, 0.0526], grad_fn=<ToCopyBackward0>), [' think', ' know', ' want', ' expect', ' have'])\n",
      "(tensor([0.7748, 0.0523, 0.0341, 0.0242, 0.0151], grad_fn=<ToCopyBackward0>), [' it', ' anything', ' that', ' they', ' there'])\n",
      "(tensor([0.6860, 0.1732, 0.1196, 0.0029, 0.0019], grad_fn=<ToCopyBackward0>), [' was', ' could', ' would', ' had', \"'d\"])\n",
      "(tensor([0.3436, 0.2614, 0.0415, 0.0362, 0.0296], grad_fn=<ToCopyBackward0>), [' as', ' going', ' funny', ' that', ' so'])\n",
      "(tensor([9.8072e-01, 3.2749e-03, 1.6218e-03, 9.0261e-04, 7.2556e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' bad', ' good', ' big', ' much', ' g'])\n",
      "(tensor([9.9283e-01, 8.8062e-04, 8.5605e-04, 8.1619e-04, 4.1351e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' as', ' a', ' it', ' or', ' in'])\n",
      "(tensor([0.4878, 0.1408, 0.1043, 0.0372, 0.0346], grad_fn=<ToCopyBackward0>), [' it', ' this', ' I', ' the', ' some'])\n",
      "(tensor([0.5142, 0.0740, 0.0671, 0.0653, 0.0080], grad_fn=<ToCopyBackward0>), [' movie', ' one', '.', ' film', '....'])\n",
      "(tensor([0.5973, 0.0809, 0.0678, 0.0430, 0.0346], grad_fn=<ToCopyBackward0>), ['.', ' is', ' was', ' makes', '....'])\n",
      "(tensor([0.3488, 0.1170, 0.0939, 0.0590, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' And'])\n",
      "(tensor([0.2233, 0.2183, 0.0524, 0.0356, 0.0229], grad_fn=<ToCopyBackward0>), [' movie', ' acting', ' story', ' only', ' worst'])\n",
      "(tensor([0.5495, 0.1604, 0.1023, 0.0571, 0.0291], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' in', ' and'])\n",
      "(tensor([0.1191, 0.1041, 0.0854, 0.0713, 0.0704], grad_fn=<ToCopyBackward0>), [' terrible', ' pretty', ' so', ' bad', ' horrible'])\n",
      "(tensor([0.5574, 0.1742, 0.1342, 0.0394, 0.0112], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' in', ';'])\n",
      "(tensor([0.6140, 0.1070, 0.0759, 0.0159, 0.0153], grad_fn=<ToCopyBackward0>), [' the', ' but', ' and', ' especially', ' I'])\n",
      "(tensor([0.3787, 0.2168, 0.0895, 0.0222, 0.0187], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' writing', ' directing'])\n",
      "(tensor([0.6185, 0.0510, 0.0426, 0.0205, 0.0193], grad_fn=<ToCopyBackward0>), [' is', ' was', ' makes', ' and', ','])\n",
      "(tensor([0.1003, 0.0894, 0.0618, 0.0482, 0.0423], grad_fn=<ToCopyBackward0>), [' ridiculous', ' terrible', ' even', ' just', ' stupid'])\n",
      "(tensor([0.4276, 0.4093, 0.0929, 0.0168, 0.0071], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' (', '...'])\n",
      "(tensor([0.4886, 0.3552, 0.0296, 0.0218, 0.0198], grad_fn=<ToCopyBackward0>), [' and', ' the', ' but', ' it', ' there'])\n",
      "(tensor([0.1751, 0.0600, 0.0452, 0.0431, 0.0409], grad_fn=<ToCopyBackward0>), [' special', ' editing', ' ending', ' script', ' acting'])\n",
      "(tensor([0.7277, 0.0484, 0.0321, 0.0180, 0.0161], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' makes', ' and'])\n",
      "(tensor([0.1567, 0.0601, 0.0488, 0.0443, 0.0413], grad_fn=<ToCopyBackward0>), [' ridiculous', ' terrible', ' awful', ' stupid', ' just'])\n",
      "(tensor([0.5285, 0.2214, 0.1700, 0.0135, 0.0110], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' ('])\n",
      "(tensor([0.2720, 0.1334, 0.1303, 0.0405, 0.0298], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' And'])\n",
      "(tensor([0.4207, 0.3291, 0.0640, 0.0239, 0.0217], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' thing', ' has'])\n",
      "(tensor([0.3904, 0.0942, 0.0677, 0.0584, 0.0277], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' makes', ' should'])\n",
      "(tensor([0.2509, 0.1483, 0.0951, 0.0619, 0.0395], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' not', ' like'])\n",
      "(tensor([0.1756, 0.0674, 0.0605, 0.0497, 0.0496], grad_fn=<ToCopyBackward0>), [' a', ' terrible', ' so', ' ridiculous', ' stupid'])\n",
      "(tensor([0.3162, 0.2921, 0.1500, 0.0413, 0.0329], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' as'])\n",
      "(tensor([0.2461, 0.1262, 0.1175, 0.0259, 0.0245], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' There', ' And'])\n",
      "(tensor([0.0981, 0.0864, 0.0830, 0.0558, 0.0479], grad_fn=<ToCopyBackward0>), [\"'m\", ' can', ' don', ' mean', ' was'])\n",
      "(tensor([0.2211, 0.1167, 0.0416, 0.0348, 0.0323], grad_fn=<ToCopyBackward0>), [' really', ' actually', ' just', ' very', ' so'])\n",
      "(tensor([0.3002, 0.1691, 0.1380, 0.0518, 0.0303], grad_fn=<ToCopyBackward0>), [' surprised', ' disappointed', ' looking', ',', ' hoping'])\n",
      "(tensor([0.2744, 0.1854, 0.1495, 0.0966, 0.0851], grad_fn=<ToCopyBackward0>), [' by', ' at', ' that', ' when', ' to'])\n",
      "(tensor([0.2300, 0.2116, 0.1752, 0.1646, 0.1567], grad_fn=<ToCopyBackward0>), [' the', ' this', ' how', ' it', ' that'])\n",
      "(tensor([0.6165, 0.1483, 0.0642, 0.0182, 0.0179], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' too', ' when'])\n",
      "(tensor([0.4631, 0.1288, 0.0592, 0.0243, 0.0190], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.4772, 0.1149, 0.0818, 0.0320, 0.0226], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' just'])\n",
      "(tensor([0.2541, 0.1551, 0.0927, 0.0700, 0.0508], grad_fn=<ToCopyBackward0>), [' not', ' just', ' like', ' a', ' one'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was a good movie. I thought this movie was pretty funny and pretty interesting and I really liked it and that's what counts I guess. I mean, if you have to give it a rating... I guess it's a 3. It\n",
      "(tensor([0.3840, 0.1721, 0.0902, 0.0769, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4369, 0.2442, 0.1965, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7770, 0.0502, 0.0283, 0.0262, 0.0100], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1276, 0.0770, 0.0661, 0.0556, 0.0421], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.0788, 0.0623, 0.0522, 0.0454, 0.0446], grad_fn=<ToCopyBackward0>), [' very', ' waste', ' good', ' great', ' really'])\n",
      "(tensor([0.1428, 0.1274, 0.1010, 0.0736, 0.0526], grad_fn=<ToCopyBackward0>), [' movie', ' idea', ' one', ' film', ' way'])\n",
      "(tensor([0.3473, 0.0974, 0.0895, 0.0643, 0.0426], grad_fn=<ToCopyBackward0>), ['.', '...', ',', '....', '!'])\n",
      "(tensor([0.3258, 0.2215, 0.0889, 0.0332, 0.0159], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.1989, 0.1136, 0.0601, 0.0521, 0.0468], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', ' think', ' just'])\n",
      "(tensor([0.5857, 0.1807, 0.0680, 0.0612, 0.0097], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', ' I'])\n",
      "(tensor([0.3818, 0.3420, 0.1154, 0.0523, 0.0084], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' is', ' one'])\n",
      "(tensor([0.7894, 0.0359, 0.0158, 0.0149, 0.0143], grad_fn=<ToCopyBackward0>), [' was', ' had', ' is', ' did', ' could'])\n",
      "(tensor([0.1701, 0.0884, 0.0710, 0.0474, 0.0457], grad_fn=<ToCopyBackward0>), [' a', ' pretty', ' boring', ' funny', ' bad'])\n",
      "(tensor([0.4902, 0.1040, 0.0665, 0.0388, 0.0377], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' cool', ' bad', ' entertaining'])\n",
      "(tensor([0.5999, 0.1552, 0.0979, 0.0198, 0.0159], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' too', ' but'])\n",
      "(tensor([0.3388, 0.2381, 0.0553, 0.0441, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' pretty', ' interesting', ' the', ' funny'])\n",
      "(tensor([0.1896, 0.0987, 0.0887, 0.0617, 0.0329], grad_fn=<ToCopyBackward0>), [' entertaining', ' interesting', ' funny', ' good', ' well'])\n",
      "(tensor([0.4280, 0.3663, 0.0773, 0.0177, 0.0173], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' to', ' but'])\n",
      "(tensor([0.3605, 0.1531, 0.0689, 0.0385, 0.0344], grad_fn=<ToCopyBackward0>), [' pretty', ' I', ' interesting', ' the', ' good'])\n",
      "(tensor([0.3856, 0.1071, 0.0528, 0.0481, 0.0465], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' liked', ' was', ' think'])\n",
      "(tensor([0.1847, 0.1666, 0.1445, 0.0803, 0.0592], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' enjoyed', ' wanted', ' didn'])\n",
      "(tensor([0.3652, 0.2517, 0.0225, 0.0110, 0.0102], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' how', ' seeing'])\n",
      "(tensor([0.7226, 0.0834, 0.0313, 0.0280, 0.0158], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '...'])\n",
      "(tensor([0.6799, 0.0553, 0.0423, 0.0391, 0.0195], grad_fn=<ToCopyBackward0>), [' I', ' it', ' that', ' the', ' so'])\n",
      "(tensor([0.5167, 0.2680, 0.0637, 0.0113, 0.0104], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' it', ' made'])\n",
      "(tensor([0.2515, 0.2509, 0.0665, 0.0630, 0.0607], grad_fn=<ToCopyBackward0>), [' all', ' why', ' pretty', ' what', ' always'])\n",
      "(tensor([0.3629, 0.0548, 0.0436, 0.0415, 0.0412], grad_fn=<ToCopyBackward0>), [' I', ' really', ' kept', ' it', ' counts'])\n",
      "(tensor([0.6578, 0.0561, 0.0497, 0.0398, 0.0367], grad_fn=<ToCopyBackward0>), ['.', ',', ' for', ' in', ' I'])\n",
      "(tensor([0.8989, 0.0648, 0.0172, 0.0032, 0.0018], grad_fn=<ToCopyBackward0>), [' guess', ' suppose', ' think', ' don', ' really'])\n",
      "(tensor([0.7814, 0.0738, 0.0151, 0.0133, 0.0105], grad_fn=<ToCopyBackward0>), ['.', ',', ' I', '...', ' and'])\n",
      "(tensor([0.2260, 0.0921, 0.0601, 0.0407, 0.0306], grad_fn=<ToCopyBackward0>), [' I', ' But', ' It', ' The', ' And'])\n",
      "(tensor([0.0775, 0.0773, 0.0720, 0.0696, 0.0655], grad_fn=<ToCopyBackward0>), [' really', ' mean', \"'m\", ' was', ' just'])\n",
      "(tensor([0.1782, 0.1473, 0.1163, 0.0851, 0.0470], grad_fn=<ToCopyBackward0>), [',', ' it', ' I', ' the', ' if'])\n",
      "(tensor([0.2793, 0.1301, 0.0731, 0.0590, 0.0469], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' the', ' this'])\n",
      "(tensor([0.6691, 0.1204, 0.0852, 0.0214, 0.0172], grad_fn=<ToCopyBackward0>), [' you', ' I', ' it', ' people', ' the'])\n",
      "(tensor([0.1827, 0.1540, 0.0905, 0.0694, 0.0415], grad_fn=<ToCopyBackward0>), [' like', \"'re\", ' have', ' make', ' don'])\n",
      "(tensor([0.2765, 0.2426, 0.0478, 0.0393, 0.0326], grad_fn=<ToCopyBackward0>), [' to', ' a', ' an', ' something', ' the'])\n",
      "(tensor([0.3523, 0.0828, 0.0505, 0.0438, 0.0412], grad_fn=<ToCopyBackward0>), [' give', ' be', ' have', ' say', ' tell'])\n",
      "(tensor([0.2630, 0.1608, 0.1185, 0.0919, 0.0408], grad_fn=<ToCopyBackward0>), [' a', ' it', ' your', ' something', ' an'])\n",
      "(tensor([0.4540, 0.2603, 0.0738, 0.0436, 0.0202], grad_fn=<ToCopyBackward0>), [' a', ' away', ' to', ' an', ' some'])\n",
      "(tensor([0.1875, 0.0517, 0.0477, 0.0416, 0.0402], grad_fn=<ToCopyBackward0>), [' rating', ' 1', ' vote', ' 10', ' score'])\n",
      "(tensor([0.3897, 0.0854, 0.0696, 0.0636, 0.0558], grad_fn=<ToCopyBackward0>), [',', '...', ' I', ' you', ' it'])\n",
      "(tensor([0.0625, 0.0504, 0.0379, 0.0338, 0.0302], grad_fn=<ToCopyBackward0>), ['I', ' I', 'it', 'this', 'the'])\n",
      "(tensor([0.1429, 0.0998, 0.0730, 0.0659, 0.0552], grad_fn=<ToCopyBackward0>), [' mean', ' guess', ' don', \"'m\", ' really'])\n",
      "(tensor([0.2146, 0.2054, 0.0674, 0.0617, 0.0570], grad_fn=<ToCopyBackward0>), [' I', ' it', ' if', ' you', ','])\n",
      "(tensor([0.6243, 0.0734, 0.0442, 0.0336, 0.0196], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' would', ' could', ' has'])\n",
      "(tensor([0.6347, 0.0402, 0.0390, 0.0247, 0.0184], grad_fn=<ToCopyBackward0>), [' a', '...', ' like', ' one', ' gonna'])\n",
      "(tensor([0.3324, 0.0858, 0.0810, 0.0758, 0.0617], grad_fn=<ToCopyBackward0>), [' 4', ' 1', ' 2', ' 3', ' 7'])\n",
      "(tensor([0.5006, 0.0659, 0.0605, 0.0492, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ' because', ',', ' or', ' out'])\n",
      "(tensor([0.1177, 0.0854, 0.0803, 0.0586, 0.0526], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' And', ' The'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was a bad joke from the start. The idea that the U.S.A. is in the middle of a war is not only insulting but also stupid. The acting was horrible and the story was stupid. I was expecting a lot\n",
      "(tensor([0.3831, 0.1726, 0.0904, 0.0772, 0.0470], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4370, 0.2453, 0.1957, 0.0165, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6533, 0.0595, 0.0361, 0.0355, 0.0261], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1375, 0.0701, 0.0664, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.0843, 0.0617, 0.0526, 0.0430, 0.0406], grad_fn=<ToCopyBackward0>), [' good', ' joke', ' bad', ' waste', ' big'])\n",
      "(tensor([0.2204, 0.1733, 0.1359, 0.0941, 0.0499], grad_fn=<ToCopyBackward0>), [' idea', ' joke', ' copy', ' rip', ' movie'])\n",
      "(tensor([0.2679, 0.1327, 0.1157, 0.0396, 0.0387], grad_fn=<ToCopyBackward0>), [' from', ' when', '.', ' until', ' on'])\n",
      "(tensor([0.7962, 0.0363, 0.0109, 0.0103, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' start', ' day', ' beginning', ' conception'])\n",
      "(tensor([0.2804, 0.2572, 0.2045, 0.0717, 0.0469], grad_fn=<ToCopyBackward0>), [' get', ' start', ' beginning', ' word', ' very'])\n",
      "(tensor([0.7963, 0.0906, 0.0143, 0.0128, 0.0117], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', '!'])\n",
      "(tensor([0.2011, 0.1785, 0.1209, 0.0242, 0.0164], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' What'])\n",
      "(tensor([0.1104, 0.0741, 0.0606, 0.0362, 0.0343], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' premise', ' idea'])\n",
      "(tensor([0.3084, 0.2165, 0.1754, 0.1152, 0.0301], grad_fn=<ToCopyBackward0>), [' was', ' of', ' that', ' is', ' behind'])\n",
      "(tensor([0.1889, 0.0671, 0.0324, 0.0323, 0.0224], grad_fn=<ToCopyBackward0>), [' a', ' the', ' it', ' they', ' this'])\n",
      "(tensor([0.0952, 0.0520, 0.0469, 0.0446, 0.0375], grad_fn=<ToCopyBackward0>), [' US', ' government', ' CIA', ' guy', ' U'])\n",
      "(tensor([9.8095e-01, 2.5514e-03, 1.6330e-03, 9.2014e-04, 7.1674e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['.', ' of', 'gly', '-', 'men'])\n",
      "(tensor([0.9538, 0.0367, 0.0037, 0.0014, 0.0013], grad_fn=<ToCopyBackward0>), ['S', 'N', ' S', 'F', 'K'])\n",
      "(tensor([9.9326e-01, 7.6604e-04, 7.3731e-04, 5.9985e-04, 3.1816e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['.', ' is', '.,', '-', ','])\n",
      "(tensor([0.1570, 0.1366, 0.0576, 0.0363, 0.0268], grad_fn=<ToCopyBackward0>), [' military', 'A', 'S', ' Army', ' is'])\n",
      "(tensor([0.9515, 0.0116, 0.0068, 0.0055, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' is', ' would', ' was', ','])\n",
      "(tensor([0.1619, 0.1246, 0.0722, 0.0702, 0.0670], grad_fn=<ToCopyBackward0>), [' is', ' has', ' (', ' can', ' needs'])\n",
      "(tensor([0.2369, 0.0679, 0.0651, 0.0636, 0.0491], grad_fn=<ToCopyBackward0>), [' so', ' in', ' a', ' going', ' the'])\n",
      "(tensor([0.1604, 0.1191, 0.1110, 0.0818, 0.0516], grad_fn=<ToCopyBackward0>), [' a', ' the', ' some', ' fact', ' trouble'])\n",
      "(tensor([0.6108, 0.0279, 0.0231, 0.0193, 0.0180], grad_fn=<ToCopyBackward0>), [' middle', ' process', ' midst', ' Middle', ' early'])\n",
      "(tensor([0.9339, 0.0378, 0.0060, 0.0051, 0.0024], grad_fn=<ToCopyBackward0>), [' of', ' east', ' East', ' and', '-'])\n",
      "(tensor([0.3163, 0.1219, 0.1072, 0.0821, 0.0691], grad_fn=<ToCopyBackward0>), [' a', ' World', ' WW', ' the', ' some'])\n",
      "(tensor([0.5850, 0.0648, 0.0344, 0.0154, 0.0139], grad_fn=<ToCopyBackward0>), [' war', ' \"', ' civil', ' battle', \" '\"])\n",
      "(tensor([0.2336, 0.0965, 0.0823, 0.0721, 0.0607], grad_fn=<ToCopyBackward0>), [' with', ' and', ' is', ' against', ','])\n",
      "(tensor([0.1678, 0.1042, 0.0703, 0.0659, 0.0489], grad_fn=<ToCopyBackward0>), [' not', ' laughable', ' ridiculous', ' just', ' so'])\n",
      "(tensor([0.3575, 0.1393, 0.1137, 0.0459, 0.0426], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' only', ' a', ' really'])\n",
      "(tensor([0.1462, 0.0933, 0.0807, 0.0745, 0.0622], grad_fn=<ToCopyBackward0>), [' ridiculous', ' absurd', ' stupid', ' laughable', ' insulting'])\n",
      "(tensor([0.6191, 0.1710, 0.0862, 0.0570, 0.0093], grad_fn=<ToCopyBackward0>), [' to', ',', ' but', ' and', '.'])\n",
      "(tensor([0.0813, 0.0760, 0.0640, 0.0548, 0.0546], grad_fn=<ToCopyBackward0>), [' laughable', ' insulting', ' also', ' prep', ' stupid'])\n",
      "(tensor([0.1619, 0.0744, 0.0619, 0.0480, 0.0454], grad_fn=<ToCopyBackward0>), [' laughable', ' stupid', ' insulting', ' ridiculous', ' prep'])\n",
      "(tensor([0.7694, 0.0554, 0.0329, 0.0234, 0.0138], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' to'])\n",
      "(tensor([0.2701, 0.1216, 0.0494, 0.0346, 0.0252], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' Why', ' There'])\n",
      "(tensor([0.2607, 0.0678, 0.0495, 0.0481, 0.0268], grad_fn=<ToCopyBackward0>), [' idea', ' fact', ' acting', ' war', ' only'])\n",
      "(tensor([0.5268, 0.2068, 0.0488, 0.0355, 0.0252], grad_fn=<ToCopyBackward0>), [' was', ' is', ' and', ',', ' in'])\n",
      "(tensor([0.0947, 0.0943, 0.0747, 0.0566, 0.0490], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' so', ' horrible'])\n",
      "(tensor([0.3147, 0.2484, 0.2482, 0.0423, 0.0201], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' as', ' too'])\n",
      "(tensor([0.6368, 0.0914, 0.0269, 0.0255, 0.0143], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' there', ' they'])\n",
      "(tensor([0.3504, 0.1639, 0.0898, 0.0342, 0.0263], grad_fn=<ToCopyBackward0>), [' plot', ' story', ' script', ' storyline', ' dialogue'])\n",
      "(tensor([0.4756, 0.0437, 0.0304, 0.0263, 0.0215], grad_fn=<ToCopyBackward0>), [' was', ' line', ' just', ' is', ' seemed'])\n",
      "(tensor([0.0813, 0.0566, 0.0521, 0.0482, 0.0461], grad_fn=<ToCopyBackward0>), [' stupid', ' so', ' not', ' just', ' even'])\n",
      "(tensor([0.6829, 0.1504, 0.0532, 0.0194, 0.0188], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' as', ' too'])\n",
      "(tensor([0.2732, 0.2237, 0.0525, 0.0307, 0.0192], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'I'])\n",
      "(tensor([0.0834, 0.0789, 0.0771, 0.0449, 0.0443], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' don', ' would', ' really'])\n",
      "(tensor([0.1027, 0.0730, 0.0720, 0.0684, 0.0434], grad_fn=<ToCopyBackward0>), [' not', ' so', ' really', ' very', ' expecting'])\n",
      "(tensor([0.5387, 0.1095, 0.0679, 0.0562, 0.0377], grad_fn=<ToCopyBackward0>), [' a', ' more', ' something', ' some', ' to'])\n",
      "(tensor([0.1794, 0.1296, 0.1124, 0.0850, 0.0806], grad_fn=<ToCopyBackward0>), [' better', ' lot', ' good', ' movie', ' real'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was terrible. I was really disappointed. The only reason I watched it was because it's the last movie in the series, and I thought it might make a good movie for a Halloween party. But it's not even funny, it was\n",
      "(tensor([0.3842, 0.1714, 0.0899, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4382, 0.2437, 0.1956, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6520, 0.0597, 0.0362, 0.0355, 0.0263], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1369, 0.0703, 0.0660, 0.0549, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.4709, 0.1414, 0.0500, 0.0453, 0.0377], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', ' when'])\n",
      "(tensor([0.3021, 0.1328, 0.1062, 0.0267, 0.0193], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'I'])\n",
      "(tensor([0.0939, 0.0804, 0.0794, 0.0625, 0.0457], grad_fn=<ToCopyBackward0>), [' thought', ' really', ' was', \"'m\", ' have'])\n",
      "(tensor([0.3007, 0.0942, 0.0737, 0.0569, 0.0554], grad_fn=<ToCopyBackward0>), [' really', ' very', ' so', ' in', ' actually'])\n",
      "(tensor([0.2353, 0.1590, 0.0865, 0.0732, 0.0358], grad_fn=<ToCopyBackward0>), [' disappointed', ' looking', ',', ' surprised', ' excited'])\n",
      "(tensor([0.4088, 0.1697, 0.1310, 0.1051, 0.0574], grad_fn=<ToCopyBackward0>), ['.', ' with', ' in', ' by', ' when'])\n",
      "(tensor([0.3614, 0.1403, 0.1087, 0.0231, 0.0222], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' This'])\n",
      "(tensor([0.1558, 0.1000, 0.0836, 0.0695, 0.0479], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' movie', ' story', ' plot'])\n",
      "(tensor([0.2823, 0.1977, 0.1301, 0.0436, 0.0370], grad_fn=<ToCopyBackward0>), [' thing', ' reason', ' good', ' funny', ' part'])\n",
      "(tensor([0.7831, 0.0624, 0.0368, 0.0344, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' why', ' that', ' i', ' this'])\n",
      "(tensor([0.3920, 0.0744, 0.0559, 0.0500, 0.0434], grad_fn=<ToCopyBackward0>), [' watched', ' even', \"'m\", ' gave', ' didn'])\n",
      "(tensor([0.8642, 0.0744, 0.0438, 0.0073, 0.0009], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' that', ' a'])\n",
      "(tensor([0.6311, 0.1526, 0.0379, 0.0237, 0.0132], grad_fn=<ToCopyBackward0>), [' was', ' is', ',', ' again', ' in'])\n",
      "(tensor([0.6426, 0.0866, 0.0565, 0.0386, 0.0379], grad_fn=<ToCopyBackward0>), [' because', ' to', ' I', ' that', ' for'])\n",
      "(tensor([0.4977, 0.0825, 0.0801, 0.0796, 0.0654], grad_fn=<ToCopyBackward0>), [' I', ' the', ' my', ' of', ' it'])\n",
      "(tensor([0.4043, 0.1265, 0.0637, 0.0458, 0.0447], grad_fn=<ToCopyBackward0>), [' was', ' starred', \"'s\", ' had', ' stars'])\n",
      "(tensor([0.1405, 0.0926, 0.0687, 0.0655, 0.0404], grad_fn=<ToCopyBackward0>), [' a', ' the', ' on', ' been', ' so'])\n",
      "(tensor([0.4447, 0.0554, 0.0524, 0.0444, 0.0363], grad_fn=<ToCopyBackward0>), [' only', ' last', ' movie', ' first', ' M'])\n",
      "(tensor([0.6309, 0.0884, 0.0189, 0.0141, 0.0129], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' Batman', ' sequel', ' time'])\n",
      "(tensor([0.4644, 0.1174, 0.0896, 0.0319, 0.0308], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' on', ' of'])\n",
      "(tensor([0.8441, 0.0462, 0.0213, 0.0127, 0.0077], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' this', ' what'])\n",
      "(tensor([0.4637, 0.0231, 0.0219, 0.0188, 0.0179], grad_fn=<ToCopyBackward0>), [' series', ' first', ' last', ' trilogy', ' 3'])\n",
      "(tensor([0.3090, 0.2074, 0.1201, 0.0807, 0.0492], grad_fn=<ToCopyBackward0>), ['.', ',', ' I', ' and', ' ('])\n",
      "(tensor([0.4480, 0.2253, 0.1519, 0.0266, 0.0127], grad_fn=<ToCopyBackward0>), [' and', ' so', ' but', ' I', ' the'])\n",
      "(tensor([0.6586, 0.0832, 0.0499, 0.0261, 0.0188], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' my', ' they'])\n",
      "(tensor([0.1347, 0.1217, 0.0880, 0.0784, 0.0473], grad_fn=<ToCopyBackward0>), [' was', ' really', ' thought', \"'m\", ' wanted'])\n",
      "(tensor([0.4693, 0.1514, 0.0896, 0.0701, 0.0655], grad_fn=<ToCopyBackward0>), [' it', ' maybe', ' I', ' the', ' that'])\n",
      "(tensor([0.4017, 0.2784, 0.1401, 0.0458, 0.0277], grad_fn=<ToCopyBackward0>), [' would', ' was', ' might', ' could', ' had'])\n",
      "(tensor([0.6216, 0.1414, 0.0357, 0.0337, 0.0202], grad_fn=<ToCopyBackward0>), [' be', ' get', ' make', ' have', ' give'])\n",
      "(tensor([0.3388, 0.1494, 0.1345, 0.0592, 0.0436], grad_fn=<ToCopyBackward0>), [' a', ' some', ' me', ' for', ' up'])\n",
      "(tensor([0.4993, 0.0612, 0.0434, 0.0321, 0.0302], grad_fn=<ToCopyBackward0>), [' good', ' great', ' comeback', ' better', ' movie'])\n",
      "(tensor([0.1337, 0.0556, 0.0543, 0.0478, 0.0458], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' one', ' comedy'])\n",
      "(tensor([0.1986, 0.1478, 0.0770, 0.0601, 0.0513], grad_fn=<ToCopyBackward0>), [' if', ' for', '.', '...', ','])\n",
      "(tensor([0.3566, 0.1752, 0.0574, 0.0383, 0.0190], grad_fn=<ToCopyBackward0>), [' a', ' the', ' Halloween', ' my', ' people'])\n",
      "(tensor([0.0823, 0.0761, 0.0478, 0.0366, 0.0359], grad_fn=<ToCopyBackward0>), [' Halloween', ' kids', ' rainy', ' school', ' college'])\n",
      "(tensor([0.3162, 0.0453, 0.0422, 0.0416, 0.0381], grad_fn=<ToCopyBackward0>), [' party', ' special', ' movie', ' treat', ' night'])\n",
      "(tensor([0.3414, 0.0493, 0.0489, 0.0476, 0.0439], grad_fn=<ToCopyBackward0>), ['.', ' when', ',', ' if', ' where'])\n",
      "(tensor([0.3231, 0.1209, 0.0917, 0.0502, 0.0262], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', 'I'])\n",
      "(tensor([0.2461, 0.1570, 0.1365, 0.0310, 0.0259], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' after', ' that'])\n",
      "(tensor([0.3685, 0.2258, 0.0740, 0.0558, 0.0546], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' didn', ' really'])\n",
      "(tensor([0.4613, 0.1884, 0.0840, 0.0402, 0.0324], grad_fn=<ToCopyBackward0>), [' not', ' just', ' really', ' a', ' so'])\n",
      "(tensor([0.5314, 0.1108, 0.0726, 0.0329, 0.0321], grad_fn=<ToCopyBackward0>), [' even', ' funny', ' a', '.', ' entertaining'])\n",
      "(tensor([0.2223, 0.1713, 0.1333, 0.0934, 0.0599], grad_fn=<ToCopyBackward0>), [' funny', ' worth', ' entertaining', ' good', ' a'])\n",
      "(tensor([0.5088, 0.1850, 0.0412, 0.0268, 0.0239], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', ' in'])\n",
      "(tensor([0.3714, 0.1504, 0.0674, 0.0542, 0.0403], grad_fn=<ToCopyBackward0>), [' it', ' not', ' and', ' the', ' I'])\n",
      "(tensor([0.9517, 0.0102, 0.0078, 0.0049, 0.0036], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' doesn', ' has'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that the movie had a pretty good story to tell. It had a great cast. It had good acting. But when I watched it, I found that the movie had no humor. I found that the story just seemed to be telling the same jokes\n",
      "(tensor([0.3839, 0.1719, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3323, 0.2361, 0.0582, 0.0561, 0.0228], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2612, 0.0541, 0.0274, 0.0218, 0.0214], grad_fn=<ToCopyBackward0>), [' movie', ' real', ' first', ' film', ' main'])\n",
      "(tensor([0.5319, 0.0603, 0.0538, 0.0231, 0.0204], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' should'])\n",
      "(tensor([0.2545, 0.1236, 0.0659, 0.0474, 0.0430], grad_fn=<ToCopyBackward0>), [' a', ' to', ' some', ' very', ' been'])\n",
      "(tensor([0.3698, 0.0690, 0.0638, 0.0569, 0.0384], grad_fn=<ToCopyBackward0>), [' pretty', ' very', ' really', ' lot', ' good'])\n",
      "(tensor([0.7157, 0.0511, 0.0280, 0.0280, 0.0241], grad_fn=<ToCopyBackward0>), [' good', ' interesting', ' funny', ' decent', ' cool'])\n",
      "(tensor([0.2056, 0.1281, 0.0764, 0.0559, 0.0463], grad_fn=<ToCopyBackward0>), [' story', ' premise', ' storyline', ' ending', ' idea'])\n",
      "(tensor([0.2064, 0.1308, 0.1168, 0.0903, 0.0861], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' line', ' to'])\n",
      "(tensor([0.7392, 0.0885, 0.0351, 0.0307, 0.0225], grad_fn=<ToCopyBackward0>), [' tell', ' it', ' be', ' go', ' follow'])\n",
      "(tensor([0.4480, 0.1717, 0.1197, 0.0436, 0.0301], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', ' but'])\n",
      "(tensor([0.1724, 0.1025, 0.1022, 0.0885, 0.0366], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' Unfortunately'])\n",
      "(tensor([0.3715, 0.1366, 0.1270, 0.0527, 0.0299], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' seemed', ' has'])\n",
      "(tensor([0.3361, 0.2358, 0.0487, 0.0311, 0.0253], grad_fn=<ToCopyBackward0>), [' a', ' some', ' the', ' great', ' good'])\n",
      "(tensor([0.2696, 0.1755, 0.0566, 0.0484, 0.0357], grad_fn=<ToCopyBackward0>), [' pretty', ' good', ' great', ' really', ' lot'])\n",
      "(tensor([0.1509, 0.1214, 0.1209, 0.0342, 0.0237], grad_fn=<ToCopyBackward0>), [' cast', ' plot', ' premise', ' story', ' set'])\n",
      "(tensor([0.3385, 0.2849, 0.1889, 0.0302, 0.0166], grad_fn=<ToCopyBackward0>), [',', ' and', '.', ' in', ' of'])\n",
      "(tensor([0.2495, 0.1446, 0.1275, 0.0881, 0.0784], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' And'])\n",
      "(tensor([0.3396, 0.3337, 0.0507, 0.0424, 0.0237], grad_fn=<ToCopyBackward0>), [' was', ' had', \"'s\", ' really', ' could'])\n",
      "(tensor([0.4624, 0.0981, 0.0952, 0.0445, 0.0431], grad_fn=<ToCopyBackward0>), [' a', ' great', ' good', ' an', ' some'])\n",
      "(tensor([0.2278, 0.1008, 0.0613, 0.0550, 0.0542], grad_fn=<ToCopyBackward0>), [' acting', ' cinem', ' actors', ' pacing', ' special'])\n",
      "(tensor([0.8270, 0.0471, 0.0207, 0.0151, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' performances', ' and'])\n",
      "(tensor([0.3259, 0.1307, 0.1104, 0.0891, 0.0881], grad_fn=<ToCopyBackward0>), [' It', ' But', ' I', ' The', ' And'])\n",
      "(tensor([0.2283, 0.1059, 0.0930, 0.0481, 0.0476], grad_fn=<ToCopyBackward0>), [' the', ' it', ' I', ' when', ' what'])\n",
      "(tensor([0.5964, 0.1793, 0.0706, 0.0607, 0.0182], grad_fn=<ToCopyBackward0>), [' I', ' you', ' it', ' the', ' this'])\n",
      "(tensor([0.2805, 0.0943, 0.0903, 0.0516, 0.0512], grad_fn=<ToCopyBackward0>), [' saw', ' started', ' watched', ' got', ' went'])\n",
      "(tensor([0.6178, 0.2844, 0.0465, 0.0105, 0.0069], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' that', \" '\"])\n",
      "(tensor([0.2798, 0.1439, 0.1295, 0.1080, 0.0423], grad_fn=<ToCopyBackward0>), [',', ' I', ' in', ' on', ' for'])\n",
      "(tensor([0.5443, 0.1211, 0.0581, 0.0322, 0.0264], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' there', ' what'])\n",
      "(tensor([0.1582, 0.1395, 0.1131, 0.0958, 0.0587], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' just', ' found', ' really'])\n",
      "(tensor([0.4178, 0.2128, 0.1897, 0.0564, 0.0329], grad_fn=<ToCopyBackward0>), [' that', ' myself', ' it', ' out', ' the'])\n",
      "(tensor([0.3411, 0.2894, 0.1076, 0.0941, 0.0235], grad_fn=<ToCopyBackward0>), [' it', ' the', ' there', ' I', ' this'])\n",
      "(tensor([0.2429, 0.2149, 0.0492, 0.0358, 0.0297], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' direction', ' acting', ' script'])\n",
      "(tensor([0.4066, 0.1214, 0.1163, 0.0489, 0.0368], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' didn', ' is'])\n",
      "(tensor([0.2809, 0.2126, 0.0418, 0.0382, 0.0374], grad_fn=<ToCopyBackward0>), [' no', ' a', ' been', ' not', ' the'])\n",
      "(tensor([0.1853, 0.1512, 0.0942, 0.0680, 0.0514], grad_fn=<ToCopyBackward0>), [' story', ' plot', ' point', ' real', ' humor'])\n",
      "(tensor([0.5555, 0.2224, 0.0706, 0.0507, 0.0378], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' to', ' at'])\n",
      "(tensor([0.2668, 0.1184, 0.1035, 0.0842, 0.0521], grad_fn=<ToCopyBackward0>), [' It', ' I', ' There', ' The', ' That'])\n",
      "(tensor([0.4538, 0.0508, 0.0400, 0.0375, 0.0348], grad_fn=<ToCopyBackward0>), [' found', ' mean', ' thought', ' was', ' don'])\n",
      "(tensor([0.6523, 0.0827, 0.0732, 0.0481, 0.0390], grad_fn=<ToCopyBackward0>), [' that', ' no', ' it', ' the', ' nothing'])\n",
      "(tensor([0.5741, 0.2006, 0.0521, 0.0290, 0.0178], grad_fn=<ToCopyBackward0>), [' the', ' it', ' there', ' I', ' when'])\n",
      "(tensor([0.7674, 0.0497, 0.0315, 0.0181, 0.0151], grad_fn=<ToCopyBackward0>), [' movie', ' humor', ' story', ' acting', ' comedy'])\n",
      "(tensor([0.5925, 0.0881, 0.0749, 0.0265, 0.0235], grad_fn=<ToCopyBackward0>), [' was', ' had', ' just', ' and', ' didn'])\n",
      "(tensor([0.2134, 0.1871, 0.1348, 0.0442, 0.0284], grad_fn=<ToCopyBackward0>), [' didn', ' wasn', ' seemed', ' was', ' went'])\n",
      "(tensor([0.4632, 0.0566, 0.0291, 0.0285, 0.0259], grad_fn=<ToCopyBackward0>), [' to', ' like', ' really', ' very', ' boring'])\n",
      "(tensor([0.4133, 0.2131, 0.0696, 0.0333, 0.0266], grad_fn=<ToCopyBackward0>), [' be', ' go', ' have', ' get', ' drag'])\n",
      "(tensor([0.0906, 0.0369, 0.0303, 0.0250, 0.0224], grad_fn=<ToCopyBackward0>), [' boring', ' about', ' trying', ' telling', ' pointless'])\n",
      "(tensor([0.2927, 0.1464, 0.1230, 0.1184, 0.0276], grad_fn=<ToCopyBackward0>), [' the', ' itself', ' you', ' a', ' me'])\n",
      "(tensor([0.4390, 0.1890, 0.0623, 0.0183, 0.0147], grad_fn=<ToCopyBackward0>), [' same', ' audience', ' viewer', ' story', ' most'])\n",
      "(tensor([0.2443, 0.1863, 0.1637, 0.0718, 0.0435], grad_fn=<ToCopyBackward0>), [' jokes', ' old', ' story', ' thing', ' joke'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was very boring and predictable, the actors are not good and the whole movie is predictable and boring. It had no point to it. The acting is not good at all. There was no point to the movie, it was boring and there\n",
      "(tensor([0.3854, 0.1706, 0.0892, 0.0769, 0.0476], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4391, 0.2421, 0.1960, 0.0167, 0.0138], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6509, 0.0600, 0.0362, 0.0354, 0.0265], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1367, 0.0702, 0.0658, 0.0547, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5082, 0.0405, 0.0342, 0.0341, 0.0323], grad_fn=<ToCopyBackward0>), [' boring', ' well', ' funny', ' disappointing', ','])\n",
      "(tensor([0.4156, 0.2425, 0.1167, 0.0409, 0.0137], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', '!'])\n",
      "(tensor([0.2281, 0.0495, 0.0361, 0.0340, 0.0293], grad_fn=<ToCopyBackward0>), [' predictable', ' was', ' had', ' the', ' un'])\n",
      "(tensor([0.6118, 0.1099, 0.1071, 0.0211, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' from', ' to'])\n",
      "(tensor([0.1843, 0.0962, 0.0838, 0.0519, 0.0335], grad_fn=<ToCopyBackward0>), [' and', ' with', ' but', ' the', ' I'])\n",
      "(tensor([0.2022, 0.0683, 0.0561, 0.0542, 0.0451], grad_fn=<ToCopyBackward0>), [' acting', ' story', ' only', ' same', ' actors'])\n",
      "(tensor([0.3779, 0.0518, 0.0502, 0.0499, 0.0490], grad_fn=<ToCopyBackward0>), [' were', ' seemed', ' didn', ' are', ' did'])\n",
      "(tensor([0.3312, 0.0748, 0.0574, 0.0377, 0.0221], grad_fn=<ToCopyBackward0>), [' not', ' all', ' very', ' so', ' the'])\n",
      "(tensor([0.1774, 0.1287, 0.1097, 0.0933, 0.0589], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' convincing', ' believable', ' that'])\n",
      "(tensor([0.2678, 0.2038, 0.1661, 0.0864, 0.0828], grad_fn=<ToCopyBackward0>), [' and', ' at', ' enough', '.', ','])\n",
      "(tensor([0.5201, 0.0483, 0.0386, 0.0381, 0.0310], grad_fn=<ToCopyBackward0>), [' the', ' there', ' it', ' this', ' I'])\n",
      "(tensor([0.3064, 0.1791, 0.1166, 0.0824, 0.0334], grad_fn=<ToCopyBackward0>), [' story', ' script', ' plot', ' storyline', ' whole'])\n",
      "(tensor([0.5534, 0.1374, 0.0702, 0.0488, 0.0322], grad_fn=<ToCopyBackward0>), [' movie', ' thing', ' story', ' idea', ' film'])\n",
      "(tensor([0.3099, 0.2506, 0.0696, 0.0367, 0.0301], grad_fn=<ToCopyBackward0>), [' is', ' was', ' seemed', ' felt', ' just'])\n",
      "(tensor([0.2951, 0.0770, 0.0594, 0.0369, 0.0269], grad_fn=<ToCopyBackward0>), [' predictable', ' very', ' just', ' not', ' a'])\n",
      "(tensor([0.4503, 0.2114, 0.0909, 0.0501, 0.0204], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' as', ' with'])\n",
      "(tensor([0.3193, 0.0999, 0.0494, 0.0437, 0.0385], grad_fn=<ToCopyBackward0>), [' boring', ' predictable', ' stupid', ' not', ' dumb'])\n",
      "(tensor([0.6023, 0.1148, 0.0590, 0.0332, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' to', '!'])\n",
      "(tensor([0.2361, 0.1495, 0.0646, 0.0432, 0.0337], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', 'The'])\n",
      "(tensor([0.2512, 0.1789, 0.1381, 0.0528, 0.0294], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' had'])\n",
      "(tensor([0.1264, 0.1244, 0.0949, 0.0827, 0.0491], grad_fn=<ToCopyBackward0>), [' no', ' a', ' some', ' the', ' to'])\n",
      "(tensor([0.2134, 0.0908, 0.0849, 0.0595, 0.0485], grad_fn=<ToCopyBackward0>), [' point', ' plot', ' suspense', ' real', ' story'])\n",
      "(tensor([0.2333, 0.1791, 0.1741, 0.1015, 0.0414], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' at'])\n",
      "(tensor([0.6336, 0.1081, 0.0645, 0.0386, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' the', ' be', ' make', ' tell'])\n",
      "(tensor([0.3429, 0.2532, 0.1149, 0.1041, 0.0168], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' at', ' except'])\n",
      "(tensor([0.2211, 0.1518, 0.1094, 0.0267, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', 'The'])\n",
      "(tensor([0.1355, 0.1297, 0.1069, 0.0923, 0.0421], grad_fn=<ToCopyBackward0>), [' movie', ' only', ' whole', ' acting', ' story'])\n",
      "(tensor([0.6333, 0.1729, 0.0298, 0.0285, 0.0160], grad_fn=<ToCopyBackward0>), [' was', ' is', ' wasn', ' and', ' in'])\n",
      "(tensor([0.2573, 0.0809, 0.0788, 0.0622, 0.0480], grad_fn=<ToCopyBackward0>), [' not', ' bad', ' so', ' very', ' terrible'])\n",
      "(tensor([0.3422, 0.1938, 0.0853, 0.0693, 0.0443], grad_fn=<ToCopyBackward0>), [' convincing', ' good', ' even', ' believable', ' bad'])\n",
      "(tensor([0.2319, 0.1480, 0.1310, 0.1309, 0.1170], grad_fn=<ToCopyBackward0>), [' at', ',', ' and', '.', ' enough'])\n",
      "(tensor([0.9783, 0.0049, 0.0033, 0.0027, 0.0021], grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' least', ' any', ' the'])\n",
      "(tensor([0.3467, 0.2942, 0.1530, 0.0244, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' the'])\n",
      "(tensor([0.3396, 0.0993, 0.0810, 0.0308, 0.0296], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.4172, 0.3251, 0.0864, 0.0574, 0.0334], grad_fn=<ToCopyBackward0>), [' is', ' was', ' are', \"'s\", ' were'])\n",
      "(tensor([0.4594, 0.0982, 0.0768, 0.0527, 0.0412], grad_fn=<ToCopyBackward0>), [' no', ' not', ' nothing', ' a', ' one'])\n",
      "(tensor([0.7492, 0.0430, 0.0243, 0.0115, 0.0109], grad_fn=<ToCopyBackward0>), [' point', ' plot', ' acting', ' chemistry', ' real'])\n",
      "(tensor([0.5194, 0.1785, 0.1018, 0.0380, 0.0371], grad_fn=<ToCopyBackward0>), [' to', ' in', '.', ' at', ' of'])\n",
      "(tensor([0.3641, 0.3506, 0.2354, 0.0097, 0.0033], grad_fn=<ToCopyBackward0>), [' the', ' this', ' it', ' any', ' make'])\n",
      "(tensor([0.9065, 0.0521, 0.0150, 0.0101, 0.0031], grad_fn=<ToCopyBackward0>), [' movie', ' story', ' film', ' whole', ' plot'])\n",
      "(tensor([0.4315, 0.1512, 0.1424, 0.0490, 0.0177], grad_fn=<ToCopyBackward0>), ['.', ' at', ',', ' and', ' except'])\n",
      "(tensor([0.2118, 0.1206, 0.0698, 0.0564, 0.0413], grad_fn=<ToCopyBackward0>), [' the', ' it', ' and', ' there', ' just'])\n",
      "(tensor([0.4492, 0.1269, 0.1024, 0.0720, 0.0329], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' seemed'])\n",
      "(tensor([0.4278, 0.1661, 0.0843, 0.0433, 0.0246], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' all', ' a'])\n",
      "(tensor([0.7684, 0.0936, 0.0550, 0.0210, 0.0085], grad_fn=<ToCopyBackward0>), [' and', '.', ',', ' to', ' as'])\n",
      "(tensor([0.3754, 0.0590, 0.0543, 0.0529, 0.0492], grad_fn=<ToCopyBackward0>), [' predictable', ' there', ' the', ' I', ' not'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I'd seen the worst of it by now. I was wrong. This was not a good film. The story was disjointed and confusing. There was no character development. The acting was awful. I could not get past the fact that I\n",
      "(tensor([0.3836, 0.1722, 0.0899, 0.0772, 0.0473], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2251, 0.1946, 0.1572, 0.0695, 0.0536], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.2087, 0.1044, 0.0617, 0.0394, 0.0298], grad_fn=<ToCopyBackward0>), [' seen', ' never', ' watched', ' give', ' like'])\n",
      "(tensor([0.4487, 0.0924, 0.0917, 0.0738, 0.0531], grad_fn=<ToCopyBackward0>), [' it', ' everything', ' the', ' all', ' a'])\n",
      "(tensor([0.7168, 0.1595, 0.0178, 0.0121, 0.0107], grad_fn=<ToCopyBackward0>), [' worst', ' last', ' end', ' whole', ' worse'])\n",
      "(tensor([0.4058, 0.0581, 0.0570, 0.0556, 0.0545], grad_fn=<ToCopyBackward0>), [' of', ',', ' movie', ' film', '.'])\n",
      "(tensor([0.2855, 0.0880, 0.0864, 0.0245, 0.0200], grad_fn=<ToCopyBackward0>), [' this', ' it', ' the', ' The', ' horror'])\n",
      "(tensor([0.4809, 0.1287, 0.1254, 0.0516, 0.0408], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' in', ' by'])\n",
      "(tensor([0.3674, 0.0941, 0.0917, 0.0432, 0.0332], grad_fn=<ToCopyBackward0>), [' now', ' watching', ' this', ' the', ' that'])\n",
      "(tensor([0.5853, 0.2794, 0.0467, 0.0166, 0.0156], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' when', '!'])\n",
      "(tensor([0.2189, 0.1177, 0.0792, 0.0483, 0.0356], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' Not'])\n",
      "(tensor([0.1785, 0.1131, 0.0945, 0.0653, 0.0290], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' mean', \"'m\", ' really'])\n",
      "(tensor([0.9707, 0.0077, 0.0045, 0.0013, 0.0011], grad_fn=<ToCopyBackward0>), [' wrong', ' right', ' so', ' in', ' really'])\n",
      "(tensor([0.9023, 0.0475, 0.0103, 0.0071, 0.0026], grad_fn=<ToCopyBackward0>), ['.', '!', ',', ' on', ';'])\n",
      "(tensor([0.2489, 0.1580, 0.0829, 0.0746, 0.0327], grad_fn=<ToCopyBackward0>), [' This', ' I', ' It', ' The', ' There'])\n",
      "(tensor([0.4950, 0.1013, 0.0893, 0.0463, 0.0338], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' thing', ' was'])\n",
      "(tensor([0.1681, 0.0795, 0.0662, 0.0478, 0.0436], grad_fn=<ToCopyBackward0>), [' just', ' not', ' a', ' the', ' only'])\n",
      "(tensor([0.2389, 0.1639, 0.1127, 0.0882, 0.0408], grad_fn=<ToCopyBackward0>), [' the', ' a', ' one', ' even', ' good'])\n",
      "(tensor([0.1769, 0.1699, 0.1591, 0.0535, 0.0526], grad_fn=<ToCopyBackward0>), [' horror', ' good', ' bad', ' one', ' movie'])\n",
      "(tensor([0.5409, 0.4053, 0.0095, 0.0043, 0.0026], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' sequel', ' documentary', ' parody'])\n",
      "(tensor([0.6974, 0.0540, 0.0396, 0.0322, 0.0321], grad_fn=<ToCopyBackward0>), ['.', ',', ' at', '...', ' to'])\n",
      "(tensor([0.1890, 0.1764, 0.1412, 0.0663, 0.0375], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' This', ' Not'])\n",
      "(tensor([0.1514, 0.0845, 0.0675, 0.0597, 0.0402], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' only', ' story', ' script'])\n",
      "(tensor([0.4594, 0.1671, 0.0423, 0.0382, 0.0147], grad_fn=<ToCopyBackward0>), [' was', ' is', ' line', ' had', ' has'])\n",
      "(tensor([0.0652, 0.0628, 0.0437, 0.0305, 0.0287], grad_fn=<ToCopyBackward0>), [' weak', ' predictable', ' dis', ' a', ' not'])\n",
      "(tensor([9.9236e-01, 1.0485e-03, 9.5275e-04, 6.3678e-04, 3.6920e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['j', '-', 'organized', 'ordered', 'em'])\n",
      "(tensor([9.9937e-01, 5.3734e-04, 2.3728e-05, 1.2437e-05, 3.3222e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['ointed', 'oint', 'uked', 'unct', 'acent'])\n",
      "(tensor([0.4976, 0.3813, 0.0446, 0.0175, 0.0131], grad_fn=<ToCopyBackward0>), [' and', ',', ' at', '.', ' from'])\n",
      "(tensor([0.1882, 0.0866, 0.0725, 0.0299, 0.0200], grad_fn=<ToCopyBackward0>), [' the', ' dis', ' confusing', ' had', ' seemed'])\n",
      "(tensor([0.4025, 0.3153, 0.1182, 0.0255, 0.0193], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' at', ';'])\n",
      "(tensor([0.5078, 0.1173, 0.0895, 0.0437, 0.0166], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.5116, 0.3728, 0.0284, 0.0188, 0.0164], grad_fn=<ToCopyBackward0>), [' was', ' were', ' wasn', ' seemed', ' are'])\n",
      "(tensor([0.6217, 0.0518, 0.0423, 0.0348, 0.0199], grad_fn=<ToCopyBackward0>), [' no', ' a', ' not', ' little', ' nothing'])\n",
      "(tensor([0.0972, 0.0521, 0.0415, 0.0402, 0.0385], grad_fn=<ToCopyBackward0>), [' plot', ' real', ' character', ' central', ' cohesion'])\n",
      "(tensor([0.8880, 0.0226, 0.0090, 0.0064, 0.0057], grad_fn=<ToCopyBackward0>), [' development', ' to', ' or', ' that', ' focus'])\n",
      "(tensor([0.3815, 0.2332, 0.1226, 0.0455, 0.0386], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' for', ' in'])\n",
      "(tensor([0.5404, 0.0835, 0.0762, 0.0564, 0.0296], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' There', ' And'])\n",
      "(tensor([0.4597, 0.0423, 0.0334, 0.0312, 0.0267], grad_fn=<ToCopyBackward0>), [' acting', ' plot', ' dialogue', ' ending', ' script'])\n",
      "(tensor([0.8634, 0.0374, 0.0109, 0.0058, 0.0054], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ',', ' and', ' seemed'])\n",
      "(tensor([0.0718, 0.0578, 0.0536, 0.0451, 0.0442], grad_fn=<ToCopyBackward0>), [' awful', ' terrible', ' wooden', ' bad', ' weak'])\n",
      "(tensor([0.8095, 0.0948, 0.0552, 0.0038, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' -', ' to'])\n",
      "(tensor([0.5696, 0.0806, 0.0778, 0.0527, 0.0200], grad_fn=<ToCopyBackward0>), [' The', ' And', ' I', ' It', ' There'])\n",
      "(tensor([0.1211, 0.0641, 0.0441, 0.0391, 0.0363], grad_fn=<ToCopyBackward0>), [' was', \"'m\", ' could', ' couldn', ' found'])\n",
      "(tensor([0.5886, 0.0909, 0.0443, 0.0344, 0.0302], grad_fn=<ToCopyBackward0>), [' not', ' barely', ' see', ' only', ' feel'])\n",
      "(tensor([0.1825, 0.1387, 0.1298, 0.0407, 0.0301], grad_fn=<ToCopyBackward0>), [' believe', ' get', ' understand', ' even', ' care'])\n",
      "(tensor([0.2556, 0.1637, 0.1525, 0.0953, 0.0842], grad_fn=<ToCopyBackward0>), [' out', ' over', ' into', ' past', ' through'])\n",
      "(tensor([0.8890, 0.0152, 0.0146, 0.0116, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' it', ' that', ' how', ' this'])\n",
      "(tensor([0.5221, 0.0140, 0.0125, 0.0114, 0.0099], grad_fn=<ToCopyBackward0>), [' fact', ' script', ' obvious', ' awful', ' terrible'])\n",
      "(tensor([0.9674, 0.0080, 0.0068, 0.0037, 0.0032], grad_fn=<ToCopyBackward0>), [' that', ' I', ' the', ' this', ' there'])\n",
      "(tensor([0.3696, 0.1836, 0.0799, 0.0490, 0.0409], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' this', ' there'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought I should watch some horror films, but I found it more interesting than watching a good scary movie like \"Silent Hill\" or \"Silent Hill: A New Beginning\". The reason is that in the movies the characters are acting out some sort of\n",
      "(tensor([0.3837, 0.1723, 0.0901, 0.0770, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.2253, 0.1946, 0.1572, 0.0696, 0.0534], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' should'])\n",
      "(tensor([0.1264, 0.1032, 0.0823, 0.0576, 0.0531], grad_fn=<ToCopyBackward0>), [' warn', ' give', ' never', ' have', ' watch'])\n",
      "(tensor([0.6118, 0.1875, 0.0457, 0.0328, 0.0197], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ' some'])\n",
      "(tensor([0.2048, 0.1229, 0.0380, 0.0331, 0.0195], grad_fn=<ToCopyBackward0>), [' horror', ' more', ' other', ' of', ' old'])\n",
      "(tensor([0.6993, 0.1277, 0.0246, 0.0234, 0.0230], grad_fn=<ToCopyBackward0>), [' movies', ' films', ' f', '.', ' movie'])\n",
      "(tensor([0.1738, 0.1349, 0.0883, 0.0858, 0.0622], grad_fn=<ToCopyBackward0>), [' to', ',', ' as', ' just', '.'])\n",
      "(tensor([0.2341, 0.1360, 0.0562, 0.0370, 0.0358], grad_fn=<ToCopyBackward0>), [' but', ' so', ' and', ' since', ' because'])\n",
      "(tensor([0.2431, 0.1636, 0.1315, 0.0512, 0.0382], grad_fn=<ToCopyBackward0>), [' this', ' after', ' I', ' it', ' the'])\n",
      "(tensor([0.1539, 0.0943, 0.0744, 0.0631, 0.0547], grad_fn=<ToCopyBackward0>), [' was', ' found', ' didn', ' just', ' watched'])\n",
      "(tensor([0.2089, 0.1307, 0.1305, 0.1013, 0.0746], grad_fn=<ToCopyBackward0>), [' this', ' them', ' myself', ' it', ' out'])\n",
      "(tensor([0.1404, 0.1352, 0.1124, 0.0776, 0.0763], grad_fn=<ToCopyBackward0>), [' to', ' more', ' was', ' too', ' a'])\n",
      "(tensor([0.1845, 0.1705, 0.1210, 0.0583, 0.0347], grad_fn=<ToCopyBackward0>), [' entertaining', ' enjoyable', ' interesting', ' fun', ' frightening'])\n",
      "(tensor([0.8485, 0.0485, 0.0332, 0.0095, 0.0066], grad_fn=<ToCopyBackward0>), [' to', ' watching', ' than', ' and', ' when'])\n",
      "(tensor([0.2680, 0.1357, 0.1026, 0.0605, 0.0534], grad_fn=<ToCopyBackward0>), [' watching', ' anything', ' I', ' horror', ' scary'])\n",
      "(tensor([0.1930, 0.1419, 0.1126, 0.0980, 0.0356], grad_fn=<ToCopyBackward0>), [' a', ' scary', ' paint', ' them', ' the'])\n",
      "(tensor([0.1289, 0.1071, 0.0616, 0.0489, 0.0443], grad_fn=<ToCopyBackward0>), [' scary', ' good', ' boring', ' movie', ' lot'])\n",
      "(tensor([0.4414, 0.1516, 0.1155, 0.1059, 0.0236], grad_fn=<ToCopyBackward0>), [' horror', ' movie', ' scary', ' one', ' film'])\n",
      "(tensor([0.8666, 0.0346, 0.0216, 0.0120, 0.0091], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' flick', ' story', ' one'])\n",
      "(tensor([0.8096, 0.0330, 0.0299, 0.0184, 0.0155], grad_fn=<ToCopyBackward0>), ['.', ' like', ',', '...', '....'])\n",
      "(tensor([0.1131, 0.0580, 0.0475, 0.0386, 0.0218], grad_fn=<ToCopyBackward0>), [' The', ' \"', ' Friday', ' the', ' House'])\n",
      "(tensor([0.2156, 0.0655, 0.0489, 0.0451, 0.0371], grad_fn=<ToCopyBackward0>), ['Sil', 'Dead', 'Sc', 'The', 'House'])\n",
      "(tensor([8.9905e-01, 9.2085e-02, 1.8513e-03, 1.0458e-03, 2.3593e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ent', 'ence', 'k', 'ents', 'o'])\n",
      "(tensor([0.7072, 0.1143, 0.0514, 0.0164, 0.0111], grad_fn=<ToCopyBackward0>), [' Hill', ' Night', ' House', ' Scream', ' Hills'])\n",
      "(tensor([0.5382, 0.1893, 0.1572, 0.0393, 0.0314], grad_fn=<ToCopyBackward0>), ['\"', '\".', '.\"', '\",', ',\"'])\n",
      "(tensor([0.3533, 0.0988, 0.0699, 0.0634, 0.0302], grad_fn=<ToCopyBackward0>), [' or', ' because', ' and', ' (', '....'])\n",
      "(tensor([0.8033, 0.0349, 0.0207, 0.0182, 0.0126], grad_fn=<ToCopyBackward0>), [' \"', ' even', ' the', ' any', ' a'])\n",
      "(tensor([0.2365, 0.1029, 0.0865, 0.0504, 0.0245], grad_fn=<ToCopyBackward0>), ['Sil', 'Friday', 'The', 'Dead', 'Sc'])\n",
      "(tensor([9.6776e-01, 2.6325e-02, 9.6940e-04, 6.2785e-04, 5.2416e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ent', 'ence', 'ents', 'k', 'ently'])\n",
      "(tensor([0.9418, 0.0114, 0.0093, 0.0033, 0.0010], grad_fn=<ToCopyBackward0>), [' Hill', ' Night', ' Hills', ' House', ' Scope'])\n",
      "(tensor([0.3510, 0.0695, 0.0651, 0.0240, 0.0236], grad_fn=<ToCopyBackward0>), [' 2', ':', ' Down', ' 3', ' II'])\n",
      "(tensor([0.2823, 0.2166, 0.0540, 0.0195, 0.0159], grad_fn=<ToCopyBackward0>), [' The', ' Home', ' Down', ' A', ' the'])\n",
      "(tensor([0.0425, 0.0250, 0.0122, 0.0117, 0.0108], grad_fn=<ToCopyBackward0>), [' New', ' Game', ' Child', ' Daughter', ' Nightmare'])\n",
      "(tensor([0.3002, 0.1229, 0.0965, 0.0850, 0.0197], grad_fn=<ToCopyBackward0>), [' Beginning', ' Hope', ' Reck', ' Nightmare', ' Dawn'])\n",
      "(tensor([0.6077, 0.1832, 0.1117, 0.0601, 0.0155], grad_fn=<ToCopyBackward0>), ['\".', '.\"', '\"', '\",', ',\"'])\n",
      "(tensor([0.3830, 0.0782, 0.0588, 0.0326, 0.0273], grad_fn=<ToCopyBackward0>), [' I', ' The', ' This', ' It', 'I'])\n",
      "(tensor([0.1057, 0.0802, 0.0775, 0.0614, 0.0448], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' first', ' reason'])\n",
      "(tensor([0.2287, 0.2053, 0.1351, 0.1321, 0.0590], grad_fn=<ToCopyBackward0>), [' is', ' for', ' I', ' why', ' being'])\n",
      "(tensor([0.6399, 0.0927, 0.0722, 0.0333, 0.0277], grad_fn=<ToCopyBackward0>), [' that', ' because', ',', ' I', ' the'])\n",
      "(tensor([0.3150, 0.0722, 0.0576, 0.0549, 0.0518], grad_fn=<ToCopyBackward0>), [' I', ' the', ' in', ' this', ' it'])\n",
      "(tensor([0.1659, 0.1269, 0.1219, 0.1130, 0.0721], grad_fn=<ToCopyBackward0>), [' the', ' those', ' most', ' these', ' a'])\n",
      "(tensor([0.1605, 0.1432, 0.0867, 0.0774, 0.0380], grad_fn=<ToCopyBackward0>), [' first', ' movies', ' movie', ' beginning', ' majority'])\n",
      "(tensor([0.3313, 0.1050, 0.0863, 0.0806, 0.0673], grad_fn=<ToCopyBackward0>), [',', ' I', ' the', ' you', ' they'])\n",
      "(tensor([0.1901, 0.0871, 0.0587, 0.0552, 0.0515], grad_fn=<ToCopyBackward0>), [' characters', ' acting', ' actors', ' story', ' main'])\n",
      "(tensor([0.4504, 0.0499, 0.0364, 0.0332, 0.0264], grad_fn=<ToCopyBackward0>), [' are', ' don', ' were', ' always', ' do'])\n",
      "(tensor([0.0887, 0.0637, 0.0597, 0.0535, 0.0407], grad_fn=<ToCopyBackward0>), [' acting', ' always', ' not', ' so', ' very'])\n",
      "(tensor([0.2064, 0.1424, 0.0580, 0.0443, 0.0355], grad_fn=<ToCopyBackward0>), [' in', ' like', ' stupid', ' out', ' to'])\n",
      "(tensor([0.3257, 0.1247, 0.1241, 0.0582, 0.0418], grad_fn=<ToCopyBackward0>), [' their', ' the', ' a', ' some', ' what'])\n",
      "(tensor([0.3743, 0.2029, 0.0934, 0.0171, 0.0124], grad_fn=<ToCopyBackward0>), [' kind', ' sort', ' type', ' very', ' story'])\n",
      "(tensor([9.9635e-01, 2.2042e-04, 1.8647e-04, 1.4733e-04, 1.3916e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' or', ' psychological', ' mental', 'a'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought that I had a good enough knowledge about the movie to make a plot out of it. I thought it would be more interesting to watch it from the standpoint of a viewer than from the standpoint of someone involved in the movie. I was wrong. The\n",
      "(tensor([0.3841, 0.1714, 0.0899, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.3312, 0.2367, 0.0584, 0.0560, 0.0229], grad_fn=<ToCopyBackward0>), [' this', ' the', ' I', ' it', ' a'])\n",
      "(tensor([0.2143, 0.1251, 0.1123, 0.0828, 0.0744], grad_fn=<ToCopyBackward0>), [' was', ' had', ' would', ' could', ' should'])\n",
      "(tensor([0.5596, 0.1900, 0.0347, 0.0333, 0.0087], grad_fn=<ToCopyBackward0>), [' to', ' seen', ' a', ' watched', ' read'])\n",
      "(tensor([0.3044, 0.2187, 0.0642, 0.0439, 0.0289], grad_fn=<ToCopyBackward0>), [' good', ' pretty', ' chance', ' really', ' brain'])\n",
      "(tensor([0.1436, 0.1232, 0.0807, 0.0278, 0.0258], grad_fn=<ToCopyBackward0>), [' idea', ' enough', ' story', ' life', ' cast'])\n",
      "(tensor([0.1261, 0.0690, 0.0638, 0.0617, 0.0401], grad_fn=<ToCopyBackward0>), [' story', ' mind', ' idea', ' knowledge', ' horror'])\n",
      "(tensor([0.2557, 0.2257, 0.2183, 0.1463, 0.0653], grad_fn=<ToCopyBackward0>), [' of', ' to', ' about', ' base', ' on'])\n",
      "(tensor([0.4462, 0.0507, 0.0500, 0.0193, 0.0095], grad_fn=<ToCopyBackward0>), [' the', ' quantum', ' this', ' what', ' human'])\n",
      "(tensor([0.1363, 0.0582, 0.0519, 0.0267, 0.0257], grad_fn=<ToCopyBackward0>), [' Titanic', ' movie', ' subject', ' technical', ' DVD'])\n",
      "(tensor([0.2681, 0.1682, 0.0612, 0.0602, 0.0577], grad_fn=<ToCopyBackward0>), [' to', ' industry', ',', '.', ' that'])\n",
      "(tensor([0.0992, 0.0801, 0.0785, 0.0691, 0.0415], grad_fn=<ToCopyBackward0>), [' make', ' comment', ' be', ' at', ' not'])\n",
      "(tensor([0.6242, 0.1000, 0.0634, 0.0551, 0.0197], grad_fn=<ToCopyBackward0>), [' a', ' an', ' it', ' some', ' the'])\n",
      "(tensor([0.1830, 0.1601, 0.1095, 0.1013, 0.0535], grad_fn=<ToCopyBackward0>), [' good', ' decent', ' plot', ' movie', ' better'])\n",
      "(tensor([0.1653, 0.1095, 0.0909, 0.0330, 0.0317], grad_fn=<ToCopyBackward0>), [' summary', ' synopsis', ' out', ' about', ' outline'])\n",
      "(tensor([9.8199e-01, 2.8496e-03, 1.4776e-03, 1.3560e-03, 8.1518e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' for', '.', ' myself', ' it'])\n",
      "(tensor([0.9105, 0.0366, 0.0051, 0.0048, 0.0046], grad_fn=<ToCopyBackward0>), [' it', ' the', ' all', ' them', ' that'])\n",
      "(tensor([0.6598, 0.1829, 0.0145, 0.0111, 0.0096], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' but'])\n",
      "(tensor([0.2740, 0.0575, 0.0548, 0.0383, 0.0350], grad_fn=<ToCopyBackward0>), [' I', ' But', ' The', ' It', ' So'])\n",
      "(tensor([0.1248, 0.0978, 0.0754, 0.0385, 0.0332], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' even', ' didn'])\n",
      "(tensor([0.4017, 0.1758, 0.1613, 0.0631, 0.0185], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ' the', ' this'])\n",
      "(tensor([0.5781, 0.2094, 0.0682, 0.0297, 0.0282], grad_fn=<ToCopyBackward0>), [' would', ' was', ' could', ' might', ' had'])\n",
      "(tensor([0.8602, 0.0313, 0.0196, 0.0106, 0.0099], grad_fn=<ToCopyBackward0>), [' be', ' make', ' have', ' take', ' just'])\n",
      "(tensor([0.2178, 0.1074, 0.0720, 0.0716, 0.0388], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' entertaining', ' more', ' easy'])\n",
      "(tensor([0.2759, 0.2371, 0.0845, 0.0523, 0.0436], grad_fn=<ToCopyBackward0>), [' interesting', ' entertaining', ' enjoyable', ' fun', ' difficult'])\n",
      "(tensor([0.5733, 0.0615, 0.0614, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' to', ' if', ' than', ' for', ' and'])\n",
      "(tensor([0.1711, 0.1116, 0.0494, 0.0470, 0.0409], grad_fn=<ToCopyBackward0>), [' make', ' see', ' write', ' watch', ' have'])\n",
      "(tensor([0.2632, 0.2365, 0.0753, 0.0385, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' as', ' a'])\n",
      "(tensor([0.1398, 0.1370, 0.1292, 0.1012, 0.0607], grad_fn=<ToCopyBackward0>), [' in', ' as', ' from', '.', ' with'])\n",
      "(tensor([0.5059, 0.1618, 0.0334, 0.0299, 0.0223], grad_fn=<ToCopyBackward0>), [' the', ' a', ' my', ' beginning', ' an'])\n",
      "(tensor([0.2181, 0.2160, 0.1882, 0.0621, 0.0543], grad_fn=<ToCopyBackward0>), [' perspective', ' point', ' beginning', ' start', ' standpoint'])\n",
      "(tensor([9.7117e-01, 2.3119e-02, 1.1106e-03, 5.9009e-04, 3.8072e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' that', ' as', ',', ' where'])\n",
      "(tensor([0.3494, 0.2811, 0.0627, 0.0514, 0.0258], grad_fn=<ToCopyBackward0>), [' the', ' a', ' an', ' someone', ' what'])\n",
      "(tensor([0.2703, 0.0492, 0.0489, 0.0341, 0.0227], grad_fn=<ToCopyBackward0>), [' viewer', ' spectator', ' person', ' movie', ' child'])\n",
      "(tensor([0.1990, 0.1812, 0.1716, 0.1192, 0.0467], grad_fn=<ToCopyBackward0>), [' who', ' than', '.', ',', ' rather'])\n",
      "(tensor([0.2632, 0.1682, 0.1346, 0.1101, 0.0565], grad_fn=<ToCopyBackward0>), [' to', ' from', ' a', ' as', ' someone'])\n",
      "(tensor([0.8544, 0.0504, 0.0122, 0.0118, 0.0116], grad_fn=<ToCopyBackward0>), [' the', ' a', ' any', ' my', ' that'])\n",
      "(tensor([0.8406, 0.0594, 0.0336, 0.0293, 0.0038], grad_fn=<ToCopyBackward0>), [' standpoint', ' point', ' perspective', ' viewpoint', ' director'])\n",
      "(tensor([9.9648e-01, 1.8590e-03, 3.3466e-04, 9.9308e-05, 9.7374e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [' of', ' as', ' that', ' the', ' a'])\n",
      "(tensor([0.5019, 0.1406, 0.1096, 0.0998, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' someone', ' an', ' the', ' somebody'])\n",
      "(tensor([0.6364, 0.1916, 0.0315, 0.0204, 0.0118], grad_fn=<ToCopyBackward0>), [' who', ' involved', ' that', ' in', ' actually'])\n",
      "(tensor([0.8120, 0.1324, 0.0355, 0.0034, 0.0029], grad_fn=<ToCopyBackward0>), [' in', ' with', '.', ',', ' the'])\n",
      "(tensor([0.4392, 0.4288, 0.0353, 0.0124, 0.0122], grad_fn=<ToCopyBackward0>), [' the', ' making', ' it', ' this', ' its'])\n",
      "(tensor([0.5794, 0.2146, 0.0471, 0.0458, 0.0178], grad_fn=<ToCopyBackward0>), [' movie', ' making', ' film', ' production', ' plot'])\n",
      "(tensor([0.8567, 0.0288, 0.0125, 0.0103, 0.0082], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' making', ' in'])\n",
      "(tensor([0.2475, 0.0755, 0.0481, 0.0477, 0.0435], grad_fn=<ToCopyBackward0>), [' I', ' So', ' The', ' It', ' But'])\n",
      "(tensor([0.0918, 0.0751, 0.0571, 0.0425, 0.0394], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' didn', ' really', ' think'])\n",
      "(tensor([0.0916, 0.0670, 0.0570, 0.0474, 0.0417], grad_fn=<ToCopyBackward0>), [' wrong', ' really', ' interested', ' looking', ' very'])\n",
      "(tensor([0.7019, 0.0823, 0.0532, 0.0519, 0.0248], grad_fn=<ToCopyBackward0>), ['.', ' on', ',', ' in', ' about'])\n",
      "(tensor([0.1811, 0.1560, 0.0890, 0.0621, 0.0462], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', 'I'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was so bad. I really thought it would be funny to watch this movie, but it wasn't. It was just terrible. It was not funny at all, it was not scary at all. The actors are so annoying. It's\n",
      "(tensor([0.3837, 0.1721, 0.0902, 0.0772, 0.0472], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4371, 0.2442, 0.1963, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6531, 0.0595, 0.0361, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1376, 0.0700, 0.0662, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.5186, 0.0474, 0.0458, 0.0419, 0.0296], grad_fn=<ToCopyBackward0>), [' bad', ' stupid', ' terrible', ' awful', ' boring'])\n",
      "(tensor([0.2378, 0.2247, 0.2059, 0.0697, 0.0468], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ',', '.'])\n",
      "(tensor([0.3492, 0.1174, 0.1151, 0.0204, 0.0175], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' Not'])\n",
      "(tensor([0.0852, 0.0764, 0.0707, 0.0677, 0.0598], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' really', \"'m\", ' can'])\n",
      "(tensor([0.2358, 0.1742, 0.0632, 0.0517, 0.0475], grad_fn=<ToCopyBackward0>), [' thought', ' did', ' didn', ',', ' wanted'])\n",
      "(tensor([0.4202, 0.3375, 0.1273, 0.0291, 0.0192], grad_fn=<ToCopyBackward0>), [' it', ' this', ' that', ' the', ' they'])\n",
      "(tensor([0.7448, 0.1362, 0.0206, 0.0172, 0.0139], grad_fn=<ToCopyBackward0>), [' was', ' would', ' wasn', ' could', ' sucked'])\n",
      "(tensor([0.7375, 0.0607, 0.0573, 0.0352, 0.0326], grad_fn=<ToCopyBackward0>), [' be', ' have', ' get', ' make', ' go'])\n",
      "(tensor([0.3335, 0.1804, 0.0505, 0.0331, 0.0328], grad_fn=<ToCopyBackward0>), [' funny', ' better', ' a', ' hilarious', ' more'])\n",
      "(tensor([0.2566, 0.2162, 0.1950, 0.0507, 0.0412], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' if', ' and'])\n",
      "(tensor([0.4649, 0.1957, 0.0385, 0.0302, 0.0190], grad_fn=<ToCopyBackward0>), [' see', ' watch', ' make', ' have', ' be'])\n",
      "(tensor([0.2460, 0.2446, 0.1502, 0.1057, 0.0139], grad_fn=<ToCopyBackward0>), [' this', ' it', ' a', ' the', ','])\n",
      "(tensor([0.4148, 0.1819, 0.0340, 0.0279, 0.0256], grad_fn=<ToCopyBackward0>), [' movie', '.', ',', ' because', ' in'])\n",
      "(tensor([0.1356, 0.1260, 0.1091, 0.1086, 0.0672], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' with', ' because'])\n",
      "(tensor([0.4314, 0.1096, 0.0960, 0.0261, 0.0193], grad_fn=<ToCopyBackward0>), [' but', ' and', ' because', ' so', ' since'])\n",
      "(tensor([0.2989, 0.2315, 0.0544, 0.0362, 0.0323], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' after', ' instead'])\n",
      "(tensor([0.4322, 0.2034, 0.0766, 0.0713, 0.0435], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' really', \"'s\", ' just'])\n",
      "(tensor([9.9714e-01, 9.8731e-04, 3.3121e-04, 2.0618e-04, 1.2204e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', \"'\", '´', '\"'])\n",
      "(tensor([0.6562, 0.1372, 0.0599, 0.0466, 0.0235], grad_fn=<ToCopyBackward0>), ['.', ' funny', ' even', ' at', ','])\n",
      "(tensor([0.3634, 0.2457, 0.1128, 0.0304, 0.0185], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.5269, 0.1603, 0.1156, 0.0241, 0.0196], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' didn', ' just'])\n",
      "(tensor([0.2999, 0.1335, 0.0675, 0.0421, 0.0332], grad_fn=<ToCopyBackward0>), [' just', ' so', ' not', ' really', ' a'])\n",
      "(tensor([0.1130, 0.0914, 0.0882, 0.0803, 0.0599], grad_fn=<ToCopyBackward0>), [' boring', ' a', ' horrible', ' stupid', ' terrible'])\n",
      "(tensor([0.7775, 0.0711, 0.0552, 0.0375, 0.0073], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', ' to'])\n",
      "(tensor([0.3261, 0.1638, 0.1235, 0.0216, 0.0216], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', ' And'])\n",
      "(tensor([0.4370, 0.1692, 0.0739, 0.0352, 0.0279], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' had', ' made'])\n",
      "(tensor([0.2104, 0.1660, 0.0951, 0.0468, 0.0408], grad_fn=<ToCopyBackward0>), [' just', ' so', ' not', ' like', ' a'])\n",
      "(tensor([0.8065, 0.1117, 0.0099, 0.0094, 0.0062], grad_fn=<ToCopyBackward0>), [' funny', ' even', ' scary', ' a', ' worth'])\n",
      "(tensor([0.4981, 0.2449, 0.1279, 0.0297, 0.0215], grad_fn=<ToCopyBackward0>), [' at', '.', ',', ' to', ' in'])\n",
      "(tensor([9.9769e-01, 7.9790e-04, 6.2531e-04, 2.9205e-04, 1.6533e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' ALL', ' the', ' any', ' least'])\n",
      "(tensor([0.7312, 0.1572, 0.0294, 0.0220, 0.0074], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', '...'])\n",
      "(tensor([0.2845, 0.1861, 0.0939, 0.0559, 0.0544], grad_fn=<ToCopyBackward0>), [' it', ' not', ' and', ' I', ' but'])\n",
      "(tensor([0.7244, 0.1995, 0.0139, 0.0135, 0.0127], grad_fn=<ToCopyBackward0>), [' was', ' wasn', \"'s\", ' didn', ' just'])\n",
      "(tensor([0.4460, 0.3174, 0.0449, 0.0178, 0.0147], grad_fn=<ToCopyBackward0>), [' not', ' just', ' stupid', ' really', ' boring'])\n",
      "(tensor([0.2412, 0.2169, 0.1606, 0.0858, 0.0515], grad_fn=<ToCopyBackward0>), [' entertaining', ' funny', ' interesting', ' even', ' scary'])\n",
      "(tensor([0.4528, 0.3830, 0.0458, 0.0340, 0.0221], grad_fn=<ToCopyBackward0>), [' at', ',', '.', ' or', ' and'])\n",
      "(tensor([9.9787e-01, 6.3514e-04, 4.2604e-04, 2.6307e-04, 1.6337e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' any', ' ALL', ' least', ' the'])\n",
      "(tensor([0.5719, 0.3128, 0.0321, 0.0304, 0.0049], grad_fn=<ToCopyBackward0>), [',', '.', '...', ' and', '!'])\n",
      "(tensor([0.3269, 0.3052, 0.0684, 0.0368, 0.0204], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.2436, 0.1197, 0.0492, 0.0385, 0.0329], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' movie', ' actors'])\n",
      "(tensor([0.4169, 0.0560, 0.0490, 0.0369, 0.0319], grad_fn=<ToCopyBackward0>), [' were', ' weren', ' are', ' did', ' in'])\n",
      "(tensor([0.2371, 0.0564, 0.0562, 0.0542, 0.0386], grad_fn=<ToCopyBackward0>), [' not', ' so', ' really', ' all', ' very'])\n",
      "(tensor([0.1971, 0.1443, 0.1041, 0.0276, 0.0240], grad_fn=<ToCopyBackward0>), [' annoying', ' bad', ' wooden', ' fake', ' stupid'])\n",
      "(tensor([0.2410, 0.1867, 0.1755, 0.1220, 0.0868], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', ' in'])\n",
      "(tensor([0.2816, 0.1517, 0.1244, 0.1147, 0.0356], grad_fn=<ToCopyBackward0>), [' I', ' They', ' It', ' The', ' And'])\n",
      "(tensor([0.4733, 0.3322, 0.0342, 0.0272, 0.0172], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' just'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this movie was pretty funny. It is not a good movie but it is funny. I think I laughed my butt off the whole time. I really enjoyed it. I would like to see the movie again. I really do enjoy it. It is\n",
      "(tensor([0.3837, 0.1722, 0.0902, 0.0770, 0.0471], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4372, 0.2445, 0.1961, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.6532, 0.0595, 0.0359, 0.0355, 0.0262], grad_fn=<ToCopyBackward0>), [' was', ' would', ' sucked', ' had', ' is'])\n",
      "(tensor([0.1373, 0.0701, 0.0661, 0.0548, 0.0468], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' so', ' terrible', ' very'])\n",
      "(tensor([0.1757, 0.1514, 0.1088, 0.0942, 0.0897], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' awful', ' lame', ' boring'])\n",
      "(tensor([0.4484, 0.1299, 0.0922, 0.0257, 0.0246], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' when', ' in'])\n",
      "(tensor([0.2600, 0.1845, 0.0998, 0.0193, 0.0174], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2974, 0.2128, 0.0685, 0.0679, 0.0555], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' has', ' is'])\n",
      "(tensor([0.1760, 0.0936, 0.0745, 0.0351, 0.0297], grad_fn=<ToCopyBackward0>), [' not', ' a', ' supposed', ' pretty', ' kind'])\n",
      "(tensor([0.1602, 0.1096, 0.1017, 0.0951, 0.0512], grad_fn=<ToCopyBackward0>), [' as', ' a', ' the', ' that', ' at'])\n",
      "(tensor([0.1835, 0.1684, 0.0859, 0.0391, 0.0367], grad_fn=<ToCopyBackward0>), [' great', ' good', ' very', ' perfect', ' comedy'])\n",
      "(tensor([0.7206, 0.1627, 0.0710, 0.0070, 0.0051], grad_fn=<ToCopyBackward0>), [' movie', ' comedy', ' film', ' satire', ' plot'])\n",
      "(tensor([0.3334, 0.2433, 0.0654, 0.0578, 0.0525], grad_fn=<ToCopyBackward0>), ['.', ',', ' to', ' but', ' in'])\n",
      "(tensor([0.4094, 0.0924, 0.0442, 0.0316, 0.0289], grad_fn=<ToCopyBackward0>), [' it', ' I', ' at', ' the', ' a'])\n",
      "(tensor([0.3708, 0.1740, 0.1115, 0.0517, 0.0414], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' does'])\n",
      "(tensor([0.4530, 0.1302, 0.0676, 0.0474, 0.0245], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' not', ' entertaining', ' pretty'])\n",
      "(tensor([0.6032, 0.1323, 0.0441, 0.0247, 0.0233], grad_fn=<ToCopyBackward0>), ['.', ' and', ' as', '!', ','])\n",
      "(tensor([0.2835, 0.1470, 0.1406, 0.0262, 0.0247], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.0958, 0.0872, 0.0641, 0.0525, 0.0502], grad_fn=<ToCopyBackward0>), [' think', ' really', ' like', ' thought', ' am'])\n",
      "(tensor([0.2327, 0.1534, 0.1330, 0.0538, 0.0342], grad_fn=<ToCopyBackward0>), [' it', ' the', ' that', ' this', ' I'])\n",
      "(tensor([0.6072, 0.0464, 0.0310, 0.0276, 0.0270], grad_fn=<ToCopyBackward0>), [' laughed', ' was', ' am', ' would', ' just'])\n",
      "(tensor([0.3893, 0.1047, 0.0757, 0.0621, 0.0398], grad_fn=<ToCopyBackward0>), [' a', ' the', ' my', ' out', ' most'])\n",
      "(tensor([0.2569, 0.1714, 0.0709, 0.0459, 0.0321], grad_fn=<ToCopyBackward0>), [' butt', ' ass', ' head', ' way', ' nose'])\n",
      "(tensor([0.9187, 0.0193, 0.0077, 0.0047, 0.0043], grad_fn=<ToCopyBackward0>), [' off', ' out', ' right', ' to', ' away'])\n",
      "(tensor([0.1632, 0.1109, 0.0580, 0.0527, 0.0517], grad_fn=<ToCopyBackward0>), [' at', ' the', '.', ' for', ' a'])\n",
      "(tensor([0.5785, 0.2640, 0.0452, 0.0157, 0.0092], grad_fn=<ToCopyBackward0>), [' whole', ' entire', ' first', ' majority', ' 1'])\n",
      "(tensor([0.5840, 0.1903, 0.1416, 0.0352, 0.0211], grad_fn=<ToCopyBackward0>), [' time', ' way', ' movie', ' film', ' thing'])\n",
      "(tensor([0.6093, 0.1819, 0.0425, 0.0276, 0.0198], grad_fn=<ToCopyBackward0>), ['.', ' I', ' and', ' watching', ','])\n",
      "(tensor([0.3186, 0.1473, 0.1348, 0.0230, 0.0218], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', 'I', 'The'])\n",
      "(tensor([0.1199, 0.0847, 0.0617, 0.0500, 0.0407], grad_fn=<ToCopyBackward0>), [' really', ' think', ' was', ' thought', ' am'])\n",
      "(tensor([0.1165, 0.0690, 0.0667, 0.0597, 0.0556], grad_fn=<ToCopyBackward0>), [' enjoyed', ' think', ' like', ' wanted', ' do'])\n",
      "(tensor([0.7248, 0.1077, 0.0614, 0.0271, 0.0253], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' myself', ' watching'])\n",
      "(tensor([0.5729, 0.0767, 0.0754, 0.0432, 0.0318], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' I', ' though'])\n",
      "(tensor([0.4021, 0.1395, 0.0969, 0.0289, 0.0224], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', 'I'])\n",
      "(tensor([0.1442, 0.1084, 0.0648, 0.0494, 0.0463], grad_fn=<ToCopyBackward0>), [' think', ' really', ' thought', ' would', ' was'])\n",
      "(tensor([0.5658, 0.1609, 0.0523, 0.0387, 0.0230], grad_fn=<ToCopyBackward0>), [' recommend', ' like', ' not', ' suggest', ' say'])\n",
      "(tensor([0.9190, 0.0361, 0.0077, 0.0067, 0.0062], grad_fn=<ToCopyBackward0>), [' to', ' it', ' the', ' my', ' a'])\n",
      "(tensor([0.7922, 0.0230, 0.0192, 0.0180, 0.0146], grad_fn=<ToCopyBackward0>), [' see', ' think', ' give', ' say', ' watch'])\n",
      "(tensor([0.6116, 0.0896, 0.0826, 0.0485, 0.0474], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' a', ' more'])\n",
      "(tensor([0.3886, 0.1834, 0.0428, 0.0310, 0.0191], grad_fn=<ToCopyBackward0>), [' sequel', ' movie', ' film', ' whole', ' other'])\n",
      "(tensor([0.6106, 0.0503, 0.0358, 0.0336, 0.0188], grad_fn=<ToCopyBackward0>), [' again', ' more', ' in', ' if', ' but'])\n",
      "(tensor([0.2866, 0.1218, 0.1017, 0.0955, 0.0629], grad_fn=<ToCopyBackward0>), ['.', ' if', ' but', ',', ' though'])\n",
      "(tensor([0.4891, 0.1693, 0.0558, 0.0416, 0.0167], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' If', ' But'])\n",
      "(tensor([0.1434, 0.1328, 0.1326, 0.0414, 0.0395], grad_fn=<ToCopyBackward0>), [' think', ' really', ' would', ' thought', ' like'])\n",
      "(tensor([0.1581, 0.0868, 0.0803, 0.0769, 0.0713], grad_fn=<ToCopyBackward0>), [' enjoyed', ' would', ' liked', ' do', ' think'])\n",
      "(tensor([0.3820, 0.1938, 0.1222, 0.1101, 0.0385], grad_fn=<ToCopyBackward0>), ['.', ' like', ' think', ' enjoy', ' not'])\n",
      "(tensor([0.2976, 0.0647, 0.0632, 0.0584, 0.0470], grad_fn=<ToCopyBackward0>), [' it', ' this', ' the', ' comed', ' movies'])\n",
      "(tensor([0.5768, 0.1068, 0.0404, 0.0390, 0.0351], grad_fn=<ToCopyBackward0>), ['.', ' though', ' but', ' when', ','])\n",
      "(tensor([0.4837, 0.1348, 0.0419, 0.0358, 0.0210], grad_fn=<ToCopyBackward0>), [' I', ' It', ' If', ' The', 'I'])\n",
      "(tensor([0.4610, 0.2129, 0.0655, 0.0588, 0.0248], grad_fn=<ToCopyBackward0>), [' is', ' was', \"'s\", ' has', ' had'])\n",
      "\n",
      "\n",
      "\n",
      "0: I thought this film was terrible. It seemed to be a big commercial effort for a TV series. The acting was terrible. The story was awful. The cinematography was terrible. The editing was terrible, too. The music was horrible. I can't even\n",
      "(tensor([0.3847, 0.1714, 0.0898, 0.0770, 0.0474], grad_fn=<ToCopyBackward0>), [' this', ' that', ' I', ' it', ' the'])\n",
      "(tensor([0.4379, 0.2437, 0.1959, 0.0166, 0.0137], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' one'])\n",
      "(tensor([0.7763, 0.0504, 0.0283, 0.0264, 0.0101], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' is', ' could'])\n",
      "(tensor([0.1274, 0.0770, 0.0659, 0.0557, 0.0419], grad_fn=<ToCopyBackward0>), [' pretty', ' a', ' very', ' terrible', ' so'])\n",
      "(tensor([0.4615, 0.1215, 0.0733, 0.0429, 0.0328], grad_fn=<ToCopyBackward0>), ['.', '!', ' and', ',', ' when'])\n",
      "(tensor([0.2959, 0.1417, 0.1014, 0.0334, 0.0190], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' Not', 'I'])\n",
      "(tensor([0.3068, 0.1546, 0.0596, 0.0532, 0.0441], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' seemed'])\n",
      "(tensor([0.5244, 0.2043, 0.0861, 0.0370, 0.0331], grad_fn=<ToCopyBackward0>), [' to', ' like', ' as', ' more', ' that'])\n",
      "(tensor([0.3259, 0.2998, 0.1299, 0.0177, 0.0147], grad_fn=<ToCopyBackward0>), [' be', ' me', ' have', ' lack', ' go'])\n",
      "(tensor([0.0885, 0.0838, 0.0716, 0.0715, 0.0410], grad_fn=<ToCopyBackward0>), [' trying', ' a', ' shot', ' made', ' written'])\n",
      "(tensor([0.0737, 0.0319, 0.0277, 0.0224, 0.0221], grad_fn=<ToCopyBackward0>), [' big', ' bunch', ' remake', ' very', ' parody'])\n",
      "(tensor([0.5330, 0.0388, 0.0301, 0.0201, 0.0124], grad_fn=<ToCopyBackward0>), [' joke', ',', ' commercial', '-', ' waste'])\n",
      "(tensor([0.1945, 0.0645, 0.0584, 0.0458, 0.0404], grad_fn=<ToCopyBackward0>), [' for', ' effort', ' piece', 'ization', ','])\n",
      "(tensor([0.3980, 0.0653, 0.0613, 0.0506, 0.0499], grad_fn=<ToCopyBackward0>), [' to', ',', ' that', ' and', ' for'])\n",
      "(tensor([0.2869, 0.2427, 0.0467, 0.0353, 0.0309], grad_fn=<ToCopyBackward0>), [' a', ' the', ' no', ' this', ' people'])\n",
      "(tensor([0.1401, 0.0882, 0.0602, 0.0571, 0.0367], grad_fn=<ToCopyBackward0>), [' bunch', ' TV', ' movie', ' film', ' big'])\n",
      "(tensor([0.1722, 0.1612, 0.1575, 0.1533, 0.0589], grad_fn=<ToCopyBackward0>), [' movie', ' station', ' show', ' series', '-'])\n",
      "(tensor([0.3011, 0.1324, 0.1025, 0.0672, 0.0614], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', ' (', ' that'])\n",
      "(tensor([0.2050, 0.1332, 0.1308, 0.0327, 0.0325], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', 'The', ' There'])\n",
      "(tensor([0.2948, 0.0450, 0.0333, 0.0319, 0.0305], grad_fn=<ToCopyBackward0>), [' acting', ' characters', ' actors', ' plot', ' story'])\n",
      "(tensor([0.7788, 0.0542, 0.0246, 0.0228, 0.0200], grad_fn=<ToCopyBackward0>), [' was', ' wasn', ' seemed', ' is', ' and'])\n",
      "(tensor([0.1290, 0.1231, 0.0767, 0.0431, 0.0394], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' bad', ' horrible', ' poor'])\n",
      "(tensor([0.4184, 0.2541, 0.2336, 0.0101, 0.0095], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' as', ';'])\n",
      "(tensor([0.5337, 0.1003, 0.0907, 0.0418, 0.0215], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.3270, 0.1303, 0.1039, 0.0402, 0.0345], grad_fn=<ToCopyBackward0>), [' plot', ' script', ' story', ' writing', ' cinem'])\n",
      "(tensor([0.6600, 0.0739, 0.0271, 0.0209, 0.0209], grad_fn=<ToCopyBackward0>), [' was', ' seemed', ' line', ' is', ' just'])\n",
      "(tensor([0.1011, 0.0912, 0.0534, 0.0463, 0.0353], grad_fn=<ToCopyBackward0>), [' terrible', ' ridiculous', ' stupid', ' not', ' awful'])\n",
      "(tensor([0.8126, 0.1128, 0.0331, 0.0057, 0.0050], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '!', '...'])\n",
      "(tensor([0.4087, 0.1597, 0.0898, 0.0585, 0.0248], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0753, 0.0622, 0.0513, 0.0457, 0.0383], grad_fn=<ToCopyBackward0>), [' cinem', ' script', ' editing', ' ending', ' plot'])\n",
      "(tensor([9.9949e-01, 3.5419e-04, 3.8048e-05, 2.9056e-05, 5.7394e-06],\n",
      "       grad_fn=<ToCopyBackward0>), ['atography', 'at', 'as', 'atics', 'atically'])\n",
      "(tensor([0.8191, 0.0298, 0.0191, 0.0133, 0.0128], grad_fn=<ToCopyBackward0>), [' was', ' and', ',', ' wasn', ' looked'])\n",
      "(tensor([0.1363, 0.1196, 0.0440, 0.0343, 0.0322], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' just', ' bad'])\n",
      "(tensor([0.9243, 0.0210, 0.0185, 0.0064, 0.0038], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', ' ('])\n",
      "(tensor([0.5557, 0.0827, 0.0733, 0.0568, 0.0293], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.2033, 0.0919, 0.0698, 0.0507, 0.0444], grad_fn=<ToCopyBackward0>), [' editing', ' script', ' plot', ' acting', ' directing'])\n",
      "(tensor([0.8936, 0.0162, 0.0130, 0.0110, 0.0096], grad_fn=<ToCopyBackward0>), [' was', ' and', ',', ' is', ' wasn'])\n",
      "(tensor([0.5092, 0.0739, 0.0678, 0.0225, 0.0218], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' horrendous', ' bad'])\n",
      "(tensor([0.9305, 0.0224, 0.0172, 0.0058, 0.0028], grad_fn=<ToCopyBackward0>), ['.', ' and', ',', '...', '!'])\n",
      "(tensor([0.2351, 0.2157, 0.1119, 0.0465, 0.0384], grad_fn=<ToCopyBackward0>), [' and', ' the', ' but', ' too', ' as'])\n",
      "(tensor([0.9414, 0.0232, 0.0064, 0.0041, 0.0034], grad_fn=<ToCopyBackward0>), ['.', ',', '.\"', ' bad', '!'])\n",
      "(tensor([0.2548, 0.1653, 0.1160, 0.0446, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.1129, 0.0703, 0.0694, 0.0519, 0.0479], grad_fn=<ToCopyBackward0>), [' only', ' editing', ' acting', ' script', ' music'])\n",
      "(tensor([0.7867, 0.0245, 0.0195, 0.0153, 0.0146], grad_fn=<ToCopyBackward0>), [' was', ',', ' just', ' and', ' is'])\n",
      "(tensor([0.2721, 0.0729, 0.0553, 0.0502, 0.0325], grad_fn=<ToCopyBackward0>), [' terrible', ' awful', ' horrible', ' bad', ' not'])\n",
      "(tensor([0.8304, 0.0700, 0.0392, 0.0110, 0.0089], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', '!'])\n",
      "(tensor([0.3994, 0.1228, 0.0934, 0.0657, 0.0286], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' And', 'The'])\n",
      "(tensor([0.0752, 0.0696, 0.0675, 0.0644, 0.0574], grad_fn=<ToCopyBackward0>), [\"'m\", ' was', ' can', ' just', ' don'])\n",
      "(tensor([0.6054, 0.1139, 0.0534, 0.0240, 0.0195], grad_fn=<ToCopyBackward0>), [\"'t\", ' honestly', ' see', ' remember', ' only'])\n",
      "(tensor([0.2801, 0.2112, 0.0937, 0.0747, 0.0506], grad_fn=<ToCopyBackward0>), [' believe', ' even', ' really', ' remember', ' imagine'])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_two_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50, sen_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93038858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_triple_alt= Model_Import_6.MultiHeadModel_PyTorch_Stacked_Triple_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_triple_alt.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22a5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-92ca123d1d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_style_w_dev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_stacked_triple_alt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_token_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mneg_token_ids_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dev implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6280d96fe765>\u001b[0m in \u001b[0;36mtrain_one_style_w_dev\u001b[0;34m(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# print(stacked_context_sample.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mnetwork_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_context_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken_ids_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ONE text id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_triple_alt, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n",
      "0: I thought it would have a lot to say, and it has. It is not just about what it says about me. I am a person, a person with feelings and a life, a human. It's about the fact I have a family and I love my wife, my kids and I have friends and family and friends of my family and I love my friends. I have friends that are not like this, friends who have a lot more in their life. It is not about what it is.\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_triple_alt, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417967d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_cross_positional= Model_Import_6.MultiHeadModel_PyTorch_Stacked_Positional(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_cross_positional.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.3605806827545166\n",
      "DEV LOSS: 3.2910168653964997\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.248650312423706\n",
      "DEV LOSS: 3.280663930606842\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.177448034286499\n",
      "DEV LOSS: 3.2733029612064364\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1034562587738037\n",
      "DEV LOSS: 3.288507435274124\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.025240421295166\n",
      "DEV LOSS: 3.30116226682663\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_cross_positional, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "WEIRD TOKEN PREDICTED\n",
      "0: I thought it was good to have some of those guys in there, but I was a big part in getting the movie to where we are now, so it's not really fair. But it's not a good movie, but I don´ve got a feeling it will get a good review when the movie is released in theaters.\"I was in a relationship at that point and we had been dating a year, so it wasn't a good time. But it wasn´ve a lot of good times and a good movie\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n",
      "0: I thought it was good to have some of those guys in there, but I was really disappointed to not see a whole lot from them in this series, I mean I was hoping to have some good things from those two guys, and I was not. It's not like I was really disappointed with the rest, I was really looking for some help. I didn´ve seen any of those other two, and it´´´´d be good for me if I got a few more guys. But it didn't\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_cross_positional, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45882328",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2936744689941406\n",
      "DEV LOSS: 3.245574431180954\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.223997116088867\n",
      "DEV LOSS: 3.2370047632217407\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1751065254211426\n",
      "DEV LOSS: 3.239879521083832\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.121413230895996\n",
      "DEV LOSS: 3.2442128241062163\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.0633127689361572\n",
      "DEV LOSS: 3.2566996613502504\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5099291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the canvas. It's not the worst movie ever made. But I can only imagine the real story. I was very young, I didn`ve just graduated highschool and was working\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n",
      "0: I now know what it's all like. It was like watching paint drying on the wall of the house of the devil, the devil's own home, and the movie is so awful, I couldn`ve never even been a fan. The movie was awful,\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdfc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_stacked_one_not_cross= Model_Import_6.MultiHeadModel_PyTorch_Stacked_One_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_stacked_one_not_cross.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921636e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 1.9614466428756714\n",
      "DEV LOSS: 0.82565540587008\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.582233190536499\n",
      "DEV LOSS: 0.5536075608596206\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.36249151825904846\n",
      "DEV LOSS: 0.45461988470181824\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.27979013323783875\n",
      "DEV LOSS: 0.4196639986075461\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 0.2359471619129181\n",
      "DEV LOSS: 0.4072770089544356\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_stacked_one_not_cross, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: The movie movie movie movie movie movie movie movie movie movie movie\n",
      "(tensor([9.9798e-01, 1.0970e-03, 1.6808e-04, 1.4440e-04, 8.4563e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', ' movies', 'movie', ' the', ' theater'])\n",
      "(tensor([9.9904e-01, 7.2250e-04, 4.6235e-05, 3.1694e-05, 1.5173e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' movies', ' picture', ' theater'])\n",
      "(tensor([9.9865e-01, 9.7820e-04, 4.1188e-05, 3.6795e-05, 2.6668e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' movies', ' game', ' picture'])\n",
      "(tensor([9.9883e-01, 7.1711e-04, 5.0114e-05, 4.7247e-05, 2.7799e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', ' picture'])\n",
      "(tensor([9.9917e-01, 3.6677e-04, 4.7966e-05, 4.2018e-05, 2.5491e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', ' picture'])\n",
      "(tensor([9.9896e-01, 3.2737e-04, 6.8437e-05, 5.9356e-05, 3.5498e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', '.'])\n",
      "(tensor([9.9859e-01, 2.8834e-04, 9.5757e-05, 8.4732e-05, 6.8489e-05],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', ' movies', '.'])\n",
      "(tensor([9.9805e-01, 2.6942e-04, 1.2380e-04, 1.1909e-04, 1.1148e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', ' game', '.', ' movies'])\n",
      "(tensor([9.9737e-01, 2.5487e-04, 1.7901e-04, 1.5267e-04, 1.3634e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', 'movie', '.', ' game', ' movies'])\n",
      "(tensor([9.9649e-01, 2.6195e-04, 2.5256e-04, 1.8308e-04, 1.7963e-04],\n",
      "       device='cuda:0', grad_fn=<TopkBackward0>), [' movie', '.', 'movie', ' game', ' guy'])\n"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_stacked_one_not_cross, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 10, sen_to_generate = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_basic_positional= Model_Import_6.MultiHeadModel_PyTorch_Positional(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_basic_positional.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e4523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.3547589778900146\n",
      "DEV LOSS: 3.295817288970947\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.257972478866577\n",
      "DEV LOSS: 3.276579099750519\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1971757411956787\n",
      "DEV LOSS: 3.2754206407546995\n",
      "----------------------------------------\n",
      "..........Epoch: 3, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.1409406661987305\n",
      "DEV LOSS: 3.2812213646888733\n",
      "----------------------------------------\n",
      "..........Epoch: 4, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.0841259956359863\n",
      "DEV LOSS: 3.29154849858284\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic_positional, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 5, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8705f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytorch_basic_positional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b22fc650e9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_style_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_basic_positional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pytorch_basic_positional' is not defined"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic_positional, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dd0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "pytorch_basic= Model_Import_6.MultiHeadModel_PyTorch(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "neg_optimizer = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39764583",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198c9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch: 0, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2852327823638916\n",
      "DEV LOSS: 3.245586234807968\n",
      "----------------------------------------\n",
      "..........Epoch: 1, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.2262442111968994\n",
      "DEV LOSS: 3.2330075203418733\n",
      "----------------------------------------\n",
      "..........Epoch: 2, Epoch Examples: 10000\n",
      "TRAIN LOSS: 3.182225227355957\n",
      "DEV LOSS: 3.2341347328186036\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 3, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0e1655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I thought this film was a good movie. I liked the cast, I liked the script. It's not the most entertaining movie ever made, but if you're looking for something to get you through a tough time in life, this one's for the job\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "/n/n\n",
      "0: I thought it was funny to watch this movie because it has nothing to do with the original, and it's not even funny because it's just so bad. The original is a classic of the genre. It's one of the greatest films ever. I don\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "/n/n\n",
      "0: I thought I'd like to watch this movie with my friends, because this movie is really funny. It was really boring when it first came out. It's really boring now. I don't know why. It's really boring. It's boring, and\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "/n/n\n",
      "0: I thought this was a pretty bad film. It was just bad in so many ways. It was just bad. It was bad. The acting was bad. The script was bad. And so many of the characters in it were bad, and it was just\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "/n/n\n",
      "0: I thought it was the worst film of all time. I think it's the worst film of all time. It's a bad movie, but the acting is horrible. It's so awful. It's like the best thing you can say about this movie is\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "/n/n\n",
      "0: I thought the worst thing about this movie was that it was so predictable and so bad, but it was so predictable I actually laughed out loud at a few moments in it. It's so predictable, I mean it's so predictable, it's like a soap\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "/n/n\n",
      "0: I thought it would be nice to be a bit of a bad-ass and go into this movie without any preconceptions about how to watch it. This was the first time in a long time that I was not intimidated by the subject matter. I was looking\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "(tensor([0.1569, 0.1451, 0.1326, 0.1002, 0.0757], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4590, 0.2953, 0.0466, 0.0197, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.9160, 0.0392, 0.0055, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' take', ' help'])\n",
      "(tensor([0.2873, 0.1349, 0.0694, 0.0633, 0.0625], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' funny', ' nice', ' fun'])\n",
      "(tensor([0.8317, 0.0872, 0.0547, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' to', ' if', ' for', ',', ' and'])\n",
      "(tensor([0.2233, 0.1080, 0.0635, 0.0575, 0.0444], grad_fn=<ToCopyBackward0>), [' have', ' see', ' show', ' give', ' be'])\n",
      "(tensor([0.4230, 0.1008, 0.0924, 0.0369, 0.0175], grad_fn=<ToCopyBackward0>), [' able', ' a', ' in', ' the', ' with'])\n",
      "(tensor([0.3518, 0.1345, 0.0694, 0.0301, 0.0295], grad_fn=<ToCopyBackward0>), [' little', ' part', ' bit', ' good', ' big'])\n",
      "(tensor([0.4939, 0.2087, 0.0232, 0.0231, 0.0135], grad_fn=<ToCopyBackward0>), [' more', ' of', ' less', ' different', ' better'])\n",
      "(tensor([0.7002, 0.1057, 0.0392, 0.0089, 0.0042], grad_fn=<ToCopyBackward0>), [' a', ' an', ' fun', ' the', ' \"'])\n",
      "(tensor([0.1252, 0.0228, 0.0207, 0.0194, 0.0133], grad_fn=<ToCopyBackward0>), [' bad', ' fan', ' cheer', ' good', ' \"'])\n",
      "(tensor([0.3039, 0.2247, 0.1490, 0.0625, 0.0448], grad_fn=<ToCopyBackward0>), [' guy', '-', ' ass', 'die', ' cop'])\n",
      "(tensor([0.7853, 0.0375, 0.0213, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), ['ass', 'guy', 'boy', 'a', 'cop'])\n",
      "(tensor([0.1451, 0.0947, 0.0941, 0.0629, 0.0334], grad_fn=<ToCopyBackward0>), [' and', ',', ' in', '.', ' for'])\n",
      "(tensor([0.0932, 0.0667, 0.0481, 0.0467, 0.0325], grad_fn=<ToCopyBackward0>), [' show', ' to', ' take', ' go', ' have'])\n",
      "(tensor([0.1624, 0.1536, 0.0694, 0.0592, 0.0535], grad_fn=<ToCopyBackward0>), [' out', ' to', ' into', ' for', ' in'])\n",
      "(tensor([0.5626, 0.1353, 0.1168, 0.0193, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' battle', ' it'])\n",
      "(tensor([0.4028, 0.1172, 0.0959, 0.0198, 0.0197], grad_fn=<ToCopyBackward0>), [' movie', ' with', ' film', ' thing', ' as'])\n",
      "(tensor([0.2565, 0.1413, 0.0597, 0.0397, 0.0372], grad_fn=<ToCopyBackward0>), [' with', ' as', ' without', ' like', ' and'])\n",
      "(tensor([0.3053, 0.1414, 0.0866, 0.0593, 0.0346], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' having', ' being'])\n",
      "(tensor([0.2164, 0.0379, 0.0337, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' precon', ' expectations', ' of', ' knowledge', ' real'])\n",
      "(tensor([6.2642e-01, 3.5667e-01, 1.4625e-02, 2.9604e-04, 2.9408e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ceived', 'ceptions', 'ception', 'c', 'ci'])\n",
      "(tensor([0.2519, 0.1710, 0.0975, 0.0791, 0.0759], grad_fn=<ToCopyBackward0>), ['.', ' about', ',', ' of', ' or'])\n",
      "(tensor([0.2766, 0.2001, 0.1954, 0.1053, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' what', ' the', ' how', ' who'])\n",
      "(tensor([0.3231, 0.1287, 0.0968, 0.0699, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' I', ' to', ' the', ' bad'])\n",
      "(tensor([0.1385, 0.1084, 0.0691, 0.0584, 0.0523], grad_fn=<ToCopyBackward0>), [' act', ' play', ' do', ' approach', ' watch'])\n",
      "(tensor([0.6693, 0.1506, 0.0336, 0.0306, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' a', ' or', ' the', ' this'])\n",
      "(tensor([0.6428, 0.1237, 0.0932, 0.0321, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', '...'])\n",
      "(tensor([0.2183, 0.0685, 0.0617, 0.0481, 0.0375], grad_fn=<ToCopyBackward0>), [' I', ' It', ' So', ' This', ' The'])\n",
      "(tensor([0.3423, 0.2492, 0.0884, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' way'])\n",
      "(tensor([0.2108, 0.1597, 0.0965, 0.0602, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' supposed', ' my'])\n",
      "(tensor([0.2119, 0.2028, 0.0604, 0.0362, 0.0348], grad_fn=<ToCopyBackward0>), [' worst', ' first', ' only', ' most', ' movie'])\n",
      "(tensor([0.5333, 0.1501, 0.1121, 0.0196, 0.0105], grad_fn=<ToCopyBackward0>), [' movie', ' time', ' film', ' horror', ' one'])\n",
      "(tensor([0.7881, 0.0596, 0.0358, 0.0097, 0.0073], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' we', ' since'])\n",
      "(tensor([0.4479, 0.2198, 0.0428, 0.0378, 0.0209], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' years', ' many'])\n",
      "(tensor([0.6355, 0.2477, 0.0271, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' long', ' while', ' very', ' few', ' really'])\n",
      "(tensor([0.7863, 0.1842, 0.0201, 0.0037, 0.0010], grad_fn=<ToCopyBackward0>), [' time', ' while', ',', ' long', '-'])\n",
      "(tensor([0.5275, 0.2670, 0.1061, 0.0269, 0.0149], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' when', ','])\n",
      "(tensor([0.9208, 0.0131, 0.0078, 0.0072, 0.0071], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' we', ' a'])\n",
      "(tensor([0.1189, 0.0911, 0.0725, 0.0655, 0.0561], grad_fn=<ToCopyBackward0>), [' was', ' didn', \"'ve\", ' had', ' watched'])\n",
      "(tensor([0.1623, 0.1010, 0.0830, 0.0684, 0.0371], grad_fn=<ToCopyBackward0>), [' able', ' actually', ' not', ' really', ' looking'])\n",
      "(tensor([0.0670, 0.0607, 0.0386, 0.0268, 0.0243], grad_fn=<ToCopyBackward0>), [' afraid', ' intimidated', ' expecting', ' disappointed', ' looking'])\n",
      "(tensor([0.7763, 0.0367, 0.0298, 0.0209, 0.0190], grad_fn=<ToCopyBackward0>), [' by', ' to', ' or', '.', ' when'])\n",
      "(tensor([0.3455, 0.1528, 0.0787, 0.0572, 0.0229], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' anything', ' something'])\n",
      "(tensor([0.1436, 0.0985, 0.0651, 0.0317, 0.0298], grad_fn=<ToCopyBackward0>), [' idea', ' prospect', ' subject', ' genre', ' thought'])\n",
      "(tensor([0.9292, 0.0202, 0.0166, 0.0120, 0.0057], grad_fn=<ToCopyBackward0>), [' matter', '.', ' of', ' material', ','])\n",
      "(tensor([0.4196, 0.1691, 0.1087, 0.0784, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' or'])\n",
      "(tensor([0.2578, 0.0957, 0.0462, 0.0431, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1626, 0.0484, 0.0475, 0.0404, 0.0304], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' watched', ' didn'])\n",
      "(tensor([0.0837, 0.0786, 0.0496, 0.0426, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' actually', ' looking', ' excited', ' able'])\n",
      "/n/n\n",
      "0: I thought it was funny. This movie is terrible and it is so bad it's funny. This movie was bad, but it was funny. The humor in this movie is just awful. I can only imagine what this movie would be like if it were written\n",
      "(tensor([0.1569, 0.1447, 0.1324, 0.1013, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4233, 0.1911, 0.1710, 0.0260, 0.0237], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.5825, 0.0998, 0.0550, 0.0250, 0.0232], grad_fn=<ToCopyBackward0>), [' was', ' would', ' had', ' could', ' is'])\n",
      "(tensor([0.1002, 0.0690, 0.0670, 0.0490, 0.0438], grad_fn=<ToCopyBackward0>), [' a', ' so', ' terrible', ' awful', ' bad'])\n",
      "(tensor([0.0743, 0.0586, 0.0508, 0.0455, 0.0369], grad_fn=<ToCopyBackward0>), [' good', ' great', ' little', ' waste', ' disappointment'])\n",
      "(tensor([0.1203, 0.1025, 0.0779, 0.0592, 0.0581], grad_fn=<ToCopyBackward0>), [' idea', ' example', ' one', ' movie', ' film'])\n",
      "(tensor([0.2707, 0.1749, 0.1378, 0.0458, 0.0344], grad_fn=<ToCopyBackward0>), ['.', ' but', ',', ' and', ' with'])\n",
      "(tensor([0.2530, 0.2433, 0.0743, 0.0463, 0.0205], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' But', ' There'])\n",
      "(tensor([0.2540, 0.1064, 0.0614, 0.0575, 0.0354], grad_fn=<ToCopyBackward0>), [' thought', ' liked', ' think', ' was', ' didn'])\n",
      "(tensor([0.4711, 0.2096, 0.0294, 0.0274, 0.0270], grad_fn=<ToCopyBackward0>), [' the', ' it', ' some', ' all', ' how'])\n",
      "(tensor([0.1189, 0.0734, 0.0517, 0.0505, 0.0445], grad_fn=<ToCopyBackward0>), [' idea', ' acting', ' characters', ' story', ' cast'])\n",
      "(tensor([0.3669, 0.2398, 0.1774, 0.0464, 0.0183], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' of', ' but'])\n",
      "(tensor([0.2774, 0.1977, 0.1268, 0.1043, 0.0238], grad_fn=<ToCopyBackward0>), [' I', ' the', ' and', ' but', ' especially'])\n",
      "(tensor([0.7581, 0.1036, 0.0470, 0.0170, 0.0095], grad_fn=<ToCopyBackward0>), [' liked', ' thought', ' like', ' enjoyed', ' think'])\n",
      "(tensor([0.8045, 0.0201, 0.0151, 0.0107, 0.0079], grad_fn=<ToCopyBackward0>), [' the', ' some', ' what', ' how', ' all'])\n",
      "(tensor([0.1449, 0.0918, 0.0913, 0.0829, 0.0425], grad_fn=<ToCopyBackward0>), [' story', ' direction', ' idea', ' script', ' premise'])\n",
      "(tensor([0.4939, 0.3292, 0.0990, 0.0142, 0.0118], grad_fn=<ToCopyBackward0>), [',', '.', ' and', '...', ' but'])\n",
      "(tensor([0.3436, 0.1459, 0.1109, 0.0678, 0.0280], grad_fn=<ToCopyBackward0>), [' I', ' It', ' But', ' The', ' And'])\n",
      "(tensor([0.3151, 0.2566, 0.0754, 0.0590, 0.0466], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.1870, 0.1832, 0.1306, 0.0235, 0.0195], grad_fn=<ToCopyBackward0>), [' a', ' not', ' just', ' hard', ' got'])\n",
      "(tensor([0.2184, 0.0765, 0.0502, 0.0415, 0.0392], grad_fn=<ToCopyBackward0>), [' a', ' the', ' bad', ' as', ' one'])\n",
      "(tensor([0.5467, 0.1109, 0.0481, 0.0319, 0.0302], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' greatest', ' kind', ' most'])\n",
      "(tensor([0.1822, 0.0797, 0.0562, 0.0199, 0.0138], grad_fn=<ToCopyBackward0>), [' original', ' interesting', ' exciting', ' entertaining', ' brilliant'])\n",
      "(tensor([0.4290, 0.2463, 0.0929, 0.0460, 0.0299], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ',', ' or'])\n",
      "(tensor([0.2937, 0.2066, 0.1223, 0.0480, 0.0453], grad_fn=<ToCopyBackward0>), [' I', ',', ' ever', '.', ' in'])\n",
      "(tensor([0.3286, 0.3099, 0.1478, 0.0827, 0.0143], grad_fn=<ToCopyBackward0>), [' made', ',', '.', ' but', ' and'])\n",
      "(tensor([0.6039, 0.1491, 0.1211, 0.0195, 0.0145], grad_fn=<ToCopyBackward0>), [',', ' but', '.', ' and', ' by'])\n",
      "(tensor([0.8211, 0.0454, 0.0273, 0.0102, 0.0093], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' I', ' or'])\n",
      "(tensor([0.6032, 0.1316, 0.0304, 0.0216, 0.0199], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' if', ' the'])\n",
      "(tensor([0.7910, 0.0359, 0.0354, 0.0151, 0.0119], grad_fn=<ToCopyBackward0>), [' you', ' it', ' I', ' there', ' the'])\n",
      "(tensor([0.2761, 0.1446, 0.1063, 0.0852, 0.0411], grad_fn=<ToCopyBackward0>), [\"'re\", ' like', ' want', ' can', ' are'])\n",
      "(tensor([0.2567, 0.1759, 0.0917, 0.0687, 0.0607], grad_fn=<ToCopyBackward0>), [' looking', ' a', ' into', ' going', ' in'])\n",
      "(tensor([9.6031e-01, 3.2702e-02, 2.6048e-03, 7.7270e-04, 4.2367e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' for', ' to', ' at', ' forward', ' in'])\n",
      "(tensor([0.4488, 0.2523, 0.0621, 0.0274, 0.0193], grad_fn=<ToCopyBackward0>), [' a', ' something', ' an', ' some', ' entertainment'])\n",
      "(tensor([0.3402, 0.1107, 0.0527, 0.0310, 0.0293], grad_fn=<ToCopyBackward0>), [' to', ' that', ' interesting', ' good', ' with'])\n",
      "(tensor([0.1493, 0.0684, 0.0675, 0.0269, 0.0260], grad_fn=<ToCopyBackward0>), [' watch', ' get', ' do', ' take', ' entertain'])\n",
      "(tensor([0.4354, 0.1505, 0.0346, 0.0223, 0.0208], grad_fn=<ToCopyBackward0>), [' you', ' your', ' into', ' the', ' off'])\n",
      "(tensor([0.1997, 0.1630, 0.0763, 0.0605, 0.0545], grad_fn=<ToCopyBackward0>), [' through', ' to', ' in', ' going', ' into'])\n",
      "(tensor([0.5495, 0.1306, 0.0598, 0.0281, 0.0277], grad_fn=<ToCopyBackward0>), [' the', ' a', ' your', ' this', ' to'])\n",
      "(tensor([0.2095, 0.1387, 0.0533, 0.0349, 0.0307], grad_fn=<ToCopyBackward0>), [' bad', ' long', ' tough', ' cold', ' dark'])\n",
      "(tensor([0.2420, 0.1200, 0.1042, 0.0356, 0.0221], grad_fn=<ToCopyBackward0>), [' time', ' day', ' week', ' period', ' weekend'])\n",
      "(tensor([0.4476, 0.2363, 0.0585, 0.0375, 0.0284], grad_fn=<ToCopyBackward0>), [',', ' in', ' or', ' and', ' of'])\n",
      "(tensor([0.5024, 0.4664, 0.0120, 0.0042, 0.0010], grad_fn=<ToCopyBackward0>), [' life', ' your', ' the', ' a', ' college'])\n",
      "(tensor([0.6620, 0.0561, 0.0529, 0.0372, 0.0222], grad_fn=<ToCopyBackward0>), [',', ' or', ' then', ' and', ' it'])\n",
      "(tensor([0.1810, 0.1586, 0.1116, 0.0750, 0.0605], grad_fn=<ToCopyBackward0>), [' this', ' it', ' then', ' I', ' or'])\n",
      "(tensor([0.4285, 0.2507, 0.0874, 0.0427, 0.0350], grad_fn=<ToCopyBackward0>), [' is', ' movie', ' film', ' one', ' could'])\n",
      "(tensor([0.3998, 0.1560, 0.1076, 0.0427, 0.0427], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' might', ' will', ' should'])\n",
      "(tensor([0.6713, 0.0690, 0.0308, 0.0274, 0.0182], grad_fn=<ToCopyBackward0>), [' for', ' a', ' worth', ' got', ' the'])\n",
      "(tensor([9.7535e-01, 1.9488e-02, 1.2270e-03, 6.5907e-04, 3.6169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' you', ' ya', ' sure', ' the', ' YOU'])\n",
      "(tensor([0.1031, 0.0569, 0.0464, 0.0382, 0.0365], grad_fn=<ToCopyBackward0>), [' kids', ' job', ' ages', ' book', ' books'])\n",
      "(tensor([0.1568, 0.1446, 0.1323, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2945, 0.0468, 0.0196, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1888, 0.1134, 0.0585, 0.0369, 0.0291], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1405, 0.1239, 0.1138, 0.0995, 0.0950], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2217, 0.1881, 0.0547, 0.0458, 0.0427], grad_fn=<ToCopyBackward0>), [' watch', ' see', ' have', ' make', ' be'])\n",
      "(tensor([0.2125, 0.1763, 0.0396, 0.0358, 0.0188], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' as', ' all'])\n",
      "(tensor([0.4092, 0.0663, 0.0207, 0.0190, 0.0171], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' in', ' show', ' little'])\n",
      "(tensor([0.1613, 0.1598, 0.0777, 0.0701, 0.0616], grad_fn=<ToCopyBackward0>), [' because', '.', ' with', ' when', ','])\n",
      "(tensor([0.2429, 0.1331, 0.1224, 0.0826, 0.0248], grad_fn=<ToCopyBackward0>), [' it', ' I', ' of', ' the', ' there'])\n",
      "(tensor([0.3085, 0.2603, 0.0764, 0.0333, 0.0294], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' had', ' has'])\n",
      "(tensor([0.1321, 0.1052, 0.0901, 0.0587, 0.0571], grad_fn=<ToCopyBackward0>), [' a', ' all', ' so', ' the', ' nothing'])\n",
      "(tensor([0.8878, 0.0173, 0.0116, 0.0090, 0.0078], grad_fn=<ToCopyBackward0>), [' to', ' but', ' whatsoever', ' in', ' at'])\n",
      "(tensor([0.9653, 0.0119, 0.0040, 0.0018, 0.0017], grad_fn=<ToCopyBackward0>), [' do', ' say', ' with', ' recommend', ' offer'])\n",
      "(tensor([9.8760e-01, 2.4418e-03, 1.3901e-03, 6.7449e-04, 4.9168e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' with', ' whatsoever', ' about', ' at', ' or'])\n",
      "(tensor([0.2152, 0.0321, 0.0307, 0.0306, 0.0306], grad_fn=<ToCopyBackward0>), [' the', ' any', ' me', ' anything', ' reality'])\n",
      "(tensor([0.1878, 0.1436, 0.0321, 0.0280, 0.0216], grad_fn=<ToCopyBackward0>), [' original', ' real', ' actual', ' book', ' first'])\n",
      "(tensor([0.2074, 0.0565, 0.0470, 0.0392, 0.0290], grad_fn=<ToCopyBackward0>), ['.', ',', ' story', ' movie', ' and'])\n",
      "(tensor([0.3236, 0.1622, 0.0591, 0.0507, 0.0507], grad_fn=<ToCopyBackward0>), [' but', ' and', ' except', ' which', ' it'])\n",
      "(tensor([0.1991, 0.1494, 0.0758, 0.0705, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' yet', ' the', ' I', ' everything'])\n",
      "(tensor([0.3762, 0.1324, 0.0703, 0.0693, 0.0382], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.1260, 0.0855, 0.0727, 0.0719, 0.0224], grad_fn=<ToCopyBackward0>), [' not', ' a', ' so', ' just', ' like'])\n",
      "(tensor([0.3975, 0.0742, 0.0549, 0.0544, 0.0308], grad_fn=<ToCopyBackward0>), [' even', ' a', ' funny', ' really', ' scary'])\n",
      "(tensor([0.2130, 0.0580, 0.0536, 0.0519, 0.0276], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' that', ' really'])\n",
      "(tensor([0.5953, 0.0661, 0.0463, 0.0380, 0.0262], grad_fn=<ToCopyBackward0>), ['.', ',', ' in', ' to', ' because'])\n",
      "(tensor([0.3695, 0.2233, 0.1038, 0.0538, 0.0307], grad_fn=<ToCopyBackward0>), [' it', ' of', ' the', ' I', ' there'])\n",
      "(tensor([0.6209, 0.1052, 0.0532, 0.0337, 0.0305], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' doesn'])\n",
      "(tensor([0.3163, 0.0987, 0.0526, 0.0484, 0.0334], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' bad', ' just'])\n",
      "(tensor([0.2173, 0.0822, 0.0622, 0.0320, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' so', ' plain', ' stupid'])\n",
      "(tensor([0.2733, 0.0758, 0.0627, 0.0375, 0.0265], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' stupid', ' awful', ' over'])\n",
      "(tensor([0.6943, 0.0557, 0.0511, 0.0387, 0.0212], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', '...'])\n",
      "(tensor([0.1609, 0.1602, 0.1347, 0.0366, 0.0215], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1341, 0.1071, 0.0819, 0.0462, 0.0318], grad_fn=<ToCopyBackward0>), [' only', ' acting', ' plot', ' original', ' characters'])\n",
      "(tensor([0.2228, 0.1236, 0.0912, 0.0526, 0.0294], grad_fn=<ToCopyBackward0>), [' was', ' is', ' movie', ' \"', ' film'])\n",
      "(tensor([0.1137, 0.1097, 0.0439, 0.0345, 0.0323], grad_fn=<ToCopyBackward0>), [' a', ' so', ' great', ' funny', ' one'])\n",
      "(tensor([0.1588, 0.1580, 0.1378, 0.0286, 0.0223], grad_fn=<ToCopyBackward0>), [' classic', ' good', ' great', ' very', ' comedy'])\n",
      "(tensor([0.2572, 0.1411, 0.0690, 0.0624, 0.0518], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' of', ' that'])\n",
      "(tensor([0.2347, 0.1169, 0.0661, 0.0339, 0.0293], grad_fn=<ToCopyBackward0>), [' the', ' its', ' horror', ' genre', ' it'])\n",
      "(tensor([0.8930, 0.0169, 0.0037, 0.0033, 0.0027], grad_fn=<ToCopyBackward0>), [' genre', ' horror', ' form', ' 80', ' first'])\n",
      "(tensor([0.4382, 0.1980, 0.1433, 0.0340, 0.0222], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' that', ' but'])\n",
      "(tensor([0.1977, 0.1488, 0.1310, 0.1219, 0.0194], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' There'])\n",
      "(tensor([0.6088, 0.0806, 0.0663, 0.0627, 0.0109], grad_fn=<ToCopyBackward0>), [\"'s\", ' has', ' is', ' was', ' just'])\n",
      "(tensor([0.1190, 0.0700, 0.0639, 0.0477, 0.0421], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' just', ' one'])\n",
      "(tensor([0.9687, 0.0121, 0.0022, 0.0020, 0.0014], grad_fn=<ToCopyBackward0>), [' of', ' that', ' I', ' the', ' thing'])\n",
      "(tensor([0.6911, 0.1948, 0.0682, 0.0035, 0.0022], grad_fn=<ToCopyBackward0>), [' the', ' those', ' my', ' a', ' these'])\n",
      "(tensor([0.2134, 0.1928, 0.0722, 0.0696, 0.0688], grad_fn=<ToCopyBackward0>), [' funn', ' best', ' most', ' worst', ' greatest'])\n",
      "(tensor([0.1755, 0.1333, 0.1064, 0.0693, 0.0402], grad_fn=<ToCopyBackward0>), [' movies', ' comed', ' films', ' horror', '.'])\n",
      "(tensor([0.4484, 0.3190, 0.0555, 0.0369, 0.0308], grad_fn=<ToCopyBackward0>), [' ever', ' of', ' I', ' in', ' that'])\n",
      "(tensor([0.6527, 0.1691, 0.0539, 0.0261, 0.0096], grad_fn=<ToCopyBackward0>), [' made', '.', ',', ' to', ' put'])\n",
      "(tensor([0.1735, 0.1479, 0.1311, 0.1028, 0.0370], grad_fn=<ToCopyBackward0>), [' It', ' This', ' I', ' The', ' But'])\n",
      "(tensor([0.0829, 0.0610, 0.0610, 0.0590, 0.0541], grad_fn=<ToCopyBackward0>), [' thought', ' think', ' don', \"'m\", ' was'])\n",
      "(tensor([0.1569, 0.1448, 0.1323, 0.1012, 0.0751], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.2757, 0.2317, 0.1134, 0.1002, 0.0480], grad_fn=<ToCopyBackward0>), [\"'d\", ' was', ' would', ' had', ' could'])\n",
      "(tensor([0.1206, 0.0990, 0.0692, 0.0494, 0.0424], grad_fn=<ToCopyBackward0>), [' like', ' give', ' seen', ' be', ' never'])\n",
      "(tensor([0.9467, 0.0137, 0.0072, 0.0034, 0.0022], grad_fn=<ToCopyBackward0>), [' to', ' a', ' the', ' it', ' see'])\n",
      "(tensor([0.1994, 0.0527, 0.0506, 0.0482, 0.0442], grad_fn=<ToCopyBackward0>), [' see', ' give', ' have', ' know', ' watch'])\n",
      "(tensor([0.4158, 0.1286, 0.0929, 0.0690, 0.0314], grad_fn=<ToCopyBackward0>), [' this', ' the', ' a', ' it', ' some'])\n",
      "(tensor([0.3818, 0.0697, 0.0557, 0.0328, 0.0315], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' one', '.', ' because'])\n",
      "(tensor([0.1498, 0.1268, 0.1115, 0.0718, 0.0420], grad_fn=<ToCopyBackward0>), ['.', ' with', ' because', ',', ' for'])\n",
      "(tensor([0.2098, 0.1288, 0.0923, 0.0845, 0.0735], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' friends', ' you'])\n",
      "(tensor([0.1733, 0.1320, 0.0911, 0.0684, 0.0531], grad_fn=<ToCopyBackward0>), [' friends', ' wife', ' kids', ' family', ' daughter'])\n",
      "(tensor([0.2545, 0.1689, 0.1509, 0.0368, 0.0285], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' because', ' but'])\n",
      "(tensor([0.3150, 0.1793, 0.1298, 0.0541, 0.0160], grad_fn=<ToCopyBackward0>), [' but', ' and', ' so', ' because', ' to'])\n",
      "(tensor([0.2490, 0.1740, 0.0663, 0.0494, 0.0330], grad_fn=<ToCopyBackward0>), [' it', ' I', ' we', ' they', ' this'])\n",
      "(tensor([0.3606, 0.3470, 0.0905, 0.0274, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.4958, 0.1351, 0.0612, 0.0181, 0.0142], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ' makes'])\n",
      "(tensor([0.2195, 0.0832, 0.0474, 0.0381, 0.0363], grad_fn=<ToCopyBackward0>), [' so', ' really', ' bad', ' a', ' terrible'])\n",
      "(tensor([0.3111, 0.1392, 0.1196, 0.0253, 0.0211], grad_fn=<ToCopyBackward0>), [' bad', ' funny', ' boring', ' good', ' stupid'])\n",
      "(tensor([0.5183, 0.1738, 0.1578, 0.0229, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' but'])\n",
      "(tensor([0.1845, 0.1476, 0.0725, 0.0639, 0.0402], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.6217, 0.1071, 0.0813, 0.0312, 0.0119], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' has', ' was', ' makes'])\n",
      "(tensor([0.1252, 0.0930, 0.0690, 0.0530, 0.0515], grad_fn=<ToCopyBackward0>), [' funny', ' a', ' really', ' very', ' so'])\n",
      "(tensor([0.5137, 0.0894, 0.0442, 0.0258, 0.0164], grad_fn=<ToCopyBackward0>), [' funny', ' bad', ' boring', ' fun', ' stupid'])\n",
      "(tensor([0.1801, 0.1575, 0.1467, 0.1041, 0.0562], grad_fn=<ToCopyBackward0>), [' to', '.', ',', ' when', ' and'])\n",
      "(tensor([0.6252, 0.1549, 0.0869, 0.0328, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' we', ' it', ' i', ' you'])\n",
      "(tensor([0.3778, 0.2529, 0.1421, 0.0818, 0.0231], grad_fn=<ToCopyBackward0>), [' was', ' first', ' came', ' started', ' comes'])\n",
      "(tensor([0.8439, 0.0768, 0.0097, 0.0092, 0.0071], grad_fn=<ToCopyBackward0>), [' came', ' started', ' aired', ' comes', ' got'])\n",
      "(tensor([0.9788, 0.0044, 0.0033, 0.0032, 0.0026], grad_fn=<ToCopyBackward0>), [' out', ' to', ' on', '.', ','])\n",
      "(tensor([0.3703, 0.3532, 0.0422, 0.0357, 0.0349], grad_fn=<ToCopyBackward0>), [',', '.', ' and', ' but', ' in'])\n",
      "(tensor([0.2197, 0.1709, 0.0855, 0.0624, 0.0314], grad_fn=<ToCopyBackward0>), [' It', ' I', ' But', ' The', ' And'])\n",
      "(tensor([0.4712, 0.2065, 0.0359, 0.0287, 0.0278], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' wasn'])\n",
      "(tensor([0.0887, 0.0745, 0.0728, 0.0660, 0.0644], grad_fn=<ToCopyBackward0>), [' not', ' really', ' funny', ' so', ' a'])\n",
      "(tensor([0.2830, 0.1260, 0.0489, 0.0402, 0.0326], grad_fn=<ToCopyBackward0>), [' funny', ' boring', ' bad', ' hard', ' not'])\n",
      "(tensor([0.5978, 0.0700, 0.0697, 0.0589, 0.0269], grad_fn=<ToCopyBackward0>), [' now', '.', ' when', ',', ' in'])\n",
      "(tensor([0.4808, 0.2283, 0.0914, 0.0474, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' because', ' when'])\n",
      "(tensor([0.2825, 0.1523, 0.0641, 0.0576, 0.0321], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.0772, 0.0769, 0.0766, 0.0502, 0.0473], grad_fn=<ToCopyBackward0>), [' think', ' don', ' like', ' thought', \"'m\"])\n",
      "(tensor([9.9761e-01, 5.7668e-04, 1.8306e-04, 1.6996e-04, 5.9707e-05],\n",
      "       grad_fn=<ToCopyBackward0>), [\"'t\", '`', '´', \"'\", '.'])\n",
      "(tensor([0.3812, 0.1781, 0.0916, 0.0625, 0.0431], grad_fn=<ToCopyBackward0>), [' know', ' think', ' understand', ' even', ' like'])\n",
      "(tensor([0.5054, 0.1980, 0.1062, 0.0636, 0.0296], grad_fn=<ToCopyBackward0>), [' why', ' what', ' if', ' how', ','])\n",
      "(tensor([0.1810, 0.1642, 0.1150, 0.0903, 0.0864], grad_fn=<ToCopyBackward0>), [' it', '.', ',', ' they', ' that'])\n",
      "(tensor([0.2807, 0.2315, 0.0449, 0.0426, 0.0388], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' Maybe'])\n",
      "(tensor([0.6731, 0.0569, 0.0444, 0.0223, 0.0183], grad_fn=<ToCopyBackward0>), [\"'s\", ' just', ' was', ' seems', ' doesn'])\n",
      "(tensor([0.1221, 0.1003, 0.0997, 0.0899, 0.0691], grad_fn=<ToCopyBackward0>), [' just', ' boring', ' not', ' really', ' like'])\n",
      "(tensor([0.6336, 0.0274, 0.0270, 0.0256, 0.0226], grad_fn=<ToCopyBackward0>), [' boring', ',', ' funny', ' bad', ' hard'])\n",
      "(tensor([0.5512, 0.1587, 0.1114, 0.0220, 0.0175], grad_fn=<ToCopyBackward0>), ['.', ' now', ',', ' to', ' because'])\n",
      "(tensor([0.2476, 0.1816, 0.0616, 0.0476, 0.0309], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' But', ' This'])\n",
      "(tensor([0.7362, 0.0424, 0.0232, 0.0202, 0.0167], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' just', ' has', ' doesn'])\n",
      "(tensor([0.2341, 0.1182, 0.0702, 0.0574, 0.0527], grad_fn=<ToCopyBackward0>), [' really', ' boring', ' not', ' just', ' so'])\n",
      "(tensor([0.3585, 0.1118, 0.1040, 0.0798, 0.0579], grad_fn=<ToCopyBackward0>), ['.', ',', ' because', ' now', ' to'])\n",
      "(tensor([0.2154, 0.1500, 0.1181, 0.0540, 0.0423], grad_fn=<ToCopyBackward0>), [' but', ' and', ' it', ' really', ' because'])\n",
      "(tensor([0.1568, 0.1449, 0.1324, 0.1010, 0.0752], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4240, 0.1909, 0.1708, 0.0260, 0.0236], grad_fn=<ToCopyBackward0>), [' movie', ' was', ' film', ' would', ' is'])\n",
      "(tensor([0.2753, 0.1607, 0.0730, 0.0365, 0.0269], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' an', ' supposed'])\n",
      "(tensor([0.1676, 0.1009, 0.0752, 0.0354, 0.0339], grad_fn=<ToCopyBackward0>), [' good', ' bad', ' great', ' really', ' pretty'])\n",
      "(tensor([0.2391, 0.1679, 0.0586, 0.0315, 0.0305], grad_fn=<ToCopyBackward0>), [' bad', ' good', ' funny', ' decent', ' lame'])\n",
      "(tensor([0.5963, 0.1767, 0.0125, 0.0120, 0.0086], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' comedy', ',', ' show'])\n",
      "(tensor([0.3837, 0.1301, 0.0620, 0.0401, 0.0348], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', '...', ' and'])\n",
      "(tensor([0.2534, 0.1619, 0.1069, 0.0275, 0.0181], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3084, 0.2537, 0.0390, 0.0374, 0.0338], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' had', ' wasn', ' is'])\n",
      "(tensor([0.1040, 0.0556, 0.0515, 0.0422, 0.0347], grad_fn=<ToCopyBackward0>), [' a', ' very', ' so', ' just', ' not'])\n",
      "(tensor([0.1518, 0.0883, 0.0859, 0.0665, 0.0472], grad_fn=<ToCopyBackward0>), [' a', ' bad', ' awful', ' so', ' terrible'])\n",
      "(tensor([0.5687, 0.0852, 0.0725, 0.0184, 0.0154], grad_fn=<ToCopyBackward0>), ['.', ' acting', ',', ' in', ' film'])\n",
      "(tensor([0.4802, 0.0918, 0.0912, 0.0796, 0.0588], grad_fn=<ToCopyBackward0>), [' every', ' a', ' so', ' the', ' all'])\n",
      "(tensor([0.9915, 0.0031, 0.0013, 0.0012, 0.0011], grad_fn=<ToCopyBackward0>), [' many', ' much', ',', ' so', ' far'])\n",
      "(tensor([0.7277, 0.1599, 0.0229, 0.0182, 0.0087], grad_fn=<ToCopyBackward0>), [' ways', ' different', ' aspects', ' areas', ' places'])\n",
      "(tensor([0.7345, 0.1039, 0.0240, 0.0237, 0.0201], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '...', ' that'])\n",
      "(tensor([0.1992, 0.1751, 0.1467, 0.0359, 0.0258], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' There', ' But'])\n",
      "(tensor([0.3895, 0.2072, 0.0532, 0.0531, 0.0416], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' just', ' wasn'])\n",
      "(tensor([0.1590, 0.0837, 0.0774, 0.0656, 0.0445], grad_fn=<ToCopyBackward0>), [' just', ' a', ' bad', ' so', ' boring'])\n",
      "(tensor([0.3218, 0.0930, 0.0612, 0.0386, 0.0346], grad_fn=<ToCopyBackward0>), [' bad', ' a', ' boring', ' so', ' awful'])\n",
      "(tensor([0.2242, 0.1597, 0.1297, 0.0493, 0.0357], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ' story', ','])\n",
      "(tensor([0.2192, 0.1858, 0.0893, 0.0378, 0.0374], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' There', ' Bad'])\n",
      "(tensor([0.5202, 0.1141, 0.0885, 0.0674, 0.0355], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' just', ' wasn', ' had'])\n",
      "(tensor([0.3730, 0.1139, 0.0576, 0.0507, 0.0300], grad_fn=<ToCopyBackward0>), [' just', ' bad', ' a', ' so', ' not'])\n",
      "(tensor([0.3262, 0.1524, 0.0724, 0.0300, 0.0287], grad_fn=<ToCopyBackward0>), [' in', '.', ' acting', ',', ' on'])\n",
      "(tensor([0.4471, 0.1192, 0.0576, 0.0287, 0.0284], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' Bad', ' There'])\n",
      "(tensor([0.3276, 0.0525, 0.0473, 0.0455, 0.0418], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' story', ' script', ' plot'])\n",
      "(tensor([0.6870, 0.0830, 0.0422, 0.0385, 0.0228], grad_fn=<ToCopyBackward0>), [' was', ',', ' wasn', ' is', '...'])\n",
      "(tensor([0.5863, 0.0882, 0.0650, 0.0443, 0.0271], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' awful', ' just', ' horrible'])\n",
      "(tensor([0.8261, 0.1112, 0.0107, 0.0098, 0.0079], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5570, 0.1509, 0.0671, 0.0278, 0.0199], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' There', ' And'])\n",
      "(tensor([0.1193, 0.1074, 0.1008, 0.0840, 0.0806], grad_fn=<ToCopyBackward0>), [' story', ' writing', ' script', ' plot', ' directing'])\n",
      "(tensor([0.8074, 0.0324, 0.0266, 0.0150, 0.0119], grad_fn=<ToCopyBackward0>), [' was', ',', ' is', ' wasn', '.'])\n",
      "(tensor([0.7879, 0.0437, 0.0302, 0.0169, 0.0124], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' just', ' awful', ' really'])\n",
      "(tensor([0.9507, 0.0233, 0.0050, 0.0049, 0.0013], grad_fn=<ToCopyBackward0>), ['.', ',', '...', ' and', ' in'])\n",
      "(tensor([0.5450, 0.1319, 0.0706, 0.0297, 0.0247], grad_fn=<ToCopyBackward0>), [' The', ' It', ' I', ' And', ' There'])\n",
      "(tensor([0.3368, 0.1446, 0.1265, 0.0518, 0.0266], grad_fn=<ToCopyBackward0>), [' the', ' I', ' it', ' then', ' so'])\n",
      "(tensor([0.1384, 0.1176, 0.0995, 0.0947, 0.0630], grad_fn=<ToCopyBackward0>), [' many', ' I', ',', ' on', ' it'])\n",
      "(tensor([0.3288, 0.2579, 0.1733, 0.0371, 0.0111], grad_fn=<ToCopyBackward0>), [' of', ' things', ' other', ' people', ' characters'])\n",
      "(tensor([0.9048, 0.0275, 0.0172, 0.0132, 0.0083], grad_fn=<ToCopyBackward0>), [' the', ' these', ' those', ' its', ' them'])\n",
      "(tensor([0.1907, 0.1289, 0.0601, 0.0596, 0.0364], grad_fn=<ToCopyBackward0>), [' characters', ' things', ' scenes', ' actors', ' people'])\n",
      "(tensor([0.6302, 0.0724, 0.0421, 0.0306, 0.0277], grad_fn=<ToCopyBackward0>), [' were', ',', ' are', ' in', ' weren'])\n",
      "(tensor([0.3466, 0.3184, 0.2488, 0.0364, 0.0161], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', ' that', ' there'])\n",
      "(tensor([0.6684, 0.0810, 0.0547, 0.0366, 0.0212], grad_fn=<ToCopyBackward0>), [' were', ',', ' just', ' are', ' weren'])\n",
      "(tensor([0.2038, 0.1904, 0.0629, 0.0621, 0.0242], grad_fn=<ToCopyBackward0>), [' bad', ' just', ' not', ' so', ' terrible'])\n",
      "(tensor([0.7426, 0.0667, 0.0463, 0.0187, 0.0180], grad_fn=<ToCopyBackward0>), ['.', ',', ' characters', ' in', ' and'])\n",
      "(tensor([0.2286, 0.0860, 0.0422, 0.0395, 0.0373], grad_fn=<ToCopyBackward0>), [' and', ' but', ' so', ' bad', ' too'])\n",
      "(tensor([0.1660, 0.1529, 0.1027, 0.0998, 0.0728], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' so', ' they'])\n",
      "(tensor([0.3757, 0.3006, 0.1050, 0.0248, 0.0240], grad_fn=<ToCopyBackward0>), [' just', ' was', \"'s\", ' wasn', ' really'])\n",
      "(tensor([0.5118, 0.0529, 0.0519, 0.0410, 0.0211], grad_fn=<ToCopyBackward0>), [' just', ' a', ' so', ' bad', ' really'])\n",
      "(tensor([0.1569, 0.1446, 0.1322, 0.1020, 0.0747], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4597, 0.2943, 0.0468, 0.0196, 0.0181], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1891, 0.1131, 0.0585, 0.0371, 0.0290], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.5756, 0.0400, 0.0353, 0.0186, 0.0117], grad_fn=<ToCopyBackward0>), [' worst', ' best', ' funn', ' most', ' biggest'])\n",
      "(tensor([0.5130, 0.0935, 0.0277, 0.0237, 0.0224], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' thing', ' horror', ' comedy'])\n",
      "(tensor([0.5931, 0.2010, 0.0567, 0.0437, 0.0209], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' of', ' i', ' in'])\n",
      "(tensor([0.6033, 0.2053, 0.0216, 0.0072, 0.0070], grad_fn=<ToCopyBackward0>), [' all', ' the', ' my', ' 2009', ' his'])\n",
      "(tensor([0.9735, 0.0098, 0.0054, 0.0028, 0.0018], grad_fn=<ToCopyBackward0>), [' time', '-', ' times', ' the', ' of'])\n",
      "(tensor([0.4571, 0.1029, 0.0499, 0.0462, 0.0436], grad_fn=<ToCopyBackward0>), ['.', ',', ' when', ' until', ' but'])\n",
      "(tensor([0.2591, 0.1611, 0.0633, 0.0203, 0.0187], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' There'])\n",
      "(tensor([0.1313, 0.1008, 0.0688, 0.0398, 0.0393], grad_fn=<ToCopyBackward0>), [' thought', ' was', ' mean', ' think', ' don'])\n",
      "(tensor([0.3192, 0.0950, 0.0896, 0.0695, 0.0319], grad_fn=<ToCopyBackward0>), [' it', ' that', ' I', ' the', ' this'])\n",
      "(tensor([0.4608, 0.2545, 0.0576, 0.0184, 0.0151], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' should'])\n",
      "(tensor([0.2942, 0.1438, 0.0867, 0.0288, 0.0197], grad_fn=<ToCopyBackward0>), [' the', ' one', ' a', ' terrible', ' awful'])\n",
      "(tensor([0.7770, 0.0709, 0.0118, 0.0111, 0.0089], grad_fn=<ToCopyBackward0>), [' worst', ' most', ' biggest', ' only', ' best'])\n",
      "(tensor([0.5470, 0.2979, 0.0163, 0.0081, 0.0074], grad_fn=<ToCopyBackward0>), [' film', ' movie', ' horror', ' of', ' thing'])\n",
      "(tensor([0.2947, 0.2530, 0.2438, 0.0501, 0.0368], grad_fn=<ToCopyBackward0>), [' of', ' I', ' ever', ' that', ' in'])\n",
      "(tensor([9.7126e-01, 1.0743e-02, 2.9678e-03, 2.4117e-03, 7.1128e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' all', ' the', ' any', ' my', ' ALL'])\n",
      "(tensor([9.8770e-01, 6.9059e-03, 2.2963e-03, 3.5015e-04, 2.9169e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' time', ' times', '-', ' of', ' the'])\n",
      "(tensor([0.6260, 0.0722, 0.0547, 0.0195, 0.0161], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' ever', ' in'])\n",
      "(tensor([0.2701, 0.1554, 0.0557, 0.0265, 0.0243], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' And', ' There'])\n",
      "(tensor([0.6044, 0.1198, 0.0689, 0.0203, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.1785, 0.0909, 0.0769, 0.0489, 0.0483], grad_fn=<ToCopyBackward0>), [' the', ' so', ' a', ' terrible', ' one'])\n",
      "(tensor([0.0637, 0.0580, 0.0524, 0.0417, 0.0373], grad_fn=<ToCopyBackward0>), [' terrible', ' total', ' complete', ' bad', ' disaster'])\n",
      "(tensor([0.3447, 0.3222, 0.0871, 0.0249, 0.0187], grad_fn=<ToCopyBackward0>), [' film', ' movie', ',', ' piece', ' picture'])\n",
      "(tensor([0.6194, 0.1302, 0.0224, 0.0220, 0.0185], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', '!', ' with'])\n",
      "(tensor([0.2169, 0.1058, 0.0826, 0.0651, 0.0613], grad_fn=<ToCopyBackward0>), [' it', ' and', ' but', ' I', ' a'])\n",
      "(tensor([0.4349, 0.1825, 0.0368, 0.0249, 0.0232], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' not', ' that'])\n",
      "(tensor([0.1416, 0.0946, 0.0422, 0.0295, 0.0271], grad_fn=<ToCopyBackward0>), [' acting', ' worst', ' only', ' script', ' story'])\n",
      "(tensor([0.5538, 0.1477, 0.0778, 0.0276, 0.0226], grad_fn=<ToCopyBackward0>), [' is', ' was', ',', ' in', ' and'])\n",
      "(tensor([0.2024, 0.1589, 0.1188, 0.0827, 0.0646], grad_fn=<ToCopyBackward0>), [' bad', ' terrible', ' so', ' awful', ' horrible'])\n",
      "(tensor([0.6161, 0.2146, 0.0640, 0.0153, 0.0134], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' in', '...'])\n",
      "(tensor([0.1871, 0.1755, 0.1691, 0.0353, 0.0285], grad_fn=<ToCopyBackward0>), [' It', ' The', ' I', ' And', ' There'])\n",
      "(tensor([0.7660, 0.0539, 0.0318, 0.0140, 0.0138], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' doesn', ' just'])\n",
      "(tensor([0.1510, 0.0754, 0.0739, 0.0650, 0.0593], grad_fn=<ToCopyBackward0>), [' a', ' the', ' just', ' so', ' terrible'])\n",
      "(tensor([0.5551, 0.0332, 0.0185, 0.0177, 0.0174], grad_fn=<ToCopyBackward0>), [' bad', ' awful', ' terrible', ' over', ' horrible'])\n",
      "(tensor([0.4585, 0.1500, 0.1262, 0.0330, 0.0309], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' it', ' and'])\n",
      "(tensor([0.2205, 0.1985, 0.1233, 0.0350, 0.0266], grad_fn=<ToCopyBackward0>), [' It', ' I', ' The', ' And', ' There'])\n",
      "(tensor([0.7473, 0.0464, 0.0321, 0.0172, 0.0147], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' makes', ' just'])\n",
      "(tensor([0.2199, 0.1021, 0.0717, 0.0581, 0.0488], grad_fn=<ToCopyBackward0>), [' so', ' a', ' just', ' the', ' like'])\n",
      "(tensor([0.2054, 0.1673, 0.0693, 0.0314, 0.0237], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' an', ' one'])\n",
      "(tensor([0.2183, 0.1492, 0.0203, 0.0168, 0.0167], grad_fn=<ToCopyBackward0>), [' worst', ' acting', ' most', ' best', ' movie'])\n",
      "(tensor([0.4242, 0.0991, 0.0700, 0.0430, 0.0245], grad_fn=<ToCopyBackward0>), [' acting', ' thing', ' movie', ' of', ' actor'])\n",
      "(tensor([0.2642, 0.1866, 0.1302, 0.0953, 0.0456], grad_fn=<ToCopyBackward0>), [' I', ' ever', ' that', ' to', ' you'])\n",
      "(tensor([0.2612, 0.2519, 0.1519, 0.1450, 0.0548], grad_fn=<ToCopyBackward0>), [' can', \"'ve\", ' ever', ' could', \"'ll\"])\n",
      "(tensor([0.2930, 0.1492, 0.1039, 0.0733, 0.0500], grad_fn=<ToCopyBackward0>), [' say', ' do', ' ever', ' imagine', ' get'])\n",
      "(tensor([0.8016, 0.0704, 0.0192, 0.0126, 0.0102], grad_fn=<ToCopyBackward0>), [' about', ' is', ' to', '.', ' in'])\n",
      "(tensor([0.3343, 0.2956, 0.0810, 0.0641, 0.0233], grad_fn=<ToCopyBackward0>), [' it', ' a', ' this', ' the', ' any'])\n",
      "(tensor([0.5761, 0.2891, 0.0475, 0.0065, 0.0054], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' piece', ' thing'])\n",
      "(tensor([0.6177, 0.1458, 0.0581, 0.0209, 0.0207], grad_fn=<ToCopyBackward0>), [' is', '.', ',', '...', ':'])\n",
      "(tensor([0.1568, 0.1446, 0.1322, 0.1018, 0.0748], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.0667, 0.0555, 0.0403, 0.0351, 0.0333], grad_fn=<ToCopyBackward0>), [' movie', ' worst', ' film', ' first', ' story'])\n",
      "(tensor([0.2026, 0.1443, 0.0678, 0.0515, 0.0402], grad_fn=<ToCopyBackward0>), [' thing', ' of', ' was', ' I', ','])\n",
      "(tensor([0.2109, 0.2025, 0.1384, 0.0692, 0.0664], grad_fn=<ToCopyBackward0>), [' about', ' that', ' I', ' to', ' was'])\n",
      "(tensor([0.5237, 0.0869, 0.0262, 0.0246, 0.0213], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' watching'])\n",
      "(tensor([0.5979, 0.1750, 0.0534, 0.0350, 0.0136], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' was', ' is', ' show'])\n",
      "(tensor([0.6420, 0.2036, 0.0319, 0.0148, 0.0067], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ',', ' were'])\n",
      "(tensor([0.3313, 0.2652, 0.0717, 0.0342, 0.0171], grad_fn=<ToCopyBackward0>), [' that', ' the', ' how', ' when', ' its'])\n",
      "(tensor([0.3128, 0.1144, 0.0889, 0.0367, 0.0352], grad_fn=<ToCopyBackward0>), [' it', ' the', ' I', ' there', ' they'])\n",
      "(tensor([0.3975, 0.0940, 0.0588, 0.0371, 0.0359], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' had', ' didn', ' wasn'])\n",
      "(tensor([0.2244, 0.0609, 0.0266, 0.0244, 0.0234], grad_fn=<ToCopyBackward0>), [' so', ' a', ' over', ' boring', ' too'])\n",
      "(tensor([0.2611, 0.1281, 0.0590, 0.0214, 0.0197], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' badly'])\n",
      "(tensor([0.6441, 0.1015, 0.0706, 0.0243, 0.0174], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' that', '...'])\n",
      "(tensor([0.2364, 0.1010, 0.0497, 0.0420, 0.0274], grad_fn=<ToCopyBackward0>), [' predictable', ' so', ' cliché', ' boring', ' the'])\n",
      "(tensor([0.3829, 0.0683, 0.0258, 0.0256, 0.0186], grad_fn=<ToCopyBackward0>), [' predictable', ' boring', ' cliché', ' bad', ' dull'])\n",
      "(tensor([0.6714, 0.0649, 0.0472, 0.0223, 0.0167], grad_fn=<ToCopyBackward0>), ['.', ',', ' that', ' at', '!'])\n",
      "(tensor([0.3010, 0.1653, 0.1314, 0.0508, 0.0484], grad_fn=<ToCopyBackward0>), [' but', ' and', ' that', ' so', ' it'])\n",
      "(tensor([0.1969, 0.1075, 0.1039, 0.0958, 0.0398], grad_fn=<ToCopyBackward0>), [' I', ' it', ' the', ' then', ' that'])\n",
      "(tensor([0.3114, 0.1961, 0.0587, 0.0486, 0.0346], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' wasn', ' turns', ' is'])\n",
      "(tensor([0.2798, 0.0943, 0.0437, 0.0371, 0.0240], grad_fn=<ToCopyBackward0>), [' so', ' also', ' the', ' actually', ' a'])\n",
      "(tensor([0.2586, 0.1509, 0.0906, 0.0364, 0.0224], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' boring', ' awful', ' terrible'])\n",
      "(tensor([0.3616, 0.3557, 0.0597, 0.0565, 0.0387], grad_fn=<ToCopyBackward0>), [' and', ' that', ' because', ',', ' I'])\n",
      "(tensor([0.1250, 0.1207, 0.0860, 0.0585, 0.0419], grad_fn=<ToCopyBackward0>), [' actually', ' was', ' didn', ' thought', ' couldn'])\n",
      "(tensor([0.1873, 0.1416, 0.1333, 0.0553, 0.0512], grad_fn=<ToCopyBackward0>), [' enjoyed', ' found', ' liked', ' laughed', ' thought'])\n",
      "(tensor([0.2388, 0.1577, 0.0838, 0.0647, 0.0391], grad_fn=<ToCopyBackward0>), [' out', ' at', '.', ' a', ' when'])\n",
      "(tensor([9.8161e-01, 1.0366e-02, 1.6645e-03, 4.5585e-04, 2.2380e-04],\n",
      "       grad_fn=<ToCopyBackward0>), [' loud', ' of', ' the', ' my', '-'])\n",
      "(tensor([0.2731, 0.1219, 0.0948, 0.0930, 0.0350], grad_fn=<ToCopyBackward0>), [' at', '.', ' when', ' a', ' several'])\n",
      "(tensor([0.2586, 0.1269, 0.0789, 0.0771, 0.0559], grad_fn=<ToCopyBackward0>), [' the', ' a', ' it', ' some', ' one'])\n",
      "(tensor([0.4903, 0.2561, 0.0834, 0.0244, 0.0144], grad_fn=<ToCopyBackward0>), [' few', ' couple', ' lot', ' certain', ' number'])\n",
      "(tensor([0.3654, 0.1118, 0.1111, 0.0831, 0.0750], grad_fn=<ToCopyBackward0>), [' of', ' parts', ' scenes', ' things', ' moments'])\n",
      "(tensor([0.5761, 0.0789, 0.0671, 0.0477, 0.0370], grad_fn=<ToCopyBackward0>), ['.', ' in', ',', ' of', ' when'])\n",
      "(tensor([0.4654, 0.3416, 0.0698, 0.0296, 0.0091], grad_fn=<ToCopyBackward0>), [' the', ' it', ' this', '.', ' there'])\n",
      "(tensor([0.7668, 0.0573, 0.0425, 0.0159, 0.0140], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' and', ' ('])\n",
      "(tensor([0.1528, 0.1436, 0.1035, 0.0351, 0.0253], grad_fn=<ToCopyBackward0>), [' I', ' The', ' It', ' This', ' There'])\n",
      "(tensor([0.4085, 0.2497, 0.0477, 0.0322, 0.0207], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' wasn', ' had'])\n",
      "(tensor([0.1321, 0.1181, 0.0942, 0.0695, 0.0432], grad_fn=<ToCopyBackward0>), [' so', ' not', ' a', ' like', ' just'])\n",
      "(tensor([0.4553, 0.2909, 0.0124, 0.0102, 0.0089], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' awful', ' hard', ' boring'])\n",
      "(tensor([0.3574, 0.1713, 0.0739, 0.0737, 0.0481], grad_fn=<ToCopyBackward0>), [' that', ',', ' and', ' it', ' I'])\n",
      "(tensor([0.1242, 0.1153, 0.1151, 0.0934, 0.0887], grad_fn=<ToCopyBackward0>), [' in', ' it', ' I', ' that', ' but'])\n",
      "(tensor([0.0934, 0.0832, 0.0750, 0.0503, 0.0494], grad_fn=<ToCopyBackward0>), [' laughed', ' actually', ' can', ' mean', ' thought'])\n",
      "(tensor([0.4534, 0.0738, 0.0405, 0.0289, 0.0277], grad_fn=<ToCopyBackward0>), [',', ' it', ' the', ' I', ' come'])\n",
      "(tensor([0.6076, 0.0689, 0.0324, 0.0222, 0.0189], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' is', ' has', ' really'])\n",
      "(tensor([0.2085, 0.1227, 0.0990, 0.0579, 0.0337], grad_fn=<ToCopyBackward0>), [' so', ' not', ' like', ' a', ' just'])\n",
      "(tensor([0.6966, 0.1313, 0.0190, 0.0063, 0.0054], grad_fn=<ToCopyBackward0>), [' predictable', ' bad', ' obvious', ' stupid', ' boring'])\n",
      "(tensor([0.3453, 0.1408, 0.0653, 0.0567, 0.0546], grad_fn=<ToCopyBackward0>), [' that', ',', '.', ' I', ' it'])\n",
      "(tensor([0.1207, 0.1069, 0.0909, 0.0811, 0.0768], grad_fn=<ToCopyBackward0>), [' it', ' I', ' that', ' but', ' the'])\n",
      "(tensor([0.6942, 0.0469, 0.0205, 0.0178, 0.0168], grad_fn=<ToCopyBackward0>), [\"'s\", ' was', ' makes', ' doesn', ' is'])\n",
      "(tensor([0.2618, 0.1608, 0.0856, 0.0454, 0.0396], grad_fn=<ToCopyBackward0>), [' so', ' like', ' not', ' just', ' predictable'])\n",
      "(tensor([0.1390, 0.1196, 0.0948, 0.0699, 0.0575], grad_fn=<ToCopyBackward0>), [' a', ' watching', ' the', ' \"', ','])\n",
      "(tensor([0.1214, 0.0338, 0.0299, 0.0216, 0.0189], grad_fn=<ToCopyBackward0>), [' bad', ' movie', ' soap', ' cartoon', ' \"'])\n",
      "(tensor([0.1569, 0.1451, 0.1326, 0.1002, 0.0757], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4590, 0.2953, 0.0466, 0.0197, 0.0183], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.9160, 0.0392, 0.0055, 0.0033, 0.0031], grad_fn=<ToCopyBackward0>), [' be', ' have', ' make', ' take', ' help'])\n",
      "(tensor([0.2873, 0.1349, 0.0694, 0.0633, 0.0625], grad_fn=<ToCopyBackward0>), [' a', ' interesting', ' funny', ' nice', ' fun'])\n",
      "(tensor([0.8317, 0.0872, 0.0547, 0.0046, 0.0032], grad_fn=<ToCopyBackward0>), [' to', ' if', ' for', ',', ' and'])\n",
      "(tensor([0.2233, 0.1080, 0.0635, 0.0575, 0.0444], grad_fn=<ToCopyBackward0>), [' have', ' see', ' show', ' give', ' be'])\n",
      "(tensor([0.4230, 0.1008, 0.0924, 0.0369, 0.0175], grad_fn=<ToCopyBackward0>), [' able', ' a', ' in', ' the', ' with'])\n",
      "(tensor([0.3518, 0.1345, 0.0694, 0.0301, 0.0295], grad_fn=<ToCopyBackward0>), [' little', ' part', ' bit', ' good', ' big'])\n",
      "(tensor([0.4939, 0.2087, 0.0232, 0.0231, 0.0135], grad_fn=<ToCopyBackward0>), [' more', ' of', ' less', ' different', ' better'])\n",
      "(tensor([0.7002, 0.1057, 0.0392, 0.0089, 0.0042], grad_fn=<ToCopyBackward0>), [' a', ' an', ' fun', ' the', ' \"'])\n",
      "(tensor([0.1252, 0.0228, 0.0207, 0.0194, 0.0133], grad_fn=<ToCopyBackward0>), [' bad', ' fan', ' cheer', ' good', ' \"'])\n",
      "(tensor([0.3039, 0.2247, 0.1490, 0.0625, 0.0448], grad_fn=<ToCopyBackward0>), [' guy', '-', ' ass', 'die', ' cop'])\n",
      "(tensor([0.7853, 0.0375, 0.0213, 0.0193, 0.0088], grad_fn=<ToCopyBackward0>), ['ass', 'guy', 'boy', 'a', 'cop'])\n",
      "(tensor([0.1451, 0.0947, 0.0941, 0.0629, 0.0334], grad_fn=<ToCopyBackward0>), [' and', ',', ' in', '.', ' for'])\n",
      "(tensor([0.0932, 0.0667, 0.0481, 0.0467, 0.0325], grad_fn=<ToCopyBackward0>), [' show', ' to', ' take', ' go', ' have'])\n",
      "(tensor([0.1624, 0.1536, 0.0694, 0.0592, 0.0535], grad_fn=<ToCopyBackward0>), [' out', ' to', ' into', ' for', ' in'])\n",
      "(tensor([0.5626, 0.1353, 0.1168, 0.0193, 0.0105], grad_fn=<ToCopyBackward0>), [' the', ' this', ' a', ' battle', ' it'])\n",
      "(tensor([0.4028, 0.1172, 0.0959, 0.0198, 0.0197], grad_fn=<ToCopyBackward0>), [' movie', ' with', ' film', ' thing', ' as'])\n",
      "(tensor([0.2565, 0.1413, 0.0597, 0.0397, 0.0372], grad_fn=<ToCopyBackward0>), [' with', ' as', ' without', ' like', ' and'])\n",
      "(tensor([0.3053, 0.1414, 0.0866, 0.0593, 0.0346], grad_fn=<ToCopyBackward0>), [' any', ' a', ' the', ' having', ' being'])\n",
      "(tensor([0.2164, 0.0379, 0.0337, 0.0335, 0.0312], grad_fn=<ToCopyBackward0>), [' precon', ' expectations', ' of', ' knowledge', ' real'])\n",
      "(tensor([6.2642e-01, 3.5667e-01, 1.4625e-02, 2.9604e-04, 2.9408e-04],\n",
      "       grad_fn=<ToCopyBackward0>), ['ceived', 'ceptions', 'ception', 'c', 'ci'])\n",
      "(tensor([0.2519, 0.1710, 0.0975, 0.0791, 0.0759], grad_fn=<ToCopyBackward0>), ['.', ' about', ',', ' of', ' or'])\n",
      "(tensor([0.2766, 0.2001, 0.1954, 0.1053, 0.0348], grad_fn=<ToCopyBackward0>), [' it', ' what', ' the', ' how', ' who'])\n",
      "(tensor([0.3231, 0.1287, 0.0968, 0.0699, 0.0355], grad_fn=<ToCopyBackward0>), [' it', ' I', ' to', ' the', ' bad'])\n",
      "(tensor([0.1385, 0.1084, 0.0691, 0.0584, 0.0523], grad_fn=<ToCopyBackward0>), [' act', ' play', ' do', ' approach', ' watch'])\n",
      "(tensor([0.6693, 0.1506, 0.0336, 0.0306, 0.0252], grad_fn=<ToCopyBackward0>), [' it', ' a', ' or', ' the', ' this'])\n",
      "(tensor([0.6428, 0.1237, 0.0932, 0.0321, 0.0210], grad_fn=<ToCopyBackward0>), ['.', ',', ' or', ' and', '...'])\n",
      "(tensor([0.2183, 0.0685, 0.0617, 0.0481, 0.0375], grad_fn=<ToCopyBackward0>), [' I', ' It', ' So', ' This', ' The'])\n",
      "(tensor([0.3423, 0.2492, 0.0884, 0.0597, 0.0243], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' way'])\n",
      "(tensor([0.2108, 0.1597, 0.0965, 0.0602, 0.0307], grad_fn=<ToCopyBackward0>), [' a', ' the', ' one', ' supposed', ' my'])\n",
      "(tensor([0.2119, 0.2028, 0.0604, 0.0362, 0.0348], grad_fn=<ToCopyBackward0>), [' worst', ' first', ' only', ' most', ' movie'])\n",
      "(tensor([0.5333, 0.1501, 0.1121, 0.0196, 0.0105], grad_fn=<ToCopyBackward0>), [' movie', ' time', ' film', ' horror', ' one'])\n",
      "(tensor([0.7881, 0.0596, 0.0358, 0.0097, 0.0073], grad_fn=<ToCopyBackward0>), [' I', ' that', ' in', ' we', ' since'])\n",
      "(tensor([0.4479, 0.2198, 0.0428, 0.0378, 0.0209], grad_fn=<ToCopyBackward0>), [' my', ' a', ' the', ' years', ' many'])\n",
      "(tensor([0.6355, 0.2477, 0.0271, 0.0112, 0.0109], grad_fn=<ToCopyBackward0>), [' long', ' while', ' very', ' few', ' really'])\n",
      "(tensor([0.7863, 0.1842, 0.0201, 0.0037, 0.0010], grad_fn=<ToCopyBackward0>), [' time', ' while', ',', ' long', '-'])\n",
      "(tensor([0.5275, 0.2670, 0.1061, 0.0269, 0.0149], grad_fn=<ToCopyBackward0>), [' that', ' I', ' where', ' when', ','])\n",
      "(tensor([0.9208, 0.0131, 0.0078, 0.0072, 0.0071], grad_fn=<ToCopyBackward0>), [' I', ' i', ' the', ' we', ' a'])\n",
      "(tensor([0.1189, 0.0911, 0.0725, 0.0655, 0.0561], grad_fn=<ToCopyBackward0>), [' was', ' didn', \"'ve\", ' had', ' watched'])\n",
      "(tensor([0.1623, 0.1010, 0.0830, 0.0684, 0.0371], grad_fn=<ToCopyBackward0>), [' able', ' actually', ' not', ' really', ' looking'])\n",
      "(tensor([0.0670, 0.0607, 0.0386, 0.0268, 0.0243], grad_fn=<ToCopyBackward0>), [' afraid', ' intimidated', ' expecting', ' disappointed', ' looking'])\n",
      "(tensor([0.7763, 0.0367, 0.0298, 0.0209, 0.0190], grad_fn=<ToCopyBackward0>), [' by', ' to', ' or', '.', ' when'])\n",
      "(tensor([0.3455, 0.1528, 0.0787, 0.0572, 0.0229], grad_fn=<ToCopyBackward0>), [' a', ' the', ' any', ' anything', ' something'])\n",
      "(tensor([0.1436, 0.0985, 0.0651, 0.0317, 0.0298], grad_fn=<ToCopyBackward0>), [' idea', ' prospect', ' subject', ' genre', ' thought'])\n",
      "(tensor([0.9292, 0.0202, 0.0166, 0.0120, 0.0057], grad_fn=<ToCopyBackward0>), [' matter', '.', ' of', ' material', ','])\n",
      "(tensor([0.4196, 0.1691, 0.1087, 0.0784, 0.0631], grad_fn=<ToCopyBackward0>), ['.', ',', ' of', ' and', ' or'])\n",
      "(tensor([0.2578, 0.0957, 0.0462, 0.0431, 0.0340], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', 'I'])\n",
      "(tensor([0.1626, 0.0484, 0.0475, 0.0404, 0.0304], grad_fn=<ToCopyBackward0>), [' was', ' thought', ' had', ' watched', ' didn'])\n",
      "(tensor([0.0837, 0.0786, 0.0496, 0.0426, 0.0394], grad_fn=<ToCopyBackward0>), [' not', ' actually', ' looking', ' excited', ' able'])\n",
      "(tensor([0.1570, 0.1450, 0.1325, 0.1008, 0.0754], grad_fn=<ToCopyBackward0>), [' I', ' this', ' it', ' that', ' the'])\n",
      "(tensor([0.4594, 0.2951, 0.0466, 0.0197, 0.0182], grad_fn=<ToCopyBackward0>), [' was', ' would', ' might', \"'d\", ' could'])\n",
      "(tensor([0.1895, 0.1132, 0.0591, 0.0363, 0.0288], grad_fn=<ToCopyBackward0>), [' a', ' funny', ' the', ' pretty', ' interesting'])\n",
      "(tensor([0.1400, 0.1229, 0.1141, 0.1006, 0.0945], grad_fn=<ToCopyBackward0>), [' when', ' that', ',', '.', ' to'])\n",
      "(tensor([0.2292, 0.1684, 0.0536, 0.0443, 0.0277], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' But'])\n",
      "(tensor([0.2715, 0.2610, 0.1030, 0.0703, 0.0180], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' was', ' film', ' one'])\n",
      "(tensor([0.3229, 0.2541, 0.0557, 0.0275, 0.0150], grad_fn=<ToCopyBackward0>), [' is', ' was', ' has', ' had', ','])\n",
      "(tensor([0.1263, 0.0581, 0.0547, 0.0456, 0.0436], grad_fn=<ToCopyBackward0>), [' so', ' supposed', ' a', ' bad', ' terrible'])\n",
      "(tensor([0.5245, 0.1393, 0.1026, 0.0431, 0.0301], grad_fn=<ToCopyBackward0>), ['.', ',', ' but', ' and', '!'])\n",
      "(tensor([0.1606, 0.0894, 0.0681, 0.0286, 0.0239], grad_fn=<ToCopyBackward0>), [' I', ' the', ' it', ' that', ' so'])\n",
      "(tensor([0.3657, 0.0968, 0.0700, 0.0522, 0.0436], grad_fn=<ToCopyBackward0>), [\"'s\", ' is', ' was', ' should', ' has'])\n",
      "(tensor([0.0726, 0.0702, 0.0696, 0.0487, 0.0309], grad_fn=<ToCopyBackward0>), [' a', ' not', ' so', ' the', ' supposed'])\n",
      "(tensor([0.1765, 0.1653, 0.0706, 0.0271, 0.0193], grad_fn=<ToCopyBackward0>), [' bad', ' predictable', ' boring', ' stupid', ' over'])\n",
      "(tensor([0.5823, 0.1199, 0.0850, 0.0444, 0.0349], grad_fn=<ToCopyBackward0>), [' that', ' it', ' I', ',', '.'])\n",
      "(tensor([0.3092, 0.2133, 0.0543, 0.0312, 0.0298], grad_fn=<ToCopyBackward0>), [' is', \"'s\", ' makes', ' was', ' has'])\n",
      "(tensor([0.4602, 0.1070, 0.0604, 0.0343, 0.0335], grad_fn=<ToCopyBackward0>), [' funny', ' good', ' hilarious', ' actually', ' not'])\n",
      "(tensor([0.7961, 0.0480, 0.0360, 0.0163, 0.0110], grad_fn=<ToCopyBackward0>), ['.', ',', '!', ' to', '...'])\n",
      "(tensor([0.1728, 0.1234, 0.1148, 0.0521, 0.0193], grad_fn=<ToCopyBackward0>), [' I', ' It', ' The', ' This', ' If'])\n",
      "(tensor([0.4624, 0.3051, 0.0422, 0.0282, 0.0096], grad_fn=<ToCopyBackward0>), [' movie', ' is', ' film', ' was', ' one'])\n",
      "(tensor([0.5903, 0.0626, 0.0476, 0.0372, 0.0236], grad_fn=<ToCopyBackward0>), [' is', ' has', ' was', ' sucks', ' makes'])\n",
      "(tensor([0.1789, 0.0911, 0.0590, 0.0558, 0.0459], grad_fn=<ToCopyBackward0>), [' so', ' a', ' terrible', ' bad', ' made'])\n",
      "(tensor([0.2456, 0.1712, 0.0852, 0.0600, 0.0543], grad_fn=<ToCopyBackward0>), [' enough', '.', ',', ' but', ' because'])\n",
      "(tensor([0.3649, 0.1292, 0.0728, 0.0634, 0.0327], grad_fn=<ToCopyBackward0>), [' but', ' bad', ' and', ' it', ' I'])\n",
      "(tensor([0.3389, 0.1153, 0.0534, 0.0328, 0.0302], grad_fn=<ToCopyBackward0>), [' it', ' I', ' the', ' at', ' this'])\n",
      "(tensor([0.3593, 0.1995, 0.1575, 0.0586, 0.0286], grad_fn=<ToCopyBackward0>), [' was', \"'s\", ' is', ' wasn', ' had'])\n",
      "(tensor([0.2421, 0.1695, 0.0517, 0.0316, 0.0271], grad_fn=<ToCopyBackward0>), [' funny', ' so', ' bad', ' fun', ' also'])\n",
      "(tensor([0.6874, 0.0639, 0.0332, 0.0304, 0.0250], grad_fn=<ToCopyBackward0>), ['.', ',', ' as', ' and', '!'])\n",
      "(tensor([0.1703, 0.1643, 0.1026, 0.0822, 0.0183], grad_fn=<ToCopyBackward0>), [' I', ' It', ' This', ' The', ' That'])\n",
      "(tensor([0.1162, 0.1029, 0.0491, 0.0441, 0.0408], grad_fn=<ToCopyBackward0>), [' acting', ' only', ' plot', ' jokes', ' humor'])\n",
      "(tensor([0.2586, 0.1958, 0.1835, 0.0983, 0.0280], grad_fn=<ToCopyBackward0>), [' is', ' in', ' was', ' of', ' comes'])\n",
      "(tensor([0.7738, 0.0792, 0.0498, 0.0112, 0.0076], grad_fn=<ToCopyBackward0>), [' this', ' the', ' it', ' \"', ' that'])\n",
      "(tensor([0.7792, 0.1114, 0.0287, 0.0149, 0.0099], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' one', ' was'])\n",
      "(tensor([0.5383, 0.2051, 0.0258, 0.0120, 0.0108], grad_fn=<ToCopyBackward0>), [' is', ' was', ' comes', ' makes', ' just'])\n",
      "(tensor([0.1952, 0.0375, 0.0361, 0.0351, 0.0300], grad_fn=<ToCopyBackward0>), [' so', ' just', ' not', ' terrible', ' pathetic'])\n",
      "(tensor([0.1387, 0.0988, 0.0812, 0.0500, 0.0443], grad_fn=<ToCopyBackward0>), [' so', ' awful', ' terrible', ' bad', ' pathetic'])\n",
      "(tensor([0.7941, 0.0860, 0.0479, 0.0107, 0.0091], grad_fn=<ToCopyBackward0>), ['.', ',', ' and', ' but', '!'])\n",
      "(tensor([0.1670, 0.1624, 0.1552, 0.0423, 0.0326], grad_fn=<ToCopyBackward0>), [' The', ' I', ' It', ' This', ' There'])\n",
      "(tensor([0.0772, 0.0706, 0.0586, 0.0473, 0.0470], grad_fn=<ToCopyBackward0>), [' don', ' can', ' mean', \"'m\", ' was'])\n",
      "(tensor([0.7923, 0.0407, 0.0247, 0.0178, 0.0105], grad_fn=<ToCopyBackward0>), [\"'t\", ' only', ' see', ' not', ' honestly'])\n",
      "(tensor([0.1459, 0.1420, 0.0688, 0.0567, 0.0472], grad_fn=<ToCopyBackward0>), [' assume', ' imagine', ' hope', ' think', ' say'])\n",
      "(tensor([0.4870, 0.1884, 0.1467, 0.0500, 0.0199], grad_fn=<ToCopyBackward0>), [' how', ' what', ' the', ' that', ' this'])\n",
      "(tensor([0.1588, 0.1540, 0.0767, 0.0554, 0.0442], grad_fn=<ToCopyBackward0>), [' it', ' the', ' this', ' a', ' kind'])\n",
      "(tensor([0.6165, 0.0531, 0.0302, 0.0222, 0.0209], grad_fn=<ToCopyBackward0>), [' movie', ' film', ' is', ' was', ' guy'])\n",
      "(tensor([0.2562, 0.1719, 0.1491, 0.1327, 0.0475], grad_fn=<ToCopyBackward0>), [' was', ' is', ' would', ' must', \"'s\"])\n",
      "(tensor([0.5026, 0.3210, 0.0721, 0.0305, 0.0124], grad_fn=<ToCopyBackward0>), [' be', ' have', \"'ve\", ' look', ' do'])\n",
      "(tensor([0.9205, 0.0527, 0.0020, 0.0015, 0.0013], grad_fn=<ToCopyBackward0>), [' like', ' if', ' with', ' in', ' called'])\n",
      "(tensor([0.8924, 0.0230, 0.0202, 0.0086, 0.0082], grad_fn=<ToCopyBackward0>), [' if', ' with', ' in', ' to', ' on'])\n",
      "(tensor([0.3246, 0.1283, 0.0680, 0.0325, 0.0258], grad_fn=<ToCopyBackward0>), [' it', ' the', ' they', ' you', ' someone'])\n",
      "(tensor([0.3551, 0.2820, 0.1471, 0.0423, 0.0242], grad_fn=<ToCopyBackward0>), [' was', ' were', ' had', ' wasn', ' weren'])\n",
      "(tensor([0.1874, 0.1113, 0.0932, 0.0733, 0.0333], grad_fn=<ToCopyBackward0>), [' made', ' written', ' directed', ' a', ' actually'])\n",
      "/n/n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 31.10 GiB already allocated; 3.56 MiB free; 31.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-69e7f6e4b24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_style_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_tokens_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_to_generate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-9017fe27f7a1>\u001b[0m in \u001b[0;36mone_style_generate\u001b[0;34m(prompt, tokenizer, SAT_model, GPT_transformer, context_to_sample, num_samples, num_tokens_to_generate, sen_to_generate)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tokens_to_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# put tokenized prompt through GPT_transformer to get GPT Logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mGPT_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tokenization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# put model_context_input and the GPT Logits into SAT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 )\n\u001b[1;32m    888\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    890\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_merge_heads\u001b[0;34m(self, tensor, num_heads, attn_head_size)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mMerges\u001b[0m \u001b[0mattn_head_size\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum_attn_heads\u001b[0m \u001b[0mdim\u001b[0m \u001b[0minto\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_head_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 31.10 GiB already allocated; 3.56 MiB free; 31.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100,  num_tokens_to_generate = 50, sen_to_generate = 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47eebc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_heads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c6a7cb34810e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpytorch_basic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mModel_Import_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadModel_PyTorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_neg_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_basic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.0001\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# could be useful transformers require warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# .0001 best WD so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_heads' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_basic= Model_Import_6.MultiHeadModel_PyTorch(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "# neg_optimizer = optim.Adam(pytorch_basic.parameters(), lr=0.00001,  weight_decay=0.001)\n",
    "optimizer = optim.RAdam(pytorch_basic.parameters(), lr=0.0001,  weight_decay=.0001) # could be useful transformers require warmup\n",
    "# .0001 best WD so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d5a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30357e03341d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one_style_w_dev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_embeds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR_embeds_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtoken_ids_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dev implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-6280d96fe765>\u001b[0m in \u001b[0;36mtrain_one_style_w_dev\u001b[0;34m(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mrandom_context_samples_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# need to make sure dev samples are the same... !!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mstacked_context_sample_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_context_samples_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mdev_network_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_context_sample_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mshifted_network_output_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_network_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mshifted_text_ids_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_token_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev_example\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Model_Import_6.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_x, decoder_x)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMulti_Head_Cross_Attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_x\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0;31m# query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# print(attention)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qkv_same_embed_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1028\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m             \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5254\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5256\u001b[0m     \u001b[0;31m# prep attention mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection\u001b[0;34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[0m\n\u001b[1;32m   4994\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mb_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"expecting key bias shape of {(Eq,)}, but got {b_k.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4995\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mb_v\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"expecting value bias shape of {(Eq,)}, but got {b_v.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "train_one_style_w_dev(pytorch_basic, optimizer, R_embeds_train, logits_train, token_ids_train, 5, logits_test, R_embeds_test,  token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0186e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, pytorch_basic, Model_Import_6.head_transformer, R_pos_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be364037",
   "metadata": {},
   "outputs": [],
   "source": [
    "MH_basic= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = num_heads, attention_dim = int(neg_logits[0].shape[1]/num_heads)).to(device) #\n",
    "# MH_basic= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8).to(device) #\n",
    "\n",
    "neg_optimizer = optim.Adam(MH_basic.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style_w_dev(MH_basic, neg_optimizer, R_neg_embeds_train, neg_logits_train, neg_token_ids_train, 4, neg_logits_test, R_neg_embeds_test,  neg_token_ids_test) ## dev implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74523d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### R_neg_embeds_train, R_neg_embeds_test, neg_logits_train, neg_logits_test, neg_token_ids_train, neg_token_ids_test\n",
    "#(model, optimizer, context_embeds_list, logits_list, token_ids_list, epochs, dev_logits, dev_context, dev_token_ids, num_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac79e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model = Model_Import_6.Test_skip_norm_model(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "neg_optimizer = optim.Adam(neg_alone_model.parameters(), lr=0.0001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fefb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_deeper = Model_Import_6.DeeperModel_skip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device)\n",
    "neg_optimizer = optim.Adam(neg_alone_model_deeper.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e265c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_deeper, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811094cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_wide = Model_Import_6.WiderModel_skip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) # 2 wide -> 1:46  4 wide ->3:00\n",
    "neg_optimizer = optim.Adam(neg_alone_model_wide.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_wide, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc350d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_alone_model_wide_deep = Model_Import_6.WiderDeeperModel_2(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(neg_alone_model_wide_deep.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e535ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(neg_alone_model_wide_deep, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a89b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(t/1000000000)\n",
    "print(r/1000000000)\n",
    "print(a/1000000000)\n",
    "print(f/1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "WiderBlock_8_model= Model_Import_6.WiderBlock_8(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(WiderBlock_8_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676af7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(WiderBlock_8_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa058d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_head_8_wide_model= Model_Import_6.ProposedModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = neg_logits[0].shape[1]*8).to(device) #\n",
    "neg_optimizer = optim.Adam(one_head_8_wide_model.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(one_head_8_wide_model, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06664633",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, WiderBlock_8_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18244c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Overall I thought\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1caef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, WiderBlock_8_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I sat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9697269",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, one_head_8_wide_model, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f50e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WiderDeeperModel_Alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672bd28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_deep_alt= Model_Import_6.WiderDeeperModel_Alt(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = neg_logits[0].shape[1]).to(device) #\n",
    "neg_optimizer = optim.Adam(Wide_deep_alt.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a426a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(Wide_deep_alt, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11beee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrow_M_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1]/4)).to(device) #\n",
    "neg_optimizer = optim.Adam(narrow_M_head.parameters(), lr=0.00001,  weight_decay=0.001) # 2:00 per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(narrow_M_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426aee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " neg_logits[0].shape[1]/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07385bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_neg_embeds[0].shape[0]/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wide_S_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 1, attention_dim = int(neg_logits[0].shape[1]*8)).to(device) #\n",
    "neg_optimizer = optim.Adam(Wide_S_head.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(Wide_S_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f83762",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_M_head= Model_Import_6.MultiHeadModel(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(wide_M_head.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(wide_M_head, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wider_8_fixedM= Model_Import_6.WiderBlock_8_FixedM(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], attention_dim = None).to(device) #\n",
    "neg_optimizer = optim.Adam(wider_8_fixedM.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(wider_8_fixedM, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda47bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, wider_8_fixedM, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fe20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_style_generate(prompt, Model_Import_6.tokenizer, narrow_M_head, Model_Import_6.head_transformer, R_neg_embeds, num_samples = 100, num_tokens_to_generate = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "middleskip_8= Model_Import_6.MultiHeadModel_WOFinalBlockSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(middleskip_8.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(middleskip_8, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoskip_8= Model_Import_6.MultiHeadModel_TwoSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(twoskip_8.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(twoskip_8, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "noskip= Model_Import_6.MultiHeadModel_WOAnyBlockSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(noskip.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(noskip, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009db93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraskip= Model_Import_6.MultiHeadModel_ExtraSkip(R_neg_embeds[0].shape[0], neg_logits[0].shape[1], heads = 8, attention_dim = int(neg_logits[0].shape[1])).to(device) #\n",
    "neg_optimizer = optim.Adam(extraskip.parameters(), lr=0.00001,  weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_style(extraskip, neg_optimizer, R_neg_embeds, neg_logits, neg_token_ids, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee0d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da4dff-a0e7-47d0-97b3-98ae9be16612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
