{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9882601a-87b4-49c1-9607-062ac9ef6e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import torch\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaModel\n",
    "import math\n",
    "import pandas as pd\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80549f8e-f25e-4f41-95e9-f013d4e01f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3125af-ff95-4814-ba24-ae151ecfa417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)\n",
    "for param in head_model.parameters():\n",
    "    \n",
    "   param.requires_grad = False\n",
    "\n",
    "\n",
    "lm_head = head_model.lm_head\n",
    "\n",
    "# head_transformer = head_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e587393-a31f-4741-8c51-6a9b59fa58b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "R_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "Roberta_model = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a732bb66-cdd4-45a4-9423-7fa0493eab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionSingle(nn.Module):\n",
    "    # def __init__(self, max_length):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim = None):\n",
    "        \"\"\"\n",
    "        Single head cross attention block scaled\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.e_dim = encoder_dim\n",
    "        self.d_dim = decoder_dim\n",
    "        if attention_dim is None:\n",
    "            self.attention_dim = decoder_dim\n",
    "        else:\n",
    "            self.attention_dim = attention_dim\n",
    "        \n",
    "        self.WQ = torch.randn((self.d_dim, self.attention_dim), requires_grad=True).to(device)\n",
    "        self.WK = torch.randn((self.e_dim, self.attention_dim), requires_grad=True).to(device)\n",
    "        self.WV = torch.randn((self.e_dim, self.attention_dim), requires_grad=True).to(device)\n",
    "        self.softmax = nn.Softmax(dim=1).to(device)\n",
    "        \n",
    "\n",
    "    def forward(self, encoder_x, decoder_x):\n",
    "        \n",
    "        #print(f\"self.WQ: {self.WQ}\")\n",
    "        Q = torch.mm(decoder_x.to(device), self.WQ ).to(device)\n",
    "        #print(f\"Q shape {Q.shape}\")\n",
    "        #print(f\"Q {Q}\")\n",
    "        K = torch.mm(encoder_x.to(device), self.WK ).to(device)\n",
    "        #print(f\"K shape {K.shape}\")\n",
    "        #print(f\"K {K}\")\n",
    "        V = torch.mm(encoder_x.to(device), self.WV ) .to(device)\n",
    "        #print(f\"V shape {V.shape}\")\n",
    "        #print(f\"V {V}\")\n",
    "        QKT = torch.mm(Q, K.t()).to(device)\n",
    "        #print(f\"QKT shape {QKT.shape}\")\n",
    "        #print(f\"QKT  {QKT}\")\n",
    "      \n",
    "        # Q d_lenXd_dim\n",
    "        # K e_lenXd_dim\n",
    "        # V e_lenXd_dim\n",
    "        QKT_div = torch.div(QKT,math.sqrt(self.d_dim))\n",
    "        \n",
    "        SM = self.softmax(QKT_div).to(device) # may need the div from my earlier transformer\n",
    "        #print(f\"SM  {SM}\")\n",
    "        \n",
    "        attention = torch.mm(SM, V).to(device) \n",
    "        #print(f\"attention shape {attention.shape}\")\n",
    "        return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991d5237-6383-4993-b811-5694a8855a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedModel(nn.Module):\n",
    "    # def __init__(self, max_length):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim = None):\n",
    "        \"\"\"\n",
    "        Part by part feed forward\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.e_dim = encoder_dim\n",
    "        self.d_dim = decoder_dim\n",
    "        if attention_dim is None:\n",
    "            self.attention_dim = decoder_dim\n",
    "        else:\n",
    "            self.attention_dim = attention_dim\n",
    "        self.cross_a = CrossAttentionSingle(self.e_dim, self.d_dim, self.attention_dim).to(device)\n",
    "        self.FF = nn.Linear(self.attention_dim, self.d_dim).to(device)\n",
    "        self.lm_head = lm_head\n",
    "        \n",
    "    def forward(self, encoder_x, decoder_x):\n",
    "        attention = self.cross_a(encoder_x, decoder_x)\n",
    "        adjustment = self.FF(attention)\n",
    "        adjusted_output = adjustment + decoder_x\n",
    "        # ######\n",
    "        # adjusted_output = decoder_x\n",
    "        # ######\n",
    "        output = self.lm_head(adjusted_output)\n",
    "        # print(attention.shape)\n",
    "        # print(adjusted_output.shape)\n",
    "        # print(output.shape)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355ac341-5dcf-4a64-ae83-079e8b499b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   670,   355,   257,  1366, 11444]], device='cuda:0')\n",
      "decoder logits shape torch.Size([1, 6, 50257])\n",
      "decoder logits sum tensor([[ 12.1287,  15.6366,  -9.6813,  ..., -31.3261, -35.7890,   6.1169]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text = \"I work as a data scientist\"\n",
    "text_ids = tokenizer.encode(text, return_tensors = 'pt').to(device)\n",
    "print(text_ids)\n",
    "# logits = head_transformer(text_ids).last_hidden_state.squeeze()\n",
    "logits = head_model(text_ids).logits\n",
    "logits_shape = logits.shape\n",
    "print(f\"decoder logits shape {logits_shape}\")\n",
    "print(f\"decoder logits sum {torch.sum(logits, dim = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0ec36d-8948-4f7b-9335-6c999535252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_head(logits).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf23adaf-ab19-4f6b-bdfd-565ca6b3111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roberta shape torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "R_tokenized = R_tokenizer(text, return_tensors = 'pt')\n",
    "R_embed = Roberta_model(**R_tokenized).last_hidden_state.squeeze()\n",
    "R_embed_shape = R_embed.shape\n",
    "print(f\"Roberta shape {R_embed_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64abf8b-a353-4aa1-9b32-c724f64c344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = ProposedModel(R_embed_shape[1], logits_shape[1], attention_dim = None)\n",
    "# test_model.forward(R_embed, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4e10c7-e9e2-47d3-a853-47686fa2a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "go_emotions_train = pd.read_csv('train.tsv.txt', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b31bf3c-5b39-423d-bf42-282fe6cf2e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "      <td>eebbqej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "      <td>ed00q6i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "      <td>eezlygj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "      <td>ed7ypvh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "      <td>ed0bdzj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43405</th>\n",
       "      <td>Added you mate well I’ve just got the bow and ...</td>\n",
       "      <td>18</td>\n",
       "      <td>edsb738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43406</th>\n",
       "      <td>Always thought that was funny but is it a refe...</td>\n",
       "      <td>6</td>\n",
       "      <td>ee7fdou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43407</th>\n",
       "      <td>What are you talking about? Anything bad that ...</td>\n",
       "      <td>3</td>\n",
       "      <td>efgbhks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43408</th>\n",
       "      <td>More like a baptism, with sexy results!</td>\n",
       "      <td>13</td>\n",
       "      <td>ed1naf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43409</th>\n",
       "      <td>Enjoy the ride!</td>\n",
       "      <td>17</td>\n",
       "      <td>eecwmbq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0   1        2\n",
       "0      My favourite food is anything I didn't have to...  27  eebbqej\n",
       "1      Now if he does off himself, everyone will thin...  27  ed00q6i\n",
       "2                         WHY THE FUCK IS BAYLESS ISOING   2  eezlygj\n",
       "3                            To make her feel threatened  14  ed7ypvh\n",
       "4                                 Dirty Southern Wankers   3  ed0bdzj\n",
       "...                                                  ...  ..      ...\n",
       "43405  Added you mate well I’ve just got the bow and ...  18  edsb738\n",
       "43406  Always thought that was funny but is it a refe...   6  ee7fdou\n",
       "43407  What are you talking about? Anything bad that ...   3  efgbhks\n",
       "43408            More like a baptism, with sexy results!  13  ed1naf8\n",
       "43409                                    Enjoy the ride!  17  eecwmbq\n",
       "\n",
       "[43410 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_emotions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abbb1458-51d4-4d32-b08c-5955b55262c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now if he does off himself, everyone will think hes having a laugh screwing with people instead of actually dead'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_emotions_train.values[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980f85ef-c338-4bcf-83e9-1f8490ce6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict_emoToidx = {} # emo -> idx\n",
    "emotions_dict_idxToemo = {} # idx -> emo\n",
    "for idx, val in enumerate(go_emotions_train.values):\n",
    "    for emotion in val[1].split(','):\n",
    "        if emotion not in emotions_dict_emoToidx:\n",
    "            emotions_dict_emoToidx[emotion] = []\n",
    "        emotions_dict_emoToidx[emotion].append(idx)\n",
    "        \n",
    "        if idx not in emotions_dict_idxToemo:\n",
    "            emotions_dict_idxToemo[idx] = []\n",
    "        emotions_dict_idxToemo[idx].append(emotion)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca3c77d8-d034-4817-b248-f61efaed0ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', '20']\n"
     ]
    }
   ],
   "source": [
    "# go_emotions_train.values[emotions_dict['27']]\n",
    "#print(go_emotions_train.values[emotions_dict_emoToidx['6']])\n",
    "print(emotions_dict_idxToemo[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3f7be-db56-4f32-afbe-550830730a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ab892f7-5762-4037-b53f-5c5447ea6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(emo_gpt2-xl.pt, \n",
    "emo_gpt_embed = torch.load('emo_gpt2-xl.pt', map_location=lambda storage, loc: storage.cuda(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c40246a-3761-4152-9a12-9db144be13f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 12.23 GiB already allocated; 4.56 MiB free; 12.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f476eaa0107e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memo_roberta_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emo_Roberta.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         loaded_storages[key] = torch.storage._TypedStorage(\n\u001b[0;32m-> 1001\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m             dtype=dtype)\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f476eaa0107e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(storage, loc)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memo_roberta_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emo_Roberta.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 39.41 GiB total capacity; 12.23 GiB already allocated; 4.56 MiB free; 12.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "emo_roberta_embed = torch.load('emo_Roberta.pt', map_location=lambda storage, loc: storage.cuda(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddbeaa-2ae7-4245-adf4-1488484bbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = ProposedModel(emo_roberta_embed[0].shape[1],emo_gpt_embed[0].shape[1], attention_dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e1c05-9b41-409a-ad27-0a40472c4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emo_roberta_embed[0].shape[1])\n",
    "print(emo_gpt_embed[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a69550-0bbe-4995-bd62-64745dc56bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model.forward(emo_roberta_embed[0], emo_gpt_embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf9626-b152-4158-abee-251976dfcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35aca54-adf6-42ac-bf22-df31500dc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(gpt_embeddings):\n",
    "    train_data_tuples = []\n",
    "    count = 0\n",
    "    for example in range(len(gpt_embeddings)):\n",
    "        emotion_list = emotions_dict_idxToemo[example]\n",
    "        for emotion in emotion_list:\n",
    "            text = go_emotions_train.values[example][0]\n",
    "\n",
    "                \n",
    "            text_ids = tokenizer.encode(text, return_tensors = 'pt', truncation=True).to(device)\n",
    "            \n",
    "            # text_id_shape = text_ids.shape\n",
    "            \n",
    "            # if count == 3487:\n",
    "            #     print(text_id_shape)\n",
    "            #     print(gpt_embeddings[example].shape)\n",
    "            # if count == 1:\n",
    "            #     print(text_id_shape)\n",
    "            #     print(gpt_embeddings[example].shape)\n",
    "            \n",
    "            if len(gpt_embeddings[example].shape) == 1:\n",
    "                print(gpt_embeddings[example].shape)\n",
    "                gpt_embeddings[example] = torch.reshape(gpt_embeddings[example], (1, len(gpt_embeddings[example])))\n",
    "                print(gpt_embeddings[example].shape)\n",
    "                \n",
    "            train_data_tuples.append((gpt_embeddings[example], emotion, text_ids))\n",
    "            count += 1\n",
    "    return train_data_tuples\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d27f96-d0ec-4e7a-80ca-d02da75361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_embeddings_emotion_tuples = prepare_train_data(emo_gpt_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02ecfe-ec33-4255-ac52-3675fdeed34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, context_embeddings, gpt_embeddings_emotion_tuples, num_context_samples, epochs):\n",
    "    model.train()\n",
    "    CELoss = nn.CrossEntropyLoss()\n",
    "    random.shuffle(gpt_embeddings_emotion_tuples)\n",
    "    # gpt_embeddings_emotion_tuples = gpt_embeddings_emotion_tuples[:100]\n",
    "    print(f\"Num examples: {len(gpt_embeddings_emotion_tuples)}\")\n",
    "    total_example_count = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        random.shuffle(gpt_embeddings_emotion_tuples)\n",
    "        count = 0\n",
    "        ag_loss = 0\n",
    "        ag_loss_epoch = 0\n",
    "        for gpt_idx_emo_tup in gpt_embeddings_emotion_tuples:\n",
    "            emotion = gpt_idx_emo_tup[1]\n",
    "            \n",
    "            #print(emotion) \n",
    "            \n",
    "            emotion_idxs = emotions_dict_emoToidx[emotion]\n",
    "\n",
    "            # for idx in emotion_idxs:\n",
    "            context_sample_list = []\n",
    "            for context_doc in range(num_context_samples): # without network training takes 23 seconds\n",
    "                # sample average and stack document samples from a particular emotion\n",
    "                context_sample_idx = random.sample(emotion_idxs,1)\n",
    "                #print(context_sample_idx)\n",
    "                single_context_sample = context_embeddings[context_sample_idx[0]]\n",
    "                mean_of_sample = torch.mean(single_context_sample, 0)\n",
    "                #\n",
    "                # mean_of_sample = torch.randn(mean_of_sample.size())\n",
    "                # mean_of_sample = torch.zeros(mean_of_sample.size())\n",
    "                #\n",
    "                context_sample_list.append(mean_of_sample)\n",
    "            agregated_stacked_context_sample = torch.stack(context_sample_list, dim = 0)\n",
    "            \n",
    "            # print(agregated_stacked_context_sample.shape)\n",
    "            # print(gpt_idx_emo_tups[0].shape)\n",
    "#             if count == 3487:\n",
    "#                 print(f\"Count: {count} Text ids: {gpt_idx_emo_tup[2]}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            network_output = model(agregated_stacked_context_sample, gpt_idx_emo_tup[0])\n",
    "            true_output = lm_head(gpt_idx_emo_tup[0])\n",
    "            \n",
    "            # https://huggingface.co/transformers/v3.5.1/_modules/transformers/modeling_gpt2.html referenced from here\n",
    "            #print(gpt_idx_emo_tup[2].shape[1])\n",
    "            if gpt_idx_emo_tup[2].shape[1] == 1:\n",
    "                #print(\"ONE text id?\")\n",
    "                #print(gpt_idx_emo_tup[2].shape[1])\n",
    "                continue\n",
    "            shifted_network_output = network_output[..., :-1, :].contiguous()\n",
    "            shifted_text_ids = gpt_idx_emo_tup[2][..., 1:].contiguous()\n",
    "            loss = CELoss(shifted_network_output.view(-1, shifted_network_output.size(-1)), shifted_text_ids.view(-1))\n",
    "            ag_loss += loss\n",
    "            ag_loss_epoch += loss\n",
    "            total_example_count += 1\n",
    "            ## extra stuff from before\n",
    "            # print(f\"True output: {torch.sum(true_output,dim =1)}\")\n",
    "            # print(f\"network_output: {network_output.shape}\")\n",
    "            # print(f\"True output: {true_output.shape}\")\n",
    "            # print(f\"network_output: {network_output.squeeze().shape}\")\n",
    "            # print(f\"True output: {true_output.squeeze().shape}\")\n",
    "            # print(f\"network_output: {torch.sum(network_output,dim =1)}\")\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if count%1000 == 0:\n",
    "                # print(f\"For Epoch: {epoch}, Example: {count}\")\n",
    "                # print(f\"TRAIN LOSS: {ag_loss/1000}\")\n",
    "                print(\".\")\n",
    "                # ag_loss = 0\n",
    "            count+=1\n",
    "        if epoch % 1 == 0:\n",
    "            if epoch == 0:\n",
    "                print(f\"FIRST epoch: {epoch}, Total Examples: {total_example_count}\")\n",
    "                print(f\"TRAIN LOSS: {ag_loss_epoch/len(gpt_embeddings_emotion_tuples)}\")\n",
    "                print(\"----------------------------------------\")\n",
    "            else:\n",
    "                print(f\"For Epoch: {epoch}, Total Examples: {total_example_count}\")\n",
    "                print(f\"TRAIN LOSS: {ag_loss_epoch/len(gpt_embeddings_emotion_tuples)}\")\n",
    "                print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e8568-890c-476e-ab21-b89dcb859249",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = ProposedModel(emo_roberta_embed[0].shape[1],emo_gpt_embed[0].shape[1], attention_dim = None)\n",
    "optimizer = optim.Adam(test_model.parameters(), lr=0.00001,  weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0dd6e-d662-450e-a50f-095fcd6c5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(test_model, optimizer, emo_roberta_embed, gpt_embeddings_emotion_tuples, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c615f5f-1678-43f6-a3ce-2d73947afad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I work as a data scientist\"\n",
    "text_ids = tokenizer.encode(text, return_tensors = 'pt').to(device)\n",
    "print(text_ids)\n",
    "# logits = head_transformer(text_ids).last_hidden_state.squeeze()\n",
    "logits = head_model(text_ids).logits\n",
    "logits_shape = logits.shape\n",
    "print(f\"decoder logits shape {logits_shape}\")\n",
    "print(f\"decoder logits sum {torch.sum(logits, dim = 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b8a49-6b11-4f36-b5ff-5306bf4d7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = head_model(text_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50b20f8-3280-4c48-88f3-01085688c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each emotion\n",
    "    # list_ofidx for that emoution = emotion_list\n",
    "    # for each shuffle(emotion_list):\n",
    "        # randomly samle from emotion_list to get context embed\n",
    "        # model(random_sample_context, current_Gpt_embed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e563b-7f1e-4771-aef9-f116dc33b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each emotion\n",
    "    # list_ofidx for that emoution = emotion_list\n",
    "    # for each shuffle(emotion_list):\n",
    "        # randomly samle from emotion_list to get context embed\n",
    "        # model(random_sample_context, current_Gpt_embed) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
