 1/1: !pip install transformers
 1/2:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
model = GPT2Model.from_pretrained('gpt2-xl')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
 1/3: head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')
 1/4: model
 1/5:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl')
 1/6: device = "cuda:0" if torch.cuda.is_available() else "cpu"
 1/7: import torch
 1/8: device = "cuda:0" if torch.cuda.is_available() else "cpu"
 1/9:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)
1/10:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl').to(device)
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/11:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/12: device = "cuda:0" if torch.cuda.is_available() else "cpu"
1/13:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/14:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)
1/15:
def pack_tensor(new_tensor, packed_tensor, max_seq_len):
    if packed_tensor is None:
        return new_tensor, True, None
    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:
        return packed_tensor, False, new_tensor
    else:
        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)
        return packed_tensor, True, None
1/16:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2-xl", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    model = model.to(device)
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
view rawlyrics4.py hosted with ❤ by GitHub
1/17:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2-xl", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    model = model.to(device)
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
1/18:
def pack_tensor(new_tensor, packed_tensor, max_seq_len):
    if packed_tensor is None:
        return new_tensor, True, None
    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:
        return packed_tensor, False, new_tensor
    else:
        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)
        return packed_tensor, True, None
1/19:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2-xl", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    model = model.to(device)
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
1/20:
import os

pos_path = "pos/"
neg_path = "neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/21:
import os

pos_path = "/pos/"
neg_path = "/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/22:
import os

pos_path = "./pos/"
neg_path = "./neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/23:
import os

pos_path = "/pos/"
neg_path = "/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/24:
import os

pos_path = "pos/"
neg_path = "neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/25:
import os

pos_path = "pos"
neg_path = "neg"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/26:
import os

pos_path = "/pos"
neg_path = "/"neg"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/27:
import os

pos_path = "/pos"
neg_path = "/neg"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/28:
import os

pos_path = "/home/ubuntu/pos"
neg_path = "/home/ubuntu/neg"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/29:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))
1/30:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(tokenizer.encode(f.read(), return_tensors = 'pt'))
1/31:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(base_tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(tokenizer.encode(f.read(), return_tensors = 'pt'))
1/32:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_token_list.append(tokenizer.encode(f.read(), return_tensors = 'pt'))

os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_token_list.append(tokenizer.encode(f.read(), return_tensors = 'pt'))
1/33:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    if len(neg_tokens) <= 1024:
        
        pos_token_list.append(pos_tokens)
    else:
        print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    if len(neg_tokens) <= 1024:
        neg_token_list.append(neg_tokens)
    else:
        print("neg too big")
1/34:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_token_list = []
neg_token_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    if len(pos_tokens) <= 1024:
        
        pos_token_list.append(pos_tokens)
    else:
        print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    if len(neg_tokens) <= 1024:
        neg_token_list.append(neg_tokens)
    else:
        print("neg too big")
1/35: print(pos_token_list)
1/36: print(len(pos_token_list))
1/37: print(len(neg_token_list))
1/38:
import torch
from sklearn.model_selection import train_test_split
1/39:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
1/40:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
1/41:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
1/42:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    output_dir="/home/ubuntu/Training_ouput", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)
1/43:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    output_dir="/home/ubuntu/Training_ouput", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)
1/44:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
1/45:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
1/46:
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)
1/47:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
1/48:
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False,
)
1/49:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    output_dir="/home/ubuntu/Training_ouput", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=pos_list,
)
1/50: trainer.train()
1/51:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
import pandas as pd
1/52:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list)
1/53: df_neg
1/54:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list)
df_pos = pd.DataFrame(pos_list)
1/55: df_neg
1/56:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    output_dir="/home/ubuntu/Training_ouput", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    )
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=df_pos,
)
1/57: trainer.train()
1/58:
training_args = TrainingArguments(
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=1, # batch size for training
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    output_dir="/home/ubuntu/Training_ouput", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    )
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=df_pos,
)
1/59: trainer.train()
1/60: dataset = NetflixDataset(df_neg, tokenizer, max_length=1024)
1/61: max_length = max([len(tokenizer.encode(text)) for text in df_neg])
1/62: df_neg.size
1/63:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list).columns = ['text'] 
df_pos = pd.DataFrame(pos_list).columns = ['text']
1/64: df_neg
1/65:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list) 
# df_neg = pd.DataFrame(neg_list).columns = ['text'] 
# df_pos = pd.DataFrame(pos_list).columns = ['text']
1/66: df_neg
1/67:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df.columns  = ['text'] 
# df_neg = pd.DataFrame(neg_list).columns = ['text'] 
# df_pos = pd.DataFrame(pos_list).columns = ['text']
1/68:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
# df_neg = pd.DataFrame(neg_list).columns = ['text'] 
# df_pos = pd.DataFrame(pos_list).columns = ['text']
1/69: df_neg
1/70:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text'] 
# df_neg = pd.DataFrame(neg_list).columns = ['text'] 
# df_pos = pd.DataFrame(pos_list).columns = ['text']
1/71: df_neg
1/72: max_length = max([len(tokenizer.encode(text)) for text in df_neg])
1/73: max_length_neg = max([len(tokenizer.encode(text)) for text in df_neg])
1/74: max_length_pos = max([len(tokenizer.encode(text)) for text in df_pos])
1/75:
class IMDB_DATA(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,
                                       max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
1/76: dataset_neg = NetflixDataset(df_neg, tokenizer, max_length=max_length_neg)
1/77:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
import pandas as pd
from torch.utils.data import Dataset, random_split
1/78:
class IMDB_DATA(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,
                                       max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
1/79: dataset_neg = NetflixDataset(df_neg, tokenizer, max_length=max_length_neg)
1/80: dataset_neg = IMDB_DATA(df_neg, tokenizer, max_length=max_length_neg)
1/81:
class IMDB_DATA(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>',,
                                       max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
1/82:
class IMDB_DATA(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>',
                                       max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
1/83: dataset_neg = IMDB_DATA(df_neg, tokenizer, max_length=max_length_neg)
1/84:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl',bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>'))
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/85:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl',bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/86:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl',bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/87:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)
1/88:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
1/89: df_neg
1/90: max_length_neg = max([len(tokenizer.encode(text)) for text in df_neg])
1/91: max_length_pos = max([len(tokenizer.encode(text)) for text in df_pos])
1/92:
class IMDB_DATA(Dataset):
    def __init__(self, txt_list, tokenizer, max_length):
        self.input_ids = []
        self.attn_masks = []
        self.labels = []
        for txt in txt_list:
            encodings_dict = tokenizer('<|startoftext|>' + txt + '<|endoftext|>', truncation=True,
                                       max_length=max_length, padding="max_length")
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]
1/93: dataset_neg = IMDB_DATA(df_neg, tokenizer, max_length=max_length_neg)
1/94:
import gc
gc.collect()
1/95: torch.cuda.empty_cache()
1/96:
training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,
                                  per_device_train_batch_size=1,
                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')
1/97:
Trainer(model=model,  args=training_args, train_dataset=train_dataset, 
         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),
                                                              'attention_mask': torch.stack([f[1] for f in data]),
                                                              'labels': torch.stack([f[0] for f in data])}).train()
1/98:
Trainer(model=model,  args=training_args, train_dataset=dataset_neg, 
         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),
                                                              'attention_mask': torch.stack([f[1] for f in data]),
                                                              'labels': torch.stack([f[0] for f in data])}).train()
1/99:
# training_args = TrainingArguments(
#     num_train_epochs=3, # number of training epochs
#     per_device_train_batch_size=1, # batch size for training
#     save_steps=800, # after # steps model is saved 
#     warmup_steps=500,# number of warmup steps for learning rate scheduler
#     output_dir="/home/ubuntu/Training_ouput", #The output directory
#     overwrite_output_dir=True, #overwrite the content of the output directory
#     )
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     data_collator=data_collator,
#     train_dataset=df_pos,
# )
1/100:
# data_collator = DataCollatorForLanguageModeling(
#     tokenizer=tokenizer, mlm=False,
# )
1/101:
Trainer(model=model,  args=training_args, train_dataset=dataset_neg, 
         data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),
                                                              'attention_mask': torch.stack([f[1] for f in data]),
                                                              'labels': torch.stack([f[0] for f in data])}).train()
1/102:
# data_collator = DataCollatorForLanguageModeling(
#     tokenizer=tokenizer, mlm=False,
# )
1/103:
# Trainer(model=model,  args=training_args, train_dataset=dataset_neg, 
#          data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),
#                                                               'attention_mask': torch.stack([f[1] for f in data]),
#                                                               'labels': torch.stack([f[0] for f in data])}).train()
1/104:
training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,
                                  per_device_train_batch_size=1,
                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')
1/105: # torch.cuda.empty_cache()
1/106:
# import gc
# gc.collect()
1/107:
training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,
                                  per_device_train_batch_size=1,
                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')
1/108:
# training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,
#                                   per_device_train_batch_size=1,
#                                   warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')
1/109:
python run_clm.py \
--model_type gpt2-medium \
--model_name_or_path gpt2-medium \
--train_file "train_tmp.txt" \
--do_train \
--validation_file "eval_tmp.txt" \
--do_eval \
--per_gpu_train_batch_size 1 \
--save_steps -1 \
--num_train_epochs 5 \
--fp16 \
--output_dir=<directory of saved model>
1/110:
!python run_clm.py \
--model_type gpt2-medium \
--model_name_or_path gpt2-medium \
--train_file "train_tmp.txt" \
--do_train \
--validation_file "eval_tmp.txt" \
--do_eval \
--per_gpu_train_batch_size 1 \
--save_steps -1 \
--num_train_epochs 5 \
--fp16 \
--output_dir=<directory of saved model>
1/111:
!python run_clm.py \
--model_type gpt2-medium \
--model_name_or_path gpt2-medium \
--train_file "train_tmp.txt" \
--do_train \
--validation_file "eval_tmp.txt" \
--do_eval \
--per_gpu_train_batch_size 1 \
--save_steps -1 \
--num_train_epochs 5 \
--fp16 \
--output_dir=<directory of saved model>
1/112:
!python run_clm.py \
--model_type gpt2-medium \
--model_name_or_path gpt2-medium \
--train_file "train_tmp.txt" \
--do_train \
--validation_file "eval_tmp.txt" \
--do_eval \
--per_gpu_train_batch_size 1 \
--save_steps -1 \
--num_train_epochs 5 \
--fp16 \
--output_dir="./results"
1/113:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    device=torch.device("cuda")
    model = model.cuda()
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
view rawlyrics4.py hosted with ❤ by GitHub
1/114:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    device=torch.device("cuda")
    model = model.cuda()
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
1/115:
def pack_tensor(new_tensor, packed_tensor, max_seq_len):
    if packed_tensor is None:
        return new_tensor, True, None
    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:
        return packed_tensor, False, new_tensor
    else:
        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)
        return packed_tensor, True, None
1/116:

def train(
    dataset, model, tokenizer,
    batch_size=16, epochs=5, lr=2e-5,
    max_seq_len=400, warmup_steps=200,
    gpt2_type="gpt2", output_dir=".", output_prefix="wreckgar",
    test_mode=False,save_model_on_epoch=False,
):
    acc_steps = 100
    device=torch.device("cuda")
    model = model.cuda()
    model.train()

    optimizer = AdamW(model.parameters(), lr=lr)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1
    )

    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    loss=0
    accumulating_batch_count = 0
    input_tensor = None

    for epoch in range(epochs):

        print(f"Training epoch {epoch}")
        print(loss)
        for idx, entry in tqdm(enumerate(train_dataloader)):
            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)

            if carry_on and idx != len(train_dataloader) - 1:
                continue

            input_tensor = input_tensor.to(device)
            outputs = model(input_tensor, labels=input_tensor)
            loss = outputs[0]
            loss.backward()

            if (accumulating_batch_count % batch_size) == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                model.zero_grad()

            accumulating_batch_count += 1
            input_tensor = None
        if save_model_on_epoch:
            torch.save(
                model.state_dict(),
                os.path.join(output_dir, f"{output_prefix}-{epoch}.pt"),
            )
    return model
1/117: model = train(dataset_neg, head_model, tokenizer)
1/118:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
import pandas as pd
from torch.utils.data import Dataset, random_split
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
import random
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import torch.nn.functional as F
import csv
1/119: device = "cuda:0" if torch.cuda.is_available() else "cpu"
1/120:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl',bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/121:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)
1/122:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
1/123:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
1/124: !pwd
1/125: !pip install transformers
1/126:
import torch
from sklearn.model_selection import train_test_split
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
import pandas as pd
from torch.utils.data import Dataset, random_split
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import numpy as np
import random
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
import torch.nn.functional as F
import csv
1/127: device = "cuda:0" if torch.cuda.is_available() else "cpu"
1/128:
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl',bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>')
model = GPT2Model.from_pretrained('gpt2-xl').to(device)
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt').to(device)
output = model(**encoded_input)
1/129:
from transformers import GPT2LMHeadModel
head_model = GPT2LMHeadModel.from_pretrained('gpt2-xl').to(device)
1/130: !pwd
1/131: %cd /home/ubuntu/
1/132: !pwd
1/133:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
1/134: !pwd
1/135: %cd /home/ubuntu/
1/136: !pwd
1/137:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
# os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
1/138:
import os

pos_path = "/home/ubuntu/pos/"
neg_path = "/home/ubuntu/neg/"

pos_list = []
neg_list = []
# os.chdir(pos_path)
  

for file in os.listdir():
  with open(pos_path+file, 'r') as f:
    pos_list.append(f.read())
#     pos_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
#     if len(pos_tokens) <= 1024:
        
#         pos_token_list.append(pos_tokens)
    # else:
    #     print("pos too big")
        
# os.chdir(neg_path)

for file in os.listdir():
  with open(neg_path+file, 'r') as f:
    neg_list.append(f.read())
    # neg_tokens = tokenizer.encode(f.read(), return_tensors = 'pt')
    # if len(neg_tokens) <= 1024:
    #     neg_token_list.append(neg_tokens)
    # else:
    #     print("neg too big")
    
df_neg = pd.DataFrame(neg_list) 
df_pos = pd.DataFrame(pos_list)
df_neg.columns  = ['text'] 
df_pos.columns  = ['text']
   1: %history -g
   2: %history -g -f notebook_file.ipynb
